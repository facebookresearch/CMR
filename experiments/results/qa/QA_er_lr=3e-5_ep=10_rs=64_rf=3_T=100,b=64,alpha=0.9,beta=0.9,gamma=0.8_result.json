{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=64_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/QA_er_lr=3e-5_ep=10_rs=64_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=64, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_er_lr=3e-5_ep=10_rs=64_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 6620, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Spain", "too much grief", "Ps. 31:5", "five", "applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Mick Mixon", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "underpinning", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "It says \"Adam Trask was born on a farm on the outskirts of a little town which was not far from a big town in Connecticut", "It's the only NBA team name that uses a state nickname", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "At one of their seances a man tied the brothers so tightly that it was neces", "What separates a Cyberpunk setting from a", "unemployment benefits"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7578004807692308}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.08, 0.16666666666666666, 0.0, 0.0, 0.33333333333333337, 0.10256410256410257]}}, "before_error_ids": ["mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-3355", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.703125, "CSR": 0.7734375, "EFR": 1.0, "Overall": 0.88671875}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "their animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "1971", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states and the Court of Justice of the European Union", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene", "war, famine, and weather", "the Wesel-Datteln Canal", "TLC", "the south side of the garden", "high cost injectable, oral, infused, or inhaled", "friendly and supportive", "Eero Saarinen", "Newton", "41", "that he may have intercepted Marconi's European experiments in July 1899", "The Lodger", "1954", "on the internet", "Fondue", "the Green Hornet", "the scrum-half", "Danskin", "the second most populous city in America", "Sanguine", "New Hampshire", "the Tennessee Valley Authority", "a boardinghouse for beagles or borzois", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.765625, "QA-F1": 0.7920800264550265}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-3699", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-6393", "mrqa_squad-validation-1529", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.765625, "CSR": 0.7708333333333334, "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "technological superiority", "four", "San Joaquin Light & Power Building", "1972", "three", "books, films, radio, TV, music, live theater, comics and video games", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "technical problems and flight delays", "the US Supreme Court", "trust God's word", "The zeta function", "those who proceed to secondary school or vocational training", "139th", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "public official", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service", "BBC HD", "Brough Park in Byker", "Genoa", "a circle", "Chickamauga", "a horse of a yellow-brown horse", "the National Center for Physical Acoustics", "Gaius Maecenas", "Christopher Tolkien", "Prussia", "the Student loan Scheme", "Penn Jillette", "the Palais Garnier", "Chicago White Stockings", "John James Osborne", "Orwell", "The Gleaners", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7638888888888888}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-412"], "SR": 0.734375, "CSR": 0.76171875, "retrieved_ids": ["mrqa_squad-train-40219", "mrqa_squad-train-12138", "mrqa_squad-train-21319", "mrqa_squad-train-79543", "mrqa_squad-train-37852", "mrqa_squad-train-45159", "mrqa_squad-train-11605", "mrqa_squad-train-49772", "mrqa_squad-train-37356", "mrqa_squad-train-16078", "mrqa_squad-train-77939", "mrqa_squad-train-56738", "mrqa_squad-train-45060", "mrqa_squad-train-50614", "mrqa_squad-train-73068", "mrqa_squad-train-36712", "mrqa_squad-train-49976", "mrqa_squad-train-23385", "mrqa_squad-train-66533", "mrqa_squad-train-45947", "mrqa_squad-train-27761", "mrqa_squad-train-12201", "mrqa_squad-train-77917", "mrqa_squad-train-14430", "mrqa_squad-train-26805", "mrqa_squad-train-24078", "mrqa_squad-train-31307", "mrqa_squad-train-43118", "mrqa_squad-train-2368", "mrqa_squad-train-58431", "mrqa_squad-train-32597", "mrqa_squad-train-41525", "mrqa_searchqa-validation-10193", "mrqa_squad-validation-236", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5963", "mrqa_squad-validation-1500", "mrqa_squad-validation-3699", "mrqa_squad-validation-3355", "mrqa_squad-validation-8558", "mrqa_squad-validation-1941", "mrqa_squad-validation-3130", "mrqa_squad-validation-3119", "mrqa_searchqa-validation-16816", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6374", "mrqa_squad-validation-4015", "mrqa_squad-validation-4528", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-4266", "mrqa_squad-validation-9145", "mrqa_squad-validation-2226", "mrqa_searchqa-validation-11770", "mrqa_squad-validation-1766", "mrqa_squad-validation-3165", "mrqa_squad-validation-4662", "mrqa_squad-validation-7574", "mrqa_squad-validation-8927", "mrqa_hotpotqa-validation-1297", "mrqa_squad-validation-8597", "mrqa_searchqa-validation-1701", "mrqa_squad-validation-6393", "mrqa_searchqa-validation-9403", "mrqa_squad-validation-2372"], "EFR": 1.0, "Overall": 0.880859375}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "Lord's Prayer", "$5 million", "hypersensitive response of plants against pathogen attack", "2.666 million", "Industry and manufacturing", "non-violent", "Parish Church of St Andrew", "1262", "New Orleans", "April 1523", "Dating of lava and volcanic ash layers", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Tuesday afternoon", "Pickawillany", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students", "16", "standard", "Lucas\u2013Lehmer", "Level 3 Communications", "Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "the Ark", "opera buffa", "Okinawa", "a poem", "the kidneys", "potato pancake", "Basin Street", "Tarsus", "Bloomingdale's", "Henry Schleiff", "Louisa May Alcott", "Walter Cronkite", "Treasure Island", "Death Watch", "Kerry Moosman", "Absinthe", "white", "Miss You Already", "As of September, 2016", "metal Bridge", "Alistair Grant", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7414772727272727}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1212121212121212]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-117", "mrqa_squad-validation-455", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-7852", "mrqa_naturalquestions-validation-1549", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.71875, "CSR": 0.753125, "EFR": 0.8888888888888888, "Overall": 0.8210069444444444}, {"timecode": 5, "before_eval_results": {"predictions": ["an ash leaf", "75,000 to 100,000", "the 1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The majority may be powerful but it is not necessarily right", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center", "one-eighth", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea", "closed system", "21 to 11", "The Earth's crustal rock", "The goal of the congress was to formalize a unified front in trade and negotiations with various Indians", "two", "the network and the connected users via leased lines (using the X.121DNIC 2041)", "a separate condenser", "to the North Sea", "Cam Newton", "The Emperor presented the final draft of the Edict of Worms on 25 May 1521", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element in the universe", "39", "Romana (Mary Tamm and Lalla Ward)", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "A\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "100\u20135,000 hp", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "UNESCO World Heritage Site", "Frederick II the Great", "the ball toward the wicket defended by.... The circumference of the ball was specified for the first time in 1838 (its weight had been dictated 60 years earlier)", "Donner", "Colonel (Tom) Parker", "the New Netherland Company", "Monrovia", "the umpire", "Taiwan", "Omaha", "Beniamino", "Nez Perce", "George Gershwin", "this state", "Oprah Winfrey", "sewing machines", "the Drazens", "Inchon", "February 29", "two weevils", "Alabama", "Bennington", "Giorgio Armani", "the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region", "the District of Columbia National Guard"], "metric_results": {"EM": 0.5, "QA-F1": 0.6060253689116948}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.19354838709677422, 0.0, 0.11764705882352941, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.6, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-9640", "mrqa_squad-validation-2976", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-4829", "mrqa_squad-validation-9320", "mrqa_squad-validation-2209", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.5, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views", "Bible", "water pump", "874.3 square miles", "53% in Botswana to -40% in Bahrain", "Scottish Parliament", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger Goodell", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "British and Europeans", "Judith Merril", "the node requires the node to look up the connection id in a table", "Von Miller", "weekly screenings of all available classic episodes", "type III secretion system", "nearly 10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "when the oxygen concentration is too high", "destroy the antichrist", "the term global village", "Sun City", "Freeport, Maine", "a dolphin", "David Bowie", "Liberty Island", "a person who has died", "the American Psychiatric Association", "Lenin", "G. McCoy", "Amtrak", "a log cabin", "The Pianist", "Patty Duke", "a frog", "a computer", "Richard Cory", "Homer J. Simpson", "South Africa", "a greyhound", "Beany and Cecil", "Reno, Nevada", "Trenton", "nickel", "Thomas Jefferson", "of Southern Spain", "Margarita", "prostate cancer", "DNA's structure", "the Pyrenees mountains"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7383742559523809}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8750000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.8571428571428572, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7449", "mrqa_squad-validation-5589", "mrqa_squad-validation-4797", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.671875, "CSR": 0.7053571428571428, "retrieved_ids": ["mrqa_squad-train-85937", "mrqa_squad-train-15946", "mrqa_squad-train-25973", "mrqa_squad-train-896", "mrqa_squad-train-35540", "mrqa_squad-train-25912", "mrqa_squad-train-56269", "mrqa_squad-train-52576", "mrqa_squad-train-7863", "mrqa_squad-train-15839", "mrqa_squad-train-69837", "mrqa_squad-train-28295", "mrqa_squad-train-54237", "mrqa_squad-train-64256", "mrqa_squad-train-36573", "mrqa_squad-train-47881", "mrqa_squad-train-53258", "mrqa_squad-train-83952", "mrqa_squad-train-41062", "mrqa_squad-train-84738", "mrqa_squad-train-20337", "mrqa_squad-train-49719", "mrqa_squad-train-68113", "mrqa_squad-train-15578", "mrqa_squad-train-54992", "mrqa_squad-train-47240", "mrqa_squad-train-43263", "mrqa_squad-train-59914", "mrqa_squad-train-43599", "mrqa_squad-train-24120", "mrqa_squad-train-63470", "mrqa_squad-train-39530", "mrqa_squad-validation-10141", "mrqa_squad-validation-2372", "mrqa_searchqa-validation-3102", "mrqa_squad-validation-4999", "mrqa_newsqa-validation-1289", "mrqa_hotpotqa-validation-1297", "mrqa_squad-validation-4829", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-9733", "mrqa_squad-validation-3699", "mrqa_searchqa-validation-4674", "mrqa_squad-validation-9640", "mrqa_searchqa-validation-15847", "mrqa_squad-validation-1441", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-491", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-14398", "mrqa_squad-validation-1766", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-14838", "mrqa_squad-validation-2226"], "EFR": 1.0, "Overall": 0.8526785714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["New Orleans' Mercedes-Benz Superdome", "1994 Works Council Directive", "the Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "the Pittsburgh Steelers", "McManus", "Gemini", "Dave Logan", "Northern Europe and the Mid-Atlantic", "Africa", "X-rays", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "Mongols and Semuren", "civil disobedients", "Because oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "the holy catholic (or universal) church", "competition", "1516", "decrease in wages", "Prudhoe Bay", "a cat's eye", "cigar", "William Godwin", "Lucy Hayes", "a ribonucleic acid", "Poor family", "Eight Is Enough", "Madrid", "Bacall", "The Name of the Rose", "Thomas Paine", "the sea", "the Silver Surfer", "G4", "Karl Shapiro", "Marcus Junius Brutus", "malaria", "Ann-margret", "Hairspray", "Johann Wolfgang von Goethe", "masks", "the Oneida Community", "a seaplane", "Sherman Antitrust Act", "Hafnium", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "economic opportunities", "Joe Harn", "his dismissal"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6525195868945869}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-132", "mrqa_squad-validation-490", "mrqa_squad-validation-694", "mrqa_squad-validation-1407", "mrqa_squad-validation-8412", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.609375, "CSR": 0.693359375, "EFR": 1.0, "Overall": 0.8466796875}, {"timecode": 8, "before_eval_results": {"predictions": ["1970s", "an electrical exhibition at Madison Square Garden.", "Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties", "Lucas Horenbout", "its safaris", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "leaves", "him to return to his side", "Kevin Harlan", "30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "the Anglican tradition's Book of Common Prayer", "the Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "leptospirosis", "a little blue engine", "the NanoFrazor", "tango", "a cave", "bamboo", "Nevil Shute", "Claudius", "Vlad Tepes", "a rail", "ginseng", "coffee", "Depeche Mode", "carbohydrates", "a battery pack that generates electrical impulses", "Vanna White", "a hippopotamus", "1492", "the Madding Crowd", "(M) Baryshakov.", "Mars", "the Boston Massacre Trials", "a hoo", "a submachine gun", "Redolfi", "Maycommemorates the Mexican army", "Jimmy Durante", "Carl Sagan", "In February 2011, while overseas, she discovered that she was pregnant.", "General Paulus", "John Ford", "CNN", "a donor molecule", "Sylvester Stallone", "The Mongol - led Yuan dynasty"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6206473214285715}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.5, 0.13333333333333333, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3478", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6321"], "SR": 0.5625, "CSR": 0.6788194444444444, "EFR": 0.9642857142857143, "Overall": 0.8215525793650793}, {"timecode": 9, "before_eval_results": {"predictions": ["the Metropolitan Police Authority", "Francis Marion", "parallel importers", "the first Block II CSM and LM", "the Tangut relief army", "five", "governmental entities", "the Great Yuan", "Mario Addison", "improved response is then retained after the pathogen has been eliminated", "more than 70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination systems", "megaprojects", "James Lofton", "gurus", "by limiting aggregate demand", "five", "Danny Lane", "5,500,000", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "an alleged robbery", "George Jetson", "Deus", "an arboretum", "pommel horse", "President William McKinley", "PSP", "Daphne du Maurier", "Turkish", "a witty remark", "the saguaro cactuses", "the American Revolution", "Morrie Schwartz", "Jimmy", "Mercury and Venus", "Tokyo", "an entry-level restaurant job", "a gorillas", "The Pentagon", "a cereals", "4", "China", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "the pancreas", "the mid-1990s", "the Hudson Bay.", "Dr Ichak Adizes", "Melpomene", "Boston", "James Lofton", "can't afford to pay for cable or satellite TV service.", "he was letting the likes of Mr. Clemmons out."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5940104166666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.4, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.16666666666666669, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1849", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_naturalquestions-validation-4124", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.515625, "CSR": 0.6625, "retrieved_ids": ["mrqa_squad-train-4902", "mrqa_squad-train-23808", "mrqa_squad-train-50756", "mrqa_squad-train-41627", "mrqa_squad-train-10733", "mrqa_squad-train-52708", "mrqa_squad-train-55816", "mrqa_squad-train-50875", "mrqa_squad-train-12227", "mrqa_squad-train-51330", "mrqa_squad-train-80598", "mrqa_squad-train-35613", "mrqa_squad-train-36633", "mrqa_squad-train-85417", "mrqa_squad-train-14191", "mrqa_squad-train-46435", "mrqa_squad-train-66542", "mrqa_squad-train-59705", "mrqa_squad-train-18334", "mrqa_squad-train-32126", "mrqa_squad-train-43586", "mrqa_squad-train-41447", "mrqa_squad-train-32303", "mrqa_squad-train-31249", "mrqa_squad-train-26251", "mrqa_squad-train-28611", "mrqa_squad-train-35136", "mrqa_squad-train-14682", "mrqa_squad-train-50660", "mrqa_squad-train-62098", "mrqa_squad-train-35767", "mrqa_squad-train-30358", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-14148", "mrqa_hotpotqa-validation-2600", "mrqa_squad-validation-2976", "mrqa_squad-validation-7527", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-2214", "mrqa_squad-validation-117", "mrqa_squad-validation-6393", "mrqa_squad-validation-9320", "mrqa_searchqa-validation-10964", "mrqa_squad-validation-4662", "mrqa_squad-validation-3130", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-9116", "mrqa_squad-validation-1456", "mrqa_searchqa-validation-15795", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-9109", "mrqa_squad-validation-7574", "mrqa_searchqa-validation-11091", "mrqa_hotpotqa-validation-1297", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-10318", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-4386", "mrqa_squad-validation-1500", "mrqa_triviaqa-validation-2363"], "EFR": 1.0, "Overall": 0.83125}, {"timecode": 10, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.9140625, "KG": 0.48046875, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "Waal", "technical problems and flight delays", "Fermat's little theorem", "Virgin Media.", "unless he were removed from the school, Tesla would be killed through overwork.", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations of this perishing world", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "physical control or full-fledged colonial rule", "30 July 1891", "the Bible", "Lower Lorraine", "parish churches", "kinetic friction", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "photoelectric", "Peggy", "the handles of the World Health Organization", "Memoirs of a Geisha", "stability control", "a bolt-action rifle", "Gothic Names", "Aluminium", "Taylor Swift", "the Cenozoic Era", "the Maghreb", "Reddi-wip", "Jeopardy", "tea", "Larry Fortensky", "the National Research Council", "Shakira", "Aimee Semple McPherson", "Kamehameha", "Time magazine", "The Jeffersons", "Tony Soprano", "The Crucible", "Muhammad Ali", "Impressionists", "Willa Cather", "Aida", "Walden", "the Burgundy wine region", "the handles of the American Civil Liberties Union", "Take It Easy", "0 - Search-ID.com", "Australian & New Zealand", "Vermont", "Doug Diemoz", "can easily flow from the faucet into the sink", "Hal Beatty", "John Ford", "119", "the Vigor, Prelude, CR-X, and Quint.", "a skilled hacker", "Sonia Sotomayor"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6292548627002288}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.6666666666666666, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9023", "mrqa_squad-validation-1326", "mrqa_squad-validation-2455", "mrqa_squad-validation-9734", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-5864", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-9725", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-2708"], "SR": 0.515625, "CSR": 0.6491477272727273, "EFR": 1.0, "Overall": 0.7657670454545455}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "the vBNS had grown to connect more than 100 universities and research and engineering institutions via 12 national points of presence with DS-3 (45 Mbit/s),", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "97", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock", "oppidum Ubiorum", "The entrance to studio 5 at the City Road complex", "1.7 million", "August 4, 2000", "the mastermind behind the September 11, 2001, terrorist attacks on the United States", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "Bob Dole", "1959", "Stratfor", "three men with suicide vests who were plotting to carry out the attacks, said Interior Minister Rehman Malik.", "137", "the green grump", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "three", "the insurgency", "Chinese", "he would actively engage Arab media.", "war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Hearst Castle", "it's all claws, all the time.", "The Rev. Alberto Cutie", "Aeneh Bahrami is blind, the victim of an acid attack by a spurned suitor.", "military trials for some Guant Bay detainees", "opium", "Obama's race in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "named his company Polo because \"it was the sport of kings. It was glamorous, sexy and international.\"", "Hawass", "Arabic, French and English", "a minor league baseball team in that stadium", "seven", "Roberto Micheletti", "Suba Kampong township on the Philippine island of Basilan", "six", "the chaos and horrified reactions after the July 7, 2005, London transit bombings were shown to jurors Thursday in the trial of three men charged with conspiracy in the case.", "A Florida man is using billboards with an image of the burning World Trade Center to encourage votes for a Republican presidential candidate,", "the middle of the 15th century", "1966", "Antonio Vivaldi", "Brainy", "Fitzroya cupressoides", "Stephanie Plum", "Sweeney Todd", "Andorra", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6287012878167948}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.10526315789473685, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.10256410256410256, 1.0, 0.9375, 0.0, 0.0, 1.0, 0.16, 1.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.09090909090909091, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4911", "mrqa_squad-validation-3805", "mrqa_squad-validation-1313", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-6939", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10090"], "SR": 0.53125, "CSR": 0.6393229166666667, "EFR": 1.0, "Overall": 0.7638020833333334}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches", "Doctor Who", "Prime ideals", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax in Valencia", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport.", "23 years.", "Pyongyang and Seoul has increased in recent weeks, with North Korea announcing it would scrap peace agreements with the South, warning of a war on the Korean peninsula and threatening to test a missile capable of hitting the western United States.", "Jared Polis", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN Moscow Correspondent at Star City, the Russian cosmonaut training facility.", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi, who is the best-selling poet in America.", "The crash destroyed four homes and killed two people who lived in at least one of the homes", "militant warriors in his take on American involvements in Afghanistan and Iraq, takes Islam -- and Islam alone -- to task for having a diabolic roughness on its fringes.", "U.S. senators who couldn't resist taking the vehicles for a spin.", "died peacefully with Mildred and two other females. Breeders are hoping he'll show interest in Lucy, who is about the same age as Mildred, later this year.", "Muslim", "California, Texas and Florida", "Robert De Niro", "New Zealand and Tonga", "Three searches", "creation of an Islamic emirate in Gaza", "Jeddah, Saudi Arabia", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "Azzam the American,", "His treatment met the legal definition of torture.", "Apple employees", "scout who proudly wears a Stetson hat and spurs on his boots", "Haiti", "Building falls down", "test-launched a rocket capable of carrying a satellite", "Nieb\u00fcll", "Del Potro.", "20% tax credit", "Seoul", "John Wayne", "Afghanistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "Charles Haley", "Ytterby", "King George III,", "Philadelphia", "Alien Resurrection", "Fester", "Moscow", "dressage"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6426859598734598}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.923076923076923, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5657", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_searchqa-validation-266"], "SR": 0.578125, "CSR": 0.6346153846153846, "retrieved_ids": ["mrqa_squad-train-12826", "mrqa_squad-train-72997", "mrqa_squad-train-15495", "mrqa_squad-train-14499", "mrqa_squad-train-83097", "mrqa_squad-train-15960", "mrqa_squad-train-13091", "mrqa_squad-train-37927", "mrqa_squad-train-69746", "mrqa_squad-train-60070", "mrqa_squad-train-935", "mrqa_squad-train-74718", "mrqa_squad-train-27588", "mrqa_squad-train-59068", "mrqa_squad-train-80604", "mrqa_squad-train-53468", "mrqa_squad-train-58953", "mrqa_squad-train-61444", "mrqa_squad-train-22621", "mrqa_squad-train-42536", "mrqa_squad-train-36041", "mrqa_squad-train-53242", "mrqa_squad-train-67017", "mrqa_squad-train-25184", "mrqa_squad-train-61554", "mrqa_squad-train-16999", "mrqa_squad-train-7334", "mrqa_squad-train-54679", "mrqa_squad-train-60471", "mrqa_squad-train-37740", "mrqa_squad-train-48425", "mrqa_squad-train-6453", "mrqa_searchqa-validation-10017", "mrqa_squad-validation-1092", "mrqa_squad-validation-8400", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-10823", "mrqa_newsqa-validation-3049", "mrqa_squad-validation-2455", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-9725", "mrqa_hotpotqa-validation-2600", "mrqa_searchqa-validation-4266", "mrqa_newsqa-validation-267", "mrqa_squad-validation-1313", "mrqa_naturalquestions-validation-7203", "mrqa_searchqa-validation-5539", "mrqa_squad-validation-3165", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-4386", "mrqa_squad-validation-9489", "mrqa_searchqa-validation-11427", "mrqa_newsqa-validation-2611", "mrqa_squad-validation-1500", "mrqa_squad-validation-2346", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-6992", "mrqa_squad-validation-2289", "mrqa_searchqa-validation-409", "mrqa_newsqa-validation-937", "mrqa_squad-validation-10274", "mrqa_newsqa-validation-893", "mrqa_squad-validation-3355"], "EFR": 1.0, "Overall": 0.762860576923077}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I", "war, famine, and weather", "British progressive folk-rock band Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "the City of Edinburgh Council", "Osama", "rural California", "Hearst Castle", "\"Larry King Live.\"", "Al Gore.", "the shoreline of the city of Quebradillas", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Martin Aloysius Culhane", "Gadahn", "Tuesday's iPhone 4S news", "in the southern port city of Karachi, Pakistan's largest city and the capital of Sindh province.", "John McCain", "South Africa", "1960s", "Iran's nuclear program.", "North Korea", "Sunday", "random events", "Haeftling", "ireport form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\" to the case.", "San Diego", "\"it was the sport of kings. It was glamorous, sexy and international.\"", "At least 40", "$1,500", "25", "137", "suppress the memories and to live as normal a life as possible", "Copts", "poor", "Tom Hanks", "ancient Egyptian antiquities in the world", "27", "165", "\"It was incredible. We've had so much rain, and yet today it was beautiful.", "\"mud\"", "16,801", "Tyler, Ali, and Lydia", "Kansas", "October", "modern dance", "Melanie Owen", "Lusitania", "the Earth", "The UK's longest-running TV soap, Coronation Street", "Turkey"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5564366870840896}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.9333333333333333, 0.6666666666666666, 0.5, 0.0, 0.5263157894736842, 0.0, 1.0, 0.0, 0.8, 1.0, 0.25, 0.0, 1.0, 0.8, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0689655172413793, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5360", "mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-9660", "mrqa_triviaqa-validation-2202", "mrqa_hotpotqa-validation-5850", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.4375, "CSR": 0.6205357142857143, "EFR": 0.9722222222222222, "Overall": 0.7544890873015874}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart", "between September and November 1946,", "$2.50 per AC horsepower", "1990s", "solvents", "Stagg Field", "2010", "Reuben Townroe", "\"it would appear to be some form of the ordinary Eastern or bubonic plague\"", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh,", "At least 88 people had been hurt,", "bankruptcies", "Inter Milan", "98", "the European Alps may melt as soon as 2050, some scientists say.", "established legal precedent,", "The Ski Train", "severe", "Naples home.", "top designers, such as Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "shut down, and desperately needed aid cannot be unloaded quickly.\"", "onstage demos.", "\"A total of seven died on our property,\"", "impeachment", "Kearny, New Jersey", "Thessaloniki and Athens,", "The elections are slated for Saturday.", "bard", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces by Sunday,", "killing rampage.", "genocide, crimes against humanity, and war crimes.", "bikinis", "Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "\"Don't Ask, don't tell\"", "Consumer Reports", "a woman", "Sheikh Abu al-Nour al-Maqdessi,", "an independent homeland since 1983.", "The Everglades,", "88-year-old", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth w\u0101", "Magnavox Odyssey", "The Lone Ranger", "the robin", "Russell Humphreys,", "The Guest", "\"When I Come Around\"", "a skull", "The Raiders'move to Las Vegas comes after years of failed efforts to renovate or replace the Oakland", "6 January 793"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5618990384615384}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.5, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_triviaqa-validation-2022", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.515625, "CSR": 0.6135416666666667, "EFR": 1.0, "Overall": 0.7586458333333332}, {"timecode": 15, "before_eval_results": {"predictions": ["moist tropical", "90%", "1966,", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions", "proportionally", "North", "Mohammed Ali al-Moayad", "they are \"still trying to absorb the impact of this week's stunning events,\"", "Obama", "Friday,", "CNN affiliate WFTV.", "The cause of the deaths has not been determined, and necropsies and blood tests were underway,", "Brett Cummins,", "sculptures", "along the equator between South America and Africa.", "four university students and a safety officer", "200.", "the ancient Greek site of Olympia", "Patrick McGoohan,", "parents", "$627,", "27-year-old's", "Virgin America", "know what's important in life,", "gossip Girl\"", "Paktika province in southeastern Afghanistan,", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "Polo", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "says he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Secretary of State Hillary Clinton,", "will look at how the universe formed by analyzing particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "in Haiti.", "\"Dancing With the Stars.\"", "estimated 1 million", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "more than 1.2 million", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "his mother.", "pigs", "Matt Flinders", "Isar", "East of Eden", "Sam Bettley.", "33-member", "the Sea of Galilee", "liquid", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.5, "QA-F1": 0.632334646517633}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.5, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 0.04651162790697675, 0.33333333333333337, 0.9473684210526316, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.10526315789473685, 0.6666666666666666, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_searchqa-validation-5504", "mrqa_triviaqa-validation-5573"], "SR": 0.5, "CSR": 0.6064453125, "retrieved_ids": ["mrqa_squad-train-25686", "mrqa_squad-train-55865", "mrqa_squad-train-36782", "mrqa_squad-train-3928", "mrqa_squad-train-30427", "mrqa_squad-train-17489", "mrqa_squad-train-25555", "mrqa_squad-train-32682", "mrqa_squad-train-72411", "mrqa_squad-train-34624", "mrqa_squad-train-31206", "mrqa_squad-train-26171", "mrqa_squad-train-77116", "mrqa_squad-train-16397", "mrqa_squad-train-27258", "mrqa_squad-train-32628", "mrqa_squad-train-85187", "mrqa_squad-train-79376", "mrqa_squad-train-55438", "mrqa_squad-train-18891", "mrqa_squad-train-15410", "mrqa_squad-train-49865", "mrqa_squad-train-60622", "mrqa_squad-train-75176", "mrqa_squad-train-29043", "mrqa_squad-train-58948", "mrqa_squad-train-75354", "mrqa_squad-train-40910", "mrqa_squad-train-63911", "mrqa_squad-train-4438", "mrqa_squad-train-36805", "mrqa_squad-train-23754", "mrqa_newsqa-validation-258", "mrqa_naturalquestions-validation-5649", "mrqa_searchqa-validation-11710", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-1216", "mrqa_squad-validation-7719", "mrqa_newsqa-validation-937", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-14838", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-541", "mrqa_naturalquestions-validation-1549", "mrqa_squad-validation-6115", "mrqa_squad-validation-8294", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-6374", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-667", "mrqa_squad-validation-4908", "mrqa_newsqa-validation-2844", "mrqa_naturalquestions-validation-5297", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-2871", "mrqa_newsqa-validation-3406", "mrqa_squad-validation-6393", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2558"], "EFR": 1.0, "Overall": 0.7572265625}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p)", "adjustable spring-loaded valve,", "Grumman", "Synthetic aperture radar (SAR) and Thematic Mapper (TM)", "A fundamental error", "recant his writings", "topographic, and natural ecosystem", "one can include arbitrarily many instances of 1", "136", "union membership", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "with Lebanese heritage,", "shooting a limo driver", "2nd Lt. Holley Wimunc.", "1918-1919.", "Ben Kingsley", "U.S. Holocaust Memorial Museum,", "from Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "Barnes & Noble", "deployment of unmanned drones,", "the first American team to win yachting's most prestigious trophy since 1992.", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two", "Larry Ellison,", "Taher Nunu", "Dick Cheney,", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "they recently killed eight Indians whom the rebels accused of collaborating with the Colombian government,", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.", "South African ministers and the deputy president", "Seoul,", "Haiti", "The United States", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Daytime Emmy Lifetime Achievement Award", "Republican", "\"Gandhi\"", "Eleven", "Hugo Chavez", "Four bodies", "translocation Down syndrome", "starch", "the UK", "Diptera", "100th anniversary of the first \"Tour de France\" bicycle race", "acid techno and drum and bass electronic musician", "fibrous tissue", "Johannes Brahms", "the 17th century", "Orson Welles"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6377157937843889}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8421052631578948, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09090909090909091, 0.47058823529411764, 1.0, 0.25, 1.0, 1.0, 1.0, 0.10526315789473682, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.7692307692307693, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4509", "mrqa_squad-validation-2788", "mrqa_squad-validation-8921", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_searchqa-validation-2260", "mrqa_hotpotqa-validation-4478"], "SR": 0.53125, "CSR": 0.6020220588235294, "EFR": 1.0, "Overall": 0.7563419117647059}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "1498", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "multi-cultural", "father of the house when in his home", "John Fox", "US$1,000,000", "Annual Conference", "Colonel Monckton", "thermodynamic", "\"CNN Moscow Correspondent at Star City,", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "northwestern Montana", "a delegation of American Muslim and Christian leaders", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "after they ambushed a convoy carrying supplies for NATO forces in southern Afghanistan,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "white beltedikini", "2006,", "the FBI.", "as many as 250,000", "release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "abuse", "Pakistan", "St. Louis, Missouri,", "\"I'm just getting started.\"", "a older generation", "heavy flooding and scattered debris.", "Oxbow,", "Dolgorsuren Dagvadorj", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "a former Navy captain", "\"aesthetic environment\"", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford", "Ginger Rogers", "five", "Marine Corps", "Garfield", "Cutpurse", "seven", "a vigorous deciduous tree", "point-contact transistors"], "metric_results": {"EM": 0.546875, "QA-F1": 0.686297123015873}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.5714285714285715, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-8337", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-2925"], "SR": 0.546875, "CSR": 0.5989583333333333, "EFR": 0.9655172413793104, "Overall": 0.7488326149425287}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline", "specialty pharmacy", "Doctor of Theology", "Christ", "The Prince of P\u0142ock", "multi-stage centrifugal pumps", "Pet Sounds", "40", "Sax Rohmer", "Aug 24,", "algebra", "a sperm whale", "\u00ef\u00bf\u00bd", "Naboth", "Jeffrey Archer", "C N Trueman", "Anne Boleyn", "David Ben-Gurion", "a fur hat", "Jonas Bernanke", "Thai", "Parsley the Lion", "Japan", "Runic", "plutonium", "Andy Murray", "blancmange", "fraxadella", "fraxage", "recorder", "\" fravelin weigh less,", "Microsoft", "Austria", "Brunel", "Edward learning - Poem Hunter", "Jamaica", "John Ford", "Petronas", "Beyonce", "Microsoft", "Otto I", "metallic", "The Battle of the Three Emperors", "southern Pacific Ocean,", "Trimdon,", "Midnight Cowboy", "Dada", "FIFA World Cup", "Southwest Airlines", "Afghanistan", "Matt Jones", "Rudolf H\u00f6ss", "3 May 1958", "Ewan McGregor", "off Somalia's coast.", "frax Wonka", "\"Royal\"", "Ford Motor Company", "Banff", "a calves"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5909970238095239}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.53125, "CSR": 0.5953947368421053, "retrieved_ids": ["mrqa_squad-train-52748", "mrqa_squad-train-24962", "mrqa_squad-train-72639", "mrqa_squad-train-126", "mrqa_squad-train-18183", "mrqa_squad-train-65938", "mrqa_squad-train-716", "mrqa_squad-train-68089", "mrqa_squad-train-67207", "mrqa_squad-train-43904", "mrqa_squad-train-48785", "mrqa_squad-train-58886", "mrqa_squad-train-24229", "mrqa_squad-train-24790", "mrqa_squad-train-47988", "mrqa_squad-train-47957", "mrqa_squad-train-21659", "mrqa_squad-train-79077", "mrqa_squad-train-47464", "mrqa_squad-train-27284", "mrqa_squad-train-71756", "mrqa_squad-train-2664", "mrqa_squad-train-21976", "mrqa_squad-train-80744", "mrqa_squad-train-44192", "mrqa_squad-train-81073", "mrqa_squad-train-39439", "mrqa_squad-train-7729", "mrqa_squad-train-31411", "mrqa_squad-train-72045", "mrqa_squad-train-44725", "mrqa_squad-train-44056", "mrqa_triviaqa-validation-2251", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1289", "mrqa_triviaqa-validation-2754", "mrqa_searchqa-validation-198", "mrqa_squad-validation-6680", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-5679", "mrqa_newsqa-validation-2435", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-9725", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-1297", "mrqa_newsqa-validation-1740", "mrqa_squad-validation-2564", "mrqa_triviaqa-validation-412", "mrqa_newsqa-validation-3527", "mrqa_naturalquestions-validation-9660", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-2791", "mrqa_squad-validation-2920", "mrqa_squad-validation-4068", "mrqa_newsqa-validation-1641", "mrqa_squad-validation-9528", "mrqa_searchqa-validation-5539", "mrqa_newsqa-validation-2945", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-2768", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-1126"], "EFR": 0.9666666666666667, "Overall": 0.7483497807017544}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "the Sky Q Silver set top boxes", "\"ash tree\"", "24 September 2007", "2001", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands,", "Tony Blair", "The Flintstones", "there was no standard emergency number", "Jonathan Swift", "South Sudan", "Maria Bueno", "utensils", "Frankie Laine", "July 28, 1948", "Thor", "bulgaria", "Goosnargh", "a bear suit", "dna structure", "Montreal", "Ruda", "a toast", "The Rocky and Bullwinkle Show", "Ray Winstone", "a Lackawanna Six", "Poland", "Indiana Jones", "John Philip Sousa", "Hyde Park Corner", "Sydney", "Alabama", "jura", "a crawler-propelled cannon", "finger", "a meteoroid", "Lew Hoad", "bobbyjo", "lola", "Bodhidharma", "Klaus Barbie", "Albert Reynolds", "a fishing gaff", "Baltic Sea", "Singapore", "cathead", "yellow", "alpo", "alan vespa", "Squamish, British Columbia, Canada", "65", "Theme Park World", "Cape Cod", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "10 percent", "Tommy Tutone", "\"D\" Briefing Flashcards", "a medium", "the small intestine", "Prince Siddhartha"], "metric_results": {"EM": 0.53125, "QA-F1": 0.603125}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4634", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1772", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-3139"], "SR": 0.53125, "CSR": 0.5921875, "EFR": 1.0, "Overall": 0.754375}, {"timecode": 20, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.892578125, "KG": 0.41875, "before_eval_results": {"predictions": ["red algal derived chloroplast", "pathogens", "1525\u201332", "a few", "solution", "2011", "random noise", "trans-Atlantic wireless telecommunications facility", "passepartout", "Ogaden", "paul paul", "prefecture", "Steve Biko", "pewter", "paulsylvanica", "acute", "nellig", "sour", "czech humbert", "dennis humingway", "Oliver!", "kunsky", "Bolton", "Hawaii", "czarevitch", "gregor v. Helvering", "junk", "Hartford", "your Excellency", "George III", "Lincoln", "severn", "cairn", "pon farr", "preston", "preston", "Jesse Garon Presley", "komando Pasukan Khusus", "lithium", "40", "The Duchess", "preston", "white", "China", "Salt Lake City, Utah", "paul paul", "Capricorn", "rugby", "sergio Garc\u00eda Fern\u00e1ndez", "butterfly", "preston", "The Savoy", "Steve Jobs", "habitat", "2", "729", "Amazon.com", "right-wing extremist groups.", "Rocky Ford brand cantaloupes", "Heartbreak Hotel", "rhinoceros", "Wes Craven", "Australian", "\"$10,000 Kelly,\""], "metric_results": {"EM": 0.46875, "QA-F1": 0.5373511904761905}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6916", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-5252", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.46875, "CSR": 0.5863095238095238, "EFR": 1.0, "Overall": 0.7349962797619047}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "the league", "gold", "the Chinese", "Surrey", "tESLAR Satellite", "wED", "Buzz Aldrin", "paul", "Niger", "backgammon", "Instagram", "Home alone", "Columbus", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "Crusades", "nicky Henderson", "curb-roof", "jagger", "perseus", "piu forte", "Socrates", "selenium", "Stephen King", "chestnut", "Catskill Mountains", "spiky & Boo", "volt-amperes", "fluid", "Jordan", "jerry huggins", "London", "Husqvarna", "Poland", "treble clef", "forehead", "dill", "eucharist", "spindle", "plum", "Washington, D.C.", "paulcadilly", "base", "Melbourne", "meowbank thistle", "Tangled", "Vincent Motorcycle Company", "daffy Duck", "inner core", "novella", "The Prodigy", "Jack White", "Michelle Rounds", "21-year-old", "jesse", "Daytona", "nick Reiner", "Mickey's PhilharMagic", "hiphop"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5338541666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-170", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_searchqa-validation-1488", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-550"], "SR": 0.515625, "CSR": 0.5830965909090908, "retrieved_ids": ["mrqa_squad-train-9440", "mrqa_squad-train-38956", "mrqa_squad-train-75305", "mrqa_squad-train-39606", "mrqa_squad-train-40191", "mrqa_squad-train-15308", "mrqa_squad-train-13898", "mrqa_squad-train-69587", "mrqa_squad-train-15662", "mrqa_squad-train-73585", "mrqa_squad-train-23594", "mrqa_squad-train-51213", "mrqa_squad-train-47737", "mrqa_squad-train-25310", "mrqa_squad-train-25956", "mrqa_squad-train-67118", "mrqa_squad-train-54477", "mrqa_squad-train-82922", "mrqa_squad-train-84096", "mrqa_squad-train-50264", "mrqa_squad-train-64349", "mrqa_squad-train-32379", "mrqa_squad-train-52686", "mrqa_squad-train-52394", "mrqa_squad-train-22505", "mrqa_squad-train-11689", "mrqa_squad-train-42216", "mrqa_squad-train-46077", "mrqa_squad-train-33931", "mrqa_squad-train-25829", "mrqa_squad-train-6940", "mrqa_squad-train-7974", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1273", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-5056", "mrqa_newsqa-validation-2408", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-4383", "mrqa_squad-validation-1384", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-1101", "mrqa_searchqa-validation-3139", "mrqa_newsqa-validation-3011", "mrqa_squad-validation-4908", "mrqa_squad-validation-8927", "mrqa_searchqa-validation-9116", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-3406", "mrqa_hotpotqa-validation-3949", "mrqa_naturalquestions-validation-3427", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1008", "mrqa_squad-validation-1766", "mrqa_squad-validation-4861", "mrqa_squad-validation-639", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1428", "mrqa_searchqa-validation-16848", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1011"], "EFR": 1.0, "Overall": 0.7343536931818182}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army", "63,523", "faith in Christ", "Ticonderoga Point", "a seal illegally", "in Season 4", "Tyrion", "1972 -- 81", "Dottie West", "October 1980", "Jamie Lee Curtis", "the Central and South regions", "Garbi\u00f1e Muguruza", "Missi Hale", "in 2018", "California beach intercut with scenes of them driving an orange campervan", "modern genetics", "Baltimore", "The United States is the only Western country currently applying the death penalty", "Second Battle of Manassas", "Paspahegh Indians", "left atrium and ventricle", "black comedy revolving around the Brewster family", "starting in 1560s", "Davos", "Prince James", "jazz", "in 2002", "U.S. service members who have died without their remains being identified", "There's a Riot Going On", "Narendra Modi", "Sohrai", "an explosion", "a pop and R&B ballad", "Annette", "The season was ordered in May 2017", "rhinoceros", "ABC", "cell nucleus", "the biological synthesis of new proteins in accordance with the genetic code", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "between 8.7 % and 9.1 %", "hero", "37.7", "1954", "1922 to 1991", "neil helfgott", "preston", "Ethiopia", "Mountain West Conference", "Sydney", "Talib Kweli", "look at how the universe formed by analyzing particle collisions.", "five female pastors", "returning combat veterans", "The Mill on the Floss", "Antarctica", "cherry bombs"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5184009594795884}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.6666666666666666, 0.0, 0.19999999999999998, 0.6666666666666666, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.14285714285714288, 1.0, 0.2222222222222222, 0.0, 0.0, 0.8, 0.12903225806451613, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2919", "mrqa_squad-validation-2373", "mrqa_squad-validation-3408", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_newsqa-validation-2275"], "SR": 0.390625, "CSR": 0.5747282608695652, "EFR": 1.0, "Overall": 0.7326800271739131}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in the 1980s", "the publication of such works as Sant\u014d Ky\u014dden's picturebook Shiji no yukikai ( 1798 )", "almost 3,000 stores", "`` Audrey II ''", "T'Pau", "Millerlite", "comedy web television series", "Universal Pictures and Focus Features", "LED illuminated display", "a line of committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "the retina", "IBM", "Felicity Huffman", "Djokovic", "84", "the United States economy first went into an economic recession", "the Welsh Borders and Shropshire area of the UK", "In 1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Ishaani Ishaan Sinha", "very important", "in the United States", "Jodie Foster", "the head of state", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "`` It ain't over'til it's over", "Massillon, Ohio", "the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "the third-most - massive planet", "the RAF", "15,000 BC", "in New York City", "Egypt", "20 July 2015", "Coroebus of Elis", "Tami Lynn", "Phil Simms", "1", "Nepal", "Elton John", "lung cancer", "Pakistan", "Sam Raimi", "7 October 1978", "would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "Alabama", "wiki", "gaffer"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6174348455598455}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.2, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.972972972972973, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-7352", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.53125, "CSR": 0.5729166666666667, "EFR": 0.9333333333333333, "Overall": 0.718984375}, {"timecode": 24, "before_eval_results": {"predictions": ["22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors.", "German creedal hymn \"Wir glauben all an einen Gott\" (\"We All Believe in One True God\")", "April 20", "Tanzania", "March 29, 2018", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "northern China", "Missouri", "a wand - maker Ollivander being tortured by Potter", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1950, 1955, 1956, 1974, 1975", "May 3, 2005", "David Hemmings as Nigel", "Vijaya Mulay", "a global cruise line that was founded in Italy, is registered in Switzerland, and has its headquarters in Geneva", "1977, 1986, 1987, 1989, 1997, 1998 ( XXXIII ), 2015", "Cody Fern", "22 November 1970", "The Star Spangled Banner", "2007", "Camping World Stadium in Orlando, Florida", "Aldis Hodge as Basketball teen", "US $11,770", "Steve Mazzaro & Missi Hale", "two alkyl halides are reacted with sodium metal in dry ether solution to form a higher alkane", "James", "Kimberlin Brown", "based by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "a single, very long DNA helix on which thousands of genes are encoded", "in either Tagalog or English", "American rock band R.E.M.", "a blend of ground beef and other ingredients and is usually served with gravy or brown sauce", "Juliet", "a semi-independent State of Vietnam", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "In nature as part of the orthophosphate ion ( PO ), consisting of a P atom and 4 oxygen atoms", "various submucosal membrane sites of the body", "Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "its vast territory was divided into several successor polities", "in the Tremont neighborhood of Cleveland, Ohio", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Ahmad ( Real ) selected Doll", "a `` skin - changer ''", "alex based on a Kashmir cardigan", "a large beetle", "Copenhagen", "Super Bowl XXIX", "Vladimir Menshov", "Elbow", "41,", "Fareed Zakaria", "Afghan National Security Forces at the site.", "a Pioneer", "a Welch rabbit", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5558946983560294}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6875000000000001, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2, 0.41379310344827586, 0.8571428571428571, 0.7741935483870968, 1.0, 0.2, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-5910", "mrqa_hotpotqa-validation-3362", "mrqa_newsqa-validation-1795", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.421875, "CSR": 0.566875, "retrieved_ids": ["mrqa_squad-train-4082", "mrqa_squad-train-85730", "mrqa_squad-train-21961", "mrqa_squad-train-44557", "mrqa_squad-train-44959", "mrqa_squad-train-19819", "mrqa_squad-train-69337", "mrqa_squad-train-32743", "mrqa_squad-train-39283", "mrqa_squad-train-64766", "mrqa_squad-train-78998", "mrqa_squad-train-14435", "mrqa_squad-train-3436", "mrqa_squad-train-60457", "mrqa_squad-train-26856", "mrqa_squad-train-58414", "mrqa_squad-train-51329", "mrqa_squad-train-51253", "mrqa_squad-train-8330", "mrqa_squad-train-3794", "mrqa_squad-train-62067", "mrqa_squad-train-15395", "mrqa_squad-train-64463", "mrqa_squad-train-23963", "mrqa_squad-train-43019", "mrqa_squad-train-50268", "mrqa_squad-train-7388", "mrqa_squad-train-62022", "mrqa_squad-train-69963", "mrqa_squad-train-67965", "mrqa_squad-train-29426", "mrqa_squad-train-10451", "mrqa_naturalquestions-validation-7896", "mrqa_searchqa-validation-13899", "mrqa_squad-validation-5911", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-1301", "mrqa_squad-validation-4861", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-1420", "mrqa_triviaqa-validation-1325", "mrqa_newsqa-validation-1963", "mrqa_triviaqa-validation-6684", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-4386", "mrqa_newsqa-validation-1008", "mrqa_squad-validation-2209", "mrqa_naturalquestions-validation-10040", "mrqa_newsqa-validation-268", "mrqa_squad-validation-7449", "mrqa_newsqa-validation-2068", "mrqa_hotpotqa-validation-4271", "mrqa_searchqa-validation-15224", "mrqa_triviaqa-validation-2994", "mrqa_searchqa-validation-3258", "mrqa_squad-validation-3559", "mrqa_searchqa-validation-3478", "mrqa_newsqa-validation-2338", "mrqa_searchqa-validation-2871", "mrqa_triviaqa-validation-6277", "mrqa_searchqa-validation-6722", "mrqa_naturalquestions-validation-7962"], "EFR": 1.0, "Overall": 0.7311093750000001}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "9:00 a.m.", "6.4 nanometers", "1894", "the means of production", "Atlanta, Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public", "the pyloric valve", "Martin Roberts", "Julia Ormond", "Incudomalleolar", "The Satavahanas", "March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Asuka", "when matching regions on matching chromosomes break and then reconnect to the other chromosome", "Hathi Jr.", "the Lower Mainland in Vancouver", "the development of electronic computers in the 1950s", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway", "Wisconsin", "declared neutrality and worked to broker a peace", "2018", "1981", "USS Chesapeake", "arcade mode -- an offline single player or local co-op where players can choose which side to play on and which battle to play in", "a spiritual conversion", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Hedwig", "Lee Mack", "a house edge of between 0.5 % and 1 %", "in the United Kingdom", "1898", "Clarence Anglin", "April 1st", "18 m ( 59.05 ft )", "the Northeast Monsoon", "Michael Crawford", "in the United States sometime during the 1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "the 17th episode in the third season of the television series How I Met Your Mother", "The Parlement de Bretagne", "robert ivy", "phosphorus", "Spencer Perceval", "ancient herding dogs, some dating back to the Roman occupation, which may have included Roman Cattle Dogs, Native Celtic Dogs and Viking Herding Spitzes.", "chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley,", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Stark County, Ohio", "Prince Edward VI", "New Orleans"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5919971113445379}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.07142857142857144, 1.0, 1.0, 0.0, 0.0, 0.4, 0.25, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.7272727272727273, 0.8, 0.5, 1.0, 1.0, 0.3333333333333333, 0.5714285714285715, 1.0, 0.5, 1.0, 0.09090909090909093, 0.5555555555555556, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.4375, "CSR": 0.5618990384615384, "EFR": 0.9722222222222222, "Overall": 0.7245586271367521}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "99", "Thomas Piketty", "vector quantities", "the southwestern United States", "Thomas Alva Edison", "Andy Serkis", "England", "virtual reality simulator", "the five - year time jump", "December 24, 1836", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "`` I Believe ''", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "metamorphic rock, which is created from the transformation of pre-existing rock types through high pressures, high temperatures, or both", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions (FFIs ) to search their records for customers with indicia of'U.S", "the Western world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Ben Rosenbaum", "Zilphia Horton", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "president since creation of the office in 1789", "the liver and kidneys", "the lumbar cistern, a subarachnoid space inferior to the conus medullaris", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the Indian Olympic Committee", "Geoffrey Zakarian", "Tommy James", "a fictionalized version of Sparta, Mississippi", "Bonnie Aarons", "March 31, 2018", "Jay Baruchel", "De Wayne Warren", "the previous week", "rear - view mirror", "the New World", "2015", "in the biosphere", "1937", "the 2017 season", "Beijing", "the court from its members", "to convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "October", "5\u00d75", "Famous Players-Lasky Corporation", "Tiffany & Company", "politician and environmentalist", "villanelle", "a man's lifeless, naked body", "a man's lifeless, naked body", "four months", "a submersible", "Christopher Newport", "rotunda"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5506214597155219}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.33333333333333337, 0.09999999999999999, 0.6153846153846153, 0.0, 1.0, 0.5283018867924527, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.9, 0.0, 1.0, 0.6666666666666666, 0.1111111111111111, 1.0, 0.3333333333333333, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.8, 1.0, 0.4, 0.7272727272727273, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7547", "mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.40625, "CSR": 0.5561342592592593, "EFR": 0.9210526315789473, "Overall": 0.7131717531676414}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature", "Dane", "Albert C. Outler", "Colonel (later Major General) Henry Young Darracott Scott,", "Seminole Tribe", "about 12 million", "Tuesday", "The pilot, whose name has not yet been released,", "the estate with its 18th-century sights, sounds, and scents.", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "said Saturday that he is committed to equality, citing the repeal of the military's \"don't ask, don't tell\" policy as an example.", "leftist Workers' Party", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels", "step up", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "a lizard-like creature from New Zealand", "at a Little Rock military recruiting center", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part", "blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "a national telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe,", "Ankara", "Bill Stanton", "humans", "Herman Thomas", "Bayern", "a lightning strike", "Deputy Treasury Secretary", "St. Louis, Illinois,", "Arizona", "two weeks after Black History Month was mocked in an off-campus party that was condemned by the school.", "al Qaeda,", "Tom Hanks", "outside his house in Najaf's Adala neighborhood", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "both from St. Louis, Missouri, were shooting an independent documentary on poverty in Africa.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Briton Carl Froch", "Abdullah Gul,", "1979", "Heshmatollah Attarzadeh", "Richard Masur", "Jughead Jones", "Sarah Josepha Hale", "1998", "violinist.com", "a single arrow pointing to the left and is used to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Al Capone", "a cabinetmaker", "Sh shrimp", "the phylum Cnidarians"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5870080223595848}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 1.0, 0.2222222222222222, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18749999999999997, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.484375, "CSR": 0.5535714285714286, "retrieved_ids": ["mrqa_squad-train-53004", "mrqa_squad-train-63383", "mrqa_squad-train-26746", "mrqa_squad-train-22472", "mrqa_squad-train-31550", "mrqa_squad-train-23245", "mrqa_squad-train-5832", "mrqa_squad-train-69103", "mrqa_squad-train-48357", "mrqa_squad-train-4916", "mrqa_squad-train-11600", "mrqa_squad-train-78826", "mrqa_squad-train-47866", "mrqa_squad-train-57162", "mrqa_squad-train-39650", "mrqa_squad-train-51870", "mrqa_squad-train-18197", "mrqa_squad-train-75981", "mrqa_squad-train-68825", "mrqa_squad-train-65172", "mrqa_squad-train-65319", "mrqa_squad-train-32473", "mrqa_squad-train-77066", "mrqa_squad-train-4759", "mrqa_squad-train-40091", "mrqa_squad-train-59910", "mrqa_squad-train-67551", "mrqa_squad-train-75305", "mrqa_squad-train-19157", "mrqa_squad-train-66513", "mrqa_squad-train-10705", "mrqa_squad-train-50890", "mrqa_triviaqa-validation-6287", "mrqa_newsqa-validation-279", "mrqa_triviaqa-validation-5659", "mrqa_squad-validation-6390", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-4888", "mrqa_hotpotqa-validation-2141", "mrqa_naturalquestions-validation-5564", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2473", "mrqa_triviaqa-validation-3090", "mrqa_newsqa-validation-4117", "mrqa_naturalquestions-validation-5662", "mrqa_newsqa-validation-1392", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4255", "mrqa_naturalquestions-validation-7896", "mrqa_hotpotqa-validation-5831", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-4266", "mrqa_naturalquestions-validation-9931", "mrqa_triviaqa-validation-4973", "mrqa_hotpotqa-validation-3949", "mrqa_triviaqa-validation-5775", "mrqa_squad-validation-8558", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4323", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-3354", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2617"], "EFR": 1.0, "Overall": 0.7284486607142858}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "President Sheikh Sharif Sheikh Ahmed", "Maersk Alabama", "Thursday and Friday", "Rod Blagojevich", "gasoline", "Denver, Colorado", "Dolgorsuren Dagvadorj", "it does not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned in the Pacific Ocean", "Kurt Cobain's", "Peshawar", "The Casalesi Camorra clan", "President Clinton.", "\"I just think the case speaks for itself.\"", "Nick Adenhart", "Carnival", "unemployment", "environmental", "2009", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "France's", "More than 15,000", "Tens of thousands of new voters", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam,", "Juan Martin Del Potro.", "Misty Croslin,", "Zed,", "Iran to Nazi Germany", "Sharon Bialek", "Kurdish militant group in Turkey", "fallen comrades lost in the heat of battle.", "41,", "the job bill's controversial millionaire's surtax,", "Sabina Guzzanti", "Booches Billiard Hall,", "15,000", "Nearly eight in 10", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "a cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "the town of Acolman, just north of Mexico City", "early 1974", "rugby", "rabies", "Parkinson's", "ten", "Disha Patani", "Anah\u00ed", "UK", "The Ignorance of Bedivere", "witchcraft"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5921335151953758}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.08695652173913043, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.11764705882352942, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-2496", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-2678", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.53125, "CSR": 0.552801724137931, "EFR": 0.9333333333333333, "Overall": 0.7149613864942529}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance (SAR)", "Broncos quarterback Broncos", "teach by rote", "opposed to meat consumption", "\"Dance Your Ass Off.\"", "Robert Barnett,", "business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "British troops", "Jacob Zuma,", "Simon Cowell", "jazz", "\"falling space debris,\"", "Obama's", "30 in Quetta, the capital of Balochistan province,", "Monday night", "prison inmates.", "Franklin, Tennessee,", "The BBC,", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Christmas parade", "football", "consumer confidence", "Republican Party,", "normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "intravenous vitamin \"drips\" are part of the latest quick-fix, health fad catching on in Japan: the IV cafe.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "Paul Ryan,", "top designers,", "about 5:20 p.m. at Terminal C", "the \"Mata Zetas,\" or Zeta Killers.", "Darrel Mohler", "Casalesi Camorra clan", "the Obama and McCain camps", "Sen. Barack Obama", "heavy brush,", "more than 30 Latin American and Caribbean nations", "Empire of the Sun,", "30-minute recorded message", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday,", "Ghana", "Caylee Anthony,", "reached an agreement late Thursday to form a government of national reconciliation.", "six innings,", "Don Valley Parkway / Highway 402 Junction in Toronto", "the Western Bloc ( the United States, its NATO allies and others )", "annually", "Galileo Galilei", "c. 282 BCE", "paper sales company", "Chancellor Christian Kern", "Mississippi Delta", "Wayne County, Michigan", "Diff'rent Strokes", "Akihito,", "the Algonquin Round Table"], "metric_results": {"EM": 0.421875, "QA-F1": 0.6080048495435724}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.14285714285714288, 0.4, 0.6666666666666666, 0.125, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.16, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.5, 0.5, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 0.608695652173913, 0.3636363636363636, 0.0, 1.0, 0.0, 0.5, 0.8, 0.0, 0.8, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-2389"], "SR": 0.421875, "CSR": 0.5484375, "EFR": 1.0, "Overall": 0.727421875}, {"timecode": 30, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.884765625, "KG": 0.49921875, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "letters", "Wendell, North Carolina", "Queen Mary II", "The Roman Arena", "Maggie", "Google", "(IE1)", "HIV", "a Claw", "Jeopardy!", "The Last Starfighter", "(to)", "the House of Romanov", "a mirror", "fermentation", "Godot", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "a bad one", "Han Solo", "(Gutzon) Borglum", "( Katharine) of Aragon", "Paris", "St. Mark", "Oklahoma", "Salman Rushdie", "the United Nations", "Tycho Brahe", "a sitcom", "the Interior", "elephants", "cloister", "\" Mail to the Chief\"", "Pakistan", "DOS for Dummies", "Clue", "Heath", "(Lovely Rita) Rita", "President Theodore Roosevelt", "herbicides", "tornado", "Omaha", "The Greatest Gift", "the Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "Gda\u0144sk", "Bobby Kennedy", "Mercury", "a rash", "Nivetha Thomas", "1967", "the Harris Fire.", "president", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6244791666666667}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-5879", "mrqa_hotpotqa-validation-4689", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3688"], "SR": 0.578125, "CSR": 0.5493951612903225, "retrieved_ids": ["mrqa_squad-train-16903", "mrqa_squad-train-10918", "mrqa_squad-train-55950", "mrqa_squad-train-12300", "mrqa_squad-train-63557", "mrqa_squad-train-46148", "mrqa_squad-train-59259", "mrqa_squad-train-67062", "mrqa_squad-train-1143", "mrqa_squad-train-74753", "mrqa_squad-train-54205", "mrqa_squad-train-33622", "mrqa_squad-train-38198", "mrqa_squad-train-56665", "mrqa_squad-train-71825", "mrqa_squad-train-80241", "mrqa_squad-train-42077", "mrqa_squad-train-35008", "mrqa_squad-train-8529", "mrqa_squad-train-53692", "mrqa_squad-train-22745", "mrqa_squad-train-54203", "mrqa_squad-train-40740", "mrqa_squad-train-81580", "mrqa_squad-train-47337", "mrqa_squad-train-64867", "mrqa_squad-train-57574", "mrqa_squad-train-75677", "mrqa_squad-train-55065", "mrqa_squad-train-9959", "mrqa_squad-train-61268", "mrqa_squad-train-82957", "mrqa_triviaqa-validation-86", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4143", "mrqa_squad-validation-9918", "mrqa_searchqa-validation-16911", "mrqa_newsqa-validation-2148", "mrqa_triviaqa-validation-7343", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-4370", "mrqa_squad-validation-627", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2684", "mrqa_searchqa-validation-3887", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3088", "mrqa_triviaqa-validation-6930", "mrqa_newsqa-validation-1185", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-554", "mrqa_searchqa-validation-10318", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-5252", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-5504", "mrqa_naturalquestions-validation-1015", "mrqa_newsqa-validation-2641", "mrqa_naturalquestions-validation-4124", "mrqa_newsqa-validation-25", "mrqa_triviaqa-validation-2856", "mrqa_searchqa-validation-3267"], "EFR": 1.0, "Overall": 0.7331602822580645}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "divisor", "Carson", "hail", "Cordillera de Merida", "Florida", "the Hippocratic Oath", "Queen Latifah", "Golden Retriever", "Shropshire", "the Aegean Sea", "fingers", "a eagle", "\" Buzz\" Windrip", "a crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a conscientious objector", "Trans-Alaska Pipeline System", "trout", "Friday the 13th", "Dixie Chicks", "Carl Bernstein", "a buffalo", "America", "Istanbul", "Blue Horse", "Blinding light", "Rehab", "the Golden Hind", "Administrative Professionals Week", "Gamal Abdel Nasser", "Montrose", "a black bear, moose, and deer", "dams", "Djibouti", "pyrite", "a cyclone", "Ted Morgan", "cashmere", "Diana", "spilled milk", "grasshopper", "carat", "Robin Hood", "White Cliffs", "Tom", "September 29, 2017", "Wake County", "July 1790", "Nicolas Sarkozy,", "the Republican Party", "a quarter", "Rabies", "Environmental Protection Agency (EPA)", "Robert Gibson", "Mogadishu", "30 minutes, five days a week.", "400 years"], "metric_results": {"EM": 0.59375, "QA-F1": 0.695610119047619}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-15383", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-12318", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.59375, "CSR": 0.55078125, "EFR": 1.0, "Overall": 0.7334375}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "a tornado", "the Trump Taj Mahal", "a banana", "a frying", "John", "Liverpool", "The Andy Griffith Show", "Nassau", "the Mediterranean", "Celsius", "Janet Reno", "Santiago", "Seinfeld", "steroids", "Atlantic City", "John Galt", "Clinton", "Iraq", "the taro", "Sanssouci", "\"Mr. Incredible\"", "Pyotr Ilyich Tchaikovsky", "Philip", "the Stone Age", "\"Some Things Bear Fruit\"", "Billy Pilgrim", "Louis XIV", "it wasn't an animal sacrifice", "Prince Charles", "the Sacred Heart", "whiskers", "a cigarette lighter", "Elmer", "the Volcanism", "Peggy Fleming", "Panama", "the metric system", "the United Kingdom", "Castle Rock Entertainment", "fuchsia", "the Mediterranean Sea", "Neil Bush", "a alcoholic student", "Sinclair Lewis", "Daphne du Maurier", "Starsky & Hutch", "King Willem - Alexander", "the New England Patriots", "a person's speech or language must be significantly impaired in one ( or several ) of the four communication modalities following acquired brain injury or have significant decline over a short time period ( progressive aphasia )", "james republican", "krak\u00f3w", "Ken Burns", "the Wabanaki Confederacy", "Flashback", "Manchester United", "in the Yemeni port city of Aden", "into the Atlantic Ocean.", "four decades"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5278062929717342}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-7455", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-16617", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-15067", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_naturalquestions-validation-3840", "mrqa_triviaqa-validation-1459", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.4375, "CSR": 0.5473484848484849, "EFR": 1.0, "Overall": 0.7327509469696969}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual teachers", "echinos", "poker", "cajun tuna", "Nigeria", "the Bronze Age", "Sulphur Island", "Thomas Merton", "ex-wife", "the phantom", "Crystal Car Fathers Day Auto Show", "Elbe", "74.3", "donut", "volcanoes", "deor", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "ducks, hummingbirds", "Columbia University", "Punky Night", "Sexuality", "Greece", "the Inca", "contagious", "Vin Diesel", "the \"National Crime Syndicate\"", "New Mexico", "the reorganization of French politics", "the Purple Heart", "the Arkansas Diamond", "the 7090 mainframe computer", "Lasky", "wakizashi", "Elvis Presley", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "knitting", "Atonement", "receipt", "Damascus", "Kung", "Innsbruck", "deluge", "SeaWorld", "the chest, back, shoulders, torso and / or legs", "Article Two", "Andy Cole", "genghis Khan.", "phil daniels", "African violet", "the Great Northern Railway", "25 October 1921", "daniels", "\"The Orchid Thief\"", "as a guard in the jails of Washington, D.C. and on the streets of post- Katrina New Orleans,", "died in the Holmby Hills, California, mansion he rented."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5455729166666667}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9258", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13753", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-11208", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-6086", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-7681", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-11473", "mrqa_naturalquestions-validation-6442", "mrqa_triviaqa-validation-7627", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.484375, "CSR": 0.5454963235294117, "retrieved_ids": ["mrqa_squad-train-74018", "mrqa_squad-train-17696", "mrqa_squad-train-74732", "mrqa_squad-train-74604", "mrqa_squad-train-50438", "mrqa_squad-train-718", "mrqa_squad-train-26315", "mrqa_squad-train-59774", "mrqa_squad-train-48688", "mrqa_squad-train-65154", "mrqa_squad-train-74187", "mrqa_squad-train-30782", "mrqa_squad-train-29815", "mrqa_squad-train-13795", "mrqa_squad-train-13581", "mrqa_squad-train-17595", "mrqa_squad-train-14856", "mrqa_squad-train-46659", "mrqa_squad-train-79925", "mrqa_squad-train-76980", "mrqa_squad-train-72707", "mrqa_squad-train-72906", "mrqa_squad-train-26093", "mrqa_squad-train-696", "mrqa_squad-train-6818", "mrqa_squad-train-69441", "mrqa_squad-train-2908", "mrqa_squad-train-20896", "mrqa_squad-train-14682", "mrqa_squad-train-31255", "mrqa_squad-train-85605", "mrqa_squad-train-61622", "mrqa_newsqa-validation-2854", "mrqa_squad-validation-6115", "mrqa_squad-validation-2919", "mrqa_naturalquestions-validation-1433", "mrqa_searchqa-validation-13657", "mrqa_squad-validation-1941", "mrqa_searchqa-validation-12876", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-1126", "mrqa_searchqa-validation-5128", "mrqa_newsqa-validation-3775", "mrqa_naturalquestions-validation-1476", "mrqa_searchqa-validation-14398", "mrqa_squad-validation-7535", "mrqa_naturalquestions-validation-2319", "mrqa_searchqa-validation-9137", "mrqa_newsqa-validation-3655", "mrqa_squad-validation-4634", "mrqa_newsqa-validation-4077", "mrqa_searchqa-validation-9403", "mrqa_triviaqa-validation-5771", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-6011", "mrqa_hotpotqa-validation-2205", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-1441", "mrqa_squad-validation-1407", "mrqa_hotpotqa-validation-1681", "mrqa_newsqa-validation-3911", "mrqa_squad-validation-3130", "mrqa_triviaqa-validation-5322"], "EFR": 1.0, "Overall": 0.7323805147058823}, {"timecode": 34, "before_eval_results": {"predictions": ["independent", "cortisol and catecholamines", "Moon River", "Mighty Joe Young", "Robert the Devil", "the Dutch West India Company", "Hans Christian Andersen", "Luffa", "Hershey", "a snail", "a crossword", "Muhammad Ali", "deodorant", "the Supreme Court", "the north magnetic pole", "Calvin Coolidge", "thunderstorms", "Kennebunkport", "a satellite", "the Black Death", "the Triassic", "elia Earhart", "Hoover Dam", "Panty Raid", "French", "cricket", "The Pythian Games", "Miami Dolphins", "Tonto", "a rodent", "white", "Flying the Unfriendly Skies", "a keypunch", "the Amazons", "The Fugitive", "China", "a blacksmith", "Harper\\'s Ferry", "eye vision", "lilac", "a snaky letter", "Tampa", "zinc", "The Lord Chamberlain\\'s Men", "Leo", "first anniversary", "Nautilus", "salaam", "Bigfoot", "Juris Doctorate", "buy a put option", "The Thing", "Sebastian Lund", "Stephen Curry", "Kusha", "Mars", "Captain America", "Black Tuesday", "South America", "1998", "Picric acid", "Nineteen", "emergency aid", "Siri."], "metric_results": {"EM": 0.5625, "QA-F1": 0.628125}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10398", "mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930", "mrqa_newsqa-validation-3365"], "SR": 0.5625, "CSR": 0.5459821428571429, "EFR": 0.9642857142857143, "Overall": 0.7253348214285713}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "How I Met Your Mother,\"", "the two-state solution", "blue-purple", "little blue booties.", "forgery and flying without a valid license,", "Kurdistan Freedom Falcons,", "the underprivileged.", "end of a biology department", "Malawi", "\"fusion teams,\"", "Her husband and attorney, James Whitehouse,", "all buses, subways and trolleys that carry almost a million people daily.", "Muslim", "Muslim festival", "Caster Semenya", "Fiona MacKeown", "magazine, GospelToday,", "death of cardiac arrest", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "about 30 miles southwest of Nashville,", "The BBC,", "Plymouth Rock", "$55.7 million", "seven", "Karen Floyd", "Expedia", "Robert Redford", "economic and political engagement", "death squad killings", "modern and classic designs", "July", "first night in his car.", "piano", "Amy Bishop,", "\"The Little Couple,\"", "her landlord", "job training", "State Department employee", "two years,", "stopping militant rocket fire", "Diego Maradona", "21-year-old", "bartering -- trading goods and services without exchanging money", "Rawalpindi", "the need for reconciliation in a country that endured a brutal civil war lasting nearly three decades.", "Leo Frank,", "Port-au-Prince", "Buddhism", "The Tupolev Tu-160 strategic bombers", "President George Bush", "independently in different parts of the globe", "Ajax", "a charbagh", "Vito Corleone", "western Caribbean Sea", "Valletta", "Eisenhower Executive Office Building", "Tottenham Hotspur", "Jeri Lynn Zimmermann", "the Palatine Hill", "petrol", "The Incredible Shrinking Man"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5409000721500721}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.36363636363636365, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 0.8, 1.0, 1.0, 0.2222222222222222, 1.0, 0.05555555555555555, 1.0, 1.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.14285714285714288, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-3013", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-1912", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-1743", "mrqa_searchqa-validation-5633"], "SR": 0.4375, "CSR": 0.54296875, "EFR": 1.0, "Overall": 0.7318749999999999}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "sustain future exploration of the moon and beyond.", "Nothing But Love", "Mississippi", "Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million.", "Southern California shoot", "to step up.", "glass shards", "one", "Jaipur", "Barack Obama", "April 6, 1994", "Biden", "Cologne, Germany", "34", "Los Angeles, California.", "one of 10 gunmen who attacked several targets in Mumbai on November 26,", "Barack Obama", "Immigration Minister Eric Besson", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Kevin Evans", "Some truly mind-blowing structures", "FARC rebels.", "Dan Brown", "The pilot, whose name has not yet been released,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\" Mary Procidano,", "Starbucks", "finance", "Monday.", "diagnosed with skin cancer.", "as he exercised in a park in a residential area of Mexico City,", "in Germany'sOre Mountains, and the mountain where the treasure hunters were looking was a copper mine until the 19th century.", "more than 5,600", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "Nearly eight in 10", "Yoko Ono Lennon,", "at least $20 million to $30 million,", "five masked men dressed in black appear on the video, sitting behind a long table. The spokesman explains that they are a group called the \"Mata Zetas,\" or Zeta Killers.", "typically intended so that the best don't meet until later in the competition", "fovea centralis", "10 years", "jerry caribbean", "a bodice or corset", "Jack Nicholson", "Flatbush Zombies", "Crane Wilbur", "Venice", "bagpipe", "reconnaissance", "Magic Johnson Jr.", "Fix You"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6315144513746487}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.2222222222222222, 0.33333333333333337, 1.0, 0.0, 0.9166666666666666, 1.0, 0.5, 1.0, 1.0, 0.8421052631578948, 0.0, 0.5, 0.1, 1.0, 0.0, 0.4444444444444445, 0.0, 0.045454545454545456, 0.8, 0.0, 0.0, 0.5, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-946", "mrqa_newsqa-validation-351", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-1127"], "SR": 0.484375, "CSR": 0.5413851351351351, "retrieved_ids": ["mrqa_squad-train-54170", "mrqa_squad-train-43570", "mrqa_squad-train-70770", "mrqa_squad-train-77033", "mrqa_squad-train-21292", "mrqa_squad-train-19719", "mrqa_squad-train-22203", "mrqa_squad-train-68990", "mrqa_squad-train-68281", "mrqa_squad-train-61584", "mrqa_squad-train-42648", "mrqa_squad-train-44053", "mrqa_squad-train-29003", "mrqa_squad-train-67818", "mrqa_squad-train-63199", "mrqa_squad-train-21534", "mrqa_squad-train-14442", "mrqa_squad-train-23891", "mrqa_squad-train-78440", "mrqa_squad-train-31333", "mrqa_squad-train-80689", "mrqa_squad-train-17847", "mrqa_squad-train-11126", "mrqa_squad-train-37695", "mrqa_squad-train-20759", "mrqa_squad-train-45989", "mrqa_squad-train-10890", "mrqa_squad-train-25548", "mrqa_squad-train-27503", "mrqa_squad-train-55088", "mrqa_squad-train-75246", "mrqa_squad-train-57412", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-4910", "mrqa_triviaqa-validation-6939", "mrqa_newsqa-validation-1383", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1961", "mrqa_squad-validation-170", "mrqa_triviaqa-validation-2754", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-11260", "mrqa_triviaqa-validation-2431", "mrqa_squad-validation-8412", "mrqa_searchqa-validation-2022", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-6374", "mrqa_squad-validation-2226", "mrqa_squad-validation-1583", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-1930", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-8291", "mrqa_squad-validation-132", "mrqa_newsqa-validation-2613", "mrqa_naturalquestions-validation-5366", "mrqa_newsqa-validation-2791", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-9284", "mrqa_searchqa-validation-11704"], "EFR": 1.0, "Overall": 0.731558277027027}, {"timecode": 37, "before_eval_results": {"predictions": ["in all health care settings", "Ricardo Valles de la Rosa,", "500", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "Kgalema Motlanthe,", "ferry", "1994,", "Belfast, Northern Ireland", "Sharon Bialek", "U.S. filmmakers", "Clarkson", "CEO of an engineering and construction company", "the British capital's other two airports,", "40 lash after he was convicted of drinking alcohol in Sudan where he plays for first division side Al-Merreikh of Omdurman.", "breathe through her nose, smell, eat solid foods and drink out of a cup,", "almost 9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "Bergdahl, was captured June 30 from Paktika province in southeastern Afghanistan,", "some of the best stunt ever pulled off", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "3,000 kilometers (1,900 miles)", "strangled his wife in his sleep while dreaming that she was an intruder", "nuclear", "Iran's parliament speaker", "highest ever position", "\"services to film, theater and the arts and to activism for equal rights for the gay and lesbian community.\"", "cars have chosen their rides based on what their cars say", "10", "artificial intelligence.", "no chance", "10", "April 13,", "Samuel Herr,", "London", "Obama", "16", "Ralph Lauren", "$10 billion", "2,800", "three", "David Ben - Gurion", "Kiss", "20 years", "jor-El", "noises Off", "aeoline", "Mauthausen\u2013Gusen", "Delilah Rene", "james gruden", "Pope John Paul II", "art deco", "Invisible Man", "pembrokeshire Coast National Park"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6398679143923709}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.08695652173913042, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.2727272727272727, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.08695652173913043, 0.16666666666666669, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.515625, "CSR": 0.5407072368421053, "EFR": 0.967741935483871, "Overall": 0.7249710844651952}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "an American ship captain held hostage by Somali pirates", "the U.S. Holocaust Memorial Museum", "Ireland.", "At least 33 people", "Sunday", "heavy turbulence", "Sophia Stellatos.", "the Gaslight Theater.", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "Wednesday evening", "Haiti", "The Israeli Navy", "Achmat Dangor, CEO of the Nelson Mandela Foundation,", "84-year-old", "John Kiriakou.", "President Bill Clinton", "humans", "The island's dining scene", "chairman of the House Budget Committee", "a crew of Grayback forest-firefighters walk up the sides of what most people would consider a cliff, to chop down underbrush in preparation for a controlled burn.", "President Robert Mugabe's", "he rejected the option of committing more forces for an undefined mission of nation-building without any deadlines.", "more than 30", "Lisa Brown", "The number of deaths linked to cantaloupes contaminated with the Listeria monocytogenes bacteria has risen to 28,", "it would", "A severe famine swept the nation in 1991-1993, devastating crops, killing up to 280,000 people and displacing up to 2 million,", "Italian Serie A title", "Superman brought down the Ku Klux Klan,", "A tall 34-year-old, slouching exhausted in a Johannesburg church that has become a de facto transit camp,", "mental health and recovery.", "The National Infrastructure Program,", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "Russian flights were carried out in strict accordance with international rules governing airspace above neutral waters, and that the aircraft did not violate the borders of other states.", "Pervez Musharraf", "two", "a 16th grand Slam title.", "the MS Columbus,", "\"Friday the 13th\"", "The local Republican Party", "1 October 2006", "1834", "endocytosis", "jazz", "Scafell Pike", "a growth factor called GCSF (granulocytes colony stimulating factor)", "The Cambridge History of Iran: The Timurid and Safavid Periods", "9,984", "Smithfield, Rhode Island, U.S.", "a vacuum", "Donna Rice Hughes", "a albatross", "actor"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5525873243471928}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.29629629629629634, 0.0, 0.16, 0.5, 1.0, 0.0, 1.0, 0.1904761904761905, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.10256410256410257, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.8571428571428571, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1882", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-5393", "mrqa_searchqa-validation-7185"], "SR": 0.46875, "CSR": 0.5388621794871795, "EFR": 0.9117647058823529, "Overall": 0.7134066270739065}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Iranian consulate,", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week,", "ties", "Addis Ababa,", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "the market makers", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "the painting titled \"The Book\"", "the fact that the teens were charged as adults.", "Palestinian-Israeli issue", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "Saturday,", "flexibility and compassion for patients who have a few alternatives for the alleviation of their pain.", "Robert", "suicides", "Songs penned by Harrison included \"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades.", "Oprah Winfrey.", "They're big, strong, and fierce", "over 1,000 pounds", "two satellites", "the most gigantic pumpkins in the world,", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three", "$50", "walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "1,300 meters in the Mediterranean Sea.", "phone calls or by text messaging,", "Pakistan", "Thursday", "he wants a \"happy ending\" to the case.He told CNN a family friend was paying for his services. \"I will be here to seek the truth.\"", "fluoroquinolone drugs,", "to ensure that detainees are not drugged unless there is a medical reason to do so.", "Empire of the Sun", "digging", "1000 square meters", "President Obama", "North Korea,", "White Hills, Arizona,", "Henrik Stenson", "Rev. Alberto Cutie", "2001 -- 2002 season", "786 -- 802", "31 March 2018", "Muhammad Ali", "tallest building in the world", "1961", "goalkeeper", "the Secret Intelligence Service", "75 mi", "alzheimer", "grasshopper", "the Kneset", "Secretary of the Interior"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6231809363154239}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.1935483870967742, 1.0, 1.0, 0.0, 1.0, 0.7058823529411764, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.15384615384615383, 0.7142857142857143, 0.0, 0.0, 1.0, 0.35714285714285715, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-3032", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-1791", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-6954"], "SR": 0.53125, "CSR": 0.538671875, "retrieved_ids": ["mrqa_squad-train-69628", "mrqa_squad-train-38056", "mrqa_squad-train-1454", "mrqa_squad-train-16004", "mrqa_squad-train-61689", "mrqa_squad-train-864", "mrqa_squad-train-44467", "mrqa_squad-train-38544", "mrqa_squad-train-31617", "mrqa_squad-train-10331", "mrqa_squad-train-74437", "mrqa_squad-train-33090", "mrqa_squad-train-40932", "mrqa_squad-train-67712", "mrqa_squad-train-54659", "mrqa_squad-train-63176", "mrqa_squad-train-82358", "mrqa_squad-train-82838", "mrqa_squad-train-14374", "mrqa_squad-train-77304", "mrqa_squad-train-41107", "mrqa_squad-train-71992", "mrqa_squad-train-56876", "mrqa_squad-train-42683", "mrqa_squad-train-576", "mrqa_squad-train-23549", "mrqa_squad-train-3922", "mrqa_squad-train-10688", "mrqa_squad-train-36827", "mrqa_squad-train-14082", "mrqa_squad-train-50358", "mrqa_squad-train-63435", "mrqa_newsqa-validation-1101", "mrqa_triviaqa-validation-7635", "mrqa_newsqa-validation-621", "mrqa_searchqa-validation-16625", "mrqa_newsqa-validation-2371", "mrqa_triviaqa-validation-86", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-6649", "mrqa_squad-validation-4911", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4742", "mrqa_searchqa-validation-13326", "mrqa_newsqa-validation-464", "mrqa_hotpotqa-validation-1239", "mrqa_naturalquestions-validation-276", "mrqa_searchqa-validation-7455", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1740", "mrqa_searchqa-validation-5539", "mrqa_squad-validation-3165", "mrqa_newsqa-validation-3732", "mrqa_naturalquestions-validation-4359", "mrqa_newsqa-validation-4011", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-616", "mrqa_naturalquestions-validation-9789", "mrqa_newsqa-validation-2709", "mrqa_squad-validation-3408", "mrqa_triviaqa-validation-6930", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-3758", "mrqa_squad-validation-4068"], "EFR": 1.0, "Overall": 0.731015625}, {"timecode": 40, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.845703125, "KG": 0.4515625, "before_eval_results": {"predictions": ["1985", "Dr. Murray", "eight", "Austin Wuennenberg,", "in a canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher", "Republican Gov. Bobby Jindal", "Afghan security forces", "Lieberman", "the meter", "the Gulf", "Petionville, Haiti,", "northwest Pakistan", "Basel", "Pyongyang and Seoul", "\"I'm not afraid to say it, sometimes she was a pain in the ass,\"", "Kurt Cobain's", "using recreational drugs", "1983", "22-10.", "Los Ticos in Cairo", "The newest Miss USA,", "delivers a big speech", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "Justicialist Party, or PJ by its Spanish acronym,", "at a construction site in the heart of Los Angeles.", "The Falklands, known as Las Malvinas", "86", "future relations between the Middle East and Washington.", "Hundreds of contraband cell phones", "six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "alternative-energy vehicles", "the Joint Chiefs of Staff,", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "melt", "Communist", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka", "he said Chaudhary's death should serve as a warning to management,", "summer", "Rev. Alberto Cutie", "since 1983.", "the child might still be alive,", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "third generation", "Jack Ruby", "The Altamont Speedway Free Festival", "james cosmo, Ewan McGregor", "Nicol Williamson", "25, 2010", "Latin American culture", "Dolly Parton", "the Sarajevo Haggadah", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5939046148220224}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.923076923076923, 0.5, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7142857142857143, 0.9411764705882353, 0.4, 0.33333333333333337, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 0.9767441860465117, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-1870", "mrqa_triviaqa-validation-6801", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.4375, "CSR": 0.5362042682926829, "EFR": 1.0, "Overall": 0.7178658536585366}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "Amsterdam.", "in the mouth.", "Security officer Stephen Johns reportedly opened the door for the man", "Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "the company", "drama that pulls in the crowds", "across Greece", "a monthly allowance,", "U.S. Navy", "videos and commentaries.", "Marcell Jansen", "he believed he was about to be attacked himself.", "Ross Perot.", "outside the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "a missing 38-foot boat", "The FBI's", "Tuesday in Los Angeles.", "Honduran", "curfew in Jaipur", "Sri Lankan", "Robert", "in a park in a residential area of Mexico City,", "16 times.", "Disney", "in the picturesque Gamla Vaster neighborhood", "the Russian air force,", "an Italian and six Africans", "three masked men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "an auxiliary lock", "Chancellor Angela Merkel", "2,700-acre sanctuary in rural Tennessee.", "Missouri.", "the Dalai Lama", "Ketamine", "Haleigh Cummings,", "at least two and a half hours.", "Bobby Darin,", "Queen Elizabeth's", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama", "an obscure story of flowers", "Schalke", "Kris Allen,", "World Wide Village,", "2", "Supplemental oxygen", "Iran", "Harley", "jerry mix", "George Washington", "lion", "German", "Forbes", "black magic", "cholesterol", "Orlando", "Italian Agostino Bassi"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5617277412628022}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.4, 0.4878048780487806, 0.4, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.4, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.1, 1.0, 0.8571428571428571, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-13584", "mrqa_naturalquestions-validation-8733"], "SR": 0.4375, "CSR": 0.5338541666666667, "EFR": 0.9444444444444444, "Overall": 0.7062847222222223}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product / market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Nick Grimshaw", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Palmer Williams Jr.", "Chicago metropolitan area", "Coldplay", "$5.4 trillion", "3,000 metres ( 9,800 ft )", "Ann Gillespie", "Brooklyn Heights", "Dr. Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fred E. Ahlert", "Institute of Chartered Accountants of India ( ICAI )", "2012", "Bette Midler", "Peristaltic contractions", "Walter Mondale", "Nick Sager", "long - standing policy of neutrality", "18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "Michael Madhusudan Dutta", "one", "Bill Patriots", "extremely inadequate representation of Scheduled Castes, Scheduled Tribes and Other Backward Castes in employment and education due to historic, societal and cultural reasons", "Brobee", "January 15, 2007", "John Garfield", "active absorption of water from the soil by the root", "pH ( / pi\u02d0\u02c8e\u026at\u0283 / ) ( potential of hydrogen )", "geophysicists", "Billy Colman", "360", "November 17, 2017", "Lulu", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay", "1932", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Jean-Claude Van Damme", "The Screening Room", "supermodel", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson's", "surrogate", "salt", "Rocky Marciano", "consumer confidence"], "metric_results": {"EM": 0.5, "QA-F1": 0.5958448992635331}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.21052631578947367, 0.7499999999999999, 1.0, 0.19999999999999998, 0.5, 0.4, 1.0, 0.33333333333333337, 0.7272727272727273, 0.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.16, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.19999999999999998, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.1, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-1351", "mrqa_searchqa-validation-10233"], "SR": 0.5, "CSR": 0.5330668604651163, "retrieved_ids": ["mrqa_squad-train-41124", "mrqa_squad-train-69130", "mrqa_squad-train-70564", "mrqa_squad-train-42233", "mrqa_squad-train-23429", "mrqa_squad-train-55583", "mrqa_squad-train-43475", "mrqa_squad-train-50082", "mrqa_squad-train-80387", "mrqa_squad-train-5409", "mrqa_squad-train-42212", "mrqa_squad-train-46345", "mrqa_squad-train-38926", "mrqa_squad-train-11302", "mrqa_squad-train-59412", "mrqa_squad-train-24188", "mrqa_squad-train-46197", "mrqa_squad-train-53393", "mrqa_squad-train-78791", "mrqa_squad-train-16118", "mrqa_squad-train-21125", "mrqa_squad-train-52360", "mrqa_squad-train-43627", "mrqa_squad-train-74153", "mrqa_squad-train-11868", "mrqa_squad-train-32536", "mrqa_squad-train-63403", "mrqa_squad-train-7518", "mrqa_squad-train-17086", "mrqa_squad-train-71440", "mrqa_squad-train-46374", "mrqa_squad-train-34520", "mrqa_triviaqa-validation-2296", "mrqa_squad-validation-8597", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-4028", "mrqa_naturalquestions-validation-3993", "mrqa_newsqa-validation-4033", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_triviaqa-validation-1972", "mrqa_searchqa-validation-4383", "mrqa_newsqa-validation-1902", "mrqa_squad-validation-4634", "mrqa_newsqa-validation-1458", "mrqa_hotpotqa-validation-1298", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_newsqa-validation-2614", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-10719", "mrqa_hotpotqa-validation-4485", "mrqa_searchqa-validation-11091", "mrqa_newsqa-validation-158", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-6737", "mrqa_newsqa-validation-3190", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-11473", "mrqa_squad-validation-1407", "mrqa_newsqa-validation-1702"], "EFR": 1.0, "Overall": 0.7172383720930233}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational", "A witness", "34", "Miami Beach, Florida,", "eight surgeons", "Somalia's piracy problem was fueled by environmental and political events.", "Cash for Clunkers", "Kim Clijsters", "it has witnessed only normal maritime traffic around Haiti,", "California-based Current TV", "I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Matt Kuchar and Bubba Watson", "Columbia", "Ali Bongo's", "the outdoors,", "mother.", "Casablanca, Morocco,", "1940's", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "pizza", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "our new cars and trucks", "Chinese", "Passers-by", "He hears what I'm saying, but there's just no coming through,\"", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Larry Ellison,", "Mexican military", "Sporting Lisbon", "The Kirchners", "she sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "July 1999,", "CNN's \"Piers Morgan Tonight\"", "\"I hope for the sake of our kids that he gets the psychological help for himself and the safety of others,\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "most of those who managed to survive the incident hid in a boiler room and storage closets during the rampage.", "his parents", "nearly 28 years", "minus 20 degrees", "Claude Monet", "Dodi Fayed's", "Consumer Reports", "Cash for Clunkers", "nine-wicket", "Chicago,", "Plymouth Rock", "more than 40 years", "Michael Schumacher", "freedom of speech, the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "Medicare", "Julia Roberts", "line code", "Harry Bailley", "The Muffin Man", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "94%", "FRAM", "the Ross Ice Shelf", "jests at Scars"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5225178139560865}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.8, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.0, 0.06896551724137931, 0.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.18181818181818185, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6792452830188679, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.4375, "CSR": 0.5308948863636364, "EFR": 0.9722222222222222, "Overall": 0.7112484217171717}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "U.S.-based Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.", "40", "more than 700", "Mandi Hamlin", "breast", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "The State Department calls it \"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "South Africa", "The Obama campaign says that when he makes up his mind, he'll send a text message and e-mail to his supporters to let them know who his sidekick will be.", "the way their business books were being handled.", "The Bronx County District Attorneys Office", "Ma Khin Khin Leh,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation", "North Korea may be trying to prevent attempted defections as the country goes through a tumultuous transition,", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-2", "70,000 or so", "France", "Manuel Mejia Munera", "2,700-acre", "his comments", "The safety of our students, faculty, and staff is my primary concern.", "smiley", "Wanda E Elaine Barzee.", "pro-democracy activists", "Kim", "3,000 kilometers (1,900 miles)", "The agency wants to ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "typically closes for two and half weeks in late summer", "euro", "Asia", "piscinae", "Bible", "Gen. Douglas MacArthur", "PlayStation 4", "Sky News", "cricket fighting", "The Goonies", "Galileo", "Carson McCullers", "fearful man, all in coarse gray with a great iron on his leg"], "metric_results": {"EM": 0.546875, "QA-F1": 0.640173350041771}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.10526315789473682, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.19999999999999998, 0.4, 0.0, 0.5, 1.0, 0.15789473684210525, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-2770", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-226", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-3649", "mrqa_searchqa-validation-10445", "mrqa_triviaqa-validation-3284"], "SR": 0.546875, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.716875}, {"timecode": 45, "before_eval_results": {"predictions": ["sports", "0-0", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend.", "The conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "a treadmill", "Bahrain.", "Piers Morgan", "Mary Phagan,", "well over two decades.", "100,000", "drowned in the Pacific Ocean on November 29, 1981,", "more than a million residents", "9-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her", "\"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "15-year-old", "100% of its byproducts", "it really like to be a new member of the world's most powerful legislature?", "participate in Iraq's government.", "The Rosie Show", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"Reusable Lessons\"", "Rolling Stone", "dogs who walk on ice in Alaska.", "Ralph Lauren", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "problems with the weather, or within the leadership,", "\"a striking blow to due process and the rule of law\"", "a half-brother, who they say is a top-ranking member of La Familia Michoacana drug cartel.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's", "Maria Reisch,", "Sunday", "Rwanda", "cancer", "Jose Manuel Zelaya", "around 10:30 p.m. October 3,", "Monterrey is in Nuevo Leon, one of two states in northeastern Mexico where drug cartel members blocked roads with hijacked vehicles Thursday and Friday to prevent military reinforcements from arriving.", "200", "a full garden and pool, a tennis court, or several heli-pads.", "\"They just were all good little soldiers and pulled right over,\"", "Brian Mabry", "was depressed over a recent breakup, grabbed the gun and took her own life.", "Sunday", "December 2, 2013, and the third season concluded on October 1, 2017", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "Christopher Lloyd", "Claudius", "Ethiopia", "Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "P.M.S. Blackett", "state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6986956619769119}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.16666666666666669, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.8363636363636363, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.59375, "CSR": 0.5326086956521738, "retrieved_ids": ["mrqa_squad-train-496", "mrqa_squad-train-81634", "mrqa_squad-train-9023", "mrqa_squad-train-40023", "mrqa_squad-train-75625", "mrqa_squad-train-1179", "mrqa_squad-train-50385", "mrqa_squad-train-38242", "mrqa_squad-train-59798", "mrqa_squad-train-5403", "mrqa_squad-train-57881", "mrqa_squad-train-15488", "mrqa_squad-train-54277", "mrqa_squad-train-31637", "mrqa_squad-train-3351", "mrqa_squad-train-42927", "mrqa_squad-train-76254", "mrqa_squad-train-70183", "mrqa_squad-train-70646", "mrqa_squad-train-64396", "mrqa_squad-train-24371", "mrqa_squad-train-15511", "mrqa_squad-train-75312", "mrqa_squad-train-18785", "mrqa_squad-train-5145", "mrqa_squad-train-39599", "mrqa_squad-train-6120", "mrqa_squad-train-14630", "mrqa_squad-train-74209", "mrqa_squad-train-62352", "mrqa_squad-train-53555", "mrqa_squad-train-37544", "mrqa_newsqa-validation-2779", "mrqa_triviaqa-validation-5038", "mrqa_searchqa-validation-8976", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-8737", "mrqa_triviaqa-validation-6527", "mrqa_searchqa-validation-4888", "mrqa_triviaqa-validation-7311", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-2684", "mrqa_squad-validation-2920", "mrqa_naturalquestions-validation-7165", "mrqa_newsqa-validation-1616", "mrqa_naturalquestions-validation-6849", "mrqa_triviaqa-validation-2994", "mrqa_naturalquestions-validation-10040", "mrqa_newsqa-validation-1101", "mrqa_naturalquestions-validation-4147", "mrqa_searchqa-validation-266", "mrqa_triviaqa-validation-1363", "mrqa_searchqa-validation-11888", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-692", "mrqa_squad-validation-627", "mrqa_squad-validation-694", "mrqa_newsqa-validation-2586", "mrqa_searchqa-validation-5456", "mrqa_hotpotqa-validation-1703", "mrqa_newsqa-validation-2844", "mrqa_searchqa-validation-5028", "mrqa_newsqa-validation-3817"], "EFR": 0.9615384615384616, "Overall": 0.7094544314381271}, {"timecode": 46, "before_eval_results": {"predictions": ["Islam", "an anti-doping test", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "science fiction", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "eight.", "around Ciudad Juarez,", "former U.S. secretary of state.", "South Africa", "Communist", "Charlotte Gainsbourg", "U.N.", "Ike", "The military commission decision", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "al Qaeda", "debris", "8,", "new materials", "Barack Obama", "Djibouti,", "in the mouth.", "1000 square meters in forward deck space,", "Alfredo Astiz,", "\"It didn't matter if you were 60, 40 or 20 like I am.", "14 years", "1979", "at least 300", "100% of its byproducts", "prostate cancer,", "EU naval force", "vice-chairman of Hussein's Revolutionary Command Council.", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan.", "black, red or white,", "Seoul.", "make life a little easier for these families by organizing the distribution of wheelchair,", "Muqtada al-Sadr", "a house party in Crandon, Wisconsin,", "Ozzy Osbourne", "almost 100", "$83,27014", "Hungary", "over 800 chapters and more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Christmas", "Ernest Hemingway", "n\u00famero", "Ellie Kemper", "a personalized certificate, an official pin, medallion, and/or a congratulatory letter from the President", "nursery rhyme", "the Equator", "St. Mary's", "Holly", "Lundy Island"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6025445474664225}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.375, 1.0, 0.28571428571428575, 1.0, 0.8, 0.0, 1.0, 0.5, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-4204", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-1315", "mrqa_searchqa-validation-12477"], "SR": 0.515625, "CSR": 0.5322473404255319, "EFR": 0.967741935483871, "Overall": 0.7106228551818806}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "Body Works\"", "\"a striking blow to due process and the rule of law.", "make the new truck safer, but also could make it more expensive to repair after a collision.", "200", "Alexey Pajitnov", "1959.", "lightning strike", "Harrison Ford", "at least 18 federal agents and two soldiers", "$17,000", "The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "\"Zed,\"", "$3 billion,", "Les Bleus", "Samoa", "more than 100.", "The controversial technique that simulates drowning -- and which President Obama calls torture -- was used at least 83 times in August 2002 on suspected al Qaeda leader Abu Zubaydah,", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "The dog, Bo, has triggered \"the most publicity the breed has ever had since its introduction into the U.S. in the late 1960s,\"", "Long Island convenience store", "launched a criminal investigation into the statements and reports given by the woman.", "Damon Bankston", "Fayetteville, North Carolina,", "hand-painted", "Herds of tiny pine beetles are munching away at Colorado's forests, turning the evergreens a sickly red and destroying large patches of trees.", "guard in the jails of Washington, D.C. and on the streets of post- Katrina New Orleans,", "Ventures", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals. The LEDs", "Deputy Treasury Secretary", "an Italian and six Africans", "Damon Bankston", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick. That's when I see the mud coming out of the top of the derick,\"", "The food, music, culture and language of Latin America", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday", "she's in love,", "Miguel Cotto", "Zac Efron", "US Airways Flight 1549", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game )", "the most recent Super Bowl champions", "Kosova", "czarevitch", "auk", "Portland, OR", "from 1993 to 1996", "Minette Walters", "Tom Sennett", "Frank", "photoelectric", "March 23, 2018"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6722628953574481}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0851063829787234, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8695652173913044, 0.0, 0.8571428571428571, 0.0, 0.6, 0.19047619047619047, 1.0, 0.4, 1.0, 1.0, 0.0, 0.8571428571428571, 0.4, 0.8, 0.07692307692307691, 0.9642857142857143, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-452", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7763", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-13582"], "SR": 0.546875, "CSR": 0.5325520833333333, "EFR": 0.9310344827586207, "Overall": 0.7033423132183908}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "North Korea intends to launch a long-range missile in the near future,", "Lindsey Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "The 27-year-old American has made a name for himself singing enka, a traditional form of lounge music that flourished in 1940's Japan.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "Peppermint oil, soluble fiber, and antispasmodic drugs", "his own death by crashing his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Kris Allen,", "her fetus were found beneath in a fire pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "in a 4-1 Serie A win at Bologna on Sunday", "Haitians", "suppress the memories and to live as normal a life as possible;", "1981,", "in terms of the country's most-wanted list,", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "Iran test-launched a rocket capable of carrying a satellite,", "$279 for weeklong classes in which you log 30 hours; 877/444-2252.", "his brother to surrender.", "helping to plan the September 11, 2001,", "fighter-pilot", "in the picturesque Gamla Vaster neighborhood", "it really like to be a new member of the world's most powerful legislature?", "in Arabic, Russian and Mandarin that led police to 86 suspects in a series of raids that started Tuesday,", "NATO fighters", "Michelle Obama", "two people,", "$250,000", "his record breaking victory as he claimed his sixth world title at a different weight by beating Cotto", "Courtney Love,", "Hu Jintao", "Bahrain,", "54", "Anil Kapoor has long been a Bollywood luminary, but after his award-winning performance in global hit \"Slumdog Millionaire,\"", "murder in the beating death of a company boss who fired them.", "African National Congress", "60 euros", "Carl and Ellie", "maintain an \"aesthetic environment\" and ensure public safety,", "Oklahoma", "season seven", "BeBe Winans", "jerry tony blair", "her old friend", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "lethal", "small-town rabbi", "Cheers Boned the Fish", "Shep Meyers"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6479560343570894}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.4166666666666667, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 1.0, 0.2, 0.9565217391304348, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 0.8, 0.2222222222222222, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-7105", "mrqa_searchqa-validation-11020", "mrqa_searchqa-validation-11614"], "SR": 0.515625, "CSR": 0.5322066326530612, "retrieved_ids": ["mrqa_squad-train-36564", "mrqa_squad-train-68314", "mrqa_squad-train-14523", "mrqa_squad-train-5832", "mrqa_squad-train-37296", "mrqa_squad-train-4215", "mrqa_squad-train-3084", "mrqa_squad-train-55944", "mrqa_squad-train-50562", "mrqa_squad-train-23552", "mrqa_squad-train-52144", "mrqa_squad-train-535", "mrqa_squad-train-15052", "mrqa_squad-train-84515", "mrqa_squad-train-14369", "mrqa_squad-train-59275", "mrqa_squad-train-79838", "mrqa_squad-train-25760", "mrqa_squad-train-79856", "mrqa_squad-train-63849", "mrqa_squad-train-7877", "mrqa_squad-train-21809", "mrqa_squad-train-56706", "mrqa_squad-train-48966", "mrqa_squad-train-55386", "mrqa_squad-train-33892", "mrqa_squad-train-57168", "mrqa_squad-train-38341", "mrqa_squad-train-8117", "mrqa_squad-train-33145", "mrqa_squad-train-17753", "mrqa_squad-train-23746", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-1309", "mrqa_hotpotqa-validation-1952", "mrqa_searchqa-validation-13899", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2473", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1119", "mrqa_triviaqa-validation-5659", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-2605", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-418", "mrqa_triviaqa-validation-1972", "mrqa_newsqa-validation-293", "mrqa_naturalquestions-validation-9523", "mrqa_newsqa-validation-3621", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-4027", "mrqa_naturalquestions-validation-486", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-12775", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-452", "mrqa_naturalquestions-validation-519", "mrqa_squad-validation-3119", "mrqa_newsqa-validation-4204", "mrqa_triviaqa-validation-3110", "mrqa_searchqa-validation-6638"], "EFR": 0.967741935483871, "Overall": 0.7106147136273864}, {"timecode": 49, "before_eval_results": {"predictions": ["delegation of American Muslim and Christian leaders", "\"an Afghan patriot\" who \"has sacrificed his life for the sake of Afghanistan and for the peace of our country.", "35,000.", "curfew", "Martin Luther King Jr.", "Four", "country's best-kept summer secret.", "The Falklands,", "Pyongyang and Seoul", "in Japan: the IV cafe.", "Africa", "Haiti", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip,", "cancerous tumor.", "Mark Hampton", "\"It was a wrong thing to say, something that we both acknowledge,\"", "his former caddy,", "David McKenzie", "\"If we're going to revise our policies here, we need to make it so for all the camps,\"", "Daniel Radcliffe", "\"The Da Vinci Code,\"", "Ferraris, a Lamborghini and an Acura NSX", "\"The Da Vinci Code\"", "al Qaeda,", "Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$8.8 million", "attacks that started in April 1994,", "$60 million", "4,000 credit cards and the company's \"private client\" list,", "Alice Horton", "33", "the Carrousel du Louvre,", "137", "bartering", "Austin Wuennenberg,", "wanted to change the music on the CD player and the 34-year-old McGee said the football star had acted aggressively in trying to grab the device.", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old's", "almost 100 vessels", "Matthew Fisher,", "southern city of Naples", "Hackers who commandeer your computer are bad enough.", "Saturday", "Both women", "Andy Serkis", "in the 1970s", "a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "the \u00d8resund Bridge", "Richard Attenborough", "eclipse", "Ehrlichman novel", "London", "Comanche County, Oklahoma", "Kevin Nealon", "Protestant", "Virginia Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6756510416666667}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.625, 0.0, 1.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.9523809523809523, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-3953", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2370", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-4033", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-703", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-6297"], "SR": 0.578125, "CSR": 0.5331250000000001, "EFR": 1.0, "Overall": 0.71725}, {"timecode": 50, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.853515625, "KG": 0.471875, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria", "11", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shanghai,", "\"Den of Spies\"", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad,", "March 8", "female soldier,", "remote highway in Michoacan state,", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the all-white public high school.", "the U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "strong work ethic", "12", "Arabic, French and English,", "40", "South Africa,", "L'Aquila", "\" Body Works\"", "North Korea,", "at least 27", "racially-tinged remark made by his former caddy,", "Amsterdam, in the Netherlands,", "burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "shoot down the object whether it is a satellite.", "posting a $1,725 bail,", "Cal Ripken Jr.", "more than 78,000 parents", "Apple Inc.", "London's", "the assassination program,", "martial arts,", "the couple's", "Operation Crank Call,\"", "Orwell", "Guwahati", "the winter solstice", "Frenchman", "sheep", "daisy", "1812", "musicologist", "1902,", "Folly", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.578125, "QA-F1": 0.675380608974359}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9743589743589743, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.25, 0.8181818181818181, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1384", "mrqa_triviaqa-validation-7329", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.578125, "CSR": 0.5340073529411764, "EFR": 0.9259259259259259, "Overall": 0.7062835307734204}, {"timecode": 51, "before_eval_results": {"predictions": ["Kenyan and Somali governments", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "1.2 million", "Arizona", "Kenyan and Somali governments", "meter reader", "Diego Maradona", "London", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "in rural Tennessee.", "Fakih", "as many as 50,000", "14", "Former Mobile County Circuit Judge Herman Thomas", "18", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Nook tablet", "Viceroy,", "Dolgorsuren Dagvadorj,", "said they would not be making any further comments,", "41,", "Anil Kapoor is part of the Kapoor family that is a veritable Bollywood dynasty. His father Surinder and elder brother Boney are film producers, his daughter Sonam, sister-in-law Sridevi", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "the pirates", "the estate with its 18th-century sights, sounds, and scents.", "Isabella", "March 22,", "Hamas,", "about 3,000 kilometers (1,900 miles),", "September 21.", "cell phones", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "the prime minister's handling of the L'Aquila earthquake,", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs", "Charlton Heston", "administrative supervision", "saint-deity manaman mac L\u00ea L\u1ee3i", "national militia", "The Landlord\\'s Game", "in the world", "U.S. states of Kentucky, Virginia, and Tennessee.", "1999", "beans", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7064723523743555}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4615384615384615, 0.8, 1.0, 0.4, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.11764705882352941, 0.2222222222222222, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3004", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5512", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-2485", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-4624"], "SR": 0.578125, "CSR": 0.5348557692307692, "retrieved_ids": ["mrqa_squad-train-45612", "mrqa_squad-train-103", "mrqa_squad-train-63786", "mrqa_squad-train-77438", "mrqa_squad-train-60567", "mrqa_squad-train-51674", "mrqa_squad-train-65295", "mrqa_squad-train-18932", "mrqa_squad-train-86089", "mrqa_squad-train-82139", "mrqa_squad-train-2723", "mrqa_squad-train-80761", "mrqa_squad-train-56931", "mrqa_squad-train-26022", "mrqa_squad-train-84825", "mrqa_squad-train-66878", "mrqa_squad-train-83560", "mrqa_squad-train-30212", "mrqa_squad-train-69206", "mrqa_squad-train-86294", "mrqa_squad-train-72889", "mrqa_squad-train-12682", "mrqa_squad-train-49120", "mrqa_squad-train-5402", "mrqa_squad-train-20204", "mrqa_squad-train-16379", "mrqa_squad-train-24048", "mrqa_squad-train-24377", "mrqa_squad-train-36578", "mrqa_squad-train-3852", "mrqa_squad-train-24289", "mrqa_squad-train-10349", "mrqa_newsqa-validation-1154", "mrqa_triviaqa-validation-2022", "mrqa_squad-validation-825", "mrqa_naturalquestions-validation-9457", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-3860", "mrqa_naturalquestions-validation-8502", "mrqa_newsqa-validation-1224", "mrqa_naturalquestions-validation-64", "mrqa_newsqa-validation-1022", "mrqa_naturalquestions-validation-5942", "mrqa_newsqa-validation-4078", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-4893", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3375", "mrqa_triviaqa-validation-3824", "mrqa_newsqa-validation-2653", "mrqa_triviaqa-validation-1972", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6234", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-3441", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2753", "mrqa_searchqa-validation-6954", "mrqa_newsqa-validation-1947", "mrqa_naturalquestions-validation-276", "mrqa_searchqa-validation-6992"], "EFR": 1.0, "Overall": 0.7212680288461538}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "education", "No. 4", "\"The Bergdahl family is not speaking with media, but Baker said prayer is helping. \"Prayer means that we are extremely powerful because God is not limited by where we are when we pray.", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "\"The Angels family has suffered a tremendous loss today,\"", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "Manchester City", "planned attacks in the southern port city of Karachi,", "\"falling space debris,\"", "Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Alfredo Astiz,", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides, but certainly we will look at every opportunity,\"", "Kingman Regional Medical Center,", "bronze medal in the women's figure skating final,", "Long Island", "5,600", "Pew Research Center", "Sharon Bialek", "Brad Blauser,", "two", "humans", "Muslim", "New York appeals court Thursday overturned terrorism convictions for a Yemeni cleric and his personal assistant,", "Evans", "near the Somali coast", "$24,000-30,000", "2008,", "killing rampage.", "\"Twilight\"", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "the Obama and McCain camps", "flooding was so fast that the thing flipped over,\"", "five", "Trevor Rees,", "Dubai", "June 6, 1944,", "the bill", "free laundry service.", "to bring", "the sex organs", "Aidan Gallagher", "Rebecca Adlington", "Cheshire", "15", "high-ranking", "1994", "The entity", "The Suite Life of Zack & Cody", "American erotic thriller film", "launch one ship", "northern latitudes"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6305912745879851}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.578125, "CSR": 0.5356721698113207, "EFR": 0.9629629629629629, "Overall": 0.7140239015548567}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "Lance Cpl. Maria Lauterbach", "throwing three punches", "Argentine", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "the simple puzzle video game,", "\"Dancing With the Stars\"", "Time's Most Influential People", "across Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "155 people aboard in the frigid river waters", "he failed to return home,", "Jiverly Wong,", "Ireland", "the Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred", "Sunday,", "help nations trapped by hunger and extreme poverty,", "$10 billion", "prosecutors of buckling under pressure from the ruling party.", "April 22.", "Mitt Romney", "twice.", "Long troop deployments in Iraq,", "Mary Phagan,", "pesos", "judge", "Herman Cain,", "$89", "$60 billion on America's infrastructure.", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "The BBC", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "an independent homeland since 1983.", "Nafees A. Syed,", "Sunday", "a share in the royalties for the tune.", "drug cartels", "in a canyon in the path of the blaze Thursday.", "a number of calls,", "Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluated", "Anatomy ( Greek anatom\u0113, `` dissection ''", "seven", "gregorfried Trebitsch", "celebrity footwear", "Herbert Lom Dies", "Battle of Prome", "Shawnee Mission Parkway", "Jean- Marc Vall\u00e9e", "a chance/community chest card", "\"Pudge\"", "Rhonda Revelle", "Kwame Nkrumah"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6325585533126294}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.4, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.4, 0.3333333333333333, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1813", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-6536", "mrqa_triviaqa-validation-7313", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-8941"], "SR": 0.546875, "CSR": 0.5358796296296297, "EFR": 0.9310344827586207, "Overall": 0.70767969747765}, {"timecode": 54, "before_eval_results": {"predictions": ["$249", "diabetes and hypertension,", "Jet Republic,", "many different", "at least 27", "last week,", "Peru's", "She's been a comedian, talk-show host and feared red carpet fashion critic.", "\"Watchmen's\"", "sovereignty over the islands", "NATO's Membership Action Plan, or MAP,", "Bangladesh", "250,000", "complicated and deeply flawed man", "scored his sixth Test century", "Jenny Sanford,", "would slow economic growth with higher taxes.", "voluntaryinson after witnesses identified him and he was interviewed by police.", "be a song-and-dance man.", "South Africa", "The noose incident occurred two weeks after Black History Month", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "500 feet down an embankment", "Marxist guerrillas", "1918-1919.", "Rwanda", "Osama bin Laden's sons", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "58 minutes.", "graduate from this school district.", "CNN", "Jobs", "bribing other wrestlers to lose bouts,", "his comments", "Juan Martin Del Potro.", "Tehran, Iran,", "gasoline", "Thirty to 40 ships", "the country music superstar", "President Obama drew big laughs at the annual White House Correspondents' Association dinner Saturday,", "Tuesday in Los Angeles.", "jut out from the dam", "No", "Kenyan forces who have entered Somalia,", "his health", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "2017", "October 2", "quartz or feldspar", "the Kursk nuclear submarine", "squash", "she's coached by her father, Louis-Paul,", "Caesars Entertainment Corporation", "Premier League club Manchester United", "March", "Eudora Welty", "Richard Nixon", "sousaphone", "National Lottery"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5376602564102564}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.8333333333333333, 0.0, 0.07692307692307693, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.2666666666666667, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-3638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1771", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-655", "mrqa_triviaqa-validation-2064", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.421875, "CSR": 0.5338068181818182, "retrieved_ids": ["mrqa_squad-train-39613", "mrqa_squad-train-238", "mrqa_squad-train-14589", "mrqa_squad-train-86003", "mrqa_squad-train-39460", "mrqa_squad-train-84385", "mrqa_squad-train-35759", "mrqa_squad-train-38477", "mrqa_squad-train-2974", "mrqa_squad-train-5063", "mrqa_squad-train-75875", "mrqa_squad-train-6534", "mrqa_squad-train-84611", "mrqa_squad-train-59097", "mrqa_squad-train-59448", "mrqa_squad-train-18413", "mrqa_squad-train-59621", "mrqa_squad-train-71944", "mrqa_squad-train-73357", "mrqa_squad-train-61114", "mrqa_squad-train-44240", "mrqa_squad-train-28699", "mrqa_squad-train-76987", "mrqa_squad-train-51392", "mrqa_squad-train-12877", "mrqa_squad-train-61440", "mrqa_squad-train-21757", "mrqa_squad-train-53254", "mrqa_squad-train-53569", "mrqa_squad-train-74166", "mrqa_squad-train-43504", "mrqa_squad-train-7777", "mrqa_triviaqa-validation-3450", "mrqa_newsqa-validation-1762", "mrqa_searchqa-validation-11208", "mrqa_newsqa-validation-2740", "mrqa_naturalquestions-validation-3285", "mrqa_squad-validation-1456", "mrqa_searchqa-validation-15778", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3121", "mrqa_squad-validation-7527", "mrqa_searchqa-validation-8941", "mrqa_newsqa-validation-1968", "mrqa_searchqa-validation-11207", "mrqa_newsqa-validation-23", "mrqa_searchqa-validation-5539", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2677", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-13584", "mrqa_triviaqa-validation-391", "mrqa_newsqa-validation-1339", "mrqa_searchqa-validation-11427", "mrqa_hotpotqa-validation-4027", "mrqa_newsqa-validation-3097", "mrqa_squad-validation-825", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2005", "mrqa_naturalquestions-validation-4079", "mrqa_newsqa-validation-2791", "mrqa_naturalquestions-validation-5004", "mrqa_searchqa-validation-6297"], "EFR": 1.0, "Overall": 0.7210582386363635}, {"timecode": 55, "before_eval_results": {"predictions": ["a bond hearing", "without the", "Mexico", "American businessman Ken Plunkett,", "five", "customers are lining up for vitamin injections that promise to improve health and beauty.", "\"to be with you at all times -- even if it's in the bathroom or your bed,\"", "Emmy-winning Patrick McGoohan,", "\"We want to reset our relationship and so we will do it together.'\"", "Cambodian territory", "general astonishment", "June 6, 1944,", "a lightning strike", "81st minute", "Sen. Barack Obama", "money or other discreet aid", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Steve Williams", "200 human bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "Elisabeth Fritzl,", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "the 3rd District of Utah.", "Golfer", "organizing the distribution of wheelchairs,", "raising its alert level,", "\"She was focused so much on learning that she didn't notice,\"", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "the piracy incident", "the western United States.", "Robert Park", "Djibouti,", "HPV (human papillomavirus)", "Six", "North Korea", "delivers a big speech", "Twitter", "Somali President Sheikh Sharif Sheikh Ahmed", "2006,", "five", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "Empire of the Sun", "New Zealand", "a model of sustainability.", "Michael Douglas film, The Jewel of the Nile", "summer", "73", "neoclassic", "squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort in Lake Buena Vista, Florida", "Frank Sinatra", "mass", "a snout beetle", "mowlam"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6229541214787643}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.782608695652174, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 0.5, 1.0, 0.5581395348837209, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2124", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.53125, "CSR": 0.5337611607142857, "EFR": 0.9666666666666667, "Overall": 0.7143824404761905}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "those traveling near the Somali coast", "\"To My Mother\"", "the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "anaphylaxis", "Pat Quinn", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees,", "the most-wanted man in the world", "the Carrousel du Louvre,", "three men with suicide vests who were plotting to carry out the attacks,", "don't have to visit laundromats because they enjoy the luxury of a free", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "shows the world that you love the environment and hate using fuel,\"", "The remains of Cologne's archive building following the collapse", "11", "Henrik Stenson", "CEO of an engineering and construction company", "Milan", "strife in Somalia,", "cancerous tumor.", "provided Syria and Iraq 500 cubic meters of water a second,", "Abdullah Gul,", "traces of tablets in Winehouse's stomach", "11th year in a row.", "\"Ninety-one children said they had been living with their family, consisting of at least one adult they considered to be their parent,\"", "Gov. Rod Blagojevich", "national telephone", "\"rightly insisted that the alarm that was raised be resolved,\"", "about the shootings,", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "it's historical, inspiring, creative, romantic and beautiful.", "gasoline", "a Utah jail", "Swansea Crown Court,", "Carol Browner", "the Dominican Republic", "U.S. and Pakistani officials", "Monday", "a Celtic people living in northern Asia Minor", "diastema", "to manage the characteristics of the beer's head", "can be \u201cfrozen\u201d and then revived in the future", "Cambridge", "Mercury", "13 October 1958", "bassline", "Pansexuality", "\"Invisibility\"", "Zachary Taylor", "a sci-fi series", "Marilyn Monroe"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6372087293692897}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 0.6153846153846153, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.2222222222222222, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.06896551724137931, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3553", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-3408", "mrqa_searchqa-validation-10329", "mrqa_searchqa-validation-15020"], "SR": 0.546875, "CSR": 0.5339912280701755, "EFR": 0.9655172413793104, "Overall": 0.7141985688898972}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "Mad Men", "5,600", "AMD", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "American pop star's", "we will be back,\"", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce", "Carol Browner", "U.N. Security Council", "Department of Homeland Security Secretary Janet Napolitano", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad as", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack by a spurned suitor.", "Congress", "the southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "Petra Nemcova", "South Africa", "Somali", "returning combat veterans could be recruited by right-wing extremist groups.", "opposition supporters in Libreville, Gabon.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "a cardio to ensure he had access to workout equipment at all times without limiting himself to going to the gym or facing days of bad weather.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "the RheinEnergieStadion.", "free milk.", "tennis", "No. 1 slot at the box office.", "Republican Gov. Jan Brewer.", "remote part of northwestern Montana", "securities", "$150 billion", "experimental", "Michael Crawford", "the beginning", "coconut shy", "Fenn Street School", "external ear canal", "Australian", "Argentinian", "fibre optic cable with TOSLINK connectors", "hip-hop", "inducere", "Harvard Law School", "129,007"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7439403044871795}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.5833333333333334, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 0.0, 0.125, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-9174", "mrqa_searchqa-validation-3718"], "SR": 0.65625, "CSR": 0.5360991379310345, "retrieved_ids": ["mrqa_squad-train-15496", "mrqa_squad-train-18937", "mrqa_squad-train-13194", "mrqa_squad-train-11814", "mrqa_squad-train-22138", "mrqa_squad-train-34763", "mrqa_squad-train-42909", "mrqa_squad-train-41590", "mrqa_squad-train-40290", "mrqa_squad-train-83870", "mrqa_squad-train-72874", "mrqa_squad-train-8200", "mrqa_squad-train-70671", "mrqa_squad-train-14171", "mrqa_squad-train-66366", "mrqa_squad-train-76866", "mrqa_squad-train-63713", "mrqa_squad-train-73414", "mrqa_squad-train-49272", "mrqa_squad-train-46835", "mrqa_squad-train-58924", "mrqa_squad-train-77820", "mrqa_squad-train-42305", "mrqa_squad-train-6880", "mrqa_squad-train-15298", "mrqa_squad-train-20909", "mrqa_squad-train-21690", "mrqa_squad-train-63064", "mrqa_squad-train-40909", "mrqa_squad-train-102", "mrqa_squad-train-77012", "mrqa_squad-train-82605", "mrqa_newsqa-validation-1137", "mrqa_naturalquestions-validation-7848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-1384", "mrqa_newsqa-validation-1083", "mrqa_newsqa-validation-3088", "mrqa_searchqa-validation-5339", "mrqa_newsqa-validation-3351", "mrqa_searchqa-validation-198", "mrqa_naturalquestions-validation-2037", "mrqa_newsqa-validation-3072", "mrqa_triviaqa-validation-5969", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3277", "mrqa_squad-validation-1766", "mrqa_triviaqa-validation-7426", "mrqa_newsqa-validation-4023", "mrqa_naturalquestions-validation-4552", "mrqa_squad-validation-2372", "mrqa_triviaqa-validation-6916", "mrqa_naturalquestions-validation-10265", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1418", "mrqa_squad-validation-8921", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3953", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-4075", "mrqa_squad-validation-4861"], "EFR": 1.0, "Overall": 0.7215167025862069}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "Jackson sitting in Renaissance-era clothes and holding a book.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "its fifth season", "Arthur E. Morgan III,", "Jason Chaffetz", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal", "No 4,", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"an incompetent and rude president who is senseless and ignorant as he does not know even elementary diplomatic etiquette and lacks diplomatic ability.\"", "President Obama.", "Jacob Zuma,", "1937,", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "the Southeast,", "\"Up,\"", "getting into that Lexus, Lincoln, Infiniti or Porsche you always wanted, without laying out $70,000 or $80,000 for something you're not actually going to live in.", "capture that fascinating transformation that takes place when carving a pumpkin.", "school,", "a motor scooter", "they can learn in safer surroundings.", "$50 less,", "J.Crew", "$106.5 million", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\"", "Cipro", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "Olivia Newton-John recalls of the skin-tight black trousers she worn in the film \"Grease\"", "Virgin America", "to humiliate herself by standing next to a story,\" said Cyndi Mosteller,", "I sound like a basket case. It's funny with acting -- we all wear masks in our normal life.", "Kenyan and Somali", "export value of this year's poppy harvest stood at around $4 billion, a 29 per cent increase over 2006.", "1980,", "a pool of blood beneath his head.", "Africa", "Osama bin Laden", "left - sided heart failure", "the kidnapper tells Shawn to tell `` Abigail '' that he loved her", "Devastator", "Madness", "Jelly Roll Morton", "vice-admiral", "George Lawrence Mikan, Jr.", "Kait Parker", "Centre-du-Qu\u00e9bec area.", "Nguyen", "doughboy", "United We Stand, Divided We Fall", "professor henry higgins"], "metric_results": {"EM": 0.59375, "QA-F1": 0.685094246031746}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [0.4444444444444445, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.058823529411764705, 0.33333333333333337, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.05714285714285714, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1801", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_hotpotqa-validation-2951"], "SR": 0.59375, "CSR": 0.5370762711864407, "EFR": 0.8461538461538461, "Overall": 0.6909428984680573}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "1913.", "$40 and a loaf of bread.", "9:20 p.m. ET Wednesday.", "U Win Tin,", "543", "Knox's parents", "11 healthy eggs", "four", "64,", "the mammoth's skull,", "at least two and a half hours.", "partially submerged in a stream in shark River Park in Monmouth County", "improve the environment by taking on greenhouse gas emissions.", "a gift to the Obama girls from Sen. Ted Kennedy.", "\"The old-timers who were there when Kurt was around really took offense to some of the things he said about the area, so they had no real reason to honor him,\"", "More than 15,000", "0300 (All times GMT)", "Muslim countries,", "\"Piers Morgan Tonight\"", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty and nonproliferation.", "sumo wrestling", "10 below", "\"The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "recall", "Roy", "VBS.TV", "Dr. Albert Reiter,", "Marxist guerrillas", "Greeley, Colorado,", "five", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "an unprecedented wave of buying", "34", "3,000", "Workers' Party.", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "in the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Brazil remains the largest coffee exporting nation", "Theodore Roosevelt", "vice-admiral", "Phillies", "the Big Bopper", "Greek-American", "feats of exploration", "uncle", "Monarch", "the Ivy League", "Truman", "BBC Formula One coverage"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6935881370091896}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [0.3157894736842105, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9, 0.8421052631578948, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1755", "mrqa_naturalquestions-validation-5620", "mrqa_triviaqa-validation-105", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.59375, "CSR": 0.5380208333333334, "EFR": 0.9615384615384616, "Overall": 0.714208733974359}, {"timecode": 60, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.849609375, "KG": 0.4875, "before_eval_results": {"predictions": ["183", "Ed McMahon,", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000.", "Martin \"Al\" Culhane,", "normal maritime", "\"It feels great to be back at work,\"", "Iran", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "teen former Argentine", "Obama", "Matthew Chance", "34", "five victims by helicopter,", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "Henley-on-Klip, near Johannesburg.", "the Russian air company Vertikal-T,", "to comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Michael Brewer,", "Sunday's", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Brewer", "it is not just $3 billion of new money into the economy.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "he wants a \"happy ending\" to the case.", "the Obama and McCain camps", "the U.S. ship that was hijacked off Somalia's coast.", "in Fayetteville, North Carolina,", "the only goal of the game", "France", "Roberto Micheletti,", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "in Somalia.", "in response to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "Operation Pipeline Express.", "Islamabad", "Williams' body", "Kris Allen,", "ConAgra Foods plant", "Lalo Schifrin", "1982", "Billy Idol", "diagnostic", "David Cameron", "every ten years", "five", "\"The Dragon\"", "1994", "a magnolia", "August 1", "Jupiter", "mural"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6513525028611236}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.3333333333333333, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-681", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3950", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5502", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-16357"], "SR": 0.546875, "CSR": 0.5381659836065573, "retrieved_ids": ["mrqa_squad-train-24358", "mrqa_squad-train-80811", "mrqa_squad-train-17101", "mrqa_squad-train-13519", "mrqa_squad-train-8582", "mrqa_squad-train-71020", "mrqa_squad-train-57416", "mrqa_squad-train-82665", "mrqa_squad-train-44481", "mrqa_squad-train-61185", "mrqa_squad-train-59872", "mrqa_squad-train-66565", "mrqa_squad-train-1407", "mrqa_squad-train-71474", "mrqa_squad-train-42420", "mrqa_squad-train-33473", "mrqa_squad-train-55466", "mrqa_squad-train-81177", "mrqa_squad-train-81815", "mrqa_squad-train-15588", "mrqa_squad-train-69308", "mrqa_squad-train-40661", "mrqa_squad-train-26666", "mrqa_squad-train-47525", "mrqa_squad-train-69328", "mrqa_squad-train-4717", "mrqa_squad-train-9686", "mrqa_squad-train-37560", "mrqa_squad-train-63106", "mrqa_squad-train-21456", "mrqa_squad-train-37994", "mrqa_squad-train-71128", "mrqa_squad-validation-2226", "mrqa_searchqa-validation-1843", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-237", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3343", "mrqa_triviaqa-validation-2925", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-10265", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2708", "mrqa_triviaqa-validation-7233", "mrqa_newsqa-validation-2976", "mrqa_triviaqa-validation-6198", "mrqa_newsqa-validation-1791", "mrqa_naturalquestions-validation-10199", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-2209", "mrqa_triviaqa-validation-3110", "mrqa_naturalquestions-validation-2781", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1022", "mrqa_hotpotqa-validation-486", "mrqa_squad-validation-5657", "mrqa_triviaqa-validation-3354", "mrqa_searchqa-validation-8117", "mrqa_naturalquestions-validation-2124", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1273", "mrqa_naturalquestions-validation-8441"], "EFR": 0.9655172413793104, "Overall": 0.7158147699971735}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "prisoners at the South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "a drug lord with ties to paramilitary groups,", "a baseball bat", "six", "a book.", "Venezuela", "Kerstin", "$1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi.", "Daniel Radcliffe", "the privileged ethnicity, thus giving them better opportunities.", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures by famous artists.", "Shanghai", "the BBC's central London offices", "reduced their carbon footprint by 132 tons.", "engineering and construction", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"It's very new and involves repairing my leaky valve using a clip device, without open heart surgery,", "civilians,", "outstanding performance by a female actor in a drama series for her role as Deputy Chief Brenda Johnson.", "9:20 p.m. ET Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979 Iranian", "three out of four questioned say that things are going well for them personally.", "The island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "that he spent the first night in his car.", "businesses hiring veterans as well as job training for all service members leaving the military.", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "UK", "bipartisan", "has a thicker consistency and a deeper flavour than sauce", "skeletal muscle and the brain", "1985 -- 1993", "Dublin", "goldfinger", "czechoslovakia", "Baltimore", "Wynonna Ellen Judd", "youngest publicly documented", "the Italians", "a razorback", "Canada", "Bolton and the North West"], "metric_results": {"EM": 0.625, "QA-F1": 0.6901041666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 0.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-1548", "mrqa_naturalquestions-validation-2943", "mrqa_triviaqa-validation-6013", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753", "mrqa_triviaqa-validation-4952"], "SR": 0.625, "CSR": 0.5395665322580645, "EFR": 0.9583333333333334, "Overall": 0.7146580981182795}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "greater than 14", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Egypt", "1820s", "Turkey and the western fringes of Iran", "third", "Andrew Garfield", "The Fixx", "in digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "7.6 mm", "March 8, 2018", "Camping World Stadium", "George Harrison", "Kristy Swanson", "Chairman of the Monetary Policy Committee", "simulation", "James Martin Lafferty", "Kenny Anderson", "agriculture", "the vas deferens", "the Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "C\u03bc and C\u03b4", "Ward Productions", "Tbilisi", "dry lake beds", "quota", "Consular Report of Birth Abroad", "Africa and Asia", "Frank Theodore `` Ted '' Levine", "IIII", "a hydrolysis reaction", "France", "Gustav Bauer", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card verification data", "alcohol or smoking", "Sondheim", "Queensland", "Laura Robson", "can be traced back 5000 years.", "Todd McFarlane", "Massachusetts", "one season", "\"significant skeletal remains\"", "the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "the giant mega-yacht 'Wally Island'", "syrup", "palate", "locoweed", "December 1974"], "metric_results": {"EM": 0.5, "QA-F1": 0.6201746323529411}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.9166666666666666, 0.0, 1.0, 0.2, 0.6666666666666666, 1.0, 0.33333333333333337, 0.9411764705882353, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.1, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-5082", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-7184", "mrqa_hotpotqa-validation-5594", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-1699"], "SR": 0.5, "CSR": 0.5389384920634921, "EFR": 1.0, "Overall": 0.7228658234126983}, {"timecode": 63, "before_eval_results": {"predictions": ["Lady Agnes", "the Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Jessica Sanders", "Article 1, Section 2, Clause 3", "Rick Rude", "November 2, 2010", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Mark Lowry", "1877", "a maximum possible strength of 31", "c. 1000 AD", "a bow bridge with 16 arches shielded by ice guards", "Dick Rutan and Jeana Yeager", "a stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "December 1800", "King Saud University", "Hugo Weaving", "Book of Exodus", "a take - it - or - leave - it contract, or a boilerplate contract", "Bart Howard", "transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Toby Kebbell", "1078", "Simon Peter", "Stefanie Scott", "glycine and arginine", "book and architecture", "Stephen A. Douglas", "the Dolby Theatre in Hollywood, Los Angeles, California", "The 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII", "The Republic of Tecala", "during meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22, 2017", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "mitte, in central Berlin", "Marjorie McGinnis", "the Electorate", "fourth-ranking", "Anne Frank,", "Sunday", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone: The Movie", "The Benchwarmers", "the No Child Left Behind Act", "part of the proceeds"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6238512442986581}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, false, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.5714285714285715, 0.8387096774193549, 1.0, 1.0, 1.0, 1.0, 0.326530612244898, 1.0, 0.15789473684210525, 0.4, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.21052631578947367, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.0, 1.0, 0.4, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8, 1.0, 0.888888888888889, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9922", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_triviaqa-validation-2963", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-311", "mrqa_searchqa-validation-7607"], "SR": 0.46875, "CSR": 0.537841796875, "retrieved_ids": ["mrqa_squad-train-35294", "mrqa_squad-train-36991", "mrqa_squad-train-80603", "mrqa_squad-train-45719", "mrqa_squad-train-14426", "mrqa_squad-train-53980", "mrqa_squad-train-37318", "mrqa_squad-train-72152", "mrqa_squad-train-80592", "mrqa_squad-train-22278", "mrqa_squad-train-24706", "mrqa_squad-train-70964", "mrqa_squad-train-7931", "mrqa_squad-train-2727", "mrqa_squad-train-47674", "mrqa_squad-train-3590", "mrqa_squad-train-71561", "mrqa_squad-train-10769", "mrqa_squad-train-21298", "mrqa_squad-train-32093", "mrqa_squad-train-24744", "mrqa_squad-train-33463", "mrqa_squad-train-48316", "mrqa_squad-train-75982", "mrqa_squad-train-17318", "mrqa_squad-train-37137", "mrqa_squad-train-12611", "mrqa_squad-train-50008", "mrqa_squad-train-69957", "mrqa_squad-train-71588", "mrqa_squad-train-75126", "mrqa_squad-train-80758", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1531", "mrqa_naturalquestions-validation-3381", "mrqa_newsqa-validation-3762", "mrqa_searchqa-validation-5028", "mrqa_newsqa-validation-1056", "mrqa_searchqa-validation-13787", "mrqa_newsqa-validation-34", "mrqa_searchqa-validation-33", "mrqa_naturalquestions-validation-8095", "mrqa_squad-validation-9145", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-2104", "mrqa_naturalquestions-validation-7352", "mrqa_searchqa-validation-10681", "mrqa_squad-validation-8749", "mrqa_naturalquestions-validation-6091", "mrqa_searchqa-validation-9915", "mrqa_newsqa-validation-1506", "mrqa_searchqa-validation-1317", "mrqa_triviaqa-validation-959", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-2941", "mrqa_triviaqa-validation-7151", "mrqa_searchqa-validation-1384", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-490", "mrqa_triviaqa-validation-6684", "mrqa_naturalquestions-validation-9457", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-1076"], "EFR": 0.8823529411764706, "Overall": 0.6991170726102941}, {"timecode": 64, "before_eval_results": {"predictions": ["winter", "19 July 1990", "senators", "Rex Harrison", "a manufacturing operation", "Turducken", "Patrick Warburton", "the chief priests", "1960", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "The Seattle Center", "Yuzuru Hanyu", "Tracy McConnell", "Kenny Rogers", "between the stomach and the large intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "a minimum number of hours defined as such by his / her employer", "Naomi", "a hope that belongs to your call one Lord, one faith, one baptism, one God and Father of all, who is over all and through all and in all", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "in 1993", "in the mid - to late 1920s", "Far Away", "John C. Reilly", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "the Battle of Culloden", "Cyanea capillata", "Bonnie Lipton", "Paleolithic", "Tom Brady", "Dawn French", "translator", "Ut\u00f8ya", "Flyweight", "Old World fossil representatives", "1964", "pesos", "North Korea", "at \"E! News\"", "carbon fiber", "income tax", "The Greatest Show on Earth", "ilhtstratioiu"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6903135085958274}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.11764705882352942, 0.0, 0.13953488372093023, 1.0, 0.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-887", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4435", "mrqa_hotpotqa-validation-1810", "mrqa_newsqa-validation-70", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.59375, "CSR": 0.538701923076923, "EFR": 1.0, "Overall": 0.7228185096153845}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "season seven finale", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "lithium", "Pebe Sebert", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae", "Lesley Gore", "Paul", "comic book series", "warplanes", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "December 1, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "`` It's My Party ''", "the Director of National Intelligence", "Liam Cunningham", "Timothy B. Schmit", "a cylinder of glass or plastic that runs along the fiber's length", "the British group Ace", "Goths", "H CO ( equivalently OC (OH ) )", "StubHub Center in Carson, California", "the Maryland Senate", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "between the Mediterranean Sea to the north and the Red Sea in the south", "Germanic elements `` hrod '' meaning renown and `` beraht '' meaning bright", "1888", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "performance marker", "in Super Bowl LII", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge in the U.S. states of Oregon and Washington", "International Campaign to Abolish Nuclear Weapons ( ICAN )", "John Joseph Patrick Ryan", "1912", "the book of Acts", "Ric Flair", "Around 1200, Tahitian explorers found and began settling the area. This began the rise of the Hawaiian civilization", "continental units", "2010", "Adam Werritty", "the Jets", "her white halter dress", "Kim Jong-hyun", "Edward II", "Harrods", "she's grateful that a 40-year water diversion project is nearly complete.", "tax incentives for businesses hiring veterans as well as job training", "Arnold Drummond", "Nixon", "Great Expectations", "cathode", "No Surprises"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6364051940706352}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 0.07999999999999999, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428572, 0.5, 1.0, 1.0, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 0.9, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.4615384615384615, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.058823529411764705, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.515625, "CSR": 0.5383522727272727, "EFR": 0.9032258064516129, "Overall": 0.7033937408357771}, {"timecode": 66, "before_eval_results": {"predictions": ["a substitute good", "May 1980", "IX", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "tourneys or slow wheels", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "decreases as the soil becomes saturated", "Kathy Najimy", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "smoking", "Richard Crispin Armitage", "Himalayas", "Harry Potter and the Deathly Hallows", "mid-ocean ridges", "Sir Rowland Hill", "late - September through early January", "1991", "Joseph Sherrard Kearns", "Union forces", "1931 Statute of Westminster", "a loop ( also called a self - loop or a `` buckle ''", "Carroll O'Connor", "West Egg on prosperous Long Island", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "an `` extraordinary writ ''", "after World War II, ending the Empire of Japan's 35 - year rule over Korea in 1945", "Guwahati", "the largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "Cheap trick", "October 29, 2015", "The Pir Panjal Railway Tunnel", "16", "~ 3.5 million years old from Idaho, USA", "The federal government received only those powers which the colonies had recognized as belonging to king and parliament", "Tigris and Euphrates rivers ; and the Levant, the eastern coast of the Mediterranean Sea", "bicameral Congress", "In the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells", "Michael Crawford", "Microsoft Windows", "east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Los Angeles", "moral tale", "Lana Del Rey", "NBA", "greyhound, gazelle hound or tazi", "Aristotle", "Northwest Mall", "Supergirl", "Field Marshal Lord Gort", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "gun", "between government soldiers and Taliban militants in the Swat Valley.", "Odysseus", "Louisiana", "Boy Scouts of America", "three empty vodka bottles,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5777081616616666}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.8, 0.45454545454545453, 0.4, 0.125, 1.0, 0.9361702127659575, 1.0, 0.0, 0.36363636363636365, 0.0, 0.7272727272727273, 0.0, 0.5333333333333333, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.653061224489796, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_triviaqa-validation-4501", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-4320", "mrqa_newsqa-validation-3067"], "SR": 0.4375, "CSR": 0.5368470149253731, "retrieved_ids": ["mrqa_squad-train-68288", "mrqa_squad-train-74618", "mrqa_squad-train-2770", "mrqa_squad-train-31139", "mrqa_squad-train-13148", "mrqa_squad-train-42473", "mrqa_squad-train-65714", "mrqa_squad-train-82334", "mrqa_squad-train-34845", "mrqa_squad-train-29687", "mrqa_squad-train-47293", "mrqa_squad-train-61913", "mrqa_squad-train-50201", "mrqa_squad-train-8082", "mrqa_squad-train-65128", "mrqa_squad-train-72625", "mrqa_squad-train-27380", "mrqa_squad-train-76893", "mrqa_squad-train-14662", "mrqa_squad-train-56384", "mrqa_squad-train-59159", "mrqa_squad-train-69104", "mrqa_squad-train-34925", "mrqa_squad-train-81632", "mrqa_squad-train-79174", "mrqa_squad-train-78538", "mrqa_squad-train-40113", "mrqa_squad-train-56849", "mrqa_squad-train-72715", "mrqa_squad-train-42146", "mrqa_squad-train-70800", "mrqa_squad-train-7610", "mrqa_searchqa-validation-5857", "mrqa_triviaqa-validation-1622", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-3930", "mrqa_triviaqa-validation-5681", "mrqa_naturalquestions-validation-1415", "mrqa_squad-validation-2372", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-954", "mrqa_searchqa-validation-10193", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2517", "mrqa_triviaqa-validation-2716", "mrqa_newsqa-validation-3221", "mrqa_triviaqa-validation-1706", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-13", "mrqa_triviaqa-validation-5698", "mrqa_newsqa-validation-3499", "mrqa_searchqa-validation-5633", "mrqa_newsqa-validation-172", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7490", "mrqa_squad-validation-5589", "mrqa_newsqa-validation-3733", "mrqa_naturalquestions-validation-5082", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-1961"], "EFR": 0.9722222222222222, "Overall": 0.716891972429519}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "2010", "Clarence Darrow", "John B. Watson", "Spanish explorers", "Tara", "a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "March 6, 2018", "Erica Rivera", "McFerrin, Robin Williams, and Bill Irwin", "Donald Trump", "Matt Flinders", "Kansas and Oklahoma", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "Sir Ronald Ross", "Georgia", "Domhnall Gleeson", "Alex Drake", "March 11, 2016", "early 2017", "Thomas Mundy Peterson", "Augustus Waters", "boxing", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "the supergroup United Support of Artists ( USA ) for Africa", "Amybeth McNulty", "King James Bible", "John Goodman", "the intermembrane space", "February 25, 2004", "breast or lower chest of beef or veal", "each state's DMV, which is required to drive", "Dr. Hartwell Carver", "two", "following the 2017 season", "Arunachal Pradesh", "Charles R Ranch, County Road 24", "condemns rural depopulation and the pursuit of excessive wealth", "to feel close to his son", "the Washington metropolitan area", "euro", "Ferm\u00edn Francisco de Lasu\u00e9n", "Aslan", "Richmond in North Yorkshire", "drinking song", "the tissues of the outer third of the vagina", "Bergen County", "Cartoon Network", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "a skunk", "East Prussia", "Tommy Hilfiger", "a pitcher"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6537756996239484}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.11764705882352941, 0.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 1.0, 0.3636363636363636, 0.06666666666666667, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.375, 1.0, 0.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.7058823529411764, 0.8421052631578948, 0.25, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.1818181818181818, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9459", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-10565", "mrqa_triviaqa-validation-7430", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-597", "mrqa_searchqa-validation-808", "mrqa_triviaqa-validation-2358"], "SR": 0.515625, "CSR": 0.5365349264705883, "EFR": 0.967741935483871, "Overall": 0.7159334973908918}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Danny Elfman", "Olivia Olson", "21 June 2007", "Paul Rudd", "Kaitlyn Maher", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Kalinga Ashoka ( son of Bindusara )", "Omar Khayyam", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "personnel directors", "Davos", "Neil Patrick Harris", "1900", "Elizabeth Weber", "the stems and roots of certain vascular plants", "either late 2018 or early 2019", "R.E.M.", "Jewish audiences", "as a lustrous, purple - black metallic solid at standard conditions that sublimes readily to form a violet gas", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "Polk County, Florida", "Iran", "2001", "the inventor Bi Sheng", "Elected Emperor of the Romans", "1799", "Kid Creole & The Coconuts", "to refer to a god of the Ammonites, as well as Tyrian Melqart", "December 2, 2013, and the third season concluded on October 1, 2017", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Miracle", "Dumfries and Galloway", "High Knob", "President Nicolas Sarkozy and Canada's Prime Minister Stephen Harper", "NATO fighters", "age 19,", "a lighthouse", "lullaby", "E. E. Cummings", "Minerals Management Service Director Elizabeth Birnbaum"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6805196398401341}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.22222222222222224, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.8837209302325582, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3141", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-13013", "mrqa_newsqa-validation-2665"], "SR": 0.53125, "CSR": 0.5364583333333333, "EFR": 0.9666666666666667, "Overall": 0.7157031249999999}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Old Trafford, Greater Manchester, England", "The Intolerable Acts", "in skeletal muscle and the brain to recycle adenosine triphosphate, the energy currency of the cell", "the libretto", "prophets and beloved religious leaders", "1947, 1956, 1975, 2015 and 2017", "the St. Louis Cardinals", "Andy Serkis", "Panning", "The fourth season premiered on September 21, 2017, on Fox, while the second half premiered on March 1, 2018", "Bob Dylan", "Virginia Beach is an independent city located on the southeastern coast of the Commonwealth of Virginia in the United States", "the sidewalk between Division Street and East Broadway", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "Eastern Redbud", "eleven", "10.5 %", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four", "Benjamin Franklin", "a routing table, or routing information base ( RIB )", "James Rodr\u00edguez", "in Ephesus in AD 95 -- 110", "Johnson", "more than 2,500 locations in all states except Alaska, Hawaii, Connecticut, Maine, New Hampshire, and Vermont", "from the top of the leg to the foot on the posterior aspect", "Mary Elizabeth ( Margaret Hoard )", "Ashoka", "dermis", "Hodel", "October 27, 2017", "Wolfgang Hochstetter", "a popular medieval given throughout Europe, coming from the biblical name, Thomas being one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Agamemnon", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the Supreme Court of Canada", "September 29, 2017", "around 10 : 30am", "Angola", "Norway", "Manley", "the band released their fourth live album All My Friends We're Glorious : Death of a Bachelor Live", "Wyatt", "Wednesday", "E pluribus unum", "2006", "Steve Martin", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "to back one side or the other.", "At least 40", "Juan Martin Del Potro.", "the Caspian Sea", "Sweden", "photoelectric", "namibia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6475140821188615}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.5714285714285715, 1.0, 0.47058823529411764, 1.0, 0.7499999999999999, 0.2857142857142857, 0.0, 1.0, 1.0, 0.3, 1.0, 0.7692307692307693, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-1244", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-327", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7612", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_triviaqa-validation-5834"], "SR": 0.546875, "CSR": 0.5366071428571428, "retrieved_ids": ["mrqa_squad-train-49014", "mrqa_squad-train-692", "mrqa_squad-train-7776", "mrqa_squad-train-37111", "mrqa_squad-train-51103", "mrqa_squad-train-30697", "mrqa_squad-train-31094", "mrqa_squad-train-48724", "mrqa_squad-train-19530", "mrqa_squad-train-26634", "mrqa_squad-train-83785", "mrqa_squad-train-66762", "mrqa_squad-train-76402", "mrqa_squad-train-22068", "mrqa_squad-train-79887", "mrqa_squad-train-79627", "mrqa_squad-train-25411", "mrqa_squad-train-46347", "mrqa_squad-train-11835", "mrqa_squad-train-42715", "mrqa_squad-train-82913", "mrqa_squad-train-36698", "mrqa_squad-train-33368", "mrqa_squad-train-6360", "mrqa_squad-train-37022", "mrqa_squad-train-38421", "mrqa_squad-train-13043", "mrqa_squad-train-48629", "mrqa_squad-train-76508", "mrqa_squad-train-9927", "mrqa_squad-train-1358", "mrqa_squad-train-58289", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-1758", "mrqa_naturalquestions-validation-6931", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-647", "mrqa_triviaqa-validation-5724", "mrqa_squad-validation-973", "mrqa_searchqa-validation-33", "mrqa_newsqa-validation-4074", "mrqa_searchqa-validation-6737", "mrqa_newsqa-validation-346", "mrqa_naturalquestions-validation-754", "mrqa_newsqa-validation-2227", "mrqa_naturalquestions-validation-4247", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-3863", "mrqa_naturalquestions-validation-2037", "mrqa_newsqa-validation-3089", "mrqa_squad-validation-3699", "mrqa_newsqa-validation-3517", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-7003", "mrqa_squad-validation-7527", "mrqa_searchqa-validation-10273", "mrqa_triviaqa-validation-1622", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1057", "mrqa_squad-validation-1766", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-5004"], "EFR": 0.9655172413793104, "Overall": 0.7155030018472905}, {"timecode": 70, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.8828125, "KG": 0.49765625, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore", "from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Golgi apparatus", "a 2001 Indian epic sports - drama film, directed by Ashutosh Gowariker, produced by Aamir Khan and Mansoor Khan", "Super Bowl XXXIX", "poor hygiene", "September 2017", "Kanawha River", "12.65 m ( 41.5 ft )", "1820s", "the customer's account", "his cousin D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "as a prison from 1100 ( Ranulf Flambard ) until 1952 ( Kray twins )", "the Supreme Court of Canada", "July 1, 1923", "Firoz Shah Tughlaq", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina ; 1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia", "irsten Simone Vangsness", "Frankie Laine's `` I Believe", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "2002 Tamil film Ramanaa", "RAF Coningsby in Lincolnshire", "the President", "De pictura", "more than 2,500 locations", "1919", "September 19, 1977", "17 - year - old Augustus Waters, an ex-basketball player and amputee", "Sebastian Vettel", "Tiger Woods", "2018", "Speaker of the House of Representatives, President pro tempore of the Senate", "the final scene of the fourth season", "Lord's", "a mid-size four - wheel drive luxury Volvo", "Ingrid Bergman", "Malayalam", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "issued upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "eye", "the Vietnam war", "Rutger Hauer", "Canada", "Robert Jenrick", "Srinagar", "Jewish", "the Dalai Lama", "San Simeon, California,", "Crawford", "the Blue Ridge Mountains", "wido", "electric currents and magnetic fields"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6789471563816717}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.9180327868852458, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.8363636363636363, 0.6666666666666666, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_hotpotqa-validation-2134", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.546875, "CSR": 0.5367517605633803, "EFR": 0.7241379310344828, "Overall": 0.6763185633195726}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "Pepsi Super Bowl LII Halftime Show", "the following day", "Labour", "Judi Dench", "a mutual kill", "three", "Spanish moss", "Matt Monro", "1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "drivers who meet more exclusive criteria", "Charles Carroll", "1959", "many forested parts", "Hermia", "in and around an unnamed village", "MercyMe", "Lagaan ( English : Taxation ; also called Lagaon : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "Parashara", "the 15th century", "Hasmukh Adhia", "39 % of the country's electricity production at utility - scale facilities in 2014, 33 % in 2015, and 30.4 % in 2016", "Lorazepam", "April 1, 2016", "its absolute temperature", "electron donors", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "cake", "1890", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Cori Gonzalez - Macuer", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "main pulmonary artery", "Steve Russell", "2016 WWE draft", "1799", "Celtic", "Zachary Taylor", "Oscar Wilde", "the Galaxy S7", "The New Yorker", "Citgo", "school in South Africa", "\"[The e-mails]", "Rolling Stone", "a lump", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6705828847687401}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.6, 0.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.19999999999999998, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-7963", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-10641"], "SR": 0.5625, "CSR": 0.537109375, "EFR": 0.9285714285714286, "Overall": 0.7172767857142858}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Jason Clarke", "Toby Keith", "General George Washington", "Charles Lebrun", "Ed", "15 February 1998", "Diego Tinoco", "Bart Millard", "1978", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "First Lieutenant Israel Greene", "the nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "the Kansas City Chiefs", "Yuzuru Hanyu", "Owen Hunt", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "Representatives", "Lisa Stelly", "Holly", "Jetfire", "Rachel Kelly Tucker", "1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "HOK Sport ( now Populous )", "sixth season", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "Scott Schwartz", "Japan", "Djokovic", "won gold in the half - pipe", "Judy Collins", "2002", "Georgia Groome as Georgia Nicolson", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "Rack of lamb", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "Los Angeles Dance Theater", "100 meter", "President Sheikh Sharif Sheikh Ahmed", "Miami Beach, Florida,", "the Yamazaki distillery", "Victoria", "a yoke", "video game"], "metric_results": {"EM": 0.65625, "QA-F1": 0.709375}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-8390", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.65625, "CSR": 0.5387414383561644, "retrieved_ids": ["mrqa_squad-train-83890", "mrqa_squad-train-20123", "mrqa_squad-train-5327", "mrqa_squad-train-2272", "mrqa_squad-train-10050", "mrqa_squad-train-4987", "mrqa_squad-train-27091", "mrqa_squad-train-36931", "mrqa_squad-train-49796", "mrqa_squad-train-34915", "mrqa_squad-train-32551", "mrqa_squad-train-26018", "mrqa_squad-train-62069", "mrqa_squad-train-28720", "mrqa_squad-train-20552", "mrqa_squad-train-14837", "mrqa_squad-train-12989", "mrqa_squad-train-32245", "mrqa_squad-train-13478", "mrqa_squad-train-48834", "mrqa_squad-train-63750", "mrqa_squad-train-84597", "mrqa_squad-train-75742", "mrqa_squad-train-67052", "mrqa_squad-train-51269", "mrqa_squad-train-3601", "mrqa_squad-train-82637", "mrqa_squad-train-76097", "mrqa_squad-train-31676", "mrqa_squad-train-4153", "mrqa_squad-train-73319", "mrqa_squad-train-75225", "mrqa_searchqa-validation-16710", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-367", "mrqa_triviaqa-validation-3010", "mrqa_naturalquestions-validation-2414", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-2520", "mrqa_naturalquestions-validation-3592", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-3314", "mrqa_naturalquestions-validation-9753", "mrqa_squad-validation-1384", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-11809", "mrqa_newsqa-validation-2327", "mrqa_naturalquestions-validation-9782", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-2018", "mrqa_searchqa-validation-9116", "mrqa_hotpotqa-validation-1297", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-887", "mrqa_squad-validation-1092", "mrqa_newsqa-validation-3438", "mrqa_naturalquestions-validation-6810", "mrqa_squad-validation-4015", "mrqa_hotpotqa-validation-2951", "mrqa_newsqa-validation-1635", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-1384"], "EFR": 0.9090909090909091, "Overall": 0.7137070944894146}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "the efferent nerves that directly innervate muscles", "Johannes Gutenberg", "Shawn Wayans", "The United States of America ( USA ), commonly known as the United States ( U.S. ) or America ( / \u0259\u02c8m\u025br\u026ak\u0259 / )", "regulatory site", "3", "near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "in 1942 and 1946", "March 31 to April 8, 2018", "military units from their parent countries of Great Britain and France", "radius R of the turntable", "the Royal Air Force ( RAF )", "1945", "CeCe Drake", "April 14, 2017", "post translational modification", "1960", "the Naturalization Act of 1790", "September 6, 2019", "Bulgaria", "Kathleen Turner", "Coldplay with special guest performers Beyonc\u00e9", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "Werner Ruchti", "Brooklyn, New York", "Chris Rea", "Langdon", "pneumonoultramicroscopicsilicovolcanoconiosis", "2014", "British regulars", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350 at the 2010 census", "Uruguay", "to ordain presbyters / bishops and to exercise general oversight", "William Shakespeare's As You Like It", "2002", "Anna Faris", "Cress", "Montr\u00e9al", "Prince Edward, Duke of Kent", "Donald Trump", "the Bank of China Tower", "Mumbai, Maharashtra", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "the Queen of England", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6928709464555052}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.9411764705882353, 0.7692307692307692, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-10704", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-8023", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.5625, "CSR": 0.5390625, "EFR": 0.8928571428571429, "Overall": 0.7105245535714285}, {"timecode": 74, "before_eval_results": {"predictions": ["the Lgion d'honneur", "Shaft", "a retronym", "Arsinoe", "pharaoh", "Tony Dungy", "the Rolling Stones", "cadenza", "cayenne", "Platelet", "suffrage", "60", "Enigma", "a twister", "afternoon", "\"Lancelot and Elaine\"", "Laryngitis", "Ben", "terraces", "a zombi", "aquiline", "Hair", "a cozy", "Jalisco", "Davenport", "Sammy Sosa", "Suzuki", "ten", "Othello", "Mount Olympus", "Hematomas", "a horseman", "a Coral snake", "General William Tecumseh Sherman", "Fess Parker", "a duvet", "Baltimore", "cayfish", "Japan", "Libert, galit", "the African Union", "William Wrigley Jr.", "Nepal", "the United States Department of Agriculture", "cat scratch fever", "freezing", "(Diane) Arbus", "a kangaro court", "Whatchamacallit", "Little Richard", "The Plane! Da Plane", "avian origin", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "benjamin franklin", "Sororicide", "saint aidan", "Sulla", "Switzerland", "Parlophone Records", "keyboardist and", "150", "mental health", "the contestant"], "metric_results": {"EM": 0.5, "QA-F1": 0.5616851478494623}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-16892", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-13285", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-6289", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_naturalquestions-validation-3495", "mrqa_triviaqa-validation-1931", "mrqa_naturalquestions-validation-5636"], "SR": 0.5, "CSR": 0.5385416666666667, "EFR": 1.0, "Overall": 0.7318489583333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Eminem", "(Johnny) Griffin", "Louisiana", "a bell", "Tombs of Kobol", "The Sound and the Fury", "a sandwich", "six", "Cosmo Kramer", "Poetic Justice", "(VICT) HUGO", "the Colossus", "(Hugh) Jackman", "silver", "(Al) Lahoud", "an eagle", "the People's", "(Larry) King", "(H) Claudius", "lowering taxes", "Margot Fonteyn", "Alfred Nobel", "lifejackets", "small", "(G) Mills", "Emmitt Smith", "figurine", "a black hole", "Kampala", "Department on Agriculture", "Heisenberg", "Sin City", "(David) Hyde Pierce", "the period of social dynamics", "the Old North Church", "bones", "Red Bull", "a pirate flag", "the North West Territories", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport", "the Red River", "a shrub", "Ellen Wilson", "Esau", "skull", "Agatha Christie", "( Ronald) Reagan", "Ford Motor Company", "1947", "American actress Moira Kelly", "Zuzu", "Mt kilimanjaro", "Christian Wulff", "Zelle", "Princess Aisha bint Hussein", "French", "King James II", "Kaka", "133 people", "von Hagens", "Minnesota"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5185496794871794}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.8, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-14417", "mrqa_searchqa-validation-9504", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-8847", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-1497", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-2017", "mrqa_hotpotqa-validation-3364"], "SR": 0.421875, "CSR": 0.5370065789473684, "retrieved_ids": ["mrqa_squad-train-8605", "mrqa_squad-train-39530", "mrqa_squad-train-67612", "mrqa_squad-train-26402", "mrqa_squad-train-60426", "mrqa_squad-train-24272", "mrqa_squad-train-69213", "mrqa_squad-train-22833", "mrqa_squad-train-60637", "mrqa_squad-train-69967", "mrqa_squad-train-13177", "mrqa_squad-train-16828", "mrqa_squad-train-79875", "mrqa_squad-train-35837", "mrqa_squad-train-18044", "mrqa_squad-train-66034", "mrqa_squad-train-45242", "mrqa_squad-train-9824", "mrqa_squad-train-80344", "mrqa_squad-train-66363", "mrqa_squad-train-84932", "mrqa_squad-train-6899", "mrqa_squad-train-49532", "mrqa_squad-train-72434", "mrqa_squad-train-68801", "mrqa_squad-train-32497", "mrqa_squad-train-52859", "mrqa_squad-train-70325", "mrqa_squad-train-24761", "mrqa_squad-train-77623", "mrqa_squad-train-65658", "mrqa_squad-train-58498", "mrqa_newsqa-validation-3177", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-2552", "mrqa_searchqa-validation-11809", "mrqa_newsqa-validation-3029", "mrqa_naturalquestions-validation-1415", "mrqa_newsqa-validation-239", "mrqa_searchqa-validation-6722", "mrqa_naturalquestions-validation-3491", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3953", "mrqa_newsqa-validation-3791", "mrqa_naturalquestions-validation-5739", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-2742", "mrqa_naturalquestions-validation-2092", "mrqa_newsqa-validation-2207", "mrqa_naturalquestions-validation-8023", "mrqa_newsqa-validation-4078", "mrqa_naturalquestions-validation-554", "mrqa_newsqa-validation-3060", "mrqa_naturalquestions-validation-4124", "mrqa_newsqa-validation-3911", "mrqa_squad-validation-2008", "mrqa_newsqa-validation-2418", "mrqa_naturalquestions-validation-9009", "mrqa_searchqa-validation-16848", "mrqa_newsqa-validation-2927", "mrqa_squad-validation-7719", "mrqa_naturalquestions-validation-10040", "mrqa_newsqa-validation-467"], "EFR": 1.0, "Overall": 0.7315419407894737}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton Marsalis", "the Department of the Treasury", "Montserrat", "a cyclone", "Starland Vocal Band", "gallows", "resistance", "uffin Modern Classics", "earthquakes", "the Potomac", "Oregon", "Mary", "Hulk Hogan", "vapor", "Russia", "Adam Sandler", "Ted Koppel", "Yes I Am", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "a sack dress", "Bobby McFerrin", "Fore River Shipyard", "Capitol Hill", "a glider", "a heart", "Guyana", "jelly", "camels", "drought", "Phrases", "Jonathan Winters", "Pink", "Rhode Island", "Isaac Newton", "the African continent", "Mormon", "Theodore Roosevelt", "gold", "Joshua", "Virginia", "Lignite", "Seymour Cray", "Private Practice", "steroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "a discoverer of Neptune", "Scotland", "yellow", "chalk quarry", "SBS", "Eternal Flame", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "benzodiazepines"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7651041666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-5921", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-7238", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-8386", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-13804", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7709", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-512"], "SR": 0.71875, "CSR": 0.5393668831168832, "EFR": 0.9444444444444444, "Overall": 0.7209028905122655}, {"timecode": 77, "before_eval_results": {"predictions": ["Charles Darwin", "Eskimo", "Rome", "the Kid", "Rudyard Kipling", "Frasier Crane", "Tarzan of the Apes", "Edmund Tudor", "Leon Trotsky", "Belgium", "Sister Wendy Beckett", "1066", "ibuprofen", "thefilibuster", "Dr. George Washington Carver", "the Bulldog Drummond", "Spooky Salem, MA", "the Indus Valley", "the Baltic Sea", "\"Nolo contendere\"", "gum", "Cain", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento, California", "the Andes mountain range", "jury dutyserve", "Sigmund Freud", "Pantaloons", "Muhammad", "Paul Newman", "(Larry) Truman", "Champagne", "Rhode Island", "The Simple Life", "Laos", "Agent Orange", "the Philippines", "Kellogg's", "The Backstreet Boys", "Luxor", "Latin", "Venus", "the Hawthorne", "the Congo River", "Charles VII", "Horatio Nelson,", "a caiman", "Ferrari", "the iris", "John Adams", "1886", "Ali", "tahrir Square", "war I", "henonismbot", "ESPN College Football Friday Primetime", "R&B vocal group", "Memphis Minnie", "protective shoes", "Madonna", "Transportation Security Administration", "silver"], "metric_results": {"EM": 0.421875, "QA-F1": 0.636235119047619}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.8571428571428571, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4029", "mrqa_searchqa-validation-7100", "mrqa_searchqa-validation-13301", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-616"], "SR": 0.421875, "CSR": 0.5378605769230769, "EFR": 1.0, "Overall": 0.7317127403846153}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Christmas Eve", "The Firm", "Messerschmitt Me262", "circumnavigate", "Marilyn Monroe", "Cheddar", "a comet", "wings", "the Enigma", "a surface-to-air missile", "the igloo", "Phobos", "a dermatologist", "Kramer vs. Kramer", "The Tempest", "yellow", "Annie's", "tire", "Schwarzenegger", "Lafayette", "Iris Murdoch", "the Ironman", "Swahili", "the NHL", "lace", "a chanfrein", "wives and concubines", "The Tales of the Arabian Nights", "Scott McClellan", "Jeremiah", "Thomas Edison", "A Chorus Line", "Guadalajara", "Sydney", "flavor", "Dutchman", "\"The Janeites\"", "the Alamo", "oats", "Zlatan", "tuition bills", "Rush", "being buried alive", "Swan", "the Kansas Jayhawk", "Helsinki", "the kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan Mata", "Madeleine L'Engle", "British troops", "three", "$3 billion,", "Tom Ewell"], "metric_results": {"EM": 0.625, "QA-F1": 0.6967013888888889}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.22222222222222224, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1026", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-1167", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.625, "CSR": 0.5389636075949367, "retrieved_ids": ["mrqa_squad-train-76991", "mrqa_squad-train-27405", "mrqa_squad-train-37090", "mrqa_squad-train-591", "mrqa_squad-train-22758", "mrqa_squad-train-9884", "mrqa_squad-train-47760", "mrqa_squad-train-38312", "mrqa_squad-train-41183", "mrqa_squad-train-20056", "mrqa_squad-train-2378", "mrqa_squad-train-83120", "mrqa_squad-train-58048", "mrqa_squad-train-84742", "mrqa_squad-train-55485", "mrqa_squad-train-25903", "mrqa_squad-train-33274", "mrqa_squad-train-20175", "mrqa_squad-train-35801", "mrqa_squad-train-1915", "mrqa_squad-train-39263", "mrqa_squad-train-25521", "mrqa_squad-train-30197", "mrqa_squad-train-381", "mrqa_squad-train-75376", "mrqa_squad-train-47575", "mrqa_squad-train-3098", "mrqa_squad-train-72043", "mrqa_squad-train-75525", "mrqa_squad-train-67289", "mrqa_squad-train-54647", "mrqa_squad-train-10138", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-980", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-4973", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4850", "mrqa_triviaqa-validation-2314", "mrqa_newsqa-validation-2850", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-5185", "mrqa_searchqa-validation-2313", "mrqa_naturalquestions-validation-8610", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-421", "mrqa_squad-validation-9298", "mrqa_triviaqa-validation-2333", "mrqa_newsqa-validation-2519", "mrqa_hotpotqa-validation-1076", "mrqa_newsqa-validation-1548", "mrqa_naturalquestions-validation-7020", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-3089", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-8976", "mrqa_squad-validation-8921", "mrqa_searchqa-validation-15505", "mrqa_newsqa-validation-2525", "mrqa_naturalquestions-validation-2092", "mrqa_hotpotqa-validation-3169"], "EFR": 0.9583333333333334, "Overall": 0.7236000131856539}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte", "sport", "Peter", "litter", "New Zealand", "fontanels", "California", "Nero", "the Dalmatians", "Cecil Day-Lewis", "cotton", "erotic thriller", "South Africa", "blackjack", "along the Mediterranean", "Catherine de' Medici", "pancake", "the adder", "puzzle", "the River Thames", "\"PIE\"", "Pitcairn", "Adam Sandler", "Mayo", "\" Shut up, just shut up\"", "arrested development", "the Renaissance", "languages which have undergone language death", "Rodeo", "repent", "Denzel Washington", "Vichy", "nougat", "(Kim) Erdman", "ani", "Louis Comfort Tiffany", "Louise", "conk", "Clinton", "globalization", "Van Halen", "the Eifel", "salt", "Samsonite", "chili", "salaam", "Faraday", "necklaces", "Norse", "Niagara Falls", "the Bronx", "the National Football League ( NFL ) for the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the Baltimore Baltimore Steelers", "Ethel Merman", "Forbes Burnham", "danish", "Angus Deayton", "boston", "Russian Ark", "\"The Walking Dead\"", "237", "over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5802032019704433}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.27586206896551724, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-9831", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-8019", "mrqa_searchqa-validation-2004", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-2428", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.484375, "CSR": 0.53828125, "EFR": 0.9696969696969697, "Overall": 0.7257362689393939}, {"timecode": 80, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.86328125, "KG": 0.47578125, "before_eval_results": {"predictions": ["(George Washington) Washington", "the National Hockey League (NHL)", "blue", "Georgia", "Major General William Devereaux", "ozone", "the English Channel", "Shakespeare", "French", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "Dot- Dash", "Bartholomew", "tyrosine", "Target", "Regrets", "a possum", "(Day of the Dead)", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a staycation", "it is best not to take risks even when it seems boring or difficult", "Makkedah", "Yogi Bear", "northern Idaho", "Georgia O'Keeffe", "a highway", "1215", "Frederick Douglass", "skyscraper", "Billy the Kid", "The Killing Fields", "Oliver Twist", "a landmark", "lamb", "bread", "Boston", "Martinique", "Dr. Strangelove", "the Grand Canal", "the Sons of Liberty", "a telescope", "Messianic", "the trumpet", "a deep pass", "a dnahedron", "Nicole Gale Anderson", "`` Goodbye Toby ''", "in London's West End in 1986, and on Broadway in 1988", "spain", "16", "dragonflies", "acidic", "Roc Me Out", "\"Twice in a Lifetime\"", "10:30 p.m. October 3,", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "2006,", "he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted."], "metric_results": {"EM": 0.640625, "QA-F1": 0.7005208333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-2637", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-9111", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-3488", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-1408", "mrqa_searchqa-validation-224", "mrqa_naturalquestions-validation-1038", "mrqa_triviaqa-validation-1748", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.640625, "CSR": 0.5395447530864197, "EFR": 1.0, "Overall": 0.7280652006172839}, {"timecode": 81, "before_eval_results": {"predictions": ["taxonomy", "Warsaw", "Katrina & the Waves", "the French & Indian War", "Brady", "philosophy", "the American Red Cross", "harm", "Bonnie Raitt", "As Good As It Gets", "pickles", "Artemis", "neurons", "Evian", "a goose", "the Mayor of Casterbridge", "nervus olfactorius", "a window", "Newton", "SpeedMatch Review Game", "Andrew Jackson", "Colorado River", "Dune", "a duel", "YouTube", "heresy", "The Office", "Charlie Watts", "widow", "a button", "Virginia", "abundant", "Albert Schweitzer", "the brain", "dive bomber", "Toulouse-Lautrec", "Helen Hayes", "Dada", "a charles", "H.G. Wells", "\"Sex In Crazy Places\"", "Bill & Melinda Gates", "the hippo", "Friedrich Nietzsche", "a dog eat dog world", "Alexander Hamilton", "New York", "Niagara Falls", "a rudder", "carrots", "The Flintstones", "Abanindranath Tagore CIE", "at slightly different times when viewed from different points on Earth", "thoracic", "Carrefour", "Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano,", "national telephone", "the Catholic League.", "Ennio Morricone"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6632981601731602}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.060606060606060615, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-4889", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-16557", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-7802", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3173", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846"], "SR": 0.578125, "CSR": 0.540015243902439, "retrieved_ids": ["mrqa_squad-train-80753", "mrqa_squad-train-499", "mrqa_squad-train-40882", "mrqa_squad-train-3244", "mrqa_squad-train-52094", "mrqa_squad-train-9992", "mrqa_squad-train-30651", "mrqa_squad-train-57609", "mrqa_squad-train-1816", "mrqa_squad-train-67444", "mrqa_squad-train-30238", "mrqa_squad-train-8026", "mrqa_squad-train-48539", "mrqa_squad-train-48733", "mrqa_squad-train-38816", "mrqa_squad-train-3956", "mrqa_squad-train-33196", "mrqa_squad-train-22098", "mrqa_squad-train-24824", "mrqa_squad-train-34070", "mrqa_squad-train-9036", "mrqa_squad-train-14887", "mrqa_squad-train-39305", "mrqa_squad-train-12590", "mrqa_squad-train-56202", "mrqa_squad-train-15247", "mrqa_squad-train-60459", "mrqa_squad-train-79446", "mrqa_squad-train-4568", "mrqa_squad-train-46158", "mrqa_squad-train-43186", "mrqa_squad-train-47223", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-3232", "mrqa_newsqa-validation-1077", "mrqa_searchqa-validation-13142", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-1519", "mrqa_searchqa-validation-14398", "mrqa_newsqa-validation-3053", "mrqa_squad-validation-7547", "mrqa_naturalquestions-validation-5007", "mrqa_newsqa-validation-2587", "mrqa_searchqa-validation-11809", "mrqa_newsqa-validation-2942", "mrqa_searchqa-validation-9179", "mrqa_naturalquestions-validation-7020", "mrqa_newsqa-validation-3016", "mrqa_searchqa-validation-8756", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-2605", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-3614", "mrqa_triviaqa-validation-3423", "mrqa_squad-validation-8927", "mrqa_naturalquestions-validation-9715", "mrqa_searchqa-validation-4320", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-3351", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-1946"], "EFR": 0.9629629629629629, "Overall": 0.7207518913730804}, {"timecode": 82, "before_eval_results": {"predictions": ["Julius Caesar", "The Big Easy", "the beaver", "Dorothy", "Survivor: Fiji", "Wild Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Eternity", "Andrew Marvell", "Quiz Show", "the \"BCS Championship Game\"", "acetone", "Heart of Darkness", "Psycho", "Napoleon", "lullaby", "a capuchins", "Napoleon", "the West", "reticulated", "Munich", "digestif", "jeopardy", "Pope Benedict XVI", "Los Alamos Scientific Laboratory", "Somerset Maugham", "sapphire", "Three Coins in the Fountain", "ER", "Goldenrod", "Luke", "the rectum", "WATERS OF THE BODY", "the frequency", "Grease", "a salamander", "Alexander Solzhenitsyn", "eyebrows", "the Romance of the Rose", "Guyana", "Charlie Bartlett", "Richmond Thackeray", "the Big Sky Conference", "the beavers", "Boston", "Michelle Pfeiffer", "peanut notes of the Month Club", "Sweden", "Ajay Tyagi", "17th episode in the third season", "94 by 50 feet", "Salix", "F", "paul mccartney", "Karl- Anthony Towns", "Rarebell", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "$10 billion", "Diana,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6888888888888889}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1561", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.609375, "CSR": 0.5408509036144578, "EFR": 0.96, "Overall": 0.7203264307228915}, {"timecode": 83, "before_eval_results": {"predictions": ["the East Sea", "Stitch", "Joe Torre", "kettledrum", "P.G. Wodehouse", "Santa Fe", "Rastafarianism", "cinnamon", "I Am the Very Model of a Modern Major-General", "extreme", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Asklepius", "a troll", "The Flying Dutchman", "Dan Quayle", "Naomi", "Answer Who is", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "the Muse", "distressing", "Mercury", "the Mad Hatter", "Oceania", "Nepal", "Thomas Jefferson", "the names of God", "American Graffiti", "Hair", "cicadas", "Asbury Park", "the shaft", "the saguaro", "Zappa", "hip-hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "seven of One", "Thermopylae", "Magdalene Laundries", "\"$10,000 Kelly,\"", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons, known as TAK,", "1937"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7388020833333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1568", "mrqa_searchqa-validation-1991", "mrqa_searchqa-validation-401", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_triviaqa-validation-490", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419"], "SR": 0.671875, "CSR": 0.5424107142857143, "EFR": 1.0, "Overall": 0.7286383928571428}, {"timecode": 84, "before_eval_results": {"predictions": ["typing speed", "a crescent", "a trident", "Abercrombie & Fitch", "Jefferson", "Standard Oil", "Crustacea", "Laura Ingalls Wilder", "a carriage", "Monet", "carbon-based (organic) chemicals", "Ford", "Louis Rukeyser", "Jupiter", "Clinton", "Truisms", "tin", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia Bulldogs", "Giacomo Puccini", "the hematopoietic", "Heroes", "cramps", "Kublai Khan", "Lafitte", "Kingston", "a relic", "cyclosporine", "the Northern Mockingbird", "restrictive", "Comedy", "a owl", "imeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "a narwhal", "Stephen Hawking", "seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a star system", "1924", "741 weeks", "January 17, 1899", "general Douglas MacArthur", "Project Gutenberg", "new Guinea", "Latin American culture", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001,", "650", "$1.5 million."], "metric_results": {"EM": 0.671875, "QA-F1": 0.7239583333333334}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_searchqa-validation-6009", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-6220", "mrqa_hotpotqa-validation-3921"], "SR": 0.671875, "CSR": 0.5439338235294118, "retrieved_ids": ["mrqa_squad-train-30218", "mrqa_squad-train-48002", "mrqa_squad-train-86245", "mrqa_squad-train-57519", "mrqa_squad-train-36124", "mrqa_squad-train-8772", "mrqa_squad-train-63540", "mrqa_squad-train-7714", "mrqa_squad-train-38758", "mrqa_squad-train-32064", "mrqa_squad-train-43498", "mrqa_squad-train-278", "mrqa_squad-train-12086", "mrqa_squad-train-53529", "mrqa_squad-train-55462", "mrqa_squad-train-24884", "mrqa_squad-train-81568", "mrqa_squad-train-36203", "mrqa_squad-train-75394", "mrqa_squad-train-17131", "mrqa_squad-train-43399", "mrqa_squad-train-17702", "mrqa_squad-train-18249", "mrqa_squad-train-9564", "mrqa_squad-train-62282", "mrqa_squad-train-83610", "mrqa_squad-train-53363", "mrqa_squad-train-14107", "mrqa_squad-train-71789", "mrqa_squad-train-77741", "mrqa_squad-train-60537", "mrqa_squad-train-1915", "mrqa_newsqa-validation-6", "mrqa_squad-validation-4797", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-765", "mrqa_searchqa-validation-11530", "mrqa_newsqa-validation-3517", "mrqa_searchqa-validation-3591", "mrqa_newsqa-validation-3953", "mrqa_naturalquestions-validation-10410", "mrqa_searchqa-validation-10329", "mrqa_newsqa-validation-2936", "mrqa_naturalquestions-validation-8177", "mrqa_searchqa-validation-1415", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-8948", "mrqa_searchqa-validation-4447", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-2906", "mrqa_triviaqa-validation-2358", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-990", "mrqa_naturalquestions-validation-1426", "mrqa_searchqa-validation-4907", "mrqa_newsqa-validation-3453", "mrqa_searchqa-validation-15800", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-522", "mrqa_newsqa-validation-1856", "mrqa_naturalquestions-validation-9436", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-11020"], "EFR": 1.0, "Overall": 0.7289430147058823}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Madeleine Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Ellen Holly", "The Prince and the Pauper", "Pushing Daisies", "July", "the Reaper", "Pearl Jam", "Lent", "apples", "Solomon", "New Brunswick", "Lake County, Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "New York Luxury Real Estate", "Martin Luther", "rice", "Frasier", "Kansas City", "arteries", "\"Chinatown\"", "improv", "Hamlet", "lime", "Antichrist", "video icon", "Robert De Niro", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Brett Favre", "Chapter 6", "Fiddler on the Roof", "Pitcairn Island", "hockey", "etching", "Mars", "the skull", "the Philistine", "pay", "a cookie jar", "Babe Ruth", "Steak", "Conrad N. Hilton", "he was unable to wrest", "2016", "Jessica Simpson", "William Schuman", "tree", "indonesia", "Oklahoma", "138,535 people", "Jordan Belfort", "her son has strong values.", "\"Python Patrol\"", "Hurricane Gustav", "\"Everybody is kind of in shock and trying to figure out what happened,\""], "metric_results": {"EM": 0.609375, "QA-F1": 0.6802483974358975}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.7692307692307693, 0.0, 1.0, 0.1]}}, "before_error_ids": ["mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-6767", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-13590", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-533", "mrqa_triviaqa-validation-6864", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2753", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-4122"], "SR": 0.609375, "CSR": 0.5446947674418605, "EFR": 1.0, "Overall": 0.7290952034883721}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "Nicaragua", "Chastity", "Frank Sinatra", "Dmitri Mendeleev", "Kathleen Winsor", "poison gas", "luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "Paul Jones", "The Rolling Stones", "Jessica Leigh Shahan", "Samuel A. Alito", "kings", "Civic", "Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Yogi Berra", "courage", "a jigger", "calcium", "a constitution", "the eastern Mediterranean", "virtual reality", "bass", "The Last Remake", "hot air balloons", "Tarzan & Jane", "RBIs", "Berkowitz", "oblique", "Pecan-nuts-on-tree", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "Little Buddha", "the Bolsheviks", "April 17, 1982", "the Garden of Gethsemane", "France", "James Cameron", "one night / I Got Stung", "Japan", "wittlesey", "Kingdom of Dalmatia", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7407147233893557}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-110", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-6493", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4669"], "SR": 0.65625, "CSR": 0.5459770114942528, "EFR": 0.8181818181818182, "Overall": 0.6929880159352142}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "Don Juan", "a spinning mule", "onerous", "autographs", "Fargo", "the Dailies", "fiberboard", "the River Thames", "Napster", "a widowed mother", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "mental impairment", "lightest interchangeable lens full-frame camera", "Crossword", "the Golden Fleece", "the kingdom of God", "caution", "Macaulay Culkin", "the Tom Thumb the race", "John Edwards", "Hawaii", "John F. Kennedy", "Daniel Boone", "a quart", "Hemoglobin", "Nancy Sinatra", "Swimmer's Ear", "foxes", "tabby", "Amerigo Vespucci", "Wisconsin", "the Arabian Peninsula", "Canada", "bipolar", "a brownie", "the village clock", "Alexander Calder", "honey", "Ferris B Mueller", "Christopher Columbus", "\"other\"", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "MinneapolisSaint Paul", "axiom", "electors", "about 3.5 mya", "Tommy Shaw", "Mark Jackson", "kosher", "the sooty albatrosses", "Meta", "Agent Carter", "Parthian Empire", "\"Kill Your Darlings\"", "planning processes are urgently needed", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.625, "QA-F1": 0.6791666666666666}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-14457", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-2933", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10767", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725"], "SR": 0.625, "CSR": 0.546875, "retrieved_ids": ["mrqa_squad-train-11611", "mrqa_squad-train-24166", "mrqa_squad-train-7032", "mrqa_squad-train-21715", "mrqa_squad-train-41906", "mrqa_squad-train-19969", "mrqa_squad-train-35644", "mrqa_squad-train-33949", "mrqa_squad-train-25412", "mrqa_squad-train-14107", "mrqa_squad-train-73236", "mrqa_squad-train-27642", "mrqa_squad-train-45685", "mrqa_squad-train-53589", "mrqa_squad-train-5950", "mrqa_squad-train-78950", "mrqa_squad-train-84772", "mrqa_squad-train-55780", "mrqa_squad-train-33220", "mrqa_squad-train-69532", "mrqa_squad-train-73898", "mrqa_squad-train-54671", "mrqa_squad-train-33051", "mrqa_squad-train-8513", "mrqa_squad-train-10247", "mrqa_squad-train-6350", "mrqa_squad-train-39994", "mrqa_squad-train-6366", "mrqa_squad-train-80655", "mrqa_squad-train-61760", "mrqa_squad-train-52945", "mrqa_squad-train-41007", "mrqa_searchqa-validation-6992", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-2533", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1072", "mrqa_squad-validation-2976", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-3369", "mrqa_triviaqa-validation-5000", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1962", "mrqa_searchqa-validation-1276", "mrqa_newsqa-validation-2914", "mrqa_triviaqa-validation-3423", "mrqa_searchqa-validation-4093", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2473", "mrqa_searchqa-validation-4053", "mrqa_naturalquestions-validation-5589", "mrqa_triviaqa-validation-3555", "mrqa_searchqa-validation-1891", "mrqa_naturalquestions-validation-952", "mrqa_newsqa-validation-3970", "mrqa_searchqa-validation-3398", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-6916", "mrqa_newsqa-validation-3593", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-11372", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-160", "mrqa_triviaqa-validation-5309"], "EFR": 1.0, "Overall": 0.72953125}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "Booster", "Biggie", "John the Baptist's", "John Paul II", "Evita", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "James Strom Thurmond", "Windsor, Ontario", "Armageddon", "yellow", "The Money Line", "The Twister Game", "Spain", "Scrabble", "the Caspian Sea", "the United States", "Los Angeles Angels of Anaheim", "Cardiff", "the Blacklist", "time", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "steering the boat", "Transamerica", "China", "1976", "the Delacorte", "Henry Clay", "the wire loop", "Petsmart", "Charles Darwin", "Electric Avenue", "an outline", "Jerusalem", "Vanna White", "Toyota", "a hand bell", "Istanbul", "F. Scott Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "Elsa", "purification", "the following day", "early 1980s", "Taron Egerton", "a linesider", "Tudembroke", "undertones", "Groupe PSA", "Premier Division", "The Five", "stabbed Tate,", "Herman Cain,", "a grizzly bear", "kenya costner"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6544270833333332}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-14789", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_newsqa-validation-3714", "mrqa_triviaqa-validation-7327"], "SR": 0.5625, "CSR": 0.5470505617977528, "EFR": 0.9285714285714286, "Overall": 0.7152806480738363}, {"timecode": 89, "before_eval_results": {"predictions": ["the least weasel", "Nemo", "easel", "the body of a deceased person", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "Wales", "Denmark", "the saguaro", "Saigon", "Shinto", "\"reshit\"", "Venus", "an iris", "Chanel Iman", "Armistice", "toilet paper", "the Panama Canal", "Cesare Borgia", "pearl", "cognac", "Hangman", "\"Bleak House\"", "October", "Camptown", "Henrik Ibsen", "Linkin Park", "a doggy", "storm", "the lungs", "gravity", "Elizabeth Washington", "Robert Bruce", "Marlon Brando", "the 17th President", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-n-Harmony", "zebras", "Castroneves", "King Edward", "Hugh Grant", "Godot", "voyeurism", "Articles of Confederation", "Pavlov", "a hull", "Doll", "in great britain", "James Madison", "The Firm", "Harriet Tubman", "boston", "\" Finding Nemo\"", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\" and", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6895397167487685}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3448275862068966, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11505", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-12554", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-3566", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-2737", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.609375, "CSR": 0.5477430555555556, "EFR": 1.0, "Overall": 0.729704861111111}, {"timecode": 90, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.875, "KG": 0.496875, "before_eval_results": {"predictions": ["Wisconsin", "reddish-orange", "a stagecoach", "Henry Winkler", "faction", "Hasta la vista", "business", "pastry", "brown", "Tunisia", "a plexus", "a rattlesnake", "Catherine the Great", "Absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "\"For AJI That We Have and Are\"", "Catherine of Aragon", "flag", "(Alauddin) Khan", "Bangkok", "Spain", "archery", "oblique", "( Joe) Torre", "meatballs", "Kennedy Space Center", "Jul 19,", "Pilate", "the United States", "Marco Polo", "the adder", "rice", "Carson Palmer", "Alabama", "ayahuasca", "Queen Anne", "the banjo", "a double feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fi", "Tony Orlando and Dawn", "AD 95 -- 110", "pepsin", "Jorge Lorenzo", "1919", "Paris", "Point Place", "11", "National Aviation Hall of Fame", "Thursday", "78,000 parents of children ages 3 to 17.iReport.com:", "South Dakota State Penitentiary", "Anne boleyn"], "metric_results": {"EM": 0.75, "QA-F1": 0.7989583333333334}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3584", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-9207", "mrqa_searchqa-validation-9500", "mrqa_searchqa-validation-1555", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_newsqa-validation-1144"], "SR": 0.75, "CSR": 0.5499656593406593, "retrieved_ids": ["mrqa_squad-train-59524", "mrqa_squad-train-34713", "mrqa_squad-train-9123", "mrqa_squad-train-56695", "mrqa_squad-train-20065", "mrqa_squad-train-76442", "mrqa_squad-train-16855", "mrqa_squad-train-42252", "mrqa_squad-train-31844", "mrqa_squad-train-25435", "mrqa_squad-train-78255", "mrqa_squad-train-4672", "mrqa_squad-train-27055", "mrqa_squad-train-35531", "mrqa_squad-train-7915", "mrqa_squad-train-15794", "mrqa_squad-train-12387", "mrqa_squad-train-10915", "mrqa_squad-train-40407", "mrqa_squad-train-53236", "mrqa_squad-train-70523", "mrqa_squad-train-21803", "mrqa_squad-train-1639", "mrqa_squad-train-69008", "mrqa_squad-train-64187", "mrqa_squad-train-67463", "mrqa_squad-train-56760", "mrqa_squad-train-24269", "mrqa_squad-train-2939", "mrqa_squad-train-1687", "mrqa_squad-train-51595", "mrqa_squad-train-83850", "mrqa_newsqa-validation-1226", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4079", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-887", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-4097", "mrqa_newsqa-validation-3564", "mrqa_hotpotqa-validation-512", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-14886", "mrqa_naturalquestions-validation-1435", "mrqa_searchqa-validation-2447", "mrqa_newsqa-validation-3356", "mrqa_searchqa-validation-5339", "mrqa_naturalquestions-validation-8560", "mrqa_searchqa-validation-899", "mrqa_newsqa-validation-3527", "mrqa_triviaqa-validation-6649", "mrqa_newsqa-validation-190", "mrqa_squad-validation-3559", "mrqa_newsqa-validation-320", "mrqa_naturalquestions-validation-7336", "mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-1565", "mrqa_naturalquestions-validation-4552", "mrqa_newsqa-validation-1384", "mrqa_searchqa-validation-1701", "mrqa_squad-validation-2788", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-1656", "mrqa_hotpotqa-validation-1076"], "EFR": 1.0, "Overall": 0.7386650068681319}, {"timecode": 91, "before_eval_results": {"predictions": ["Man and Superman", "a chiles", "Oliver Twist", "the Vampire Slayer", "the Vistula", "Coriolanus", "Dallas-Fort Worth", "an aide-de-camp", "an oblique fracture", "Roman Polanski", "Court TV", "sharia", "Jake La Motta", "Mastering the Art of French Cooking", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Sunni Islam", "Madeleine Albright", "(Turpan) Pendi", "the Harlem Renaissance", "Calamity Jane", "John Lennon", "Ron Sandler", "a pitch", "daytime running lights", "Tarzan", "Once", "Warren G. Harding", "Daniel & Philip", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Man Friday", "Lord North", "Wrigley", "the euro", "the narwhal", "the wall", "John", "Wyatt Earp", "Punjabi", "Syracuse", "USDA", "heels", "Frottage", "complementary", "1999", "pretends to be Rico's father for two - thousand dollars", "2017", "oskar Schindler", "harrison ford", "Tallinn", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million in America,", "Charlotte Gainsbourg", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "Audrey Roberts"], "metric_results": {"EM": 0.578125, "QA-F1": 0.649905303030303}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.06060606060606061, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-1833", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.578125, "CSR": 0.5502717391304348, "EFR": 0.9629629629629629, "Overall": 0.7313188154186796}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler on the Roof", "Usama Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Edith Wharton", "Captains Courageous", "the handles", "Central Park", "the nave", "The Tyger", "Chinese", "( Howard) Hughes", "Pablo Escobar", "a conifer", "support Clinton", "an asteroid", "first base", "rubber", "Ichabod Crane", "T. rex", "\"Chinatown\"", "a butterfly", "Lolita", "the Rheingold", "tango", "(General) Wesley K. Clark", "a porterhouse", "a canton", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "The Museum of Modern Art", "canals", "the Galatians", "Lewis Carroll", "meters", "corn", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "Manet", "sons", "The Hairy Ape", "Jason Flemyng", "Eight", "British citizens", "robert h Humphries", "Lincoln", "France", "1968", "Vytautas \u0160apranauskas", "Humvee", "two toppled cranes on the remains of the pier", "Bright Automotive,", "Harry Nicolaides,", "1957"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7527777777777778}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333326, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-137", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236", "mrqa_newsqa-validation-2853"], "SR": 0.703125, "CSR": 0.5519153225806452, "EFR": 0.8421052631578947, "Overall": 0.707475992147708}, {"timecode": 93, "before_eval_results": {"predictions": ["All Quiet on the Western Front", "the Rhine & the Main", "Kingston", "Cheers", "Lake County, Indiana", "Walt Kelly", "a kidney", "Paris", "gangbusters", "China", "Maine", "Gertrude Stein", "Jake Barnes", "a bathroom", "The Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "Varney Air Lines", "Notre Dame", "Tiberius Nero", "Jupiter", "loverly", "rugby", "the Falklands", "1968", "Iceland", "Orwell", "a chessboard", "Heat Transfer", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "odorant", "the Mesozoic", "Eisenhower", "\"For What It's Worth\"", "the Fourteen Points", "Freddie Mercury", "Kyushu", "Harry Potter and the Order of the Phoenix", "Geronimo", "Wiley Post", "theMist Mountains", "a cantaloupe", "London", "Carl Sandburg", "a federal republic", "The Enchantress", "Eddie Murphy", "the medical profession", "kenis the Guinea Pig", "the Treaty of Waitangi", "Jessica Phyllis Lange", "Heinkel He 178", "Kenan Thompson", "269,000", "one", "8 p.m. local time Thursday", "digging ditches."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7569128787878788}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4, 0.0, 1.0, 0.0, 0.9090909090909091, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-15431", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-13140", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-10151", "mrqa_naturalquestions-validation-7166", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-590", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-591"], "SR": 0.6875, "CSR": 0.5533577127659575, "retrieved_ids": ["mrqa_squad-train-62653", "mrqa_squad-train-71459", "mrqa_squad-train-34232", "mrqa_squad-train-64458", "mrqa_squad-train-12846", "mrqa_squad-train-33372", "mrqa_squad-train-10623", "mrqa_squad-train-20318", "mrqa_squad-train-62914", "mrqa_squad-train-38646", "mrqa_squad-train-45496", "mrqa_squad-train-57782", "mrqa_squad-train-53081", "mrqa_squad-train-30287", "mrqa_squad-train-63766", "mrqa_squad-train-20584", "mrqa_squad-train-77893", "mrqa_squad-train-20798", "mrqa_squad-train-5233", "mrqa_squad-train-63777", "mrqa_squad-train-74542", "mrqa_squad-train-13469", "mrqa_squad-train-24254", "mrqa_squad-train-41477", "mrqa_squad-train-45734", "mrqa_squad-train-33458", "mrqa_squad-train-50512", "mrqa_squad-train-31667", "mrqa_squad-train-64227", "mrqa_squad-train-74335", "mrqa_squad-train-79627", "mrqa_squad-train-52475", "mrqa_searchqa-validation-12173", "mrqa_squad-validation-5589", "mrqa_searchqa-validation-4066", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-2348", "mrqa_searchqa-validation-13804", "mrqa_newsqa-validation-1977", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-11888", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-9876", "mrqa_naturalquestions-validation-327", "mrqa_searchqa-validation-2478", "mrqa_hotpotqa-validation-2896", "mrqa_triviaqa-validation-6013", "mrqa_naturalquestions-validation-1414", "mrqa_squad-validation-1891", "mrqa_newsqa-validation-3439", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-8763", "mrqa_triviaqa-validation-3284", "mrqa_naturalquestions-validation-8733", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-3229", "mrqa_searchqa-validation-86", "mrqa_squad-validation-6115", "mrqa_naturalquestions-validation-3789", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-13919"], "EFR": 0.95, "Overall": 0.7293434175531915}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Starfighter", "Muqtada al-Sadr", "outdoor", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Doolittle", "a riot", "Lon Chaney", "New York", "the \"Fargo\" clue", "Sicily", "the Boston Celtics", "wine", "Enron", "the fulcrum", "Central African Republic", "Rudolf Hess", "fight", "the hippopotamus", "an eye", "Rabbit, Run", "Ronald Reagan and former Vice President Walter Mondale", "Washington Irving", "Schulman Grove", "the Archaeology of Ancient Egypt", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "Jerry Mathers", "9 to 5", "Department of Housing and Urban Development", "extradite", "the head", "the Nutty Professor", "Michael Collins", "The Sopranos", "The Sound and the Fury", "the mother-aughters dyad", "Brazil", "obsessive-compulsive", "Michelle Pfeiffer", "oatmeal", "the arteries", "1773", "the joule", "Justice", "20 November 1989", "25 September 2007", "Andrew Moray and William Wallace", "February 8, 2015", "a window", "Crispin", "contribution", "PET", "SKUM", "12-hour-plus shifts", "Joan Rivers", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7171875}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.8333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-2638"], "SR": 0.609375, "CSR": 0.5539473684210526, "EFR": 0.96, "Overall": 0.7314613486842105}, {"timecode": 95, "before_eval_results": {"predictions": ["Viktor Fedorovych", "paul Solotaroff", "the Communist Party", "The Goonies", "Velvet Revolver", "the Haunted Mansion", "the Continental Congress", "Jimi Hendrix", "Mahlemuts", "a shank", "fish", "a parens", "Casablanca", "\"Get Behind Me\"", "the Detroit River", "(George) Sand", "Northern Exposure", "Kilimanjaro", "Nebuchadnezzar", "a flip", "the Komodo", "Jewish boy growing up in a poor neighborhood in Montreal", "The Simpsons", "The West Wing", "Tabasco", "ravens", "Mexico", "Pickren", "Pocahontas", "virus", "John Hersey", "Patricia Arquette", "Ernie Banks", "a grotto", "Prince Harry", "Elizabeth Barrett Browning", "Styx", "Whig", "Capone", "Maria Callas", "iodine", "Tournament of Kings", "Ptolemy XIII", "Tennyson", "National Geographic", "From Walt Disney", "Jerusalem", "the nativity scene", "the Edict of Nantes", "Odysseus", "Omega", "at the end of an interrogative sentence : `` How old are you? ''", "Dr. Lexie Grey ( Chyler Leigh )", "since 3, 1, and 4", "paul esterh\u00e1zy", "exponentiation", "wycestershire", "1754", "25 June 1971", "Lowe's", "snow,", "Fernando Gonzalez", "Chester Arthur Stiles,", "ants"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5775669642857142}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7142857142857143, 0.7499999999999999, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-14269", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-1498", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-7180", "mrqa_hotpotqa-validation-5354", "mrqa_triviaqa-validation-4855"], "SR": 0.515625, "CSR": 0.5535481770833333, "EFR": 0.9354838709677419, "Overall": 0.726478284610215}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "cartwheel", "assemble", "hot air balloons", "pathetic fallacy", "Nomar Garciparra", "John Glenn", "a heron", "Apollo 1", "The White Company", "New Balance", "Free Negro", "St Joan of Arc", "finale", "mollus", "Camille Claudel", "the East River", "caricaturist", "seven Years' War", "Meg & Jennifer", "Wizard of Oz", "madding", "tribes", "(Richard) Branson", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore in franto", "kumpadori", "The Stranger", "Wyoming", "Tigger", "Basel", "Frank Sinatra", "pickled", "an Islamic leadership position", "backstroke", "Makkah", "Sydney", "dermatology", "Solomon", "\"People burp me before\"", "Chirac", "6", "Polaris", "To Carrie and Irene Miner", "Guiana", "a pronoun", "Czechoslovakia", "the Corinthians", "dilithium", "the fifth studio album by English rock band the Beatles", "1997", "2010", "2014", "conchita wurst", "Proclamation of Neutrality", "Gurgaon", "Robert Gibson", "eighty-seventh", "a pregnancy", "\"The Screening Room\"", "$150 billion", "Rio Grande"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5604538690476191}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.75, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-6728", "mrqa_searchqa-validation-14458", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-15387", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-11872", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-6921", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-1387"], "SR": 0.515625, "CSR": 0.5531572164948453, "retrieved_ids": ["mrqa_squad-train-2960", "mrqa_squad-train-30802", "mrqa_squad-train-39354", "mrqa_squad-train-48352", "mrqa_squad-train-70081", "mrqa_squad-train-48233", "mrqa_squad-train-80846", "mrqa_squad-train-64188", "mrqa_squad-train-25634", "mrqa_squad-train-55982", "mrqa_squad-train-6609", "mrqa_squad-train-75857", "mrqa_squad-train-16900", "mrqa_squad-train-45152", "mrqa_squad-train-45007", "mrqa_squad-train-47181", "mrqa_squad-train-59843", "mrqa_squad-train-25476", "mrqa_squad-train-53499", "mrqa_squad-train-58780", "mrqa_squad-train-33195", "mrqa_squad-train-41960", "mrqa_squad-train-58737", "mrqa_squad-train-68676", "mrqa_squad-train-34617", "mrqa_squad-train-62883", "mrqa_squad-train-82009", "mrqa_squad-train-34959", "mrqa_squad-train-58366", "mrqa_squad-train-62055", "mrqa_squad-train-73087", "mrqa_squad-train-57489", "mrqa_newsqa-validation-293", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-11373", "mrqa_squad-validation-7514", "mrqa_searchqa-validation-9777", "mrqa_newsqa-validation-37", "mrqa_squad-validation-5835", "mrqa_searchqa-validation-1656", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-12261", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-681", "mrqa_searchqa-validation-5539", "mrqa_newsqa-validation-843", "mrqa_naturalquestions-validation-3962", "mrqa_newsqa-validation-2796", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-9109", "mrqa_newsqa-validation-627", "mrqa_searchqa-validation-7229", "mrqa_triviaqa-validation-3032", "mrqa_searchqa-validation-7676", "mrqa_squad-validation-6185", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1339", "mrqa_searchqa-validation-133", "mrqa_newsqa-validation-1041", "mrqa_naturalquestions-validation-495", "mrqa_searchqa-validation-15505", "mrqa_newsqa-validation-2813"], "EFR": 1.0, "Overall": 0.7393033182989691}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomadic", "Washington", "tribbles", "San Jose", "Two Gentlemen of Verona", "a Cobb salad", "Hydra", "Gulliver's Travels", "the Distant Early Warning Line", "Tordis", "jelly beans", "Xinjiang-Uygur Autonomous Region", "sonic boom", "Fergie", "Sacramento", "emerald", "Swiss Cheese", "Ernest Hemingway", "Blue Mountain Coffee", "Annika Sorenstam", "atoms", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "(Henry) Shrapnel", "Venezuela", "Arethusa", "Oklahoma City", "Brazil", "Chicago", "the dugong", "\"Treading Water\"", "1870", "the French & Indian War", "a checkerboard", "Waterloo", "a waterbed", "a monkey", "a bagel", "propeller", "bonnet", "660", "( Alexander) Calder", "a cruller", "helium", "Tokyo", "Mozzarella", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "bacteria", "big Dipper", "Sofia the First", "Africa", "Ben Elton", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7425595238095237}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-6393", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_searchqa-validation-9638", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.703125, "CSR": 0.5546875, "EFR": 0.8947368421052632, "Overall": 0.7185567434210527}, {"timecode": 98, "before_eval_results": {"predictions": ["Marley", "magnum", "the Ottoman Empire", "Helen of Troy", "whale", "New York", "Himalayas", "Wayne's World", "Poland", "Kwanzaa", "nuclear submarine", "Russell Crowe", "\"A Beautiful Mind,\"", "GT350", "tears", "roulette", "Scottish missionary & an American prostitute", "Christo", "Matisse", "the Sargasso Sea", "\"All Quiet on the Western Front\"", "Red Hot Chili Peppers", "Sanskrit", "one", "Montgomery Clift", "Madrid", "Ford", "Sidney Sheldon", "Surround", "Faraday", "breakfast", "Krispy Kreme", "(Museo Civico) officials", "Avery", "Death Valley", "the Cumberland Gap", "yolk", "Department of Defense", "a dwelling place", "a brown rat", "Cleveland", "Edgar Allan Poe", "Belgium", "Chirac", "U.S. President Grover Cleveland", "Destiny's Child", "Luxor", "Spain", "The Beatles", "anchovy", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "king macduff", "macbeth", "Carol Ann Duffy", "Ravenna", "travel diary", "some experts are skeptical that anything could have stopped Robert Hawkins from going on a murderous rampage at an Omaha, Nebraska, shopping mall on Wednesday.", "Sgt. Jason Bendett of the 3rd Platoon, A Company, 2nd Light armored Reconnaissance Battalion, based at Lejeune.", "an acid attack by a spurned suitor.", "make life a little easier for these families by organizing the distribution of wheelchairs,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6738782051282052}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.33333333333333337, 0.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-3546", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-1364", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.578125, "CSR": 0.5549242424242424, "EFR": 0.9259259259259259, "Overall": 0.7248419086700337}, {"timecode": 99, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.84375, "KG": 0.48984375, "before_eval_results": {"predictions": ["the Hundred Years War", "vertebral", "Alfred Binet", "Venial", "a caveat", "There's no place like home", "shrimp", "Spanish", "Vanessa Hudgens", "King Kong", "The Two Towers", "the Eocene", "Rhiannon", "Scotland", "Leave It To Beaver", "Kurdish", "Ann Richards", "half-mast", "France", "Langston Hughes", "Coke", "The Color Purple", "THX surround sound system", "Macbeth", "El Greco", "General Motors", "Michelle Williams", "a shark", "Frankie Valli", "a Dagger", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "ask for help", "Jamestown", "Joy Division", "Fondue", "VOD", "Schwarzenegger", "Edison", "Animal Crackers", "oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Finland", "Students for a Democratic Society", "All the King's Men", "(Charles) Gounod", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 10, 2017", "James Mason", "slide trumpet", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale", "Matamoros, Mexico,", "Florida", "on Capitol Hill,", "The palace has 775 rooms"], "metric_results": {"EM": 0.625, "QA-F1": 0.6979166666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-3260", "mrqa_searchqa-validation-7925", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-12741", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-4773", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2926", "mrqa_naturalquestions-validation-9572"], "SR": 0.625, "CSR": 0.555625, "retrieved_ids": ["mrqa_squad-train-66923", "mrqa_squad-train-62104", "mrqa_squad-train-78584", "mrqa_squad-train-48793", "mrqa_squad-train-82327", "mrqa_squad-train-28750", "mrqa_squad-train-68596", "mrqa_squad-train-56202", "mrqa_squad-train-37091", "mrqa_squad-train-8135", "mrqa_squad-train-33371", "mrqa_squad-train-79646", "mrqa_squad-train-28855", "mrqa_squad-train-62818", "mrqa_squad-train-56231", "mrqa_squad-train-65854", "mrqa_squad-train-20030", "mrqa_squad-train-85372", "mrqa_squad-train-51888", "mrqa_squad-train-34630", "mrqa_squad-train-72736", "mrqa_squad-train-83609", "mrqa_squad-train-73135", "mrqa_squad-train-71410", "mrqa_squad-train-41044", "mrqa_squad-train-6134", "mrqa_squad-train-2402", "mrqa_squad-train-25145", "mrqa_squad-train-73741", "mrqa_squad-train-58017", "mrqa_squad-train-24992", "mrqa_squad-train-86141", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1021", "mrqa_triviaqa-validation-5933", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-2456", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-15312", "mrqa_triviaqa-validation-6199", "mrqa_newsqa-validation-2179", "mrqa_naturalquestions-validation-10402", "mrqa_searchqa-validation-96", "mrqa_hotpotqa-validation-4129", "mrqa_searchqa-validation-10883", "mrqa_newsqa-validation-868", "mrqa_searchqa-validation-1380", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4196", "mrqa_newsqa-validation-1973", "mrqa_hotpotqa-validation-1297", "mrqa_squad-validation-4797", "mrqa_triviaqa-validation-6013", "mrqa_hotpotqa-validation-2978", "mrqa_searchqa-validation-8178", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-1325", "mrqa_newsqa-validation-774", "mrqa_searchqa-validation-5822"], "EFR": 1.0, "Overall": 0.729015625}]}