{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=1213', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=1213/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=48, save_ckpt_freq=25, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=8, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=48_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[5]_seed=1213_result.json', stream_id=5, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 9800, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a plug valve", "1550", "French Louisiana west of the Mississippi River", "2012", "carbon dioxide", "the Lisbon Treaty", "all colors", "in the chloroplasts of C4 plants", "An attorney", "democracy", "The Greens", "third", "Enthusiastic teachers", "expositions", "no French regular army troops were stationed in North America", "estates of the Holy Roman Empire", "Stromatoveris", "2011", "Louis Pasteur", "the owner", "Time Lord", "mosaic floors", "economic", "1893", "environmental factors like light color and intensity", "Gandhi", "deforestation", "Middle Rhine Valley", "pump this into the mesoglea", "low-light conditions", "No Child Left Behind", "one way streets", "\u20ac25,000 per year", "England, Wales, Scotland, Denmark, Sweden, Switzerland", "unbalanced torque", "Ulaanbaatar", "power", "very weak", "Judith Merril", "Gender pay gap", "the Ilkhanate", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "University of Chicago campus", "3D printing technology", "1957", "2000", "a certain number of teacher's salaries are paid by the State", "the Dutch Republic", "San Jose Marriott", "April 20", "the Commission", "evacuate the cylinder", "the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps", "Hurricane Beryl", "temperature and light", "terra nullius", "growth", "human", "the \u2018combs\u2019", "1978", "non-Catholics", "Sanders", "vice president", "700 employees"], "metric_results": {"EM": 0.828125, "QA-F1": 0.8576388888888888}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10143", "mrqa_squad-validation-8841", "mrqa_squad-validation-2145", "mrqa_squad-validation-739", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-7449", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-9764", "mrqa_squad-validation-7051"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["1986", "Peridinin", "standardized", "50% to 60%", "Stromatoveris", "lower incomes", "Fort Duquesne", "Katharina von Bora", "Miller", "women", "Three's Company", "Frank Marx", "architect or engineer", "$2 million", "superintendent of New York City schools", "San Francisco Bay Area at Santa Clara, California", "Kingdom of Prussia", "the same league as the Asian Economic Tigers", "Palestine", "Aristotle and Archimedes", "in the chloroplasts of C4 plants", "Outlaws", "increased blood flow into tissue", "Edgar Scherick", "14th to the 19th century", "Gibraltar and the \u00c5land islands", "the Evangelical Lutheran Church", "oxygen", "the BBC National Orchestra of Wales", "Thanksgiving", "the founding of new Protestant churches", "it is impossible to determine what the acceleration of the rope will be", "Venus", "those who proceed to secondary school or vocational training", "zoning and building code requirements", "Ikh Zasag", "Central Bridge", "the Electorate of Brandenburg and electorate of the Palatinate in the Holy Roman Empire", "William Tyndale", "1935", "seven", "Grumman", "1191", "Maciot de Bethencourt", "Euclid", "case law by the Court of Justice", "long, slender tentacles", "mesoglea", "1970s", "white", "misguided", "2014", "Reconstruction and the Gilded Age", "European Parliament and the Council of the European Union", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Manakin Episcopal Church", "Nicholas Stone", "due to ongoing tectonic subsidence", "the release of her eponymous debut album the following year", "a form of business network", "The Senate ( north ) wing was completed in 1800", "The euro is the result of the European Union's project for economic and monetary union", "Djokovic", "The White Company"], "metric_results": {"EM": 0.796875, "QA-F1": 0.841060501998002}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.2, 0.15384615384615385, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7332", "mrqa_squad-validation-27", "mrqa_squad-validation-8459", "mrqa_squad-validation-10339", "mrqa_squad-validation-10321", "mrqa_squad-validation-3021", "mrqa_squad-validation-3946", "mrqa_squad-validation-1906", "mrqa_squad-validation-5588", "mrqa_squad-validation-9166", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1187"], "SR": 0.796875, "CSR": 0.8125, "retrieved_ids": ["mrqa_squad-train-18826", "mrqa_squad-train-84430", "mrqa_squad-train-20110", "mrqa_squad-train-27764", "mrqa_squad-train-19001", "mrqa_squad-train-70786", "mrqa_squad-train-35747", "mrqa_squad-train-82748", "mrqa_squad-train-1274", "mrqa_squad-train-84698", "mrqa_squad-train-38935", "mrqa_squad-train-14804", "mrqa_squad-train-50125", "mrqa_squad-train-86318", "mrqa_squad-train-37350", "mrqa_squad-train-20008", "mrqa_squad-train-77056", "mrqa_squad-train-82497", "mrqa_squad-train-33499", "mrqa_squad-train-22203", "mrqa_squad-train-47229", "mrqa_squad-train-26669", "mrqa_squad-train-47319", "mrqa_squad-train-70228", "mrqa_squad-validation-4452", "mrqa_squad-validation-7051", "mrqa_squad-validation-9173", "mrqa_squad-validation-7364", "mrqa_squad-validation-10143", "mrqa_squad-validation-9764", "mrqa_squad-validation-2145", "mrqa_squad-validation-7449", "mrqa_squad-validation-8841", "mrqa_squad-validation-3019", "mrqa_squad-validation-739"], "EFR": 1.0, "Overall": 0.90625}, {"timecode": 2, "before_eval_results": {"predictions": ["Lek", "prohibited emigration", "The Private Education Student Financial Assistance", "highly-paid", "Labor", "time and storage", "special efforts", "rhetoric", "British", "a year", "Genghis Khan", "a supervisory church body", "35", "a cubic interpolation formula", "King Sigismund III Vasa", "1835", "the exploitation of the valuable assets and supplies of the nation that was conquered", "poor management", "five", "liquid oxygen", "Gosforth Park", "Metropolitan Police Authority", "18 February 1546", "1996", "1.7 billion years ago", "Mike Carey", "coal", "31 October", "Stanford University", "1976", "LOVE Radio", "ambiguity", "Khasar", "Sky Digital", "99.4", "about a third", "over the issue of laity having a voice and vote", "1995", "Endosymbiotic gene transfer", "rocketry and manned spaceflight", "linebacker", "feed water", "Sir Edward Poynter", "oxygen", "August 1967", "Velamen parallelum", "human rights abuses and war crimes", "three", "Lowry Digital", "worst-case time complexity", "2010", "33", "Buffalo Lookout", "Missouri", "The User State Migration Tool", "1773", "Cadmium", "October 6, 2017", "11 p.m. to 3 a.m", "Haliaeetus", "Cetshwayo", "Atticus Finch", "through the weekend", "Ewan McGregor"], "metric_results": {"EM": 0.796875, "QA-F1": 0.838261217948718}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7357", "mrqa_squad-validation-1672", "mrqa_squad-validation-335", "mrqa_squad-validation-10217", "mrqa_squad-validation-2538", "mrqa_squad-validation-6171", "mrqa_squad-validation-9876", "mrqa_squad-validation-3812", "mrqa_squad-validation-9717", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-430", "mrqa_newsqa-validation-174"], "SR": 0.796875, "CSR": 0.8072916666666666, "retrieved_ids": ["mrqa_squad-train-86500", "mrqa_squad-train-68526", "mrqa_squad-train-17574", "mrqa_squad-train-12564", "mrqa_squad-train-50106", "mrqa_squad-train-15023", "mrqa_squad-train-15956", "mrqa_squad-train-74110", "mrqa_squad-train-26370", "mrqa_squad-train-14324", "mrqa_squad-train-42969", "mrqa_squad-train-57301", "mrqa_squad-train-66019", "mrqa_squad-train-71616", "mrqa_squad-train-9232", "mrqa_squad-train-3505", "mrqa_squad-train-40464", "mrqa_squad-train-48591", "mrqa_squad-train-80221", "mrqa_squad-train-35650", "mrqa_squad-train-7064", "mrqa_squad-train-78289", "mrqa_squad-train-1292", "mrqa_squad-train-51176", "mrqa_squad-validation-8841", "mrqa_squad-validation-1906", "mrqa_naturalquestions-validation-191", "mrqa_squad-validation-5588", "mrqa_squad-validation-3019", "mrqa_squad-validation-7364", "mrqa_squad-validation-7449", "mrqa_naturalquestions-validation-1187", "mrqa_squad-validation-739", "mrqa_squad-validation-7332", "mrqa_squad-validation-10339", "mrqa_squad-validation-7051", "mrqa_squad-validation-8459", "mrqa_squad-validation-4452", "mrqa_squad-validation-9173", "mrqa_squad-validation-2145", "mrqa_squad-validation-3021", "mrqa_squad-validation-3946", "mrqa_squad-validation-9166", "mrqa_squad-validation-10321", "mrqa_squad-validation-9764", "mrqa_naturalquestions-validation-1912", "mrqa_squad-validation-27", "mrqa_squad-validation-10143"], "EFR": 1.0, "Overall": 0.9036458333333333}, {"timecode": 3, "before_eval_results": {"predictions": ["female", "1884", "Sayyid Abul Ala Maududi", "a family member, or by anyone with knowledge or skills in the wider community setting", "James E. Webb", "Lutheran and Reformed states in Germany and Scandinavia", "phycobilin phycoerytherin", "understaffed", "swimming-plates", "10 July 1856", "130 million cubic foot", "\"teleforce\"", "Heinrich Himmler", "34\u201319", "Baptism", "Decision problems", "customs of his tribe", "1953", "The Day of the Doctor", "Muhammad Khan", "Sun Life Stadium", "the Council", "February 9, 1953", "March", "sea gooseberry", "1961", "Trio Tribe", "Dai Setsen", "the Late Medieval Catholic Church", "January 1979", "phagocytic cells", "Rankine cycle", "$2.2 billion", "Seine", "Newton's Law of Gravitation", "15 February 1546", "Marquis de la Jonqui\u00e8re", "BBC Dead Ringers", "Kenyans for Kenya", "Fresno", "Israel's", "the Presiding Officer", "an intuitive understanding", "default emission factors", "Inherited wealth", "Michael P. Millardi", "Goldman Sachs", "fish", "Room Kids and Bookcase Closet", "South Dakota, 2006, Great faces, great places", "praying", "It, It S, Just Do It, This Is Us, We Have, Let It Be, Succinct Word, Initiativeate, Mamihlapinatapai", "The Abu Simbel temples", "The Leyden jar", "A better-paid legislator", "opera librettist for several of the well-known bel canto-era composers including Gioachino...   Giacomo Meyerbeer", "John, D. of Marlborogh", "70%", "November 27, 1924, and featured Macy's employees and...   Milly and the Macy's Parade", "with a shrugged, as you looked over... building which the most fertile of imaginations could consider as being at all up to date was the clock", "the British", "early 1960s", "April 1917", "poor hygiene"], "metric_results": {"EM": 0.625, "QA-F1": 0.6765391726329226}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.5454545454545454, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.15384615384615385, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-1863", "mrqa_squad-validation-3270", "mrqa_squad-validation-7525", "mrqa_squad-validation-2595", "mrqa_squad-validation-6072", "mrqa_squad-validation-5860", "mrqa_squad-validation-5262", "mrqa_squad-validation-10369", "mrqa_squad-validation-3863", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-9536", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-11367", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-10156"], "SR": 0.625, "CSR": 0.76171875, "retrieved_ids": ["mrqa_squad-train-29216", "mrqa_squad-train-81", "mrqa_squad-train-36306", "mrqa_squad-train-37837", "mrqa_squad-train-59853", "mrqa_squad-train-20240", "mrqa_squad-train-75902", "mrqa_squad-train-45926", "mrqa_squad-train-77519", "mrqa_squad-train-64391", "mrqa_squad-train-31540", "mrqa_squad-train-38410", "mrqa_squad-train-81096", "mrqa_squad-train-53926", "mrqa_squad-train-65358", "mrqa_squad-train-76720", "mrqa_squad-train-46944", "mrqa_squad-train-42365", "mrqa_squad-train-70959", "mrqa_squad-train-35646", "mrqa_squad-train-68028", "mrqa_squad-train-39930", "mrqa_squad-train-4734", "mrqa_squad-train-24320", "mrqa_squad-validation-5588", "mrqa_squad-validation-7449", "mrqa_squad-validation-2538", "mrqa_squad-validation-27", "mrqa_squad-validation-7332", "mrqa_newsqa-validation-174", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-10217", "mrqa_squad-validation-9764", "mrqa_squad-validation-4452", "mrqa_squad-validation-3019", "mrqa_squad-validation-3946", "mrqa_squad-validation-8841", "mrqa_squad-validation-3021", "mrqa_squad-validation-6171", "mrqa_squad-validation-9876", "mrqa_squad-validation-1672", "mrqa_squad-validation-7357", "mrqa_squad-validation-2145", "mrqa_squad-validation-9166", "mrqa_squad-validation-335", "mrqa_squad-validation-7364"], "EFR": 1.0, "Overall": 0.880859375}, {"timecode": 4, "before_eval_results": {"predictions": ["boom-and-bust cycles", "The Prince of P\u0142ock", "hormones", "1840", "occupational stress among teachers", "in the parts of the internal canal network", "in no way", "Tesla Electric Company", "African-American", "Thomson", "1905", "\"Nun komm, der Heiland\"", "John Fox", "in all health care settings", "cut in half", "the study of rocks", "colonies", "lower wages", "geophysical surveys", "Fontainebleau", "social power and wealth", "2,900 kilometres", "Elie Metchnikoff", "an algorithm", "Immediately after Decision Time", "Confucian propriety and ancestor veneration", "25-minute", "eight", "elude host immune responses", "Pusey Library", "inequality", "designs into reality", "cytokines", "requiring his arrest", "wide sidewalks", "other members", "Air Force", "an occupancy permit", "reactive allotrope of oxygen", "Waal and Pannerdens Kanaal", "multi-cultural", "pump water out of the mesoglea", "Zeebo", "Australia", "a judicial officer", "mathematical model", "Henry Purcell", "Ram Nath Kovind", "embryo", "Todd Griffin", "Sandy Knox and Billy Stritch", "Hudson Bay", "Etienne de Mestre", "a bow bridge with 16 arches shielded by ice guards", "from 1922 to 1991", "Nicole Gale Anderson", "1", "sedimentary", "Mrs. Wolowitz", "plate tectonics", "Colombia", "Yolande of Brienne", "Kris Allen", "Colombian telenovela Yo soy Betty, la fea"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7239109848484848}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.22222222222222224]}}, "before_error_ids": ["mrqa_squad-validation-4539", "mrqa_squad-validation-2456", "mrqa_squad-validation-6319", "mrqa_squad-validation-7338", "mrqa_squad-validation-2943", "mrqa_squad-validation-8093", "mrqa_squad-validation-3497", "mrqa_squad-validation-9176", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-4002", "mrqa_triviaqa-validation-5855", "mrqa_hotpotqa-validation-4815", "mrqa_searchqa-validation-172"], "SR": 0.65625, "CSR": 0.740625, "retrieved_ids": ["mrqa_squad-train-66034", "mrqa_squad-train-71864", "mrqa_squad-train-51983", "mrqa_squad-train-79519", "mrqa_squad-train-1343", "mrqa_squad-train-25485", "mrqa_squad-train-85658", "mrqa_squad-train-21682", "mrqa_squad-train-84137", "mrqa_squad-train-52041", "mrqa_squad-train-43167", "mrqa_squad-train-20111", "mrqa_squad-train-32114", "mrqa_squad-train-2838", "mrqa_squad-train-75789", "mrqa_squad-train-59482", "mrqa_squad-train-46644", "mrqa_squad-train-31906", "mrqa_squad-train-58052", "mrqa_squad-train-68607", "mrqa_squad-train-25688", "mrqa_squad-train-36249", "mrqa_squad-train-62575", "mrqa_squad-train-17707", "mrqa_squad-validation-1863", "mrqa_squad-validation-3019", "mrqa_squad-validation-5588", "mrqa_squad-validation-2145", "mrqa_squad-validation-2538", "mrqa_searchqa-validation-11367", "mrqa_newsqa-validation-174", "mrqa_squad-validation-9166", "mrqa_searchqa-validation-1156", "mrqa_squad-validation-7051", "mrqa_squad-validation-3946", "mrqa_squad-validation-5860", "mrqa_squad-validation-8459", "mrqa_naturalquestions-validation-844", "mrqa_squad-validation-10321", "mrqa_squad-validation-8841", "mrqa_squad-validation-10339", "mrqa_squad-validation-27", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-1912", "mrqa_searchqa-validation-13232", "mrqa_squad-validation-10143", "mrqa_squad-validation-7357", "mrqa_searchqa-validation-4319"], "EFR": 1.0, "Overall": 0.8703125}, {"timecode": 5, "before_eval_results": {"predictions": ["former flooded terraces", "beginning of the 20th century", "1974", "ABC", "dictatorial", "Ben Johnston", "quantum mechanics", "Book of Exodus", "Synthetic aperture radar", "Mission Impossible", "patients' prescriptions and patient safety issues", "No, that's no good", "1697", "3 January 1521", "a new magma", "a \"principal hostile country\"", "Jan Hus", "Newton", "Croatia", "2011", "Swynnerton Plan", "the machine gun", "Theatre Museum", "August 10, 1948", "they are distinct or equal classes", "the 2004 Treaty establishing a Constitution for Europe", "Serge Chermayeff", "Thomas Edison", "Mnemiopsis", "the flail of God", "Woodward Park", "The Melbourne Cricket Ground", "Wednesdays", "most common", "a concentration gradient", "tears and urine", "six years", "plants and algae", "Republic Day", "1913", "Yuzuru Hanyu", "Konakuppakatil Gopinathan Balakrishnan", "1942", "5 September", "Texas, Oklahoma, and the surrounding Great Plains", "a balance sheet", "Mayor Hudnut", "1995", "William the Conqueror", "1922", "an anembryonic gestation", "Bemis Heights", "9pm ET ( UTC - 5 )", "twice", "Joe Pizzulo and Leeza Miller", "the head of Lituya Bay in Alaska", "Sarah", "routing protocols", "The euro", "(beyond violet", "2000", "Rodong Sinmun", "a papermaking process that precedes the rolling of the paper", "Rodgers & Hammerstein"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6971163799968148}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3076923076923077, 0.5714285714285715, 1.0, 1.0, 0.8, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5818", "mrqa_squad-validation-1827", "mrqa_squad-validation-1566", "mrqa_squad-validation-10388", "mrqa_squad-validation-3770", "mrqa_squad-validation-1780", "mrqa_squad-validation-3985", "mrqa_squad-validation-4572", "mrqa_squad-validation-8904", "mrqa_squad-validation-6439", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-9597", "mrqa_triviaqa-validation-2749", "mrqa_searchqa-validation-14371"], "SR": 0.609375, "CSR": 0.71875, "retrieved_ids": ["mrqa_squad-train-63649", "mrqa_squad-train-6851", "mrqa_squad-train-26171", "mrqa_squad-train-29121", "mrqa_squad-train-14218", "mrqa_squad-train-26636", "mrqa_squad-train-22428", "mrqa_squad-train-37638", "mrqa_squad-train-17850", "mrqa_squad-train-10189", "mrqa_squad-train-80094", "mrqa_squad-train-78763", "mrqa_squad-train-62776", "mrqa_squad-train-66275", "mrqa_squad-train-5834", "mrqa_squad-train-43529", "mrqa_squad-train-82469", "mrqa_squad-train-36264", "mrqa_squad-train-75271", "mrqa_squad-train-47224", "mrqa_squad-train-82631", "mrqa_squad-train-58860", "mrqa_squad-train-50228", "mrqa_squad-train-55546", "mrqa_naturalquestions-validation-5780", "mrqa_searchqa-validation-1156", "mrqa_naturalquestions-validation-2476", "mrqa_squad-validation-2595", "mrqa_squad-validation-2538", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-8711", "mrqa_squad-validation-7357", "mrqa_squad-validation-1906", "mrqa_squad-validation-3946", "mrqa_squad-validation-9173", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-7338", "mrqa_squad-validation-9876", "mrqa_squad-validation-1863", "mrqa_naturalquestions-validation-4002", "mrqa_searchqa-validation-4319", "mrqa_squad-validation-10143", "mrqa_squad-validation-27", "mrqa_squad-validation-10321", "mrqa_squad-validation-10217", "mrqa_searchqa-validation-172", "mrqa_naturalquestions-validation-8792"], "EFR": 1.0, "Overall": 0.859375}, {"timecode": 6, "before_eval_results": {"predictions": ["four", "2 million", "Gender pay gap in favor of males in the labor market", "Pliocene", "relationship between teachers and children", "LeGrande", "After the sixth sermon", "10 Cloverfield Lane", "11.1%", "10,000", "University of Chicago College Bowl Team", "the decline of organized labor in the United States", "Santa Clara Marriott", "oxygen chambers", "two", "two catechisms", "Cologne", "1991", "Silk Road", "Surveyor 3 unmanned lunar probe", "145", "growth and investment", "the centers were computer service bureaus, offering batch processing services.", "Vampire bats", "antiforms", "U. S. flags left on the Moon during the Apollo missions were found to still be standing", "weight", "their country as \"Genghis Khan's Mongolia", "oil was priced in dollars, oil producers' real income decreased", "Beyonc\u00e9 and Bruno Mars", "a university or college", "More than 1 million", "hash tables and pseudorandom number generators", "Japan", "Coriolis force", "Mickey Rourke", "May 2016", "Nicklaus", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Panama Canal Authority", "silk, hair / fur ( including wool ) and feathers", "France's Legislative Assembly", "two", "Sebastian Vettel", "April 10, 2018", "Gorakhpur", "How I Met Your Mother", "directly elected", "December 15, 2016", "Abraham Gottlob Werner", "Jourdan Miller", "1991", "Samantha Jo", "the Greek name", "Broken Hill and Sydney", "159", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC )", "Judiththia Aline Keppel", "medellin", "Crown Holdings Incorporated", "Expedia", "HST", "Zed", "a failure of leadership at a critical moment in the nation's history"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6749903251941295}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.4444444444444444, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.782608695652174, 0.25, 0.25, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.13333333333333333, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7445", "mrqa_squad-validation-606", "mrqa_squad-validation-6965", "mrqa_squad-validation-5435", "mrqa_squad-validation-7422", "mrqa_squad-validation-4000", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_squad-validation-6228", "mrqa_squad-validation-3718", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-6106", "mrqa_searchqa-validation-10372", "mrqa_newsqa-validation-429"], "SR": 0.578125, "CSR": 0.6986607142857143, "retrieved_ids": ["mrqa_squad-train-27803", "mrqa_squad-train-6436", "mrqa_squad-train-48474", "mrqa_squad-train-45375", "mrqa_squad-train-61458", "mrqa_squad-train-46864", "mrqa_squad-train-50080", "mrqa_squad-train-20616", "mrqa_squad-train-8766", "mrqa_squad-train-62242", "mrqa_squad-train-64587", "mrqa_squad-train-56401", "mrqa_squad-train-53015", "mrqa_squad-train-22297", "mrqa_squad-train-18911", "mrqa_squad-train-72730", "mrqa_squad-train-43076", "mrqa_squad-train-73932", "mrqa_squad-train-28433", "mrqa_squad-train-51347", "mrqa_squad-train-15644", "mrqa_squad-train-49579", "mrqa_squad-train-83517", "mrqa_squad-train-1263", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-3558", "mrqa_squad-validation-3863", "mrqa_searchqa-validation-11367", "mrqa_squad-validation-5860", "mrqa_squad-validation-10388", "mrqa_squad-validation-1672", "mrqa_squad-validation-739", "mrqa_squad-validation-6439", "mrqa_squad-validation-2595", "mrqa_squad-validation-7051", "mrqa_squad-validation-3946", "mrqa_searchqa-validation-5631", "mrqa_naturalquestions-validation-9240", "mrqa_squad-validation-1827", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-844", "mrqa_squad-validation-10369", "mrqa_naturalquestions-validation-2466", "mrqa_searchqa-validation-172", "mrqa_squad-validation-3270", "mrqa_searchqa-validation-4319", "mrqa_squad-validation-3021", "mrqa_searchqa-validation-14371"], "EFR": 0.9629629629629629, "Overall": 0.8308118386243386}, {"timecode": 7, "before_eval_results": {"predictions": ["Director", "travel literature, cartography, geography, and scientific education", "oxygen chambers", "Graham Gano", "Two", "In 1066, Duke William II of Normandy conquered England killing King Harold II", "2008", "Mojave Desert", "Operating System Principles", "St. Lawrence and Mississippi watersheds", "27%", "4000", "Rhine Gorge", "helical thylakoid model, grana", "highest", "impact process effects", "Asian, African and Caribbean countries", "Warner Bros. Presents", "pharmacists", "high-frequency power", "4:51", "Kabaty Forest", "the seal of the Federal Communications Commission", "strong sedimentation in the western Rhine Delta", "The European Commission", "SAP Center in San Jose", "respiration", "352", "eliminate the accusing law", "October 6, 2004", "\"The Day of the Doctor\"", "Pakistan", "November 1999", "September 6, 2019", "English", "the fourth season", "four times", "Nick Kroll", "comedy web television series", "Billy Gibbons", "an apprentice of the fictional Wars Order in the Star Wars franchise", "the medulla oblongata", "31", "1970s", "the Vital Records Office of the states, capital district, territories and former territories", "Art Carney", "accomplish the objectives of the organization", "the female spends extra time basking to keep her eggs warm", "December 1922", "Category 4", "September 2017", "3 September, after a British ultimatum to Germany to cease military operations was ignored", "South Africa, Australia, northern New Zealand and the southern USA", "Terrell Owens", "since 3, 1, and 4", "five", "Dolph Lundgren", "Hampton Court Palace", "Sela Ward", "her decades-long portrayal of Alice Horton", "the Universal Licensing System", "Benjamin Britten", "isosceles triangle", "NOW Magazine"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6607958447802198}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.625, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1125", "mrqa_squad-validation-4629", "mrqa_squad-validation-8819", "mrqa_squad-validation-1938", "mrqa_squad-validation-6409", "mrqa_squad-validation-1195", "mrqa_squad-validation-451", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-62", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16130", "mrqa_triviaqa-validation-6548"], "SR": 0.59375, "CSR": 0.685546875, "retrieved_ids": ["mrqa_squad-train-31629", "mrqa_squad-train-63330", "mrqa_squad-train-20605", "mrqa_squad-train-35732", "mrqa_squad-train-22130", "mrqa_squad-train-32099", "mrqa_squad-train-75270", "mrqa_squad-train-50424", "mrqa_squad-train-42406", "mrqa_squad-train-55210", "mrqa_squad-train-38639", "mrqa_squad-train-59553", "mrqa_squad-train-43809", "mrqa_squad-train-69340", "mrqa_squad-train-12552", "mrqa_squad-train-70539", "mrqa_squad-train-13265", "mrqa_squad-train-25237", "mrqa_squad-train-36128", "mrqa_squad-train-11116", "mrqa_squad-train-1462", "mrqa_squad-train-84453", "mrqa_squad-train-24986", "mrqa_squad-train-26591", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-6461", "mrqa_searchqa-validation-4319", "mrqa_squad-validation-7357", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-2595", "mrqa_squad-validation-3985", "mrqa_naturalquestions-validation-10311", "mrqa_searchqa-validation-172", "mrqa_squad-validation-7449", "mrqa_squad-validation-7338", "mrqa_squad-validation-9764", "mrqa_squad-validation-5435", "mrqa_naturalquestions-validation-9818", "mrqa_squad-validation-7525", "mrqa_naturalquestions-validation-10039", "mrqa_squad-validation-9876", "mrqa_naturalquestions-validation-8765", "mrqa_squad-validation-6072", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-4644", "mrqa_squad-validation-3946", "mrqa_naturalquestions-validation-1912"], "EFR": 0.9615384615384616, "Overall": 0.8235426682692308}, {"timecode": 8, "before_eval_results": {"predictions": ["cytokines", "William Pitt", "North Carolina and New Mexico", "the p-adic norm", "Hassan al Banna", "Gottfried Fritschel", "the Eighth Doctor", "plastoglobulus", "pound-force", "the Song dynasty and the Ming dynasty", "Dorothy and Michael Hintze", "The Small Catechism", "36%", "Giovanni Battista Foggini", "April 20", "biomass", "their belief in the validity of the social contract", "K MJ-TV", "the Foreign Protestants Naturalization Act", "the southern and central parts of France", "10%", "not designed to fly through the Earth's atmosphere or return to Earth", "Metro Trains Melbourne", "BBC 1", "greater than $2 million", "Vince Lombardi Trophy", "Galileo", "in linked groups or chains", "Mama Said", "The Tiber", "1885", "James Madison", "Ryan Pinkston", "federal republic", "lacteal", "2005", "foreign investors", "N\u0289m\u0289n\u0289", "8ft", "Bartolomeu Dias", "William Wyler", "1961", "March 1930", "Julie Adams", "Thomas Jefferson", "the game released in February 2017 in Japan and in March 2018 in North America and Europe", "October 29, 2015", "USCS or USC", "Millerlite", "Sunday night results show", "Billy Hill", "Mara Jade", "Malina Weissman", "September 6, 2019", "1773", "A lacteal", "April 26, 2005", "Ann bawdy folk song called \"The Ballad of Lydia Pinkham,\"", "Albert", "a coma in a grave condition", "Croatia", "Drew Kesse", "16", "a group of 20 similar cars making an annual road trip"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6350942460317459}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-8958", "mrqa_squad-validation-7876", "mrqa_squad-validation-8184", "mrqa_squad-validation-5724", "mrqa_squad-validation-6706", "mrqa_squad-validation-4715", "mrqa_squad-validation-8769", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-8287", "mrqa_triviaqa-validation-4881", "mrqa_hotpotqa-validation-2800", "mrqa_newsqa-validation-3043", "mrqa_searchqa-validation-2141", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-3476"], "SR": 0.59375, "CSR": 0.6753472222222222, "retrieved_ids": ["mrqa_squad-train-6928", "mrqa_squad-train-37230", "mrqa_squad-train-38265", "mrqa_squad-train-59179", "mrqa_squad-train-10845", "mrqa_squad-train-61667", "mrqa_squad-train-12902", "mrqa_squad-train-84622", "mrqa_squad-train-20890", "mrqa_squad-train-41246", "mrqa_squad-train-61317", "mrqa_squad-train-49321", "mrqa_squad-train-2485", "mrqa_squad-train-59086", "mrqa_squad-train-8227", "mrqa_squad-train-14141", "mrqa_squad-train-34675", "mrqa_squad-train-27614", "mrqa_squad-train-49059", "mrqa_squad-train-517", "mrqa_squad-train-57728", "mrqa_squad-train-34182", "mrqa_squad-train-7163", "mrqa_squad-train-21612", "mrqa_searchqa-validation-14194", "mrqa_naturalquestions-validation-6461", "mrqa_squad-validation-9876", "mrqa_squad-validation-7445", "mrqa_searchqa-validation-11367", "mrqa_squad-validation-1906", "mrqa_squad-validation-10369", "mrqa_naturalquestions-validation-7095", "mrqa_squad-validation-8093", "mrqa_searchqa-validation-5631", "mrqa_squad-validation-9166", "mrqa_squad-validation-3985", "mrqa_naturalquestions-validation-4348", "mrqa_squad-validation-7332", "mrqa_newsqa-validation-2112", "mrqa_naturalquestions-validation-7242", "mrqa_squad-validation-9176", "mrqa_squad-validation-9717", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-8983", "mrqa_squad-validation-3998", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-4002"], "EFR": 1.0, "Overall": 0.8376736111111112}, {"timecode": 9, "before_eval_results": {"predictions": ["to emphasize academics over athletics", "3,600", "nine", "the individual states and territories", "50%", "one of his wife's ladies-in-waiting", "liquid phase", "Dirichlet's theorem on arithmetic progressions", "Europe", "the cell membrane", "Time Lord", "Laverne & Shirley", "carbohydrates", "his butchery", "Jean Ribault", "March 2011", "Continental Edison Company in France", "2010", "more equality in the income distribution", "X reduces to Y", "38", "1887", "1469", "a \"world classic of epoch-making oratory.\"", "up to half the carbon fixed by the Calvin cycle", "one octave lower than the four lower strings", "WD-40", "a tavern", "Georgie Porgie", "Vodka", "William Shakespeare", "The Fray", "Venus", "Helen Hayes", "Canberra", "The World Through More Than One lens", "Alexander Graham Bell", "Anna Pavlova", "an actuary computes premium rates", "Lasorda", "Azuki beans", "Chicago Cubs", "crayola.com", "the goat", "child molestation", "the INTJ feedback", "jennifer Wagner", "the White Nile", "Jeremy Irons", "the chimney flue", "Andrew Jackson", "Madonna", "Fauves", "a sailfish", "jennifer", "Laurie", "Egypt", "James Hutton", "Morocco", "Shepherd Neame", "Total Nonstop Action Wrestling", "the highest Hirsch index rating of all living chemists in 2011", "China and Japan", "Antonio Maria Costa"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6345552884615384}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3687", "mrqa_squad-validation-8969", "mrqa_squad-validation-7700", "mrqa_squad-validation-1240", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-15770", "mrqa_naturalquestions-validation-1378", "mrqa_hotpotqa-validation-929", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-2179"], "SR": 0.578125, "CSR": 0.665625, "retrieved_ids": ["mrqa_squad-train-28440", "mrqa_squad-train-11887", "mrqa_squad-train-68774", "mrqa_squad-train-6070", "mrqa_squad-train-56428", "mrqa_squad-train-77202", "mrqa_squad-train-12041", "mrqa_squad-train-62653", "mrqa_squad-train-86548", "mrqa_squad-train-150", "mrqa_squad-train-44095", "mrqa_squad-train-69898", "mrqa_squad-train-31734", "mrqa_squad-train-48158", "mrqa_squad-train-19096", "mrqa_squad-train-34187", "mrqa_squad-train-38798", "mrqa_squad-train-25263", "mrqa_squad-train-9639", "mrqa_squad-train-9492", "mrqa_squad-train-76154", "mrqa_squad-train-6574", "mrqa_squad-train-19738", "mrqa_squad-train-46177", "mrqa_naturalquestions-validation-430", "mrqa_triviaqa-validation-6548", "mrqa_searchqa-validation-13232", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-10039", "mrqa_squad-validation-10388", "mrqa_squad-validation-4452", "mrqa_searchqa-validation-15194", "mrqa_squad-validation-8819", "mrqa_naturalquestions-validation-5721", "mrqa_squad-validation-1827", "mrqa_squad-validation-8471", "mrqa_squad-validation-3270", "mrqa_naturalquestions-validation-3028", "mrqa_squad-validation-1780", "mrqa_squad-validation-7876", "mrqa_newsqa-validation-3476", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-2016", "mrqa_squad-validation-10321", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8277", "mrqa_squad-validation-6706", "mrqa_squad-validation-5588"], "EFR": 1.0, "Overall": 0.8328125}, {"timecode": 10, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2368", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-469", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6088", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8454", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9453", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-11532", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15169", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2579", "mrqa_searchqa-validation-3245", "mrqa_searchqa-validation-3613", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-5035", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9536", "mrqa_squad-validation-10000", "mrqa_squad-validation-10115", "mrqa_squad-validation-10136", "mrqa_squad-validation-1017", "mrqa_squad-validation-10181", "mrqa_squad-validation-10184", "mrqa_squad-validation-10217", "mrqa_squad-validation-10263", "mrqa_squad-validation-10281", "mrqa_squad-validation-10290", "mrqa_squad-validation-10321", "mrqa_squad-validation-10339", "mrqa_squad-validation-10361", "mrqa_squad-validation-10369", "mrqa_squad-validation-1038", "mrqa_squad-validation-10410", "mrqa_squad-validation-10454", "mrqa_squad-validation-10496", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-1177", "mrqa_squad-validation-1181", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1226", "mrqa_squad-validation-1240", "mrqa_squad-validation-1254", "mrqa_squad-validation-1269", "mrqa_squad-validation-1371", "mrqa_squad-validation-1499", "mrqa_squad-validation-1521", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1651", "mrqa_squad-validation-166", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1748", "mrqa_squad-validation-1780", "mrqa_squad-validation-1787", "mrqa_squad-validation-1848", "mrqa_squad-validation-1863", "mrqa_squad-validation-1892", "mrqa_squad-validation-1924", "mrqa_squad-validation-1938", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-1998", "mrqa_squad-validation-2019", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-2108", "mrqa_squad-validation-2145", "mrqa_squad-validation-2209", "mrqa_squad-validation-2233", "mrqa_squad-validation-2241", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-247", "mrqa_squad-validation-2521", "mrqa_squad-validation-2545", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2642", "mrqa_squad-validation-27", "mrqa_squad-validation-2751", "mrqa_squad-validation-2820", "mrqa_squad-validation-2885", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-3039", "mrqa_squad-validation-305", "mrqa_squad-validation-3076", "mrqa_squad-validation-3144", "mrqa_squad-validation-3164", "mrqa_squad-validation-317", "mrqa_squad-validation-3184", "mrqa_squad-validation-322", "mrqa_squad-validation-3230", "mrqa_squad-validation-3270", "mrqa_squad-validation-334", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3376", "mrqa_squad-validation-3380", "mrqa_squad-validation-3392", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3497", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3687", "mrqa_squad-validation-3703", "mrqa_squad-validation-3718", "mrqa_squad-validation-374", "mrqa_squad-validation-3769", "mrqa_squad-validation-3770", "mrqa_squad-validation-381", "mrqa_squad-validation-3824", "mrqa_squad-validation-3829", "mrqa_squad-validation-3842", "mrqa_squad-validation-3848", "mrqa_squad-validation-3852", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3917", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3985", "mrqa_squad-validation-3986", "mrqa_squad-validation-3998", "mrqa_squad-validation-4000", "mrqa_squad-validation-4009", "mrqa_squad-validation-402", "mrqa_squad-validation-4031", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4181", "mrqa_squad-validation-4187", "mrqa_squad-validation-4213", "mrqa_squad-validation-4291", "mrqa_squad-validation-4312", "mrqa_squad-validation-4348", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4452", "mrqa_squad-validation-4467", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-451", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4539", "mrqa_squad-validation-4557", "mrqa_squad-validation-4557", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4838", "mrqa_squad-validation-491", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5019", "mrqa_squad-validation-5064", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-516", "mrqa_squad-validation-5262", "mrqa_squad-validation-5396", "mrqa_squad-validation-5436", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5493", "mrqa_squad-validation-5527", "mrqa_squad-validation-5546", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5602", "mrqa_squad-validation-5631", "mrqa_squad-validation-5664", "mrqa_squad-validation-5677", "mrqa_squad-validation-57", "mrqa_squad-validation-5726", "mrqa_squad-validation-5750", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5806", "mrqa_squad-validation-5818", "mrqa_squad-validation-5852", "mrqa_squad-validation-5860", "mrqa_squad-validation-5865", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6031", "mrqa_squad-validation-6066", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6176", "mrqa_squad-validation-6206", "mrqa_squad-validation-6222", "mrqa_squad-validation-6229", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6319", "mrqa_squad-validation-6330", "mrqa_squad-validation-6347", "mrqa_squad-validation-6353", "mrqa_squad-validation-6355", "mrqa_squad-validation-6409", "mrqa_squad-validation-6439", "mrqa_squad-validation-6502", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6611", "mrqa_squad-validation-6649", "mrqa_squad-validation-6664", "mrqa_squad-validation-6694", "mrqa_squad-validation-6790", "mrqa_squad-validation-6815", "mrqa_squad-validation-6838", "mrqa_squad-validation-6875", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6898", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7036", "mrqa_squad-validation-7039", "mrqa_squad-validation-7064", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7228", "mrqa_squad-validation-7260", "mrqa_squad-validation-7261", "mrqa_squad-validation-7297", "mrqa_squad-validation-7332", "mrqa_squad-validation-7338", "mrqa_squad-validation-7357", "mrqa_squad-validation-7364", "mrqa_squad-validation-7368", "mrqa_squad-validation-7380", "mrqa_squad-validation-739", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7445", "mrqa_squad-validation-7457", "mrqa_squad-validation-7470", "mrqa_squad-validation-7492", "mrqa_squad-validation-7503", "mrqa_squad-validation-7525", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-762", "mrqa_squad-validation-7693", "mrqa_squad-validation-7700", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7775", "mrqa_squad-validation-7781", "mrqa_squad-validation-7785", "mrqa_squad-validation-779", "mrqa_squad-validation-7863", "mrqa_squad-validation-7871", "mrqa_squad-validation-7917", "mrqa_squad-validation-7943", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8016", "mrqa_squad-validation-8043", "mrqa_squad-validation-8093", "mrqa_squad-validation-8125", "mrqa_squad-validation-8154", "mrqa_squad-validation-8177", "mrqa_squad-validation-8184", "mrqa_squad-validation-8192", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8365", "mrqa_squad-validation-8414", "mrqa_squad-validation-8449", "mrqa_squad-validation-8459", "mrqa_squad-validation-8471", "mrqa_squad-validation-8484", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8568", "mrqa_squad-validation-8585", "mrqa_squad-validation-8661", "mrqa_squad-validation-8670", "mrqa_squad-validation-8670", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-8841", "mrqa_squad-validation-888", "mrqa_squad-validation-8904", "mrqa_squad-validation-8925", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8958", "mrqa_squad-validation-8985", "mrqa_squad-validation-908", "mrqa_squad-validation-9095", "mrqa_squad-validation-9161", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9403", "mrqa_squad-validation-9405", "mrqa_squad-validation-9446", "mrqa_squad-validation-9464", "mrqa_squad-validation-9556", "mrqa_squad-validation-957", "mrqa_squad-validation-9594", "mrqa_squad-validation-9615", "mrqa_squad-validation-9669", "mrqa_squad-validation-9716", "mrqa_squad-validation-9717", "mrqa_squad-validation-9764", "mrqa_squad-validation-9814", "mrqa_squad-validation-9816", "mrqa_squad-validation-9876", "mrqa_squad-validation-9907", "mrqa_squad-validation-9928", "mrqa_triviaqa-validation-2749", "mrqa_triviaqa-validation-4444", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7463"], "OKR": 0.9453125, "KG": 0.4609375, "before_eval_results": {"predictions": ["Northern Europe and the Mid-Atlantic", "$2 million", "fish stocks to collapse", "Chris Keates", "many castles and vineyards", "Cinerama Productions/Palomar theatrical library", "Antigone", "3.5 million", "Denver Broncos", "1997", "A \u2192 G deamination", "since 2001", "Streptococcus", "1784", "Narrow alleys", "another problem", "economic growth", "John and Benjamin Green", "1530", "installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators", "two", "the poor", "Irish", "Pearl Jam", "Grey's Anatomy", "black", "Bruce Springsteen", "Wounded Knee", "Greek National Opera Company", "Henry Moore", "The Teammates", "Charlotte", "an eagle", "Narcissus", "Fred Williamson", "Orange River", "a size 24 tapestry", "the Holy Grail", "Mellon Collie and the Infinite Sadness", "Albert", "Ludwig Van Beethoven", "Lake Victoria", "(quadrature) coincide with lunar apogee", "Root Beer", "FRAM", "The Country of the Pointed Firs and Other Stories", "Velvet Revolver", "Britain", "polar-front", "You Bet Your Life", "China", "Nova Scotia", "Breathless", "Narnia", "Franklin Pierce", "an estimated 120,000 Japanese immigrants and Nisei", "Michael Schumacher", "a four - page pamphlet in 1876", "baulk", "Glasgow", "Joely Richardson", "Hugh Dowding", "NATO fighters", "Congress"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6199900793650793}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.888888888888889, 1.0, 0.8571428571428571, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.26666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8990", "mrqa_squad-validation-5887", "mrqa_squad-validation-6655", "mrqa_squad-validation-7353", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3530", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-11388", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-8760", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-7269", "mrqa_searchqa-validation-4394", "mrqa_searchqa-validation-9148", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-6181", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-4307", "mrqa_triviaqa-validation-6896", "mrqa_hotpotqa-validation-1843"], "SR": 0.546875, "CSR": 0.6548295454545454, "retrieved_ids": ["mrqa_squad-train-51664", "mrqa_squad-train-77461", "mrqa_squad-train-34829", "mrqa_squad-train-63597", "mrqa_squad-train-58878", "mrqa_squad-train-25436", "mrqa_squad-train-4375", "mrqa_squad-train-26832", "mrqa_squad-train-25058", "mrqa_squad-train-44638", "mrqa_squad-train-21666", "mrqa_squad-train-70599", "mrqa_squad-train-56310", "mrqa_squad-train-74539", "mrqa_squad-train-40775", "mrqa_squad-train-32277", "mrqa_squad-train-35264", "mrqa_squad-train-74167", "mrqa_squad-train-62795", "mrqa_squad-train-35906", "mrqa_squad-train-79024", "mrqa_squad-train-5035", "mrqa_squad-train-1950", "mrqa_squad-train-2889", "mrqa_searchqa-validation-14480", "mrqa_triviaqa-validation-6548", "mrqa_squad-validation-5435", "mrqa_naturalquestions-validation-4002", "mrqa_searchqa-validation-5631", "mrqa_squad-validation-9166", "mrqa_searchqa-validation-5149", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-2102", "mrqa_squad-validation-6439", "mrqa_searchqa-validation-2568", "mrqa_triviaqa-validation-4881", "mrqa_squad-validation-7364", "mrqa_squad-validation-1240", "mrqa_squad-validation-6228", "mrqa_naturalquestions-validation-1863", "mrqa_searchqa-validation-11137", "mrqa_squad-validation-3687", "mrqa_squad-validation-7051", "mrqa_hotpotqa-validation-929", "mrqa_squad-validation-10369", "mrqa_searchqa-validation-9536"], "EFR": 0.9310344827586207, "Overall": 0.7562353056426332}, {"timecode": 11, "before_eval_results": {"predictions": ["the Horn of Africa", "Grumman", "the highest duty of a citizen", "1671", "St. Johns River", "canceled", "The President of the Council and a Commissioner can sit in on ECB meetings, but don't have voting rights", "Ismailiyah, Egypt", "phycobilisomes", "lupus erythematosus", "December 1878", "bars, caf\u00e9s and clubs", "both PNU and ODM camps", "O(n2)", "Bill Clinton", "the qu", "International Crops Research Institute for the Semi-Arid Tropics (ICRISAT)", "straight line", "Germany", "autoimmune", "January 26, 1996", "intelligent design", "Seoul", "2005", "December 24, 1973", "May 21, 2000", "the 100 metres", "January 2016", "seven", "Samuel Beckett", "Eilean Donan Castle", "Sonic Mania", "Homeland", "Carson City", "League of the Three Emperors", "Barack Obama", "Brian A. Miller", "Washington, D.C.", "December 13, 2015", "the Front Row media program", "1590", "Vixen", "Revolution Studios", "the Mach number", "1990", "Michael A. Cremo", "Gangsta's Paradise", "A41", "Bette Davis, Olivia de Havilland, Joseph Cotten, Agnes Moorehead and Mary Astor", "five", "Indiana", "Esteban Ocon", "ABC", "the British military", "National Lottery", "2018", "Alan Menken", "Milton Friedman", "Billy Wilder", "2-1", "a personal estate for its owner", "Central African Republic", "a pillar of salt", "Yahya Khan"], "metric_results": {"EM": 0.625, "QA-F1": 0.6750744047619048}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9912", "mrqa_squad-validation-6759", "mrqa_squad-validation-3113", "mrqa_squad-validation-4150", "mrqa_squad-validation-1713", "mrqa_squad-validation-4883", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-3944", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-10161", "mrqa_newsqa-validation-1700", "mrqa_searchqa-validation-11991", "mrqa_naturalquestions-validation-3485"], "SR": 0.625, "CSR": 0.65234375, "retrieved_ids": ["mrqa_squad-train-17606", "mrqa_squad-train-64181", "mrqa_squad-train-57874", "mrqa_squad-train-80835", "mrqa_squad-train-15053", "mrqa_squad-train-17555", "mrqa_squad-train-4887", "mrqa_squad-train-22913", "mrqa_squad-train-86428", "mrqa_squad-train-12661", "mrqa_squad-train-55112", "mrqa_squad-train-23917", "mrqa_squad-train-5954", "mrqa_squad-train-85325", "mrqa_squad-train-5905", "mrqa_squad-train-53699", "mrqa_squad-train-24140", "mrqa_squad-train-2847", "mrqa_squad-train-22680", "mrqa_squad-train-31658", "mrqa_squad-train-43148", "mrqa_squad-train-36360", "mrqa_squad-train-69297", "mrqa_squad-train-54892", "mrqa_searchqa-validation-8760", "mrqa_squad-validation-7876", "mrqa_squad-validation-3812", "mrqa_triviaqa-validation-6548", "mrqa_squad-validation-7700", "mrqa_squad-validation-1125", "mrqa_searchqa-validation-4169", "mrqa_squad-validation-6319", "mrqa_searchqa-validation-3613", "mrqa_squad-validation-10217", "mrqa_triviaqa-validation-6896", "mrqa_squad-validation-5435", "mrqa_newsqa-validation-3331", "mrqa_squad-validation-5887", "mrqa_squad-validation-4572", "mrqa_naturalquestions-validation-7095", "mrqa_squad-validation-7525", "mrqa_naturalquestions-validation-10039", "mrqa_squad-validation-2538", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-3863", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5817"], "EFR": 0.9583333333333334, "Overall": 0.7611979166666667}, {"timecode": 12, "before_eval_results": {"predictions": ["Sky Q Silver set top boxes", "water pump", "Tesla coil", "1946", "21 to 11", "Convening the Parliamentary Bureau", "Japan and Latin America", "sent missionaries, backed by a fund to financially reward converts to Catholicism", "Arizona Cardinals", "842 pounds", "the 1540s", "John Fox", "American Indians in the colony of Georgia", "orbit the Moon", "El Tem\u00fcr", "quickly", "the innate immune system versus the adaptive immune system", "March 1896", "The Lightning thief", "James `` Jamie '' Dornan", "W. Edwards Deming", "usually in May", "Human anatomy", "$657.4 million", "current day Poole Harbour towards mid-Channel", "Accounting Standards Board ( ASB )", "Ole Einar Bj\u00f8rndalen", "General George Washington", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Djokovic", "Longliners", "1952", "the dome", "1997", "British rock band Procol Harum", "Sheev Palpatine", "Charles Haley, five ( two as a linebacker with the San Francisco 49ers and three as a defensive end with the Dallas Cowboys", "punk rock", "the septum", "The White House Executive chef", "vaskania", "the church at Philippi", "10 May 1940", "Brenda", "bohrium", "prejudice in favour of or against one thing, person, or group compared with another", "the nasal septum", "a couple broken apart by the Iraq War", "Spanish American wars of independence", "Tristan Rogers", "Owen Vaccaro", "Walter Brennan", "1872", "Brad Johnson", "1992", "King Richard II of England", "Austria and Switzerland", "Chuck vs. First Class", "Gillian Anderson", "Spanish Davis Cup hero Fernando Verdasco", "18", "Swing Low, Sweet Chariot", "a stroke", "Puget Sound"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7023660714285713}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.88, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.4444444444444445, 0.19999999999999998, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.20000000000000004, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-978", "mrqa_squad-validation-9458", "mrqa_squad-validation-3130", "mrqa_squad-validation-3811", "mrqa_squad-validation-9863", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-10122", "mrqa_triviaqa-validation-4886", "mrqa_hotpotqa-validation-2009", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-765", "mrqa_searchqa-validation-9822"], "SR": 0.59375, "CSR": 0.6478365384615384, "retrieved_ids": ["mrqa_squad-train-36035", "mrqa_squad-train-62584", "mrqa_squad-train-58539", "mrqa_squad-train-15161", "mrqa_squad-train-30015", "mrqa_squad-train-22741", "mrqa_squad-train-74784", "mrqa_squad-train-25410", "mrqa_squad-train-70044", "mrqa_squad-train-52098", "mrqa_squad-train-29072", "mrqa_squad-train-47550", "mrqa_squad-train-43971", "mrqa_squad-train-46208", "mrqa_squad-train-33437", "mrqa_squad-train-48625", "mrqa_squad-train-24668", "mrqa_squad-train-4991", "mrqa_squad-train-68809", "mrqa_squad-train-15197", "mrqa_squad-train-33635", "mrqa_squad-train-25487", "mrqa_squad-train-77373", "mrqa_squad-train-61240", "mrqa_naturalquestions-validation-3332", "mrqa_searchqa-validation-16130", "mrqa_hotpotqa-validation-1534", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6106", "mrqa_squad-validation-9717", "mrqa_squad-validation-4539", "mrqa_searchqa-validation-3530", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-9715", "mrqa_searchqa-validation-686", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3392", "mrqa_squad-validation-8990", "mrqa_naturalquestions-validation-9712", "mrqa_squad-validation-6706", "mrqa_hotpotqa-validation-62", "mrqa_triviaqa-validation-2749", "mrqa_squad-validation-4883", "mrqa_naturalquestions-validation-2102", "mrqa_triviaqa-validation-6896", "mrqa_squad-validation-7700", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-5721"], "EFR": 0.9615384615384616, "Overall": 0.7609375}, {"timecode": 13, "before_eval_results": {"predictions": ["Protestantism", "Extension", "Riverside", "conservation of momentum", "Hamburg merchants and traders", "Department of Justice", "water flow through the body cavity", "67.9", "Fort Duquesne", "Sports Programs, Inc.", "quality rental units", "New England Patriots", "Edward Teller", "the geographical area it covers as well as the frequency of meeting", "to stay", "Andrew Lortie", "bilaterally symmetrical", "Thirty years after the Galactic Civil War", "a series of structural rearrangements in the RTK that lead to its enzymatic activation", "eight years", "Leonard Nimoy", "Longline fishing", "various locations in Redford's adopted home state of Utah", "Stephen A. Douglas", "April 2011", "around 1940", "Las Vegas Stadium", "Herman Hollerith", "Dr. Sachchidananda Sinha", "Ron Harper", "hairpin corner", "over two days", "IB Diploma Program and the IB Career - related Program for students aged 15 to 18", "when the cell is undergoing the metaphase of cell division", "it activates a relay which will handle the higher current load", "Donald Trump", "Liam Cunningham", "the spectroscopic notation for the associated atomic orbitals : sharp, principal, diffuse and fundamental, and then g which follows f in the alphabet", "Betty", "moral", "a pole", "Sauron's", "Gustav Bauer", "2002", "Mohammad Reza Pahlavi", "on the southeastern coast of the Commonwealth of Virginia in the United States", "a convergent plate boundary", "Jourdan Miller", "10,605", "84", "1773", "Jesse McCartney", "73", "Mars Hill, 150 miles ( 240 km ) to the northeast", "2005", "Catherine Zeta-Jones", "Michael Crawford", "264,152", "10,000", "those missing", "Mormon Tabernacle Choir", "carbon dioxide", "Pickwick", "Sunshine State"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5984363753936122}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.15384615384615383, 1.0, 0.21052631578947367, 1.0, 0.19999999999999998, 1.0, 0.0, 0.8, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.4799999999999999, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-259", "mrqa_squad-validation-4435", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-3066", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-585", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-5500", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4097", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2765", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-1166"], "SR": 0.515625, "CSR": 0.6383928571428572, "retrieved_ids": ["mrqa_squad-train-1291", "mrqa_squad-train-31296", "mrqa_squad-train-9795", "mrqa_squad-train-21596", "mrqa_squad-train-23626", "mrqa_squad-train-74936", "mrqa_squad-train-65379", "mrqa_squad-train-26686", "mrqa_squad-train-6696", "mrqa_squad-train-10381", "mrqa_squad-train-42282", "mrqa_squad-train-67636", "mrqa_squad-train-20423", "mrqa_squad-train-28763", "mrqa_squad-train-37049", "mrqa_squad-train-51513", "mrqa_squad-train-11527", "mrqa_squad-train-59671", "mrqa_squad-train-76353", "mrqa_squad-train-53214", "mrqa_squad-train-61529", "mrqa_squad-train-73625", "mrqa_squad-train-49115", "mrqa_squad-train-22208", "mrqa_squad-validation-3946", "mrqa_naturalquestions-validation-9597", "mrqa_squad-validation-7700", "mrqa_naturalquestions-validation-4674", "mrqa_squad-validation-6759", "mrqa_hotpotqa-validation-4815", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-3672", "mrqa_squad-validation-8769", "mrqa_squad-validation-8958", "mrqa_hotpotqa-validation-2213", "mrqa_squad-validation-1713", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-5721", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-8711", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-15194", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-3564", "mrqa_newsqa-validation-2112"], "EFR": 1.0, "Overall": 0.7667410714285714}, {"timecode": 14, "before_eval_results": {"predictions": ["Tulku", "The second, third, and fourth timelines", "King George's War", "Brad Nortman", "Behind the Sofa", "BBC Dead Ringers", "1206", "Louis Pasteur", "The Brain of Morbius (1976)", "the depths of the oceans and seas", "two", "a mainline Protestant Methodist denomination", "Albert Einstein", "Vince Lombardi Trophy", "death in body and soul", "Candice Susan Swanepoel", "AT&T", "Australian", "German", "Chris Anderson", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "1949", "\"Crooked Little Vein\"", "Australian", "Humphrey Goodman", "Jena Malone", "John M. Dowd", "twelfth", "Hawaii Republican", "New York", "Polly Hatchet", "tragedy", "Cricket fighting", "14th Street", "bass", "Ed Asner", "2012", "New Orleans, Louisiana", "Henry Hill", "May 4, 1924", "Australian", "1966", "2012", "1926", "27th congressional district", "Charles Messina", "mother goddess", "VAQ-135", "1892", "Ludwig van Beethoven", "Sam Phillips", "Manchester United", "Saudi", "1942", "October 6, 2017", "at a given temperature", "wolf", "Ganges", "January 24, 2006", "repression and dire economic circumstances", "a pager", "Baltimore", "April 6, 1917", "in the Blue Ridge Mountains of Virginia"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6388629006227691}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.10526315789473684]}}, "before_error_ids": ["mrqa_squad-validation-5070", "mrqa_squad-validation-8229", "mrqa_squad-validation-7688", "mrqa_squad-validation-218", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-2887", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-10613", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-1813"], "SR": 0.5625, "CSR": 0.6333333333333333, "retrieved_ids": ["mrqa_squad-train-75746", "mrqa_squad-train-65017", "mrqa_squad-train-41480", "mrqa_squad-train-34625", "mrqa_squad-train-32355", "mrqa_squad-train-47301", "mrqa_squad-train-41483", "mrqa_squad-train-55786", "mrqa_squad-train-32621", "mrqa_squad-train-28424", "mrqa_squad-train-1391", "mrqa_squad-train-23657", "mrqa_squad-train-8892", "mrqa_squad-train-27058", "mrqa_squad-train-32249", "mrqa_squad-train-6396", "mrqa_squad-train-61110", "mrqa_squad-train-54478", "mrqa_squad-train-74479", "mrqa_squad-train-47699", "mrqa_squad-train-76520", "mrqa_squad-train-81423", "mrqa_squad-train-77552", "mrqa_squad-train-76058", "mrqa_newsqa-validation-429", "mrqa_squad-validation-4838", "mrqa_squad-validation-2538", "mrqa_naturalquestions-validation-3332", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-4169", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8116", "mrqa_squad-validation-4715", "mrqa_naturalquestions-validation-6046", "mrqa_squad-validation-3770", "mrqa_searchqa-validation-14194", "mrqa_squad-validation-8459", "mrqa_squad-validation-3019", "mrqa_naturalquestions-validation-5531", "mrqa_squad-validation-1938", "mrqa_hotpotqa-validation-3304", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5579", "mrqa_triviaqa-validation-1166", "mrqa_naturalquestions-validation-8983", "mrqa_searchqa-validation-5725", "mrqa_squad-validation-10369"], "EFR": 1.0, "Overall": 0.7657291666666667}, {"timecode": 15, "before_eval_results": {"predictions": ["1937", "trade unions", "2014", "\"Journey's End\"", "tech-oriented", "John Houghton", "heterokontophyte", "NP-complete", "Chinggis", "128,843", "a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "56.2%", "11 points", "Chaplain to the Forces", "The 10500000 sqft Venetian Macao", "Eddie Albert", "1st Earl Mountbatten of Burma", "Alcorn State", "David", "The Today Show", "Philadelphia, Pennsylvania", "12", "A41", "Royce da 5'9\" (Bad) and Eminem (Evil)", "Pimp My Ride", "1998", "casting, job opportunities, and career advice", "Mary Harron", "Flashback", "Eenasul Fateh", "Chicago", "Australia", "2014", "Second World War", "Lismore, New South Wales", "rural areas", "child actor", "Summerlin, Nevada", "Jack Binion", "YG Entertainment", "water sprite", "Noel", "the \"Pour le M\u00e9rite\" 1", "Trey Parker and Matt Stone", "Riot Act", "Aqua", "American Longhair", "four", "Christy Walton", "Lt. Gen. Ulysses S. Grant", "Hechingen in Swabia", "N.I.B.", "manager", "8,211", "Tim Allen", "the cell nucleus", "a Bristol Box Kite", "1961", "a drug reportedly found after Michael Jackson's death", "the South Dakota State Penitentiary", "Douglas Fir", "drink wine or kiss a fool", "military commissions", "Philip Markoff"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6113095238095239}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.0, 0.5, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.8, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4298", "mrqa_squad-validation-257", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2558", "mrqa_triviaqa-validation-7461", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-13595", "mrqa_searchqa-validation-1757", "mrqa_newsqa-validation-4202"], "SR": 0.46875, "CSR": 0.623046875, "retrieved_ids": ["mrqa_squad-train-45899", "mrqa_squad-train-1860", "mrqa_squad-train-50432", "mrqa_squad-train-56937", "mrqa_squad-train-80875", "mrqa_squad-train-78066", "mrqa_squad-train-80957", "mrqa_squad-train-56111", "mrqa_squad-train-10229", "mrqa_squad-train-62330", "mrqa_squad-train-23476", "mrqa_squad-train-81755", "mrqa_squad-train-26647", "mrqa_squad-train-83388", "mrqa_squad-train-70218", "mrqa_squad-train-67444", "mrqa_squad-train-8911", "mrqa_squad-train-69406", "mrqa_squad-train-33727", "mrqa_squad-train-77444", "mrqa_squad-train-32551", "mrqa_squad-train-10528", "mrqa_squad-train-29197", "mrqa_squad-train-16590", "mrqa_searchqa-validation-11367", "mrqa_triviaqa-validation-6548", "mrqa_squad-validation-3985", "mrqa_naturalquestions-validation-7767", "mrqa_searchqa-validation-8711", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-4193", "mrqa_hotpotqa-validation-4712", "mrqa_naturalquestions-validation-430", "mrqa_searchqa-validation-6463", "mrqa_hotpotqa-validation-2117", "mrqa_triviaqa-validation-4307", "mrqa_naturalquestions-validation-5817", "mrqa_squad-validation-6655", "mrqa_naturalquestions-validation-585", "mrqa_naturalquestions-validation-5583", "mrqa_squad-validation-7364", "mrqa_naturalquestions-validation-8277", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-1843", "mrqa_naturalquestions-validation-81", "mrqa_searchqa-validation-14480", "mrqa_hotpotqa-validation-5889"], "EFR": 0.9705882352941176, "Overall": 0.7577895220588236}, {"timecode": 16, "before_eval_results": {"predictions": ["civil, military, and censorial offices", "Puritan", "James Wolfe", "March 1974", "2003", "Frederick II the Great", "Lower taxes, increased economic development, unification of the community, better public spending and effective administration by a more central authority", "the Armenian state", "redistributive taxation", "Seattle Seahawks", "paid professionals", "polynomial-time reduction", "revelry", "Krishna Rajaram", "Padre Alberto", "the BBC", "1-0", "Choi", "second-degree aggravated battery", "John McCain", "one day", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "be silent", "200", "2,000", "several weeks", "auction off one of the earliest versions of the Magna Carta later this year", "it pulls the scab and it cracks, and it starts to bleed.\"", "Michael Jackson", "Caylee Anthony", "10 below in Chicago, Illlinois", "Barbara Williams", "Manmohan Singh", "jazz", "1983", "cancer", "Al-Shabaab", "Casalesi Camorra", "KVBC", "Appathurai", "at Eintracht Frankfurt", "opium poppies", "The Bronx County District Attorneys Office", "1,073", "Arthur E. Morgan III", "Arturo Gonzalez Rodriguez", "Las Vegas", "Pakistan", "the crew of the Bainbridge", "Akio Toyoda", "boy Wizard", "Draquila -- Italy Trembles", "India", "Mumbai", "Miami Heat", "a combination of genetics and the male hormone dihydrotestosterone", "Senegal", "Windermere", "Field of Dreams", "Bill Paxton", "a star and a stripe", "KLM", "Sex Pistols", "an ambitious Jewish boy growing up in a poor neighborhood in Montreal"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5964600503663005}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7296", "mrqa_squad-validation-1136", "mrqa_squad-validation-1765", "mrqa_newsqa-validation-3982", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-4966", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-4356"], "SR": 0.53125, "CSR": 0.6176470588235294, "retrieved_ids": ["mrqa_squad-train-16933", "mrqa_squad-train-84055", "mrqa_squad-train-52291", "mrqa_squad-train-47570", "mrqa_squad-train-67629", "mrqa_squad-train-16487", "mrqa_squad-train-77837", "mrqa_squad-train-70442", "mrqa_squad-train-72829", "mrqa_squad-train-39790", "mrqa_squad-train-14820", "mrqa_squad-train-72004", "mrqa_squad-train-34834", "mrqa_squad-train-41885", "mrqa_squad-train-25352", "mrqa_squad-train-72753", "mrqa_squad-train-14706", "mrqa_squad-train-33115", "mrqa_squad-train-9228", "mrqa_squad-train-1568", "mrqa_squad-train-5690", "mrqa_squad-train-32998", "mrqa_squad-train-9125", "mrqa_squad-train-61829", "mrqa_searchqa-validation-6181", "mrqa_hotpotqa-validation-1173", "mrqa_naturalquestions-validation-2969", "mrqa_squad-validation-10388", "mrqa_naturalquestions-validation-9712", "mrqa_searchqa-validation-14480", "mrqa_naturalquestions-validation-9130", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-5631", "mrqa_newsqa-validation-3331", "mrqa_hotpotqa-validation-2800", "mrqa_squad-validation-8819", "mrqa_squad-validation-2943", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-2124", "mrqa_hotpotqa-validation-1739", "mrqa_naturalquestions-validation-6500", "mrqa_searchqa-validation-13151", "mrqa_naturalquestions-validation-123", "mrqa_squad-validation-257", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-9753", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-3613"], "EFR": 1.0, "Overall": 0.7625919117647059}, {"timecode": 17, "before_eval_results": {"predictions": ["higher economic inequality", "Ersatzschulen", "high schools", "a glass case suspended from the lid", "phagocytic cells", "2000", "five", "weight", "Leukocytes", "3D printing technology", "Ong Khan", "colonel", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "\"The oceans are kind of the last frontier for use and development,\"", "Wigan Athletic", "Vertikal-T", "Graeme Smith", "228", "the Bush administration's controversial system of military trials for some Guant Bay detainees.", "and her 4-year-old brother", "St. Francis De Sales", "The Tinkler", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "air support", "power lines downed by Saturday's winds,", "African National Congress", "United States", "bikinis", "1959", "Adam Yahiye Gadahn,", "Mark Sanford", "150", "weren't taking it well.", "the equator", "Chinese President Hu Jintao", "183", "warning", "Too many glass shards left by beer drinkers", "Cirque du Soleil", "criminal or disciplinary proceedings initiated", "11th year in a row", "two paintings by Pablo Picasso,", "fastest circumnavigation of the globe in a powerboat", "Guinea, Myanmar, Sudan and Venezuela.", "Austin Wuennenberg", "Diversity", "\"involuntarily medicated without court order,\"", "Oaxaca", "buckling under pressure from the ruling party.", "a polo match", "more than 100", "pulling on the top-knot of an opponent,", "Alfredo Astiz,", "MacFarlane", "convert single - stranded genomic RNA into double - stranded cDNA", "Harrison Ford", "Andes", "Peter Robert Auty", "2009", "Lassie", "Marilyn Monroe", "The Who", "The Band Concert", "Taco Bell"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5434526984691459}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 0.16666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8421052631578948, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4864864864864865, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2000", "mrqa_squad-validation-7845", "mrqa_squad-validation-6477", "mrqa_newsqa-validation-412", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-140", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1123", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-1968", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-15877"], "SR": 0.46875, "CSR": 0.609375, "retrieved_ids": ["mrqa_squad-train-14038", "mrqa_squad-train-30207", "mrqa_squad-train-12036", "mrqa_squad-train-10709", "mrqa_squad-train-81436", "mrqa_squad-train-25174", "mrqa_squad-train-2225", "mrqa_squad-train-54904", "mrqa_squad-train-21456", "mrqa_squad-train-47893", "mrqa_squad-train-69431", "mrqa_squad-train-86025", "mrqa_squad-train-32497", "mrqa_squad-train-52284", "mrqa_squad-train-54116", "mrqa_squad-train-46150", "mrqa_squad-train-48147", "mrqa_squad-train-36675", "mrqa_squad-train-66334", "mrqa_squad-train-19230", "mrqa_squad-train-43037", "mrqa_squad-train-10018", "mrqa_squad-train-7128", "mrqa_squad-train-66684", "mrqa_naturalquestions-validation-1911", "mrqa_searchqa-validation-15202", "mrqa_squad-validation-2538", "mrqa_searchqa-validation-5149", "mrqa_newsqa-validation-1538", "mrqa_squad-validation-4883", "mrqa_naturalquestions-validation-276", "mrqa_squad-validation-6965", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-132", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4815", "mrqa_naturalquestions-validation-6426", "mrqa_searchqa-validation-12864", "mrqa_squad-validation-1863", "mrqa_searchqa-validation-13600", "mrqa_squad-validation-8819", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9235", "mrqa_squad-validation-4539", "mrqa_searchqa-validation-12611", "mrqa_hotpotqa-validation-5014"], "EFR": 1.0, "Overall": 0.7609375}, {"timecode": 18, "before_eval_results": {"predictions": ["Jason Bourne", "the condenser", "1999", "mesoglea", "a body of treaties and legislation, such as Regulations and Directives", "liquid not as a gas", "socially", "Mark Twain", "amylopectin starch granules", "Tower District", "did not change the threat level,", "pro-democracy activists clashed Friday with Egyptian security forces in central Cairo,", "a construction site in the heart of Los Angeles", "overthrow the socialist government of Salvador Allende in Chile,", "The Pilgrims sail to Plymouth Rock", "a rally", "2,000", "Michael Schumacher", "Ventures", "seven", "Black History Month", "resigned", "\"I'm just getting started.\"", "21,", "diplomatic relations", "hand-painted Swedish wooden clogs", "Daniel Radcliffe", "Muslim", "five", "her mother", "$10 billion", "Six members of Zoe's Ark", "Galveston, Texas,", "9-week-old", "14 unapproved pain-relief drugs.", "Lucky Dube,", "children's books", "David Bowie", "40", "NATO", "Lindsey Vonn", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "a hunting party of three men,", "International Polo Club Palm Beach in Florida,", "Nevaeh (heaven spelled backward) for girls, and Messiah for boys.", "HPV (human papillomavirus)", "poor families", "88", "creation of an Islamic emirate in Gaza,", "an \"unnamed international terror group\"", "Nicole", "Manchester City", "al-Maqdessi's", "2015", "initially absent from the original game release, but were added in the January 2017 patch", "-30", "sweater", "Shooter Jennings", "people working in film and the performing arts", "Coppertone", "the bedroom", "Marlborough", "1968", "The Krypto Report"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6356346344627595}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.4444444444444445, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2564102564102564, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.375, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3444", "mrqa_squad-validation-3447", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-3697", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-2733", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-5209", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-4044", "mrqa_hotpotqa-validation-3428"], "SR": 0.5625, "CSR": 0.606907894736842, "retrieved_ids": ["mrqa_squad-train-33063", "mrqa_squad-train-74477", "mrqa_squad-train-45765", "mrqa_squad-train-73392", "mrqa_squad-train-28087", "mrqa_squad-train-46763", "mrqa_squad-train-39293", "mrqa_squad-train-81484", "mrqa_squad-train-5748", "mrqa_squad-train-11639", "mrqa_squad-train-23722", "mrqa_squad-train-78402", "mrqa_squad-train-64123", "mrqa_squad-train-50661", "mrqa_squad-train-11449", "mrqa_squad-train-77117", "mrqa_squad-train-217", "mrqa_squad-train-1264", "mrqa_squad-train-49886", "mrqa_squad-train-67384", "mrqa_squad-train-23291", "mrqa_squad-train-71251", "mrqa_squad-train-7939", "mrqa_squad-train-39573", "mrqa_naturalquestions-validation-7896", "mrqa_squad-validation-2595", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-81", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-1879", "mrqa_squad-validation-3946", "mrqa_newsqa-validation-2765", "mrqa_squad-validation-6655", "mrqa_hotpotqa-validation-4418", "mrqa_squad-validation-9458", "mrqa_newsqa-validation-1805", "mrqa_squad-validation-1780", "mrqa_searchqa-validation-4394", "mrqa_naturalquestions-validation-5960", "mrqa_squad-validation-3770", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7242", "mrqa_squad-validation-1195", "mrqa_hotpotqa-validation-16", "mrqa_naturalquestions-validation-6461", "mrqa_squad-validation-6706", "mrqa_naturalquestions-validation-1987", "mrqa_squad-validation-8904"], "EFR": 1.0, "Overall": 0.7604440789473684}, {"timecode": 19, "before_eval_results": {"predictions": ["the blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "an Executive Committee", "New Orleans", "the death of Elisabeth Sladen", "NFL Experience", "English and Swahili", "61%", "plastoglobulus", "three", "Turkey", "Wombat", "Sri Lanka", "gestation", "Manchester", "The Hope Diamond", "Gin Rummy", "Pilate", "enamel", "bone", "\"help yourself to happiness\"", "Battle of Hastings", "the Caspian Sea", "a work journal", "kabbalah", "Gannett", "a nickname", "\"Don Juan De Marco\"", "Fes", "the 1998 World Cup final", "Interlaken", "Mystic Pizza", "Princeton", "Mandy", "The top 100 largest libraries in the United States", "Malay Peninsulamakes", "Herman Wouk", "Frederick IV", "Richard Saunders", "poetry", "Napoleon Bonaparte", "stegosaurus", "fifth", "thermodynamics", "Derek Smalls", "Dalits", "Harry Houdini", "Mary Steenburgen", "\"Double\"", "Sporcle", "(Willa) van Gogh", "Camembert", "James Ross Clemens", "a hole in one", "1987", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "Hoyo de Monterrey", "\"Thrilla in Manila\"", "North America", "841", "Ike", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "Harlem River", "modern state system", "Cairo, Illinois"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5305216827745898}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.13953488372093023, 0.0, 0.5, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-3082", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-16558", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-13554", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-5938", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-2769", "mrqa_newsqa-validation-3214", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3413"], "SR": 0.421875, "CSR": 0.59765625, "retrieved_ids": ["mrqa_squad-train-18986", "mrqa_squad-train-69834", "mrqa_squad-train-73456", "mrqa_squad-train-54572", "mrqa_squad-train-2940", "mrqa_squad-train-2380", "mrqa_squad-train-50431", "mrqa_squad-train-8687", "mrqa_squad-train-45012", "mrqa_squad-train-2519", "mrqa_squad-train-75119", "mrqa_squad-train-937", "mrqa_squad-train-62256", "mrqa_squad-train-9547", "mrqa_squad-train-3339", "mrqa_squad-train-30290", "mrqa_squad-train-42232", "mrqa_squad-train-30789", "mrqa_squad-train-75033", "mrqa_squad-train-34696", "mrqa_squad-train-61645", "mrqa_squad-train-19743", "mrqa_squad-train-71494", "mrqa_squad-train-70988", "mrqa_naturalquestions-validation-6524", "mrqa_squad-validation-4629", "mrqa_triviaqa-validation-7461", "mrqa_searchqa-validation-3960", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-1974", "mrqa_searchqa-validation-5631", "mrqa_naturalquestions-validation-7242", "mrqa_newsqa-validation-2142", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-5960", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-2541", "mrqa_naturalquestions-validation-9436", "mrqa_squad-validation-8904", "mrqa_squad-validation-9176", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3476", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-8277", "mrqa_newsqa-validation-1218", "mrqa_naturalquestions-validation-2452", "mrqa_hotpotqa-validation-2213"], "EFR": 0.972972972972973, "Overall": 0.7531883445945946}, {"timecode": 20, "UKR": 0.779296875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1173", "mrqa_hotpotqa-validation-1252", "mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1739", "mrqa_hotpotqa-validation-1742", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1890", "mrqa_hotpotqa-validation-1967", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3347", "mrqa_hotpotqa-validation-3519", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-4", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4344", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5328", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4906", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9088", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-955", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1080", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-386", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-782", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-920", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-1053", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11270", "mrqa_searchqa-validation-11395", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13585", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-14361", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-14569", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-16130", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2100", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2568", "mrqa_searchqa-validation-2607", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4469", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5172", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5755", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7059", "mrqa_searchqa-validation-7086", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-971", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10143", "mrqa_squad-validation-10168", "mrqa_squad-validation-10241", "mrqa_squad-validation-10266", "mrqa_squad-validation-10370", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1141", "mrqa_squad-validation-115", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-120", "mrqa_squad-validation-1254", "mrqa_squad-validation-127", "mrqa_squad-validation-1288", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1747", "mrqa_squad-validation-1765", "mrqa_squad-validation-1827", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-1953", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2050", "mrqa_squad-validation-2059", "mrqa_squad-validation-218", "mrqa_squad-validation-22", "mrqa_squad-validation-2243", "mrqa_squad-validation-2248", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2379", "mrqa_squad-validation-2383", "mrqa_squad-validation-2411", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2463", "mrqa_squad-validation-2467", "mrqa_squad-validation-2538", "mrqa_squad-validation-2545", "mrqa_squad-validation-257", "mrqa_squad-validation-2589", "mrqa_squad-validation-2595", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2886", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3019", "mrqa_squad-validation-305", "mrqa_squad-validation-3052", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3358", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3567", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3864", "mrqa_squad-validation-3917", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3975", "mrqa_squad-validation-3986", "mrqa_squad-validation-3994", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4066", "mrqa_squad-validation-4175", "mrqa_squad-validation-4187", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4509", "mrqa_squad-validation-4530", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4583", "mrqa_squad-validation-4629", "mrqa_squad-validation-4715", "mrqa_squad-validation-4883", "mrqa_squad-validation-5004", "mrqa_squad-validation-5014", "mrqa_squad-validation-5097", "mrqa_squad-validation-5110", "mrqa_squad-validation-5140", "mrqa_squad-validation-5237", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5453", "mrqa_squad-validation-5479", "mrqa_squad-validation-5572", "mrqa_squad-validation-5588", "mrqa_squad-validation-5604", "mrqa_squad-validation-5677", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5859", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-5960", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6347", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6551", "mrqa_squad-validation-6594", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6951", "mrqa_squad-validation-6957", "mrqa_squad-validation-6965", "mrqa_squad-validation-6999", "mrqa_squad-validation-7034", "mrqa_squad-validation-7039", "mrqa_squad-validation-7051", "mrqa_squad-validation-71", "mrqa_squad-validation-7125", "mrqa_squad-validation-7136", "mrqa_squad-validation-7192", "mrqa_squad-validation-7390", "mrqa_squad-validation-7422", "mrqa_squad-validation-7449", "mrqa_squad-validation-7521", "mrqa_squad-validation-7576", "mrqa_squad-validation-7608", "mrqa_squad-validation-7612", "mrqa_squad-validation-7613", "mrqa_squad-validation-7618", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7708", "mrqa_squad-validation-7751", "mrqa_squad-validation-7814", "mrqa_squad-validation-7863", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7881", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7982", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-8282", "mrqa_squad-validation-829", "mrqa_squad-validation-8309", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8471", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8670", "mrqa_squad-validation-8710", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8809", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9095", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9367", "mrqa_squad-validation-9405", "mrqa_squad-validation-942", "mrqa_squad-validation-9594", "mrqa_squad-validation-9614", "mrqa_squad-validation-9669", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-4881", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-6421", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-7496"], "OKR": 0.90625, "KG": 0.48828125, "before_eval_results": {"predictions": ["the main contractor", "widespread education", "300", "an attack on New France's capital, Quebec", "two-thirds", "Decompression sickness", "1979", "Parliament Square, High Street and George IV Bridge", "1959", "the Superdome", "grizzly bear", "Dracula", "Nancy Spungen", "Nitrous oxide", "Tchaikovsky's", "Frederic Remington", "the Mosquito Coast", "Arkansas", "an object oriented programming", "2010", "the genie", "a Whig", "\"ER\"", "a yellow lotus", "a genie", "The Princess Diaries", "Arkansas", "Mao Zedong", "Doner", "a genie", "Wells Fargo", "the Sundance Kid", "a house of prayer", "amber", "Holly", "Umbria", "a Roth 401(k)", "Quentin Tarantino", "the Palatine Hill", "Kentucky", "a statement that is taken to be true, to serve as a premise or starting point for further reasoning and arguments", "the second Sunday in March", "a skirt", "the Airplane", "Scooter Libby", "thesaurus", "Rose", "Equatorial Guinea", "Bob Crane", "bowling ball", "Cecil John Rhodes", "Aerobic", "Anaheim", "Steve Hale", "epidemiology", "Belgium", "Jack", "the 137th edition", "Merck", "semiconductors", "English and Russian", "France", "a Hungarian Horntail", "Phil Mickelson"], "metric_results": {"EM": 0.5, "QA-F1": 0.5947916666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4918", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-3817", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-16826", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-12996", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-12477", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-3514", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-4992", "mrqa_hotpotqa-validation-409", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-4118"], "SR": 0.5, "CSR": 0.5930059523809523, "retrieved_ids": ["mrqa_squad-train-24800", "mrqa_squad-train-60723", "mrqa_squad-train-51531", "mrqa_squad-train-85777", "mrqa_squad-train-25679", "mrqa_squad-train-6453", "mrqa_squad-train-29838", "mrqa_squad-train-53554", "mrqa_squad-train-27581", "mrqa_squad-train-64868", "mrqa_squad-train-22083", "mrqa_squad-train-59335", "mrqa_squad-train-22103", "mrqa_squad-train-45805", "mrqa_squad-train-68259", "mrqa_squad-train-82428", "mrqa_squad-train-67210", "mrqa_squad-train-85827", "mrqa_squad-train-79929", "mrqa_squad-train-78281", "mrqa_squad-train-73397", "mrqa_squad-train-72006", "mrqa_squad-train-78227", "mrqa_squad-train-4920", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-6524", "mrqa_searchqa-validation-6463", "mrqa_hotpotqa-validation-230", "mrqa_squad-validation-3497", "mrqa_hotpotqa-validation-3721", "mrqa_naturalquestions-validation-3413", "mrqa_squad-validation-6439", "mrqa_squad-validation-978", "mrqa_squad-validation-8969", "mrqa_squad-validation-6409", "mrqa_searchqa-validation-16378", "mrqa_squad-validation-7876", "mrqa_newsqa-validation-1538", "mrqa_squad-validation-7357", "mrqa_squad-validation-10369", "mrqa_squad-validation-4572", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_hotpotqa-validation-1011", "mrqa_squad-validation-1672", "mrqa_searchqa-validation-4367", "mrqa_naturalquestions-validation-5583"], "EFR": 1.0, "Overall": 0.7533668154761906}, {"timecode": 21, "before_eval_results": {"predictions": ["a lesson plan", "laws of physics", "1893", "Welsh", "the people themselves", "criminal", "a monthly subscription", "15,000 BC", "novella", "the President of the United States", "above the light source and under the sample in an upright microscope", "November 3, 2007", "1939", "April 1917", "1959", "the orphanage where he was raised", "September 19 - 22, 2017", "bypasses", "can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Bobby Eli, John Freeman and Vinnie Barrett", "Nagar Haveli", "Dick Rutan and Jeana Yeager", "Paracelsus", "January 2004", "members of the gay ( LGBT ) community", "late January or early February", "they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Jerry Lee Lewis", "push the food down the esophagus", "Splodgenessabounds", "offensive player's feet are slightly wider than shoulder width and slightly on the balls of their feet", "Edd Kimber, Joanne Wheatley, John Whaite, Frances Quinn, Nancy Birtwhistle, Nadiya Hussain, Candice Brown and Stephanie Faldo", "a white one", "A diastema ( plural diastemata )", "Eddie Murphy", "video game series", "high officials", "flour and water", "San Francisco 49ers", "Gupta Empire", "card verification value ( CVV )", "T - Bone Walker", "Ray Charles", "Francis Hutcheson", "1937", "Cairo, Illinois", "Barbara Windsor", "British", "Gladys Knight & the Pips", "the Executive Residence of the White House Complex", "July 2014", "The courts", "Kanawha River", "athletics", "isosceles", "1898", "James I", "WFTV", "tennis", "Christopher Darden", "Austria", "different women coping with breast cancer", "Harry Nicolaides", "he reached under the counter, grabbed his gun and told the robber to drop the bat and get down on his knees."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6639959305952152}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6428571428571429, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10714285714285714, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5000000000000001, 0.9863013698630138, 0.0, 1.0, 1.0, 0.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 0.28571428571428575, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.07692307692307693]}}, "before_error_ids": ["mrqa_squad-validation-2336", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9330", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-4715", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-1985"], "SR": 0.59375, "CSR": 0.5930397727272727, "retrieved_ids": ["mrqa_squad-train-48384", "mrqa_squad-train-717", "mrqa_squad-train-70812", "mrqa_squad-train-13412", "mrqa_squad-train-35694", "mrqa_squad-train-1455", "mrqa_squad-train-47449", "mrqa_squad-train-80539", "mrqa_squad-train-48953", "mrqa_squad-train-41798", "mrqa_squad-train-66279", "mrqa_squad-train-38882", "mrqa_squad-train-82579", "mrqa_squad-train-69123", "mrqa_squad-train-360", "mrqa_squad-train-17016", "mrqa_squad-train-41794", "mrqa_squad-train-39773", "mrqa_squad-train-46287", "mrqa_squad-train-10371", "mrqa_squad-train-40442", "mrqa_squad-train-77008", "mrqa_squad-train-35762", "mrqa_squad-train-34113", "mrqa_naturalquestions-validation-5986", "mrqa_searchqa-validation-15770", "mrqa_hotpotqa-validation-5604", "mrqa_naturalquestions-validation-8356", "mrqa_squad-validation-3687", "mrqa_searchqa-validation-6445", "mrqa_newsqa-validation-765", "mrqa_triviaqa-validation-6896", "mrqa_newsqa-validation-3679", "mrqa_squad-validation-5262", "mrqa_triviaqa-validation-5209", "mrqa_searchqa-validation-2714", "mrqa_hotpotqa-validation-3090", "mrqa_naturalquestions-validation-7767", "mrqa_newsqa-validation-1330", "mrqa_squad-validation-3770", "mrqa_naturalquestions-validation-6148", "mrqa_squad-validation-6228", "mrqa_searchqa-validation-3514", "mrqa_naturalquestions-validation-2653", "mrqa_searchqa-validation-13600", "mrqa_searchqa-validation-12363", "mrqa_hotpotqa-validation-4712", "mrqa_squad-validation-8459"], "EFR": 0.8846153846153846, "Overall": 0.7302966564685315}, {"timecode": 22, "before_eval_results": {"predictions": ["literacy and numeracy", "bark of mulberry trees", "drama series", "1806", "\"distributive efficiency\"", "on issues related to the substance of the statement", "Tokyo", "Continental drift", "Frank Oz", "1975", "775", "Kimberlin Brown", "AD 95 -- 110", "status line", "the disk, about 26,000 light - years from the Galactic Center, on the inner edge of the Nebula Arm, one of the spiral - shaped concentrations of gas and dust", "permanently absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "handheld subscriber equipment", "Powderham Castle ( staircase, hall, music room, bedroom ; used for the aqua - turquoise stairway scenes ), Corsham Court ( library and dining room )", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "Javier Fern\u00e1ndez", "a lightning strike", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen", "Coton in the Elms", "a stem", "Wakanda and the Savage Land", "1992", "active osmotic water absorption", "shortwave radio", "a place of trade, entertainment, and education", "Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway", "Robert Hooke", "in the interstellar medium ( ISM ) and giant molecular clouds ( GMC )", "Alicia Vikander", "Holden Nowell", "Kylie Jenner's first child", "approximately 5 liters", "somatic ( body ) cell", "betty or veronica", "results from rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "June 8, 2009", "head - up display", "presidential representative democratic republic", "Ferm\u00edn Francisco de Lasu\u00e9n", "a moral tale", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2009", "Spanish / Basque origin", "Laura Jane Haddock", "Atlanta", "2002", "cartilage", "a coffee house", "Chief Inspector of Prisons", "Cheshire", "#364", "24800 mi long", "liberal revolutions of 1848", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Matamoros, Mexico", "The incident Sunday evening", "X-Files", "biometrics", "the Great Seal of North Dakota"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6673103989575102}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6842105263157895, 0.9600000000000001, 0.0, 0.1904761904761905, 0.0, 1.0, 0.0, 0.0, 0.5633802816901409, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285714, 0.5714285714285715, 1.0, 0.5, 0.4, 0.5714285714285715, 0.375, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.23076923076923078, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1802", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5113", "mrqa_hotpotqa-validation-1679", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-7662"], "SR": 0.53125, "CSR": 0.5903532608695652, "retrieved_ids": ["mrqa_squad-train-16676", "mrqa_squad-train-48793", "mrqa_squad-train-33685", "mrqa_squad-train-9605", "mrqa_squad-train-2450", "mrqa_squad-train-54147", "mrqa_squad-train-25044", "mrqa_squad-train-25572", "mrqa_squad-train-45876", "mrqa_squad-train-79107", "mrqa_squad-train-46015", "mrqa_squad-train-37159", "mrqa_squad-train-49345", "mrqa_squad-train-46470", "mrqa_squad-train-47184", "mrqa_squad-train-35864", "mrqa_squad-train-76467", "mrqa_squad-train-43187", "mrqa_squad-train-43802", "mrqa_squad-train-76028", "mrqa_squad-train-12720", "mrqa_squad-train-56422", "mrqa_squad-train-1090", "mrqa_squad-train-55013", "mrqa_newsqa-validation-1175", "mrqa_squad-validation-6706", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-10806", "mrqa_hotpotqa-validation-5889", "mrqa_squad-validation-4883", "mrqa_squad-validation-3130", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-11886", "mrqa_squad-validation-7051", "mrqa_triviaqa-validation-1166", "mrqa_searchqa-validation-217", "mrqa_squad-validation-1125", "mrqa_searchqa-validation-2052", "mrqa_naturalquestions-validation-129", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-230", "mrqa_naturalquestions-validation-9235", "mrqa_searchqa-validation-10856", "mrqa_hotpotqa-validation-4831", "mrqa_naturalquestions-validation-10161", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-11388", "mrqa_squad-validation-8459"], "EFR": 0.9333333333333333, "Overall": 0.7395029438405797}, {"timecode": 23, "before_eval_results": {"predictions": ["100,000", "Tyneside Classical", "ring theory", "his birthtown, Smiljan", "Persia", "ABC-DuMont", "bathtub curve", "First World War", "John Constable", "Charlie Harper", "Salve", "Duncan", "Everton", "October", "\u201cI came, I saw, I conquered\u201d", "Bull Moose Party", "Augusta, GA, USA", "Demi Moore", "the College of Cardinals", "Cornell University", "Robert Stroud", "Alice in Alice", "caffeine", "\"The Blind Side.\"", "11", "17", "Achille Lauro", "Samson Utivich", "Bert Jones", "Swansea", "Wyatt", "Chuck Hagel", "Hispaniola", "Bangladesh", "argument", "Sean Maddox", "one king, one queen, two bishops, two knights, and eight pawns", "Bristol", "tinctures", "Andy Murray", "Independence Day", "\"I've never had an eight-ender before,\"", "the Hanse", "Crusades", "King Henry I", "ThunderCats", "comic (Nursery Comics)", "The European Council", "Volkswagen", "George IV", "the kalavinka", "Japan", "Edward Seton", "Thomas Jefferson", "North Dakota, South Dakota, Minnesota, Iowa, Nebraska, Colorado, Kansas, Oklahoma, Vermont, Oregon, and Idaho", "William Adelin", "Barbary pirates", "Sir William Collins", "to hold onto his land", "Lunsmann in July while she was vacationing with her family on the island of Tictabon, authorities said.", "if they focused their mischief energy on neural devices, such as the deep-brain stimulators used to treat Parkinson's and depression, or electrode systems for controlling prosthetic limbs", "Port In A Storm", "the Lone Star", "Bahadur Shah Zafar"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5961717549923194}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.06451612903225808, 0.1904761904761905, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9136", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2486", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-3864", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-6584", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-7212", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-7056", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2325", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-7661", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2371", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-13686"], "SR": 0.515625, "CSR": 0.5872395833333333, "retrieved_ids": ["mrqa_squad-train-44321", "mrqa_squad-train-4089", "mrqa_squad-train-21262", "mrqa_squad-train-74290", "mrqa_squad-train-39556", "mrqa_squad-train-73639", "mrqa_squad-train-35148", "mrqa_squad-train-86288", "mrqa_squad-train-60006", "mrqa_squad-train-23986", "mrqa_squad-train-56055", "mrqa_squad-train-74657", "mrqa_squad-train-36103", "mrqa_squad-train-2360", "mrqa_squad-train-52775", "mrqa_squad-train-24534", "mrqa_squad-train-39033", "mrqa_squad-train-40062", "mrqa_squad-train-54751", "mrqa_squad-train-15400", "mrqa_squad-train-51345", "mrqa_squad-train-25966", "mrqa_squad-train-66150", "mrqa_squad-train-49625", "mrqa_searchqa-validation-217", "mrqa_naturalquestions-validation-5960", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-6453", "mrqa_squad-validation-6409", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-6916", "mrqa_searchqa-validation-12996", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-4740", "mrqa_squad-validation-9863", "mrqa_newsqa-validation-4086", "mrqa_searchqa-validation-3525", "mrqa_hotpotqa-validation-2887", "mrqa_squad-validation-3019", "mrqa_searchqa-validation-12440", "mrqa_naturalquestions-validation-7310", "mrqa_hotpotqa-validation-62", "mrqa_searchqa-validation-12477", "mrqa_searchqa-validation-14301", "mrqa_naturalquestions-validation-10490", "mrqa_newsqa-validation-2900", "mrqa_hotpotqa-validation-5499", "mrqa_searchqa-validation-13232"], "EFR": 0.967741935483871, "Overall": 0.7457619287634408}, {"timecode": 24, "before_eval_results": {"predictions": ["Napoleon", "The Victorian Alps", "skin damage", "three", "European Parliament and the Council of the European Union", "Steve McQueen", "\u00c9dith Piaf", "piano", "Midtown", "bogge", "shoes of the finest leathers", "boxer", "Geneva", "Call for the Dead", "Woodrow Wilson", "Menorca", "Wales", "new town of Livingston in West Lothian", "Bulldog Drummond", "distance selling", "Edward VI", "raw linseed oil", "mercury", "trumpet", "architecture", "james bond villain", "Iain Banks", "Andalusia", "gluten", "Jan Van Eyck", "claire", "red-green", "stephen", "wyatt Hayworth", "World War II", "Thermopylae", "carol Bateman", "Yosemite", "the Sandstone Trail", "king duncan", "8 minutes", "uranium", "pretty Betsy", "a poster", "West Point", "Saint Cecilia", "algebra", "jennifer", "Whittle", "charlie", "We Interrupt This Week", "Chester", "Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "2005", "Wes Craven", "1698", "Bill Clinton", "an Airbus A320-214", "a broken pelvis", "246", "hurricane", "The Treasure of the Sierra Madre", "smallpox"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6244357638888889}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-3769", "mrqa_triviaqa-validation-5981", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-2669", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-4073", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-6358", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-2426", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-1857"], "SR": 0.53125, "CSR": 0.585, "retrieved_ids": ["mrqa_squad-train-72187", "mrqa_squad-train-53732", "mrqa_squad-train-13792", "mrqa_squad-train-35272", "mrqa_squad-train-36095", "mrqa_squad-train-14144", "mrqa_squad-train-46564", "mrqa_squad-train-16310", "mrqa_squad-train-61617", "mrqa_squad-train-63598", "mrqa_squad-train-39525", "mrqa_squad-train-3151", "mrqa_squad-train-56728", "mrqa_squad-train-51007", "mrqa_squad-train-14524", "mrqa_squad-train-6860", "mrqa_squad-train-69585", "mrqa_squad-train-63897", "mrqa_squad-train-27586", "mrqa_squad-train-10807", "mrqa_squad-train-74151", "mrqa_squad-train-49123", "mrqa_squad-train-56762", "mrqa_squad-train-23574", "mrqa_searchqa-validation-13232", "mrqa_hotpotqa-validation-1173", "mrqa_squad-validation-10369", "mrqa_hotpotqa-validation-4047", "mrqa_naturalquestions-validation-8239", "mrqa_hotpotqa-validation-5627", "mrqa_squad-validation-2538", "mrqa_squad-validation-7338", "mrqa_newsqa-validation-1514", "mrqa_naturalquestions-validation-4071", "mrqa_hotpotqa-validation-4712", "mrqa_naturalquestions-validation-3332", "mrqa_hotpotqa-validation-4418", "mrqa_newsqa-validation-1634", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-4069", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-2491", "mrqa_triviaqa-validation-7212", "mrqa_newsqa-validation-140", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-1770", "mrqa_squad-validation-3812"], "EFR": 1.0, "Overall": 0.751765625}, {"timecode": 25, "before_eval_results": {"predictions": ["a postman", "Thomas Piketty", "1,548", "zoning and building code requirements", "Science and Discovery", "New York", "tuppenny", "Laos", "a non-speaking character", "bluebird", "wine", "300", "1894", "jaws", "jon pertleby", "jodie Foster", "Billie Holiday", "Gingerbread", "Mickelson", "jon Paul Sartre", "Len Deighton", "tARTan", "Alex Garland", "L. Pasteur", "Dionysus", "Benjamin Disraeli", "Johannesburg", "Martin Luther King", "jawn [John J] McKenna", "Changing Places", "lose points during the game", "ocellaris", "Albert Reynolds", "Newfoundland", "Eddie Cochran", "Alessandro Volta", "Andre 3000", "Wanderers", "lazing", "Biafra secession", "Anna Mae Bullock", "Flint", "Cuba", "dove", "Heston Blumenthal", "Harold Godwinson", "James Jeffries", "Ritchie Valens", "posh", "Honda", "Bristol", "Gargantua", "Krypton", "hair jelly", "if the concentration of a compound exceeds its solubility", "\"The Bob Edwards Show\"", "Franz Ferdinand", "Paul John Mueller Jr.", "underprivileged", "Aniston, Demi Moore and Alicia Keys", "80", "Maldives", "Matt Leinart", "Stephen Hawking"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6089543269230769}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6973", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-243", "mrqa_triviaqa-validation-7414", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-423", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-3582", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-3223", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2965", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4832", "mrqa_hotpotqa-validation-3714"], "SR": 0.546875, "CSR": 0.5835336538461539, "retrieved_ids": ["mrqa_squad-train-66229", "mrqa_squad-train-62223", "mrqa_squad-train-79084", "mrqa_squad-train-17984", "mrqa_squad-train-37143", "mrqa_squad-train-38352", "mrqa_squad-train-10728", "mrqa_squad-train-57690", "mrqa_squad-train-72239", "mrqa_squad-train-66220", "mrqa_squad-train-13056", "mrqa_squad-train-34840", "mrqa_squad-train-4802", "mrqa_squad-train-71453", "mrqa_squad-train-32078", "mrqa_squad-train-33637", "mrqa_squad-train-8320", "mrqa_squad-train-65184", "mrqa_squad-train-16108", "mrqa_squad-train-33531", "mrqa_squad-train-15627", "mrqa_squad-train-55979", "mrqa_squad-train-43041", "mrqa_squad-train-74071", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6896", "mrqa_naturalquestions-validation-6166", "mrqa_squad-validation-2943", "mrqa_triviaqa-validation-1802", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-4047", "mrqa_squad-validation-9484", "mrqa_naturalquestions-validation-4002", "mrqa_hotpotqa-validation-4102", "mrqa_squad-validation-3444", "mrqa_naturalquestions-validation-6453", "mrqa_triviaqa-validation-456", "mrqa_newsqa-validation-1123", "mrqa_hotpotqa-validation-5328", "mrqa_newsqa-validation-1538", "mrqa_hotpotqa-validation-1173", "mrqa_naturalquestions-validation-10614", "mrqa_squad-validation-9764", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2426", "mrqa_searchqa-validation-7923", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-2309"], "EFR": 0.9655172413793104, "Overall": 0.7445758040450928}, {"timecode": 26, "before_eval_results": {"predictions": ["paid professionals", "the centre of Basel", "\"we want to practice Christian love toward them and pray that they convert,\"", "Informal rule", "animals", "raven", "helium", "John Logie Baird", "city of chicago", "Pickwick", "Titanic", "Benjamin Britten", "taekwondo", "Gibraltar", "Rome", "florita", "the orbital floor", "johnemburey", "bury Football Club", "oxygen", "morris", "Roy Keane", "Venus", "Ben Watson", "French", "Jupiter", "if\u2013\u201d", "the Monkees", "johnson johnson", "Urania", "Australia", "cerebrospinal fluid", "Charlie Chaplin", "city of chicago", "Portugal", "Vladivostok", "Boiling", "beetles", "Phoenicia", "Norwegian", "the Gulf of Suez", "beard", "Lichfield Cathedral", "lithium", "Salema", "America", "ch.1, p. 49-50", "tempera", "Rio", "peacock", "Hong Kong", "chicago", "6ft 1in", "Judith Cynthia Aline Keppel", "94 by 50 feet", "Eli Manning", "photographs, film and television", "March 17, 2015", "AbdulMutallab", "an eye for an eye", "three people", "city's harbour", "Dennis Miller", "May"], "metric_results": {"EM": 0.5625, "QA-F1": 0.615234375}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.5, 0.375, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-2368", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-4517", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-2945", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-1605", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-6046", "mrqa_newsqa-validation-1535", "mrqa_searchqa-validation-8872"], "SR": 0.5625, "CSR": 0.5827546296296297, "retrieved_ids": ["mrqa_squad-train-50487", "mrqa_squad-train-14313", "mrqa_squad-train-66290", "mrqa_squad-train-14037", "mrqa_squad-train-25268", "mrqa_squad-train-48487", "mrqa_squad-train-61675", "mrqa_squad-train-7695", "mrqa_squad-train-79979", "mrqa_squad-train-32019", "mrqa_squad-train-41295", "mrqa_squad-train-49618", "mrqa_squad-train-71136", "mrqa_squad-train-45445", "mrqa_squad-train-47714", "mrqa_squad-train-9817", "mrqa_squad-train-75618", "mrqa_squad-train-23628", "mrqa_squad-train-28019", "mrqa_squad-train-51224", "mrqa_squad-train-61441", "mrqa_squad-train-28246", "mrqa_squad-train-52280", "mrqa_squad-train-66522", "mrqa_hotpotqa-validation-5627", "mrqa_searchqa-validation-13151", "mrqa_squad-validation-451", "mrqa_triviaqa-validation-5852", "mrqa_naturalquestions-validation-9079", "mrqa_squad-validation-1125", "mrqa_newsqa-validation-3476", "mrqa_searchqa-validation-3525", "mrqa_squad-validation-3863", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-10918", "mrqa_squad-validation-7457", "mrqa_searchqa-validation-6909", "mrqa_hotpotqa-validation-996", "mrqa_newsqa-validation-3043", "mrqa_hotpotqa-validation-5386", "mrqa_searchqa-validation-11137", "mrqa_squad-validation-4629", "mrqa_squad-validation-3812", "mrqa_squad-validation-8459", "mrqa_naturalquestions-validation-10156", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_naturalquestions-validation-10614"], "EFR": 0.9642857142857143, "Overall": 0.7441736937830689}, {"timecode": 27, "before_eval_results": {"predictions": ["CBS", "15th", "60%", "Xbox One", "Three", "not", "Herman Cain", "to \"wipe out\" the United States", "blew himself up.", "University of Pittsburgh", "netherlands", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "\"There is a tiny sliver of good news -- the number of Americans who think things are going very badly has dropped from 40 percent in December to 32 percent now,\"", "Sonia Sotomayor", "Mandi Hamlin", "750", "imminent threat of a North Korean missile strike or confrontation between the two countries at sea.", "claire Clarkson", "Expedia", "humbert", "severe", "jobs up and down the auto supply chain", "the Swat Valley", "Rawalpindi", "a Utah jail", "Sunday,", "four", "East Java", "South Africa", "$2 billion", "Six members of Zoe's Ark", "2002", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"It has never been the policy of this president or this administration to torture.\"", "Former Mobile County Circuit Judge Herman Thomas", "Egypt.", "mayor of Seoul from 2002 to 2004", "Kerstin Fritzl", "that they don't feelMisty Cummings continues to hold important answers in the case.", "was made out of either heavy flannel or wool", "Melbourne", "the Southeast", "Sunday,", "$273 million", "Salt Lake City", "millionaire's surtax", "an animal tranquilizer,", "Section 60", "NATO's International Security Assistance Force", "Jaime Andrade", "1994", "Dance Your Ass Off", "Santiago Ram\u00f3n y Cajal", "Los Angeles", "a fortified complex at the heart of Moscow", "the Magic Circle", "china", "quiver", "Lin-Manuel Miranda", "15,024", "novelist and poet", "mantle", "morocco", "marshmallows"], "metric_results": {"EM": 0.5, "QA-F1": 0.6258850218409042}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.7272727272727272, 1.0, 0.5, 0.0, 0.15384615384615385, 0.16666666666666666, 0.0, 1.0, 1.0, 0.11764705882352941, 0.5, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6, 1.0, 0.15384615384615383, 1.0, 0.15384615384615385, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_newsqa-validation-3761", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-2265", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4561", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-1094", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-1864", "mrqa_searchqa-validation-13251"], "SR": 0.5, "CSR": 0.5797991071428572, "retrieved_ids": ["mrqa_squad-train-69626", "mrqa_squad-train-40296", "mrqa_squad-train-33274", "mrqa_squad-train-23853", "mrqa_squad-train-25863", "mrqa_squad-train-44106", "mrqa_squad-train-28818", "mrqa_squad-train-524", "mrqa_squad-train-46877", "mrqa_squad-train-47208", "mrqa_squad-train-80882", "mrqa_squad-train-48604", "mrqa_squad-train-37369", "mrqa_squad-train-27232", "mrqa_squad-train-84144", "mrqa_squad-train-74615", "mrqa_squad-train-78267", "mrqa_squad-train-25633", "mrqa_squad-train-4497", "mrqa_squad-train-56133", "mrqa_squad-train-25151", "mrqa_squad-train-739", "mrqa_squad-train-27745", "mrqa_squad-train-9608", "mrqa_searchqa-validation-13232", "mrqa_naturalquestions-validation-1782", "mrqa_squad-validation-8904", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-8159", "mrqa_squad-validation-6439", "mrqa_hotpotqa-validation-1679", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5386", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-8189", "mrqa_newsqa-validation-1232", "mrqa_naturalquestions-validation-2558", "mrqa_squad-validation-4000", "mrqa_hotpotqa-validation-5042", "mrqa_naturalquestions-validation-3672", "mrqa_squad-validation-8841", "mrqa_searchqa-validation-16378", "mrqa_newsqa-validation-3915", "mrqa_searchqa-validation-5149", "mrqa_newsqa-validation-1538", "mrqa_naturalquestions-validation-289", "mrqa_searchqa-validation-7923", "mrqa_triviaqa-validation-7439"], "EFR": 0.9375, "Overall": 0.7382254464285715}, {"timecode": 28, "before_eval_results": {"predictions": ["Denver's Executive Vice President of Football Operations and General Manager", "illegal boycotts", "Pittsburgh", "Cress", "molecular clouds in interstellar space", "Stefanie Scott", "the highway between the predominantly black city of Detroit and Wayne County and the predominantly White Oakland County and Macomb County suburbs", "Ram Nath Kovind", "Joseph Nye Welch", "5 lakhs of rupees", "members of the gay ( LGBT ) community", "Copper ( Cu ), silver ( Ag ), and gold ( Au )", "Wembley Stadium", "royal society", "1776", "Continental drift", "Julie Adams", "genetics and the male hormone dihydrotestosterone", "Jonathan Cheban", "Norman Greenbaum", "De Wayne Warren", "Thirty years after the Galactic Civil War", "restoring someone's faith in love and family relationships", "Anakin", "October 22, 2017", "April 17, 1982", "the Speaker of the House of Representatives", "London, United Kingdom", "a minority report", "tourneys or slow wheels", "two senators", "Club Bijou on Chapel Street", "pre-Columbian times", "the central plains", "China (formerly the Republic of China ), Russia ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "A costume party ( American English ) or a fancy dress party ( British English )", "directly into the bloodstream", "Kenny Anderson", "beneath the liver", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "Nathan Hale", "Jesse Frederick James Conaway", "the naos", "near temples", "port of Nueva Espa\u00f1a to the Spanish coast", "September 19, 2017", "West Egg on prosperous Long Island in the summer of 1922", "it was first published on November 12, 1976 by Ballantine Books", "Butter Island off North Haven, Maine in the Penobscot Bay", "wintertime", "Charles L. Jackson", "Laura Williams", "Flanagan", "territory east of the Mississippi River", "the Tyrrhenian Sea", "Manor of the More", "Craig William Macneill", "Democratic Unionist Party", "military personnel", "1,700 year old Roman mosaic entitled Chamber of the Ten Maidens", "\"The Orchid Thieves\"", "sacrificed CLEAN animals to YHWH", "Lord Arthur Balfour", "Edgar Allan Poe"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5701036584889139}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 0.4, 1.0, 0.823529411764706, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631577, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-386", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8359", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-3145", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-2150", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1570", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-12829"], "SR": 0.453125, "CSR": 0.5754310344827587, "retrieved_ids": ["mrqa_squad-train-54480", "mrqa_squad-train-66368", "mrqa_squad-train-3726", "mrqa_squad-train-71071", "mrqa_squad-train-77068", "mrqa_squad-train-38220", "mrqa_squad-train-68119", "mrqa_squad-train-69318", "mrqa_squad-train-33826", "mrqa_squad-train-2758", "mrqa_squad-train-25465", "mrqa_squad-train-44198", "mrqa_squad-train-53491", "mrqa_squad-train-14332", "mrqa_squad-train-75574", "mrqa_squad-train-77352", "mrqa_squad-train-15863", "mrqa_squad-train-64159", "mrqa_squad-train-42881", "mrqa_squad-train-62511", "mrqa_squad-train-18435", "mrqa_squad-train-16353", "mrqa_squad-train-43151", "mrqa_squad-train-49607", "mrqa_newsqa-validation-4202", "mrqa_triviaqa-validation-423", "mrqa_naturalquestions-validation-3028", "mrqa_searchqa-validation-3479", "mrqa_squad-validation-6228", "mrqa_searchqa-validation-13003", "mrqa_hotpotqa-validation-957", "mrqa_squad-validation-978", "mrqa_squad-validation-7845", "mrqa_hotpotqa-validation-5742", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-7774", "mrqa_naturalquestions-validation-7351", "mrqa_searchqa-validation-15075", "mrqa_triviaqa-validation-1166", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-1437", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4966", "mrqa_newsqa-validation-3790", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-11367"], "EFR": 0.9428571428571428, "Overall": 0.7384232604679803}, {"timecode": 29, "before_eval_results": {"predictions": ["Antigone", "the Meuse", "1596", "New Delhi", "MacFarlane", "Super Bowl XXXIX", "Hon July Moyo", "many forested parts of the world", "Narendra Modi", "forests and animals", "Aaron Harrison", "The White House Executive Chef", "Michael Crawford", "9 February 2018", "the red bone marrow of large bones", "Emily Blunt", "Pangaea", "Jonathan Breck", "the dermis", "Joe Pizzulo and Leeza Miller", "Ming dynasty", "201", "Chuck Noland", "Montreal Montreal", "Max Martin", "Waylon Jennings", "Nancy Jean Cartwright", "Beyonc\u00e9", "Ephesus", "the New York Yankees", "1996", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "United States customary units", "Joe Spano", "Michael Moriarty", "Rock Island, Illinois", "February 2002", "September 1959", "Louis Hynes", "Bonnie Lipton", "`` Tip and Ty ''", "0.05 ( 5 % )", "Poems : Series 1", "Rebekah", "the kitchen", "Tagalog or English", "Ernest Rutherford", "Napoleon Bonaparte", "the 12th century", "Yosemite National Park", "Norman Pritchard", "2014", "Mustelidae", "neutrons, protons, electrons, anti-electrons, photons and neutrinos", "King Henry VI", "October 13, 1980", "motorsport world championship", "Niihau", "Defense of Marriage Act", "Bronx.", "almost 9 million Americans", "\"Tennessee Waltz\"", "abacus", "Cyrus"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7183758899031076}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.14285714285714288, 0.6666666666666666, 0.24000000000000002, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-4279", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-5926", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-3760", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-5106", "mrqa_hotpotqa-validation-2195", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-3606"], "SR": 0.578125, "CSR": 0.5755208333333333, "retrieved_ids": ["mrqa_squad-train-29565", "mrqa_squad-train-47713", "mrqa_squad-train-84435", "mrqa_squad-train-50703", "mrqa_squad-train-25060", "mrqa_squad-train-85600", "mrqa_squad-train-32126", "mrqa_squad-train-23580", "mrqa_squad-train-78574", "mrqa_squad-train-63319", "mrqa_squad-train-82831", "mrqa_squad-train-39351", "mrqa_squad-train-83729", "mrqa_squad-train-50796", "mrqa_squad-train-19995", "mrqa_squad-train-71892", "mrqa_squad-train-65304", "mrqa_squad-train-12324", "mrqa_squad-train-9047", "mrqa_squad-train-32361", "mrqa_squad-train-58311", "mrqa_squad-train-19115", "mrqa_squad-train-5263", "mrqa_squad-train-45431", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-3019", "mrqa_hotpotqa-validation-4832", "mrqa_searchqa-validation-16908", "mrqa_naturalquestions-validation-2969", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-1360", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-6463", "mrqa_newsqa-validation-1805", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-7661", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-9148", "mrqa_naturalquestions-validation-10037", "mrqa_searchqa-validation-11137", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-2196", "mrqa_squad-validation-4539", "mrqa_searchqa-validation-9185", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-7067"], "EFR": 0.9629629629629629, "Overall": 0.7424623842592593}, {"timecode": 30, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1317", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-16", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1747", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2058", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-3015", "mrqa_hotpotqa-validation-3635", "mrqa_hotpotqa-validation-3662", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-392", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4451", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4886", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1502", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2930", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5160", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5721", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6772", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9691", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-974", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1078", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1456", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1738", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-288", "mrqa_newsqa-validation-2900", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3333", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3594", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4201", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-671", "mrqa_newsqa-validation-755", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-9", "mrqa_searchqa-validation-10098", "mrqa_searchqa-validation-10536", "mrqa_searchqa-validation-10856", "mrqa_searchqa-validation-11137", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-11886", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-13874", "mrqa_searchqa-validation-13883", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15740", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-16076", "mrqa_searchqa-validation-1649", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-172", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1851", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-2303", "mrqa_searchqa-validation-2323", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-2743", "mrqa_searchqa-validation-2835", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3514", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-3926", "mrqa_searchqa-validation-393", "mrqa_searchqa-validation-4032", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4393", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5928", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6170", "mrqa_searchqa-validation-6463", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7774", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-7998", "mrqa_searchqa-validation-8693", "mrqa_searchqa-validation-8872", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9269", "mrqa_searchqa-validation-9390", "mrqa_searchqa-validation-971", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10135", "mrqa_squad-validation-10136", "mrqa_squad-validation-10181", "mrqa_squad-validation-10268", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10388", "mrqa_squad-validation-10477", "mrqa_squad-validation-1095", "mrqa_squad-validation-1125", "mrqa_squad-validation-1177", "mrqa_squad-validation-1195", "mrqa_squad-validation-1408", "mrqa_squad-validation-1453", "mrqa_squad-validation-1499", "mrqa_squad-validation-1533", "mrqa_squad-validation-1566", "mrqa_squad-validation-1672", "mrqa_squad-validation-1765", "mrqa_squad-validation-1791", "mrqa_squad-validation-1848", "mrqa_squad-validation-1890", "mrqa_squad-validation-1892", "mrqa_squad-validation-195", "mrqa_squad-validation-2019", "mrqa_squad-validation-2033", "mrqa_squad-validation-2041", "mrqa_squad-validation-2243", "mrqa_squad-validation-2411", "mrqa_squad-validation-2456", "mrqa_squad-validation-247", "mrqa_squad-validation-2545", "mrqa_squad-validation-2683", "mrqa_squad-validation-27", "mrqa_squad-validation-2742", "mrqa_squad-validation-305", "mrqa_squad-validation-3130", "mrqa_squad-validation-3144", "mrqa_squad-validation-3184", "mrqa_squad-validation-3241", "mrqa_squad-validation-327", "mrqa_squad-validation-3335", "mrqa_squad-validation-335", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3501", "mrqa_squad-validation-3507", "mrqa_squad-validation-358", "mrqa_squad-validation-3605", "mrqa_squad-validation-3626", "mrqa_squad-validation-3718", "mrqa_squad-validation-3770", "mrqa_squad-validation-3796", "mrqa_squad-validation-381", "mrqa_squad-validation-386", "mrqa_squad-validation-3863", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3986", "mrqa_squad-validation-4000", "mrqa_squad-validation-402", "mrqa_squad-validation-4046", "mrqa_squad-validation-4054", "mrqa_squad-validation-4175", "mrqa_squad-validation-4213", "mrqa_squad-validation-4265", "mrqa_squad-validation-4302", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4446", "mrqa_squad-validation-4452", "mrqa_squad-validation-4468", "mrqa_squad-validation-4538", "mrqa_squad-validation-4546", "mrqa_squad-validation-4572", "mrqa_squad-validation-4629", "mrqa_squad-validation-4883", "mrqa_squad-validation-4986", "mrqa_squad-validation-5004", "mrqa_squad-validation-5097", "mrqa_squad-validation-5320", "mrqa_squad-validation-5396", "mrqa_squad-validation-5435", "mrqa_squad-validation-5448", "mrqa_squad-validation-5588", "mrqa_squad-validation-5692", "mrqa_squad-validation-5724", "mrqa_squad-validation-5781", "mrqa_squad-validation-5818", "mrqa_squad-validation-5860", "mrqa_squad-validation-5887", "mrqa_squad-validation-6019", "mrqa_squad-validation-6030", "mrqa_squad-validation-6069", "mrqa_squad-validation-6171", "mrqa_squad-validation-6206", "mrqa_squad-validation-6228", "mrqa_squad-validation-6240", "mrqa_squad-validation-6243", "mrqa_squad-validation-6279", "mrqa_squad-validation-6353", "mrqa_squad-validation-6439", "mrqa_squad-validation-6490", "mrqa_squad-validation-6517", "mrqa_squad-validation-6535", "mrqa_squad-validation-6543", "mrqa_squad-validation-6543", "mrqa_squad-validation-6611", "mrqa_squad-validation-6694", "mrqa_squad-validation-6729", "mrqa_squad-validation-6790", "mrqa_squad-validation-6838", "mrqa_squad-validation-6965", "mrqa_squad-validation-6973", "mrqa_squad-validation-6999", "mrqa_squad-validation-7039", "mrqa_squad-validation-71", "mrqa_squad-validation-7192", "mrqa_squad-validation-7368", "mrqa_squad-validation-7426", "mrqa_squad-validation-7521", "mrqa_squad-validation-7612", "mrqa_squad-validation-7674", "mrqa_squad-validation-7693", "mrqa_squad-validation-7814", "mrqa_squad-validation-7872", "mrqa_squad-validation-7876", "mrqa_squad-validation-7943", "mrqa_squad-validation-7952", "mrqa_squad-validation-7954", "mrqa_squad-validation-7984", "mrqa_squad-validation-7993", "mrqa_squad-validation-8043", "mrqa_squad-validation-8229", "mrqa_squad-validation-829", "mrqa_squad-validation-8415", "mrqa_squad-validation-8417", "mrqa_squad-validation-8500", "mrqa_squad-validation-852", "mrqa_squad-validation-8561", "mrqa_squad-validation-8585", "mrqa_squad-validation-8594", "mrqa_squad-validation-8754", "mrqa_squad-validation-8769", "mrqa_squad-validation-8969", "mrqa_squad-validation-8985", "mrqa_squad-validation-9102", "mrqa_squad-validation-9166", "mrqa_squad-validation-9170", "mrqa_squad-validation-9176", "mrqa_squad-validation-9196", "mrqa_squad-validation-942", "mrqa_squad-validation-9445", "mrqa_squad-validation-957", "mrqa_squad-validation-9614", "mrqa_squad-validation-9764", "mrqa_squad-validation-985", "mrqa_squad-validation-9866", "mrqa_squad-validation-9876", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1378", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-177", "mrqa_triviaqa-validation-1785", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-2623", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3359", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4057", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4465", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5106", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5387", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-578", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6046", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-7033", "mrqa_triviaqa-validation-7220", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7461"], "OKR": 0.880859375, "KG": 0.48671875, "before_eval_results": {"predictions": ["flammable cabin and space suit materials", "1992", "at least four", "Genesis", "real estate investment trust", "Louisiana's Bayou", "the carat", "Mission: Impossible", "dikonos", "Edinburgh", "Teha'amana", "Galapagos", "Bill Murray", "Battle of Chancellorsville", "boxing", "Wii", "the Suez Canal", "Dave Matthews Band", "twins", "dentures", "Friday the 13th", "Kinko's", "a platypus", "bang", "the tongue", "Cherokee", "nekropolis", "Eleanor Roosevelt", "the Grand Central Oyster", "The World's Leading", "Doctor Who Psychology", "bamboos", "Isaac Newton", "the Unabomber", "Narnia", "Freud", "Burma Ruby stone", "librettos", "Jonathan Swift", "Tracy Letts", "Medium", "bison", "a New Orleansstyle iced coffee concentrate", "Botswana", "Susan B. Anthony dollar", "Mattel", "Little Red Riding Hood", "Splint", "milky circle", "Vojvodina", "bang", "red bull", "infection, irritation, or allergies", "Castleford", "annually ( usually in May ) at the Palais des Festivals et des Congr\u00e8s", "Sheffield Wednesday", "Sahara", "henry vi", "You're Next", "Headless Body in Topless Bar", "political correctness", "Hanin Zoabi", "Princess Diana", "homicide"], "metric_results": {"EM": 0.5625, "QA-F1": 0.625297619047619}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-5180", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3542", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-7475", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-13247", "mrqa_searchqa-validation-4851", "mrqa_searchqa-validation-382", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2855", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-6237"], "SR": 0.5625, "CSR": 0.5751008064516129, "retrieved_ids": ["mrqa_squad-train-58206", "mrqa_squad-train-85093", "mrqa_squad-train-22397", "mrqa_squad-train-54273", "mrqa_squad-train-37507", "mrqa_squad-train-86557", "mrqa_squad-train-59013", "mrqa_squad-train-58960", "mrqa_squad-train-30344", "mrqa_squad-train-23199", "mrqa_squad-train-49308", "mrqa_squad-train-60574", "mrqa_squad-train-4540", "mrqa_squad-train-54789", "mrqa_squad-train-72415", "mrqa_squad-train-2339", "mrqa_squad-train-62777", "mrqa_squad-train-63959", "mrqa_squad-train-78141", "mrqa_squad-train-70679", "mrqa_squad-train-43548", "mrqa_squad-train-550", "mrqa_squad-train-8692", "mrqa_squad-train-45814", "mrqa_squad-validation-4838", "mrqa_searchqa-validation-1279", "mrqa_triviaqa-validation-7439", "mrqa_squad-validation-5818", "mrqa_hotpotqa-validation-2058", "mrqa_squad-validation-10388", "mrqa_hotpotqa-validation-2585", "mrqa_triviaqa-validation-2684", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-9853", "mrqa_naturalquestions-validation-123", "mrqa_squad-validation-1566", "mrqa_newsqa-validation-4017", "mrqa_searchqa-validation-3322", "mrqa_naturalquestions-validation-6046", "mrqa_newsqa-validation-720", "mrqa_triviaqa-validation-2151", "mrqa_newsqa-validation-3982", "mrqa_searchqa-validation-12440", "mrqa_naturalquestions-validation-4206", "mrqa_squad-validation-3998", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-1412"], "EFR": 1.0, "Overall": 0.7393170362903226}, {"timecode": 31, "before_eval_results": {"predictions": ["the West", "Begter", "2016", "Sheev Palpatine, ( colloquial : Darth Sidious and The Emperor )", "at the University of Oxford", "July 1, 1923", "a pH indicator, a color marker, and a dye", "winter", "the Khoisan language of the \u01c0Xam people", "the present districts of East Jaintia Hills district", "either in front or on top of the brainstem", "Janie Crawford", "Rob Jacobs", "the straight - line distance from A to B", "786", "Office of Inspector General", "Carbon copy to secondary recipients", "the `` round '', the rear leg of the cow", "1957", "Nia Long", "An error does not count as a hit but still counts as an at bat for the batter", "Andreas Vesalius", "Moscazzano", "Kristy Swanson", "detritus", "Asuka", "Jay Baruchel", "Oceania", "revolution or orbital revolution", "Houston Astros", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "the retina", "in the fascia surrounding skeletal muscle", "Pangaea", "2017", "near the inner rim of the Orion Arm", "Ricky Nelson", "all the numbers required to win a prize have been marked off", "Debbie Gibson", "Mad - Eye Moody and Hedwig", "a written compendium of Rabbinic Judaism's Oral Torah", "a column - like or oval ( egg - shaped ) symbol of Shiva", "winter", "Algeria", "the King James Bible", "Harlem River", "1998", "R.E.M.", "332", "above the light sources and under the sample in an upright microscope, and above the stage and below the light source in an inverted microscope", "Georgia Bulldogs", "Illinois", "Northumberland", "Northern Ireland", "Travis County", "Boston, Massachusetts", "Adam Dawes", "Republicans", "Zed's fossil", "a tenement in the Mumbai suburb of Chembur,", "a dummy", "Aristotle", "nothing gained"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7032821014663295}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.11111111111111112, 0.5, 0.5, 1.0, 0.0, 0.2978723404255319, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2, 0.5714285714285715, 0.888888888888889, 1.0, 0.0, 0.2978723404255319, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.15384615384615383, 0.0, 1.0, 0.375, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9473684210526315, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6821", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-809", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4132", "mrqa_triviaqa-validation-3940", "mrqa_newsqa-validation-1512"], "SR": 0.578125, "CSR": 0.5751953125, "retrieved_ids": ["mrqa_squad-train-48087", "mrqa_squad-train-14086", "mrqa_squad-train-5478", "mrqa_squad-train-64339", "mrqa_squad-train-67206", "mrqa_squad-train-63436", "mrqa_squad-train-30549", "mrqa_squad-train-47471", "mrqa_squad-train-7030", "mrqa_squad-train-75485", "mrqa_squad-train-67071", "mrqa_squad-train-55065", "mrqa_squad-train-76431", "mrqa_squad-train-44233", "mrqa_squad-train-24326", "mrqa_squad-train-42957", "mrqa_squad-train-41534", "mrqa_squad-train-68515", "mrqa_squad-train-36590", "mrqa_squad-train-70078", "mrqa_squad-train-47169", "mrqa_squad-train-17575", "mrqa_squad-train-79644", "mrqa_squad-train-22212", "mrqa_newsqa-validation-340", "mrqa_triviaqa-validation-2486", "mrqa_squad-validation-9764", "mrqa_searchqa-validation-8760", "mrqa_triviaqa-validation-2735", "mrqa_hotpotqa-validation-1534", "mrqa_searchqa-validation-6909", "mrqa_naturalquestions-validation-8326", "mrqa_searchqa-validation-1757", "mrqa_naturalquestions-validation-3442", "mrqa_triviaqa-validation-3864", "mrqa_searchqa-validation-7517", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-2495", "mrqa_hotpotqa-validation-5179", "mrqa_searchqa-validation-1156", "mrqa_naturalquestions-validation-8227", "mrqa_searchqa-validation-1053", "mrqa_naturalquestions-validation-430", "mrqa_searchqa-validation-15194", "mrqa_hotpotqa-validation-4047", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-24", "mrqa_newsqa-validation-3484"], "EFR": 0.8148148148148148, "Overall": 0.702298900462963}, {"timecode": 32, "before_eval_results": {"predictions": ["magnitude and direction", "1985", "1982", "National Aviation Hall of Fame", "Giotto di Bondone", "1985", "more than 26,000", "Lakshmibai", "the Championship", "French", "2009", "\" campaign setting\"", "The bonobo", "singer", "Greg Gorman and Helmut Newton", "Shameless", "stolperstein", "1901", "Carl Zeiss AG", "YouTube", "Bambi, a Life in the Woods", "Robert \"Bobby\" Germaine", "2004", "IndyCar", "one", "\"Twice in a Lifetime\"", "the Sun", "Greg Hertz", "Ruprekha Banerjee", "\"The Walking Dead\"", "Ted Nugent", "chairman and majority owner of The Related Companies", "Gust Avrakotos", "Maleficent", "Coll\u00e8ge de France", "Miami-Dade County", "Marty Ingels", "1945", "Edward R. Murrow", "Conservatorio Verdi", "Mindy Kaling", "June 10, 1982", "beer and soft drinks", "Liga MX", "Donald Duck", "The School Boys", "Lord Chancellor of England", "President Patrick Hillery", "The English Electric Canberra", "Richa Sharma", "48,982", "The Sound of Music", "chapterwise in the manga anthology Shonen Jump since the magazine's launch in November 2002 and in tank\u014dbon format since June 2003", "Michigan State Spartans", "Frank Langella", "an elephant", "an apple core", "carbonic acid", "the British capital's other two airports, Stansted and Gatwick,", "homicide", "maintain an \"aesthetic environment\" and ensure public safety", "Marshal Ptain", "the M1 Abrams", "Hannah Montana"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6492788461538461}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_hotpotqa-validation-444", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-1506", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-1030", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4655", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-6532", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3726", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-4628"], "SR": 0.59375, "CSR": 0.5757575757575757, "retrieved_ids": ["mrqa_squad-train-59691", "mrqa_squad-train-79622", "mrqa_squad-train-34943", "mrqa_squad-train-20410", "mrqa_squad-train-79152", "mrqa_squad-train-22488", "mrqa_squad-train-57329", "mrqa_squad-train-28803", "mrqa_squad-train-8175", "mrqa_squad-train-82976", "mrqa_squad-train-63679", "mrqa_squad-train-50650", "mrqa_squad-train-82502", "mrqa_squad-train-75964", "mrqa_squad-train-56764", "mrqa_squad-train-67785", "mrqa_squad-train-85844", "mrqa_squad-train-3704", "mrqa_squad-train-44239", "mrqa_squad-train-56361", "mrqa_squad-train-4026", "mrqa_squad-train-2087", "mrqa_squad-train-10697", "mrqa_squad-train-67916", "mrqa_triviaqa-validation-7439", "mrqa_naturalquestions-validation-7301", "mrqa_squad-validation-6706", "mrqa_searchqa-validation-5725", "mrqa_squad-validation-3687", "mrqa_hotpotqa-validation-957", "mrqa_newsqa-validation-412", "mrqa_squad-validation-4298", "mrqa_squad-validation-6228", "mrqa_searchqa-validation-5149", "mrqa_newsqa-validation-1712", "mrqa_squad-validation-3497", "mrqa_naturalquestions-validation-8983", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-3564", "mrqa_squad-validation-10388", "mrqa_triviaqa-validation-3563", "mrqa_searchqa-validation-16076", "mrqa_naturalquestions-validation-4054", "mrqa_newsqa-validation-3615", "mrqa_squad-validation-6494", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-3004", "mrqa_newsqa-validation-214"], "EFR": 1.0, "Overall": 0.7394483901515152}, {"timecode": 33, "before_eval_results": {"predictions": ["CEPR", "special university classes, called Lehramtstudien (Teaching Education Studies)", "CTV Television Network", "13\u20133", "American", "July 25 to August 4", "1958", "Norway", "twenty-three", "The Crips", "The Crowned Prince of the Philadelphia Mob", "Belmont Stakes", "Charles Edward Stuart", "historic buildings, arts, and published works", "August 9, 2017", "Batman", "Tennessee", "G\u00e9rard Depardieu, Daniel Auteuil", "books, films and other media", "King Duncan", "Europop", "1835", "Mayor Ed Lee", "Ghana", "Norwegian", "Dutch", "1976", "January 23, 1898", "Motorised quadricycle", "Nazareth", "Charlyn Marie \" Chan\" Marshall", "1968", "76,416", "Father Dougal McGuire", "June 17, 2007", "Deputy F\u00fchrer", "The United States of America", "Uchinaanchu (\u6c96\u7e04\u4eba, Japanese: \"Okinawa jin\")", "coaxial", "November 15, 1903", "Ant Timpson, Ted Geoghegan and Tim League", "1961", "1952", "Indian", "one child, Lisa Brennan-Jobs", "Pablo Escobar", "ZZ Top", "Steve and Rudy", "Russian Empire", "Flex-fuel", "Great Smoky Mountains National Park", "\"King of Cool\"", "subduction zone", "Barry Bonds", "Owen Vaccaro", "Machu Picchu", "Exile", "a downtown restaurant", "it has not intercepted any Haitians attempting illegal crossings into U.S. waters.", "U Win Tin,", "$1.45 billion", "onomatopoeia", "Singapore", "the femur"], "metric_results": {"EM": 0.625, "QA-F1": 0.7178977272727272}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-3703", "mrqa_triviaqa-validation-4306", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-742", "mrqa_searchqa-validation-15477"], "SR": 0.625, "CSR": 0.5772058823529411, "retrieved_ids": ["mrqa_squad-train-74280", "mrqa_squad-train-63132", "mrqa_squad-train-30562", "mrqa_squad-train-74828", "mrqa_squad-train-13127", "mrqa_squad-train-40476", "mrqa_squad-train-38899", "mrqa_squad-train-79772", "mrqa_squad-train-27199", "mrqa_squad-train-53430", "mrqa_squad-train-55009", "mrqa_squad-train-42799", "mrqa_squad-train-36104", "mrqa_squad-train-57485", "mrqa_squad-train-82881", "mrqa_squad-train-53970", "mrqa_squad-train-8755", "mrqa_squad-train-10754", "mrqa_squad-train-5270", "mrqa_squad-train-72184", "mrqa_squad-train-23930", "mrqa_squad-train-13663", "mrqa_squad-train-67183", "mrqa_squad-train-477", "mrqa_naturalquestions-validation-3442", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-3848", "mrqa_newsqa-validation-3615", "mrqa_searchqa-validation-13247", "mrqa_triviaqa-validation-1605", "mrqa_squad-validation-9863", "mrqa_triviaqa-validation-7401", "mrqa_hotpotqa-validation-4838", "mrqa_naturalquestions-validation-2855", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-2800", "mrqa_naturalquestions-validation-2462", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-3780", "mrqa_searchqa-validation-217", "mrqa_triviaqa-validation-2945", "mrqa_naturalquestions-validation-6916", "mrqa_squad-validation-5724", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-12996", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-4863"], "EFR": 1.0, "Overall": 0.7397380514705882}, {"timecode": 34, "before_eval_results": {"predictions": ["Peyton Manning", "upper limit", "25 million", "A simple iron boar crest", "Vienna", "period dependent", "the Harpe brothers", "Bill Clinton", "Dirk Nowitzki", "Detroit, Michigan,", "Bury St Edmunds, Suffolk, England", "novelty", "Mahoning County", "16 November 1973", "Hillary Rodham's advisor", "Bohemia", "New York", "The Indianapolis Times", "400 MW", "Mauritian", "Household Words", "Gatwick Airport", "Tokyo's Narita International Airport, Amami Airport and Fukuoka Airport", "Minette Walters", "CTV", "Firestorm", "2013", "Les Miles", "40 Days and 40 Nights", "James Tinling", "2014", "Crane Wilbur", "gull-wing doors", "Terry Malloy", "Operation Neptune", "Attack the Block", "House of Commons", "Hessians", "Battle of Chester", "Wayne County, Michigan", "Samoa", "mistress of the Robes", "Duchess Eleanor", "Barack Obama", "November 13, 2015", "Guardians of the Galaxy Vol. 2", "President of the United States", "1963", "The Bologna Process", "Pittsburgh", "women's basketball", "Salman Rushdie", "the employer", "the Hongwu Emperor of the Ming Dynasty", "commemorating fealty and filial piety", "chihuahua", "Arkansas", "throat", "1979", "his father", "$8.8 million", "Red Heat", "Miriam Makeba", "a mesio-occlusal cavity"], "metric_results": {"EM": 0.625, "QA-F1": 0.6856770833333333}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-384", "mrqa_squad-validation-10316", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2975", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-5228", "mrqa_hotpotqa-validation-1420", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-8907", "mrqa_triviaqa-validation-4143", "mrqa_triviaqa-validation-2316", "mrqa_newsqa-validation-501", "mrqa_searchqa-validation-9394"], "SR": 0.625, "CSR": 0.5785714285714285, "retrieved_ids": ["mrqa_squad-train-31371", "mrqa_squad-train-65447", "mrqa_squad-train-81104", "mrqa_squad-train-628", "mrqa_squad-train-65465", "mrqa_squad-train-36796", "mrqa_squad-train-6055", "mrqa_squad-train-2094", "mrqa_squad-train-27504", "mrqa_squad-train-36985", "mrqa_squad-train-86537", "mrqa_squad-train-78126", "mrqa_squad-train-62256", "mrqa_squad-train-33478", "mrqa_squad-train-70848", "mrqa_squad-train-8249", "mrqa_squad-train-39889", "mrqa_squad-train-30565", "mrqa_squad-train-27709", "mrqa_squad-train-77747", "mrqa_squad-train-14388", "mrqa_squad-train-64673", "mrqa_squad-train-53324", "mrqa_squad-train-16882", "mrqa_newsqa-validation-3772", "mrqa_naturalquestions-validation-3922", "mrqa_squad-validation-8990", "mrqa_triviaqa-validation-2459", "mrqa_newsqa-validation-3463", "mrqa_squad-validation-6072", "mrqa_hotpotqa-validation-2769", "mrqa_searchqa-validation-8705", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-6532", "mrqa_naturalquestions-validation-8765", "mrqa_hotpotqa-validation-5320", "mrqa_hotpotqa-validation-2009", "mrqa_squad-validation-27", "mrqa_hotpotqa-validation-1996", "mrqa_naturalquestions-validation-9753", "mrqa_squad-validation-2595", "mrqa_naturalquestions-validation-9002", "mrqa_searchqa-validation-7004", "mrqa_searchqa-validation-4367", "mrqa_triviaqa-validation-1303", "mrqa_newsqa-validation-1360", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-7351"], "EFR": 1.0, "Overall": 0.7400111607142856}, {"timecode": 35, "before_eval_results": {"predictions": ["DuMont Television Network", "Mount Kenya", "Albany, New York", "1908", "3 May 1958", "1986 to 2013", "Ronald Wilson Reagan", "Chiltern Hills", "Ted 2", "Bay of Fundy", "CD Castell\u00f3n", "2001", "Sean Yseult", "country", "she was a member of the Hawaii House of Representatives from 1990-96", "Operation Watchtower", "Paul W. S. Anderson", "15 February 1970", "Yasiin Bey", "Shooter Jennings", "Cincinnati, Ohio.", "Bad Moon Rising", "Kris Kristofferson", "371.6 days", "Atomic Kitten", "Trey Parker and Matt Stone", "Matt Gonzalez", "The Gold Coast", "1979", "\u00c6thelred the Unready", "PlayStation 4", "Malta", "1966", "Key West, Florida.", "Europe", "Black Mountain College", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies.", "an Italian former professional footballer", "American actor, comedian, and writer", "Prince George's County", "Pittsburgh, Pennsylvania", "1891", "Austral L\u00edneas A\u00e9reas", "Gainsborough Trinity", "Los Angeles", "October 13, 1980", "water", "India", "Syracuse University", "FIFA Women's World Cup", "Orange County", "76,416", "a diffuse system of small concentrations of lymphoid tissue found in various submucosal membrane sites of the body", "statistical modeling and statistical estimation", "an alien mechanoid being that Will first encounters on the planet that his family crash lands on", "Deep Blue", "Albert Reynolds", "George Washington", "U.S. senators", "California-based Current TV", "two", "one bath", "The Lost Boys of Peter", "succotash"], "metric_results": {"EM": 0.515625, "QA-F1": 0.648351128038628}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.45454545454545453, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.358974358974359, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2923", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-257", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-10259", "mrqa_triviaqa-validation-1348", "mrqa_newsqa-validation-2595", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-12493"], "SR": 0.515625, "CSR": 0.5768229166666667, "retrieved_ids": ["mrqa_squad-train-11724", "mrqa_squad-train-164", "mrqa_squad-train-23051", "mrqa_squad-train-22344", "mrqa_squad-train-5825", "mrqa_squad-train-4126", "mrqa_squad-train-16748", "mrqa_squad-train-79856", "mrqa_squad-train-6191", "mrqa_squad-train-78760", "mrqa_squad-train-27843", "mrqa_squad-train-27919", "mrqa_squad-train-4372", "mrqa_squad-train-7228", "mrqa_squad-train-73907", "mrqa_squad-train-69605", "mrqa_squad-train-69032", "mrqa_squad-train-83639", "mrqa_squad-train-48661", "mrqa_squad-train-68959", "mrqa_squad-train-61031", "mrqa_squad-train-38512", "mrqa_squad-train-71363", "mrqa_squad-train-25488", "mrqa_squad-validation-2043", "mrqa_hotpotqa-validation-511", "mrqa_searchqa-validation-2743", "mrqa_triviaqa-validation-3952", "mrqa_naturalquestions-validation-2794", "mrqa_newsqa-validation-2591", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5360", "mrqa_squad-validation-3113", "mrqa_newsqa-validation-1512", "mrqa_squad-validation-9863", "mrqa_naturalquestions-validation-8359", "mrqa_squad-validation-8969", "mrqa_newsqa-validation-1538", "mrqa_naturalquestions-validation-4103", "mrqa_searchqa-validation-4641", "mrqa_searchqa-validation-2971", "mrqa_naturalquestions-validation-1911", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-6746", "mrqa_hotpotqa-validation-3428", "mrqa_searchqa-validation-7923", "mrqa_triviaqa-validation-1363"], "EFR": 0.967741935483871, "Overall": 0.7332098454301076}, {"timecode": 36, "before_eval_results": {"predictions": ["between June and September", "emergency plans", "topless.", "a bank", "July", "Giovanni Falcone and Paolo Borsellino", "Tulsa, Oklahoma.", "380,000", "Old Trafford", "\"release\" civilians", "Number Ones", "Hannah Montana", "the Indian embassy in Kabul", "wrote the word \"pig\" in blood on the door of the home", "greatest female cyclist of her generation", "Annie Duke", "brutal", "that the legislation will foster racial profiling,", "producing rock music with a country influence", "The Kirchners", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "root out terrorists within its borders.", "violent separatist", "at the ancient Greek site of Olympia", "3,000", "closing these racial gaps", "Behar", "22", "3-0", "150", "boats", "U.N. agencies", "more than 30", "the man was dead", "one", "23 million square meters (248 million square feet)", "south of Kabul in the eastern Afghan province of Logar", "Virgin America", "fuel economy and safety while boosting", "American Civil Liberties Union", "summer", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "3rd District of Utah", "mental health and recovery", "56", "\"Gruesome photos from the scene", "Frank Ricci", "the Ku Klux Klan", "90", "Cash for Clunkers", "Argentina", "1998", "103", "Carolyn Sue Jones", "vanilla", "Hercules", "six letters to divide the world into six major climate regions", "Gian Carlo Menotti", "50th anniversary of the founding of the National Basketball Association", "Semites", "v. suzanne valadon", "Daisy Miller", "Detaiils", "Apollo"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5999997601370284}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.4, 0.0, 0.15384615384615385, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.375, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-978", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-533", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4389", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-2623"], "SR": 0.515625, "CSR": 0.5751689189189189, "retrieved_ids": ["mrqa_squad-train-53450", "mrqa_squad-train-57037", "mrqa_squad-train-53699", "mrqa_squad-train-63258", "mrqa_squad-train-25043", "mrqa_squad-train-75051", "mrqa_squad-train-63035", "mrqa_squad-train-32934", "mrqa_squad-train-12488", "mrqa_squad-train-47022", "mrqa_squad-train-62174", "mrqa_squad-train-73384", "mrqa_squad-train-85273", "mrqa_squad-train-55727", "mrqa_squad-train-39551", "mrqa_squad-train-17059", "mrqa_squad-train-32411", "mrqa_squad-train-13911", "mrqa_squad-train-20990", "mrqa_squad-train-82834", "mrqa_squad-train-66768", "mrqa_squad-train-67824", "mrqa_squad-train-28353", "mrqa_squad-train-71831", "mrqa_hotpotqa-validation-5291", "mrqa_searchqa-validation-10918", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-2195", "mrqa_newsqa-validation-1456", "mrqa_squad-validation-5262", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7095", "mrqa_newsqa-validation-4086", "mrqa_squad-validation-5860", "mrqa_searchqa-validation-2463", "mrqa_squad-validation-4838", "mrqa_squad-validation-3998", "mrqa_hotpotqa-validation-3216", "mrqa_naturalquestions-validation-585", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-3633", "mrqa_triviaqa-validation-254", "mrqa_searchqa-validation-9536", "mrqa_naturalquestions-validation-7694", "mrqa_searchqa-validation-1999", "mrqa_hotpotqa-validation-1173"], "EFR": 1.0, "Overall": 0.7393306587837838}, {"timecode": 37, "before_eval_results": {"predictions": ["five", "the state's attorney", "Abdullah Gul", "Ed McMahon", "They are co-chair of the Genocide Prevention Task Force.", "off Somalia's coast.", "clogs", "an upper respiratory infection", "two", "tells stories of different women coping with breast cancer in five vignettes.", "Gov. Bobby Jindal", "and husband Bill Klein,", "Twitter", "Alwin Landry", "Venus Williams", "they learned of the death of TV news coverage,", "Robert Mugabe", "Holley Wimunc,", "surrender.", "a dress", "$17,000", "his vice president", "Al-Aqsa mosque", "\"momentous discovery\"", "a three-story residential building in downtown Nairobi.", "Robert Barnett", "Asian qualifying Group 2", "Matthew Fisher", "Zimbabwean government", "Ben Roethlisberger", "The youngest ones", "Pew Research Center", "Sgt. Jason Bendett", "Brazil", "Stratfor's website", "$14.1 million", "a jury", "Salt Lake City, Utah,", "on Sunday.", "Robert Mugabe", "13.", "One of Osama bin Laden's sons", "for security reasons and not because of their faith.", "\"We tortured (Mohammed al ) Qahtani,\"", "autonomy", "the Arctic north of Murmansk down to the southern climes of Sochi", "Long Island", "Ma Khin Khin Leh", "400 years", "Elisabeth", "Washington State's decommissioned Hanford nuclear site,", "the breast or lower chest of beef or veal", "winter", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "stephen", "a bullfight", "the waltz", "1887", "Atlantic Coast Conference", "his uncle", "the Mason-Dixon", "Rabbit", "Brunswick", "Labrador"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6125868055555556}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-2905", "mrqa_naturalquestions-validation-1823", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-15103", "mrqa_searchqa-validation-12609"], "SR": 0.515625, "CSR": 0.5736019736842105, "retrieved_ids": ["mrqa_squad-train-64204", "mrqa_squad-train-29148", "mrqa_squad-train-32180", "mrqa_squad-train-85941", "mrqa_squad-train-20328", "mrqa_squad-train-27000", "mrqa_squad-train-8339", "mrqa_squad-train-49203", "mrqa_squad-train-61459", "mrqa_squad-train-28174", "mrqa_squad-train-68297", "mrqa_squad-train-50946", "mrqa_squad-train-59732", "mrqa_squad-train-79395", "mrqa_squad-train-82928", "mrqa_squad-train-66300", "mrqa_squad-train-38751", "mrqa_squad-train-49780", "mrqa_squad-train-44918", "mrqa_squad-train-40854", "mrqa_squad-train-77319", "mrqa_squad-train-54472", "mrqa_squad-train-45666", "mrqa_squad-train-57906", "mrqa_triviaqa-validation-4143", "mrqa_triviaqa-validation-254", "mrqa_hotpotqa-validation-3431", "mrqa_squad-validation-9458", "mrqa_newsqa-validation-2371", "mrqa_hotpotqa-validation-2923", "mrqa_triviaqa-validation-7220", "mrqa_naturalquestions-validation-7484", "mrqa_hotpotqa-validation-2781", "mrqa_squad-validation-335", "mrqa_hotpotqa-validation-1173", "mrqa_naturalquestions-validation-129", "mrqa_triviaqa-validation-4992", "mrqa_searchqa-validation-8872", "mrqa_squad-validation-8769", "mrqa_newsqa-validation-3463", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-5038", "mrqa_triviaqa-validation-5294", "mrqa_hotpotqa-validation-2452", "mrqa_searchqa-validation-8705", "mrqa_hotpotqa-validation-4864", "mrqa_squad-validation-9196", "mrqa_triviaqa-validation-6046"], "EFR": 1.0, "Overall": 0.7390172697368421}, {"timecode": 38, "before_eval_results": {"predictions": ["vector quantities", "Transport Workers Union leaders", "March 24,", "Eleven", "Mexican military", "Pakistani officials,", "$7.8 million", "Stratfor,", "Madeleine K. Albright", "Barack Obama", "Polo", "Hapag-Lloyd", "10,000", "vitamin \"drips\"", "Red Lines", "in body bags on the roadway near the bus,", "40", "intricate Flemish tapestries in an east-facing sitting room called the Morning Room.", "Islamic", "Los Angeles", "last week", "Sunni Arab and Shiite tribal leaders", "information on 4,000 credit cards and the company's \"private client\" list,", "an antihistamine and an epinephrine auto-injector", "North Korea", "Hong Kong", "a brown coffin containing the remains of Israeli soldiers killed during the 2006 war.", "the release of the four men", "ties,", "President Sheikh Sharif Sheikh Ahmed", "Saturday's", "power-sharing talks", "the matron swore and scream at the girls and assaulted them,", "in Amstetten,", "9-1", "Africa", "fear of losing their licenses to fly.", "Zimbabwe's main opposition party", "FBI.", "his first grand Slam,", "\"it should stay that way.\"", "CNN", "Barack Obama", "the strength of its brand name and the diversity of its product portfolio,", "U.S. State Department and British Foreign Office", "Monday's", "Fiona MacKeown", "sculptures", "Pakistan's High Commission in India", "Bryant Purvis,", "pain-relief drugs.", "1871", "Frederick Chiluba, Levy Mwanawasa, Rupiah Banda, Michael Sata, and current President Edgar Lungu", "Gestalt psychology", "all animals", "Treaty of Utrecht", "Massachusetts", "James Harrison", "Ford Island", "Tomorrowland", "Tiger Woods", "Wall Street", "Martin Luther King", "Sesame Street"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6406791125541125}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.18181818181818182, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2666666666666667, 0.5, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2830", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-639", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1060", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-6409", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-13907"], "SR": 0.53125, "CSR": 0.5725160256410257, "retrieved_ids": ["mrqa_squad-train-15573", "mrqa_squad-train-66550", "mrqa_squad-train-25835", "mrqa_squad-train-40479", "mrqa_squad-train-39251", "mrqa_squad-train-37096", "mrqa_squad-train-26263", "mrqa_squad-train-35795", "mrqa_squad-train-5281", "mrqa_squad-train-36059", "mrqa_squad-train-62509", "mrqa_squad-train-66539", "mrqa_squad-train-68627", "mrqa_squad-train-51890", "mrqa_squad-train-73015", "mrqa_squad-train-48463", "mrqa_squad-train-9184", "mrqa_squad-train-49011", "mrqa_squad-train-74265", "mrqa_squad-train-45790", "mrqa_squad-train-3511", "mrqa_squad-train-46755", "mrqa_squad-train-85596", "mrqa_squad-train-6875", "mrqa_hotpotqa-validation-4879", "mrqa_naturalquestions-validation-2466", "mrqa_squad-validation-335", "mrqa_newsqa-validation-1021", "mrqa_hotpotqa-validation-3020", "mrqa_searchqa-validation-4724", "mrqa_triviaqa-validation-341", "mrqa_hotpotqa-validation-444", "mrqa_newsqa-validation-2020", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-2202", "mrqa_hotpotqa-validation-5328", "mrqa_naturalquestions-validation-3319", "mrqa_triviaqa-validation-3145", "mrqa_searchqa-validation-15496", "mrqa_hotpotqa-validation-3304", "mrqa_squad-validation-1938", "mrqa_newsqa-validation-2429", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2462", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-4838", "mrqa_squad-validation-1672", "mrqa_hotpotqa-validation-516"], "EFR": 1.0, "Overall": 0.7388000801282051}, {"timecode": 39, "before_eval_results": {"predictions": ["Andrew Lortie", "weather improved", "ovules", "high sewing", "Silver Hatch", "nerves", "Ethiopia", "Red Admiral", "Jets", "Harrier Mini 2", "astronaut", "Macau", "Alastair Cook", "Enterprise", "Three Little Pigs", "Asia", "hip", "clowns", "meninges", "English", "Thomas Seymour", "Munich", "Henry Mancini", "Fred Astaire", "the pyre", "woe", "Sudan", "Low Countries", "Police Procedural", "Proverbs", "stand-up comedian", "jubilee", "Tornado", "police detective", "s\u00e3o Jorge Island", "pancreas", "puff", "football", "Antoine Lavoisier", "Leon Trotsky", "Neuna", "societies or amalgamations of persons", "Pet Shop Boys", "Chris Salmon", "Algiers", "Marks & Co,", "Anabaptists", "St. Ambrose", "Hebrew", "John Virgo", "herpes virus", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Garfield Sobers", "In the mountains outside City 17", "Dan Castellaneta", "Johannes Vermeer", "O.T. Genasis", "Climatecare,", "there are several thousand drugs, mostly older products, marketed illegally without FDA approval in this country.", "Kevin Kuranyi", "Lost in America", "autumnal", "Soviet Union", "Senate Democrats"], "metric_results": {"EM": 0.5, "QA-F1": 0.5296266233766234}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.21428571428571427, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-4864", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4784", "mrqa_triviaqa-validation-2781", "mrqa_naturalquestions-validation-86", "mrqa_hotpotqa-validation-264", "mrqa_newsqa-validation-2509", "mrqa_searchqa-validation-6304", "mrqa_newsqa-validation-1550"], "SR": 0.5, "CSR": 0.570703125, "retrieved_ids": ["mrqa_squad-train-62117", "mrqa_squad-train-42895", "mrqa_squad-train-73640", "mrqa_squad-train-68206", "mrqa_squad-train-19133", "mrqa_squad-train-47462", "mrqa_squad-train-59118", "mrqa_squad-train-6845", "mrqa_squad-train-887", "mrqa_squad-train-30934", "mrqa_squad-train-6751", "mrqa_squad-train-51442", "mrqa_squad-train-58960", "mrqa_squad-train-64763", "mrqa_squad-train-21519", "mrqa_squad-train-75749", "mrqa_squad-train-78589", "mrqa_squad-train-85880", "mrqa_squad-train-69559", "mrqa_squad-train-63522", "mrqa_squad-train-26169", "mrqa_squad-train-54927", "mrqa_squad-train-79190", "mrqa_squad-train-5845", "mrqa_newsqa-validation-1535", "mrqa_squad-validation-218", "mrqa_newsqa-validation-3214", "mrqa_hotpotqa-validation-3871", "mrqa_naturalquestions-validation-5550", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-9185", "mrqa_triviaqa-validation-3145", "mrqa_searchqa-validation-1156", "mrqa_searchqa-validation-12440", "mrqa_naturalquestions-validation-7694", "mrqa_searchqa-validation-9394", "mrqa_squad-validation-3718", "mrqa_squad-validation-4435", "mrqa_triviaqa-validation-2202", "mrqa_naturalquestions-validation-8239", "mrqa_searchqa-validation-15508", "mrqa_newsqa-validation-1855", "mrqa_naturalquestions-validation-4592", "mrqa_hotpotqa-validation-409", "mrqa_searchqa-validation-3653", "mrqa_newsqa-validation-978", "mrqa_squad-validation-4572", "mrqa_newsqa-validation-3446"], "EFR": 1.0, "Overall": 0.7384375000000001}, {"timecode": 40, "UKR": 0.8203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1389", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1495", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3928", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4234", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-929", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10606", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5067", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8052", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-837", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1985", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2404", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-1264", "mrqa_searchqa-validation-12829", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-13907", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15075", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15770", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16453", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-16839", "mrqa_searchqa-validation-1770", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-686", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9596", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10263", "mrqa_squad-validation-10317", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10369", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-127", "mrqa_squad-validation-1408", "mrqa_squad-validation-1430", "mrqa_squad-validation-1453", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2094", "mrqa_squad-validation-2328", "mrqa_squad-validation-2352", "mrqa_squad-validation-2365", "mrqa_squad-validation-2438", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3124", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3435", "mrqa_squad-validation-3444", "mrqa_squad-validation-3497", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-3946", "mrqa_squad-validation-402", "mrqa_squad-validation-4047", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-4715", "mrqa_squad-validation-491", "mrqa_squad-validation-4918", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5664", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5763", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6517", "mrqa_squad-validation-6543", "mrqa_squad-validation-6706", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7147", "mrqa_squad-validation-7192", "mrqa_squad-validation-7205", "mrqa_squad-validation-7296", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7751", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8154", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-8841", "mrqa_squad-validation-893", "mrqa_squad-validation-8933", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2459", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5179", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-568", "mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6358", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-899"], "OKR": 0.916015625, "KG": 0.490625, "before_eval_results": {"predictions": ["chameleon circuit", "Jake La Motta", "danish", "Joshua", "pangrams", "Live and Let Die", "lassie", "smith", "china", "Secretary of the Society", "Hadrian", "Napier", "Sony Interactive Entertainment", "king Henry I of England", "green", "1215", "delphiniums", "Robinson Crusoe", "Charles Dickens", "Brussels", "Egypt", "a star", "grommets", "New York Yankees", "Four Tops", "hudson", "July 20,", "9", "bali", "lilac", "Hilary Swank", "redbird", "dove", "a mole", "McCarthy", "springtime for Hitler", "two", "lV", "Shaft", "the legs", "The Daily Mirror", "pino", "horse", "china", "belgian", "Vitcos", "McCartney", "Madness", "Jane Eyre", "Kansas", "Australia", "A marriage officiant", "Ricky Nelson", "Wabanaki Confederacy members Abenaki and Mi'kmaq", "Port Moresby", "A bass", "Security Management", "eight", "Russia", "\"It was more of an event held by a radio station.\"", "Malaya", "Hank Aaron", "Livin' On A Prayer", "Octopus"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5659598214285715}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5714285714285715, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-1606", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-1746", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-6843", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-6915", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-6885", "mrqa_triviaqa-validation-774", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3491", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3526", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1413", "mrqa_searchqa-validation-11621", "mrqa_naturalquestions-validation-6903"], "SR": 0.484375, "CSR": 0.5685975609756098, "retrieved_ids": ["mrqa_squad-train-14687", "mrqa_squad-train-78655", "mrqa_squad-train-29171", "mrqa_squad-train-50759", "mrqa_squad-train-3810", "mrqa_squad-train-14792", "mrqa_squad-train-37969", "mrqa_squad-train-70273", "mrqa_squad-train-73548", "mrqa_squad-train-56890", "mrqa_squad-train-53755", "mrqa_squad-train-128", "mrqa_squad-train-28311", "mrqa_squad-train-51784", "mrqa_squad-train-42597", "mrqa_squad-train-40477", "mrqa_squad-train-35389", "mrqa_squad-train-55673", "mrqa_squad-train-57740", "mrqa_squad-train-34666", "mrqa_squad-train-81311", "mrqa_squad-train-53349", "mrqa_squad-train-81688", "mrqa_squad-train-82379", "mrqa_searchqa-validation-12611", "mrqa_newsqa-validation-340", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-10271", "mrqa_triviaqa-validation-2519", "mrqa_searchqa-validation-14194", "mrqa_squad-validation-3811", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-6340", "mrqa_squad-validation-2336", "mrqa_naturalquestions-validation-9979", "mrqa_searchqa-validation-9185", "mrqa_squad-validation-5435", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1634", "mrqa_triviaqa-validation-3263", "mrqa_squad-validation-3130", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-2316", "mrqa_naturalquestions-validation-844", "mrqa_searchqa-validation-3542", "mrqa_naturalquestions-validation-4193", "mrqa_newsqa-validation-1538"], "EFR": 1.0, "Overall": 0.7591101371951219}, {"timecode": 41, "before_eval_results": {"predictions": ["Edward Teller", "food, music, culture and language of Latin America", "at least 10,000 years later, visitors in Los Angeles can see the remains of \"Zed,\" a Columbian mammoth", "they would not be making any further comments, citing the investigation.", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Tim Clark, Matt Kuchar and Bubba Watson", "philip Markoff,", "Haeftling", "forgery and flying without a valid license,", "Sea World", "Mafia", "Nat King Cole.", "the 1800s and the era of Mark Twain,", "convicts caught with phones", "16", "cancer", "$40 and a loaf of bread.", "McDonald\\'s'", "\"The Rosie Show,\"", "France", "President Obama and Britain's Prince Charles", "more than 100", "South Africa's", "Madeleine K. Albright", "that the National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "back at work", "\"It got me thinking about what I would want to do when I got out of the game.", "unknown,", "five", "the hiring of hundreds of foreign workers", "The father of Haleigh Cummings,", "Elisabeth's father,", "his club", "they", "$60 billion", "$199", "J.G. Ballard,", "Republicans", "he discussed foreplay, sexual conquests and how he picks up women", "the Airbus A330-200", "the United States, NATO member states, Russia and India", "fatally shooting a limo driver", "ties,", "dogs who walk on ice in Alaska.", "not guilty in an appearance last week in Broward County Circuit Court.", "China", "Steve Jobs", "\"Rin Tin Tin: The Life and the Legend\"", "Sri Lanka's", "The station", "the state's attorney", "Colonel Robert E. Lee", "126", "Robert E. Lee", "Telegraph Media Group Limited", "prime minister", "le Marseillaise", "Kristy Lee Cook", "John Samuel Waters Jr.", "Norman Mark Reedus", "denned Women", "Douglas MacArthur", "rice", "at least 18 or 21 years old"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5664978250915751}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 0.6153846153846153, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.08333333333333334, 1.0, 0.8571428571428571, 1.0, 0.0, 0.25, 0.13333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.4, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7880", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-371", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-2421", "mrqa_triviaqa-validation-4374", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-2819", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2138", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-149", "mrqa_naturalquestions-validation-8617"], "SR": 0.453125, "CSR": 0.5658482142857143, "retrieved_ids": ["mrqa_squad-train-35942", "mrqa_squad-train-23949", "mrqa_squad-train-72676", "mrqa_squad-train-55956", "mrqa_squad-train-6297", "mrqa_squad-train-64271", "mrqa_squad-train-73878", "mrqa_squad-train-30880", "mrqa_squad-train-18859", "mrqa_squad-train-36115", "mrqa_squad-train-39448", "mrqa_squad-train-861", "mrqa_squad-train-79509", "mrqa_squad-train-4237", "mrqa_squad-train-61200", "mrqa_squad-train-51567", "mrqa_squad-train-76992", "mrqa_squad-train-75547", "mrqa_squad-train-57196", "mrqa_squad-train-12238", "mrqa_squad-train-15816", "mrqa_squad-train-61239", "mrqa_squad-train-86268", "mrqa_squad-train-80274", "mrqa_naturalquestions-validation-2562", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-9536", "mrqa_newsqa-validation-2903", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16908", "mrqa_hotpotqa-validation-132", "mrqa_newsqa-validation-1413", "mrqa_squad-validation-6171", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-6548", "mrqa_naturalquestions-validation-4674", "mrqa_triviaqa-validation-6532", "mrqa_hotpotqa-validation-2150", "mrqa_searchqa-validation-9394", "mrqa_naturalquestions-validation-7390", "mrqa_searchqa-validation-3618", "mrqa_newsqa-validation-1265", "mrqa_triviaqa-validation-1303", "mrqa_squad-validation-9176", "mrqa_searchqa-validation-2773", "mrqa_naturalquestions-validation-4905", "mrqa_triviaqa-validation-1606", "mrqa_newsqa-validation-1103"], "EFR": 1.0, "Overall": 0.7585602678571429}, {"timecode": 42, "before_eval_results": {"predictions": ["ITT", "246", "nearly a hundred years", "The cause of the child's death will be listed as homicide by undetermined means,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Iran", "20", "\"Dance Your Ass Off\"", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "15-year-old", "AbdulMutallab,", "London", "Democratic National Convention", "his native Philippines", "Kitty Kelley", "France", "Democrats and Republicans", "Amanda Knox's aunt", "Michael Krane,", "15,000", "One out of every 17 children under 3 years old", "the Gulf", "May 4", "3-0", "Zimbabwean President Robert Mugabe", "three", "\"murder dozens of people with a focus on murdering African-Americans\"", "10 percent", "165", "\"Historically, when times get tough, you see a 50 percent-plus increase in bartering as a way for people to be able to buy things or get things and do it economically,\"", "Ignazio La Russa", "Amir Zaki", "$40 and a loaf of bread.", "rural Tennessee.", "Tulsa, Oklahoma.", "3-3", "nearly $2 billion", "Russian concerns that the defensive shield could be used for offensive aims.", "1981", "\"They are, of course, shattered.", "London's", "Prague", "more than 100", "Microsoft", "Michael Partain,", "Mitt Romney", "Islamic militants", "prisoners at the South Dakota State Penitentiary", "part", "full health-care coverage,", "season five", "in Llantrisant, Wales", "1940", "Afghanistan", "Marshal Kane", "the Spey", "yodel It!", "Cheshire", "Town of Brookhaven", "Transamerica", "shows the culture of the United States at a specific time", "pirate", "16"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5258038836163836}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.26666666666666666, 1.0, 0.0, 0.0, 0.4, 0.5384615384615384, 0.0, 1.0, 1.0, 0.5, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0606060606060606, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.5, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2503", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-1761", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-1229", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-1429", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-143", "mrqa_triviaqa-validation-1380", "mrqa_hotpotqa-validation-1212", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-5848", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-709", "mrqa_naturalquestions-validation-1640"], "SR": 0.40625, "CSR": 0.5621366279069768, "retrieved_ids": ["mrqa_squad-train-31368", "mrqa_squad-train-30740", "mrqa_squad-train-64123", "mrqa_squad-train-41543", "mrqa_squad-train-29488", "mrqa_squad-train-59025", "mrqa_squad-train-30000", "mrqa_squad-train-54690", "mrqa_squad-train-892", "mrqa_squad-train-63201", "mrqa_squad-train-13324", "mrqa_squad-train-64719", "mrqa_squad-train-40164", "mrqa_squad-train-57081", "mrqa_squad-train-79580", "mrqa_squad-train-6599", "mrqa_squad-train-53843", "mrqa_squad-train-52061", "mrqa_squad-train-81722", "mrqa_squad-train-4944", "mrqa_squad-train-33774", "mrqa_squad-train-57612", "mrqa_squad-train-57762", "mrqa_squad-train-9350", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-1693", "mrqa_triviaqa-validation-6532", "mrqa_squad-validation-7422", "mrqa_hotpotqa-validation-3714", "mrqa_triviaqa-validation-2902", "mrqa_squad-validation-6655", "mrqa_hotpotqa-validation-2452", "mrqa_searchqa-validation-3613", "mrqa_hotpotqa-validation-3844", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1456", "mrqa_naturalquestions-validation-430", "mrqa_newsqa-validation-2074", "mrqa_triviaqa-validation-6409", "mrqa_hotpotqa-validation-4712", "mrqa_naturalquestions-validation-6453", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-10613", "mrqa_searchqa-validation-10372", "mrqa_triviaqa-validation-4843", "mrqa_newsqa-validation-3594", "mrqa_searchqa-validation-14371", "mrqa_hotpotqa-validation-234"], "EFR": 1.0, "Overall": 0.7578179505813953}, {"timecode": 43, "before_eval_results": {"predictions": ["the top 400 richest Americans", "14", "3-0", "the issue to a crowd at the White House,", "scoliosis, or abnormal curving of the spine", "Oaxacan countryside of southern", "the punishment for the player", "The wings, included in the sale,", "Vernon Forrest,", "Mandi Hamlin", "U.S. State Department and British Foreign Office", "Pastor Paula White", "Phoenix, Arizona,", "\"We tortured (Mohammed al ) Qahtani,\"", "ax handles.", "Six", "Wednesday.", "the District of Columbia near Takoma Park, Maryland.", "Russia", "doctors", "Jund Ansar Allah", "a senior at Stetson University", "Michael Jackson", "1,500", "through the weekend,", "three", "\"novel\"", "Aniston, Demi Moore and Alicia Keys", "January", "his land", "Miguel Cotto", "\"Two pages -- usually high school juniors who serve Congress as messengers\"", "U.S. soldier", "shark River Park", "as many as 50,000 members of the group United Front for Democracy Against Dictatorship", "buses, subways and trolleys that carry almost a million people daily.", "five", "Long troop deployments", "St. Louis, Missouri.", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "a number of calls, and those calls were intriguing,", "Clifford Harris,", "Sweden in 1967, Iceland in 1968, Nigeria in 1972 and Ghana in 1974.", "Republican Party,", "almost 9 million", "Asashoryu", "an upper respiratory infection", "Adriano", "\"Zed,\"", "the prime minister's handling of the L'Aquila earthquake,", "for death squad killings carried out during his rule in the 1990s.", "1973", "Roanoke", "Scarlett Johansson", "the skull", "Red Sea", "Pacific Ocean", "Scotty Grainger", "Robert A. Iger", "Salgaocar", "Northanger Abbey", "Abercrombie & Fitch", "a soap opera", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens"], "metric_results": {"EM": 0.5, "QA-F1": 0.6008427017061886}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false], "QA-F1": [0.4, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 0.5, 0.14285714285714288, 0.5714285714285715, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-7454", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3933", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-2084", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-375", "mrqa_triviaqa-validation-3275", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-802", "mrqa_searchqa-validation-6252", "mrqa_naturalquestions-validation-4915"], "SR": 0.5, "CSR": 0.5607244318181819, "retrieved_ids": ["mrqa_squad-train-24695", "mrqa_squad-train-11243", "mrqa_squad-train-25783", "mrqa_squad-train-31486", "mrqa_squad-train-47390", "mrqa_squad-train-45449", "mrqa_squad-train-18436", "mrqa_squad-train-53589", "mrqa_squad-train-83163", "mrqa_squad-train-56607", "mrqa_squad-train-57900", "mrqa_squad-train-76547", "mrqa_squad-train-30690", "mrqa_squad-train-61980", "mrqa_squad-train-41537", "mrqa_squad-train-64432", "mrqa_squad-train-38256", "mrqa_squad-train-50149", "mrqa_squad-train-58558", "mrqa_squad-train-4732", "mrqa_squad-train-43024", "mrqa_squad-train-49403", "mrqa_squad-train-77278", "mrqa_squad-train-65555", "mrqa_searchqa-validation-5725", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-1496", "mrqa_naturalquestions-validation-3598", "mrqa_searchqa-validation-4169", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3442", "mrqa_searchqa-validation-1156", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-4054", "mrqa_triviaqa-validation-1380", "mrqa_searchqa-validation-6463", "mrqa_hotpotqa-validation-906", "mrqa_naturalquestions-validation-9979", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-10613", "mrqa_triviaqa-validation-774", "mrqa_searchqa-validation-6445", "mrqa_triviaqa-validation-4143", "mrqa_naturalquestions-validation-1974", "mrqa_searchqa-validation-1279", "mrqa_squad-validation-3447", "mrqa_newsqa-validation-2265", "mrqa_triviaqa-validation-5500"], "EFR": 1.0, "Overall": 0.7575355113636364}, {"timecode": 44, "before_eval_results": {"predictions": ["help transfer and dissipate excess energy", "at his waterfront home on Chesapeake Bay, south of Annapolis in Maryland", "the Isthmus of Corinth", "Alex Ryan", "Wimpy", "2001", "2015", "Archie proposes marriage to Betty", "devised by Leonard Nimoy, who portrayed the half - Vulcan character Mr. Spock on the original Star Trek television series", "Rodney Crowell", "Jason Momoa", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "a donor molecule to an acceptor molecule", "Jamie Foxx", "Iowa ( 36.6 % )", "1996", "The uvea", "the Director of National Intelligence", "biological taxonomy", "Zeebo", "1977", "Department of Health and Human Services", "France", "electronic computers", "a contemporary drama in a rural setting", "1939", "Kristy Swanson", "Jyotirindra Basu", "roughly five hundred experts across the world", "2018", "Congress in 1790 passed the first naturalization law for the United States, the Naturalization Act of 1790", "The Jamestown settlement in the Colony of Virginia", "Arkansas", "December 24, 1836", "at slightly different times", "during the American Civil War", "Executive Residence of the White House Complex", "200 to 500 mg up to 7 mg", "Timothy B. Schmit", "March 2, 2016", "Thirty years after the Galactic Civil War", "Woody Paige", "Bonnie Plunkett ( Allison Janney )", "$75,000", "four", "`` Blood is the New Black ''", "The small intestine or small bowel", "USS Chesapeake", "18 - season career", "a greet which is used by some on birthdays, and by others in response to `` Merry Christmas '' and `` Happy New Year ''", "President Lyndon Johnson", "Sarah Palin", "Carlo (Fredo) Corleone", "Passion", "Kinnairdy Castle", "Teenage Mutant Ninja Turtles", "Kona coast of the island of Hawai\u02bb i about 12 mi south of Kailua-Kona", "his business dealings for possible securities violations", "40", "16 times", "John Deere", "snowboarding", "dollop", "algiers"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6658624544315335}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.12121212121212123, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2666666666666667, 0.6, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-9361", "mrqa_triviaqa-validation-7072", "mrqa_triviaqa-validation-5874", "mrqa_hotpotqa-validation-1156", "mrqa_hotpotqa-validation-5117", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-522", "mrqa_searchqa-validation-2656"], "SR": 0.5625, "CSR": 0.5607638888888888, "retrieved_ids": ["mrqa_squad-train-3563", "mrqa_squad-train-72760", "mrqa_squad-train-8672", "mrqa_squad-train-25735", "mrqa_squad-train-66572", "mrqa_squad-train-34920", "mrqa_squad-train-50173", "mrqa_squad-train-66083", "mrqa_squad-train-85406", "mrqa_squad-train-36207", "mrqa_squad-train-29348", "mrqa_squad-train-49244", "mrqa_squad-train-57557", "mrqa_squad-train-23483", "mrqa_squad-train-7192", "mrqa_squad-train-83159", "mrqa_squad-train-69402", "mrqa_squad-train-38826", "mrqa_squad-train-24003", "mrqa_squad-train-303", "mrqa_squad-train-34665", "mrqa_squad-train-84671", "mrqa_squad-train-71226", "mrqa_squad-train-51781", "mrqa_searchqa-validation-6909", "mrqa_searchqa-validation-13710", "mrqa_searchqa-validation-6252", "mrqa_newsqa-validation-3741", "mrqa_triviaqa-validation-2478", "mrqa_newsqa-validation-1634", "mrqa_triviaqa-validation-570", "mrqa_hotpotqa-validation-4832", "mrqa_searchqa-validation-2835", "mrqa_squad-validation-259", "mrqa_searchqa-validation-15075", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-386", "mrqa_hotpotqa-validation-3304", "mrqa_searchqa-validation-14361", "mrqa_newsqa-validation-3584", "mrqa_naturalquestions-validation-291", "mrqa_newsqa-validation-3986", "mrqa_squad-validation-7845", "mrqa_naturalquestions-validation-6555", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-3290", "mrqa_triviaqa-validation-1166"], "EFR": 0.9642857142857143, "Overall": 0.7504005456349206}, {"timecode": 45, "before_eval_results": {"predictions": ["locomotion", "the physician George Huntington, after whom it is named", "Cheryl Campbell", "The Satavahanas", "Michael Moriarty", "Canada", "111", "Virginia", "random - access memory ( RAM )", "1940", "Neela Montgomery", "Charlene Holt", "Amanda Leighton", "Fred Ott", "Mad - Eye Moody", "O'Meara", "Washington Wiz", "Missi Hale", "2001", "Arsenio Hall", "Portugal. The Man", "John Frank Stevens", "1940", "parthenogenic", "Lyle Waggoner", "west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Coconut Cove", "near the end of the school year", "a Canaanite god associated with child sacrifice", "vehicles", "10 June 1940", "Bill Pullman", "Little G minor symphony '', No. 25", "using a baby as bait, allowing a child to go through a torturous treatment to gain information, and allowing Dean to become a Dracula", "1990", "786 -- 802", "eliminate or reduce the trade barriers", "Wake County, it lies just north of the state capital, Raleigh", "NBC", "generally lacks the additives common to a complete tomato sauce and does not have the thickness of paste", "Bartemius Crouch Jr", "Martin Lawrence", "Effy", "Regia and House of the Vestal Virgins", "Nurhaci", "Abu Talib", "Americans", "August 22, 1980", "Professor Kantorek", "Yondu Udonta", "the next episode, `` Seeing Red ''", "luster", "bushfires", "Alexei Sayle", "Figaro", "Big 12 Conference", "Debbie Reynolds", "Dubai", "legitimacy of that race.", "Salt Lake City, Utah,", "Java", "OPEC", "an American jazz saxophonist and composer", "Current TV"], "metric_results": {"EM": 0.46875, "QA-F1": 0.603201102216674}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9387755102040816, 0.4444444444444445, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.5555555555555556, 0.8695652173913044, 0.0, 0.967741935483871, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-138", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3339", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-4457", "mrqa_newsqa-validation-2590"], "SR": 0.46875, "CSR": 0.5587635869565217, "retrieved_ids": ["mrqa_squad-train-50652", "mrqa_squad-train-7854", "mrqa_squad-train-11928", "mrqa_squad-train-28308", "mrqa_squad-train-77822", "mrqa_squad-train-49736", "mrqa_squad-train-47601", "mrqa_squad-train-84055", "mrqa_squad-train-68387", "mrqa_squad-train-28047", "mrqa_squad-train-17689", "mrqa_squad-train-62148", "mrqa_squad-train-79376", "mrqa_squad-train-32625", "mrqa_squad-train-3508", "mrqa_squad-train-52051", "mrqa_squad-train-72004", "mrqa_squad-train-7857", "mrqa_squad-train-38538", "mrqa_squad-train-58462", "mrqa_squad-train-61604", "mrqa_squad-train-46497", "mrqa_squad-train-55576", "mrqa_squad-train-60851", "mrqa_searchqa-validation-5180", "mrqa_newsqa-validation-3774", "mrqa_hotpotqa-validation-4079", "mrqa_squad-validation-3444", "mrqa_naturalquestions-validation-5070", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4575", "mrqa_newsqa-validation-250", "mrqa_triviaqa-validation-3223", "mrqa_naturalquestions-validation-7957", "mrqa_hotpotqa-validation-1742", "mrqa_naturalquestions-validation-3491", "mrqa_searchqa-validation-14480", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-2009", "mrqa_naturalquestions-validation-4561", "mrqa_searchqa-validation-12829", "mrqa_triviaqa-validation-3208", "mrqa_newsqa-validation-2884", "mrqa_squad-validation-7845", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3096", "mrqa_searchqa-validation-2971"], "EFR": 0.9705882352941176, "Overall": 0.7512609894501279}, {"timecode": 46, "before_eval_results": {"predictions": ["the main porch", "Pastoral farming", "The Nitty Gritty Dirt Band", "the right name tape", "Luther Ingram", "Taron Egerton", "Spencer Treat Clark", "Siddharth Arora / Vibhav Roy", "Ray Harroun", "scrolls", "Clarence Anglin", "to establish an electrochemical gradient ( often a proton gradient ) across a membrane", "Copernicus", "a single, implicitly structured data item", "the President", "capillary action", "electron donors", "T.J. Miller", "Ren\u00e9 Descartes", "2006 -- 06 season", "the dealer sets the cards face - down on the table near the player designated to make the cut, typically the player to the dealer's right", "1955", "the town of Acolman, just north of Mexico City", "23 September 1889", "indigenous to many forested parts of the world", "January 2018", "Colon Street", "The higher the vapor pressure of a liquid at a given temperature", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "1923", "Hugh S. Johnson", "Somatic motor neurons", "Lord Banquo", "Schadenfreude", "lithium", "al - khimar", "67", "Anthony Hopkins", "middle of the 15th century", "Ingrid Bergman", "political ideology", "c. 1000 AD", "Definition of the problems and / or goals", "Missouri River", "Venezuela and the remainder in Colombia", "Shaw", "privatized", "the chief lawyer of the United States government", "2018", "C\u03bc and C\u03b4", "August 29, 2017", "Beaujolais", "Edward Woodward", "Black Swan", "Lake Wallace", "Paul W. S. Anderson", "1790", "9", "the world's tallest building,", "due process rights", "V", "the Vedas", "Gertrude Stein", "Ilkley"], "metric_results": {"EM": 0.59375, "QA-F1": 0.698062275985663}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4166666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 0.25, 0.0, 0.0, 0.3870967741935484, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-4197", "mrqa_triviaqa-validation-2399", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-4201"], "SR": 0.59375, "CSR": 0.5595079787234043, "retrieved_ids": ["mrqa_squad-train-72004", "mrqa_squad-train-27979", "mrqa_squad-train-6811", "mrqa_squad-train-66390", "mrqa_squad-train-29161", "mrqa_squad-train-55060", "mrqa_squad-train-72245", "mrqa_squad-train-47448", "mrqa_squad-train-2645", "mrqa_squad-train-40895", "mrqa_squad-train-66677", "mrqa_squad-train-49357", "mrqa_squad-train-4317", "mrqa_squad-train-10477", "mrqa_squad-train-37644", "mrqa_squad-train-60459", "mrqa_squad-train-8225", "mrqa_squad-train-36753", "mrqa_squad-train-18357", "mrqa_squad-train-62806", "mrqa_squad-train-78441", "mrqa_squad-train-86084", "mrqa_squad-train-22742", "mrqa_squad-train-25403", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-8896", "mrqa_newsqa-validation-2904", "mrqa_naturalquestions-validation-7242", "mrqa_searchqa-validation-14194", "mrqa_hotpotqa-validation-5014", "mrqa_naturalquestions-validation-3392", "mrqa_hotpotqa-validation-1739", "mrqa_searchqa-validation-3203", "mrqa_hotpotqa-validation-1843", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10057", "mrqa_searchqa-validation-13520", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8228", "mrqa_naturalquestions-validation-4863", "mrqa_hotpotqa-validation-3773", "mrqa_newsqa-validation-4027", "mrqa_hotpotqa-validation-674", "mrqa_naturalquestions-validation-5109", "mrqa_searchqa-validation-3618", "mrqa_hotpotqa-validation-132", "mrqa_newsqa-validation-2112"], "EFR": 0.9230769230769231, "Overall": 0.7419076053600655}, {"timecode": 47, "before_eval_results": {"predictions": ["Germany", "Tim Russert", "Sebastian Lund ( Rob Kerkovich )", "Celtic", "1978", "two", "September 6, 2019", "Cliff's father", "September 19, 2017", "the Anglo - Norman French waleis", "31 October 1972", "23 September 1889", "Speaker of the House of Representatives", "Pittsburgh", "frontal lobe", "the 1940s", "increased productivity, trade, and secular economic trends", "2 %", "approximately 5 liters, with females generally having less blood volume than males", "Games", "the 17th episode in the third season", "the financial statement showing a firm's assets, liabilities and equity ( capital ) at a set point in time, usually the end of the fiscal year reported on the accompanying income statement", "the New York Yankees", "94 by 50", "Sam Waterston", "the realm of the Valar in Aman", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "doll", "bohrium", "Ravi Shastri", "the gated community of Pebble Beach", "Mali's wealth and power soon declined", "electrons", "November 2014", "Lewis Carroll", "Janis Joplin", "the South Pacific Ocean", "T'Pau", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "Blue laws in the United States vary by state", "in South America", "Edgar Lungu", "The Paris Sisters", "An optional message body", "Yes", "Brian Steele", "the centre of Munich", "Katherine Allentuck and Christopher Norris", "1998", "Thomas Chisholm", "Tommy James", "travel sickness", "a double dip recession", "\"The best is yet to come\"", "Chattahoochee", "Patterns of Sexual Behavior", "The Simpsons", "Argentina", "an ice jam", "Facebook and Google,", "decaffeinated", "One Flew Over the Cuckoo's Nest", "Stephen Hawking", "\"420,000 tax break previously granted to MTV's most popular program"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5746714072461355}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.4444444444444445, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.4, 0.08695652173913042, 0.33333333333333337, 0.0, 0.4, 0.8, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383]}}, "before_error_ids": ["mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-6466", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-485", "mrqa_triviaqa-validation-2385", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-2819", "mrqa_newsqa-validation-3459", "mrqa_searchqa-validation-14104", "mrqa_newsqa-validation-2608"], "SR": 0.453125, "CSR": 0.5572916666666667, "retrieved_ids": ["mrqa_squad-train-71278", "mrqa_squad-train-62021", "mrqa_squad-train-43671", "mrqa_squad-train-51891", "mrqa_squad-train-20020", "mrqa_squad-train-67610", "mrqa_squad-train-35374", "mrqa_squad-train-55412", "mrqa_squad-train-76880", "mrqa_squad-train-10443", "mrqa_squad-train-5209", "mrqa_squad-train-70414", "mrqa_squad-train-66358", "mrqa_squad-train-37025", "mrqa_squad-train-12414", "mrqa_squad-train-55435", "mrqa_squad-train-34758", "mrqa_squad-train-41836", "mrqa_squad-train-46535", "mrqa_squad-train-41239", "mrqa_squad-train-75917", "mrqa_squad-train-31642", "mrqa_squad-train-34986", "mrqa_squad-train-33242", "mrqa_searchqa-validation-11388", "mrqa_hotpotqa-validation-2009", "mrqa_squad-validation-6848", "mrqa_triviaqa-validation-7690", "mrqa_triviaqa-validation-1955", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2991", "mrqa_hotpotqa-validation-3844", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-10161", "mrqa_squad-validation-4326", "mrqa_triviaqa-validation-6532", "mrqa_naturalquestions-validation-6998", "mrqa_newsqa-validation-3174", "mrqa_naturalquestions-validation-7767", "mrqa_triviaqa-validation-6407", "mrqa_newsqa-validation-4041", "mrqa_naturalquestions-validation-5739", "mrqa_hotpotqa-validation-4655", "mrqa_naturalquestions-validation-8792", "mrqa_newsqa-validation-742", "mrqa_naturalquestions-validation-868", "mrqa_squad-validation-8471", "mrqa_naturalquestions-validation-2044"], "EFR": 0.9714285714285714, "Overall": 0.7511346726190476}, {"timecode": 48, "before_eval_results": {"predictions": ["Democratic VP candidate", "Stuttgart", "Three", "Long troop deployments", "well over 1,000 pounds", "Channel 4 has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "Dennis Davern,", "in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "people", "a man", "Swedish Prime Minister Fredrik Reinfeldt", "AMD", "Ferraris, a Lamborghini and an Acura NSX", "more than 20 times during the 1992 campaign", "From Spain to the Caribbean", "frees up a place", "Michael Krane,", "Russian bombers", "three gunmen", "all day starting at 10 a.m.", "0-0", "more than 200", "it -- you know -- black is beautiful.", "a one-shot victory in the Bob Hope Classic", "whites", "a cancer-causing toxic chemical.", "nearly $2 billion in stimulus funds", "US Airways Flight 1549", "to the southern city of Naples", "racial intolerance.", "Friday", "\"Twilight\"", "Robert Kimmitt.", "22-year-old", "TLC's \"The Little Couple,\"", "in Nuevo Leon, one of two states in northeastern", "10", "Retailers who don't speak out against it", "Anil Kapoor", "Samoa", "authorizing killings and kidnappings by paramilitary death squads.", "E. coli", "Ma Khin Khin Leh,", "a traditional form of lounge music", "\"The train ride up there is spectacular.", "1998.", "supporting Afghan forces in destroying drug labs, markets and convoys.", "London's O2 arena", "The EU naval force", "Cyprus", "devoted to federal ocean planning.", "between $10,000 and $30,000", "31 December 1600", "Number 4, Privet Drive, Little Whinging in Surrey, England", "sucars", "$10,000", "a hat", "five", "E Street Band", "England", "Ojibwe", "salt", "Everybody Have Fun Tonight", "the foreign exchange market ( currency )"], "metric_results": {"EM": 0.5, "QA-F1": 0.6035872863859997}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 1.0, 0.45454545454545453, 0.0, 0.0, 0.20000000000000004, 0.15384615384615385, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.3529411764705882, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.0, 0.32, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.75]}}, "before_error_ids": ["mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-3228", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-4169", "mrqa_naturalquestions-validation-4768", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-6642", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4399", "mrqa_searchqa-validation-6597", "mrqa_searchqa-validation-12129", "mrqa_naturalquestions-validation-3236"], "SR": 0.5, "CSR": 0.5561224489795918, "retrieved_ids": ["mrqa_squad-train-77619", "mrqa_squad-train-77851", "mrqa_squad-train-63609", "mrqa_squad-train-49688", "mrqa_squad-train-5575", "mrqa_squad-train-30692", "mrqa_squad-train-48973", "mrqa_squad-train-59123", "mrqa_squad-train-28558", "mrqa_squad-train-63270", "mrqa_squad-train-23943", "mrqa_squad-train-82592", "mrqa_squad-train-576", "mrqa_squad-train-80251", "mrqa_squad-train-74646", "mrqa_squad-train-75748", "mrqa_squad-train-52402", "mrqa_squad-train-75102", "mrqa_squad-train-61337", "mrqa_squad-train-25812", "mrqa_squad-train-40928", "mrqa_squad-train-15613", "mrqa_squad-train-45873", "mrqa_squad-train-59850", "mrqa_searchqa-validation-2714", "mrqa_searchqa-validation-9183", "mrqa_searchqa-validation-4319", "mrqa_naturalquestions-validation-4134", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2777", "mrqa_hotpotqa-validation-1239", "mrqa_squad-validation-4326", "mrqa_naturalquestions-validation-4103", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3772", "mrqa_searchqa-validation-3525", "mrqa_triviaqa-validation-5874", "mrqa_naturalquestions-validation-10039", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-398", "mrqa_hotpotqa-validation-4234", "mrqa_squad-validation-4539", "mrqa_searchqa-validation-15033", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-5014", "mrqa_naturalquestions-validation-1890", "mrqa_newsqa-validation-3588"], "EFR": 0.96875, "Overall": 0.7503651147959184}, {"timecode": 49, "before_eval_results": {"predictions": ["Pierre Laval", "Beckett", "james johnson", "Michaela Tabb", "Alpha Orionis", "Edward VIII", "clare quilty", "falcon", "Stephen Fry", "Libya", "Darshaan", "Robinson", "the vinaya", "The Daily Mail", "William Shakespeare", "Handley Page", "psychology, sociology, anthropology, religious studies, medicine and forensic science", "Rod Laver", "Texas", "slower", "Strait of Messina", "hoay", "from genocide, war crimes, ethnic cleansing and crimes against humanity", "Brian Deane", "Volkswagen Golf", "Emilia Fox", "October", "I", "catherine zeta", "South Africa", "Jim Braddock", "mediterranean", "1840", "bony", "Gibeon", "humbert humbert", "vomiting", "Richard Strauss", "sperm", "Syrian-American", "puffins", "golf", "purpurea", "Amnesty International", "Spearchucker", "her skills", "the Kingdom of Lesotho", "Variations", "Mauricio Pochettino", "Sarah Ferguson", "myxoma", "the season - five premiere episode `` Second Opinion ''", "exports to other states occurring around 1858", "Parker's pregnancy at the time of filming", "Dame Eileen June Atkins", "Synod of Chester", "James Gandolfini", "Bill Stanton", "Los Angeles Angels", "56,", "Twenty three", "the Berthas", "a parabola", "Patty Duke"], "metric_results": {"EM": 0.5, "QA-F1": 0.5853061868686869}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666667, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-6271", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4875", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-687", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-3752", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-4066", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-2844", "mrqa_searchqa-validation-12134", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-4996"], "SR": 0.5, "CSR": 0.5549999999999999, "retrieved_ids": ["mrqa_squad-train-29581", "mrqa_squad-train-45783", "mrqa_squad-train-77620", "mrqa_squad-train-2597", "mrqa_squad-train-22829", "mrqa_squad-train-85098", "mrqa_squad-train-47771", "mrqa_squad-train-83906", "mrqa_squad-train-38599", "mrqa_squad-train-74985", "mrqa_squad-train-1974", "mrqa_squad-train-8405", "mrqa_squad-train-7895", "mrqa_squad-train-83011", "mrqa_squad-train-16428", "mrqa_squad-train-81861", "mrqa_squad-train-59048", "mrqa_squad-train-3855", "mrqa_squad-train-48647", "mrqa_squad-train-20638", "mrqa_squad-train-41011", "mrqa_squad-train-59747", "mrqa_squad-train-76568", "mrqa_squad-train-14516", "mrqa_searchqa-validation-3653", "mrqa_newsqa-validation-3063", "mrqa_hotpotqa-validation-2769", "mrqa_triviaqa-validation-2036", "mrqa_naturalquestions-validation-868", "mrqa_newsqa-validation-2170", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-9130", "mrqa_newsqa-validation-831", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-8359", "mrqa_squad-validation-5262", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-6015", "mrqa_squad-validation-3811", "mrqa_newsqa-validation-1403", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-5055", "mrqa_newsqa-validation-169", "mrqa_naturalquestions-validation-143"], "EFR": 0.875, "Overall": 0.731390625}, {"timecode": 50, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2848", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4295", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4431", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10298", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-9299", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1200", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-1258", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3728", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-483", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-962", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-11836", "mrqa_searchqa-validation-1227", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-12864", "mrqa_searchqa-validation-13151", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-15671", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16627", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4044", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4628", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2365", "mrqa_squad-validation-2456", "mrqa_squad-validation-2595", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3863", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4528", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5128", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5644", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6228", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7297", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7492", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8232", "mrqa_squad-validation-8282", "mrqa_squad-validation-893", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9581", "mrqa_squad-validation-959", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-3860", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-52", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-5500", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5943", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7407", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-899"], "OKR": 0.89453125, "KG": 0.48203125, "before_eval_results": {"predictions": ["A Christmas Carol", "Jerry Zaks", "Bangladesh", "Dan Dare", "louis armstrong", "neath, South Wales", "Berlin", "Rocky Horror Picture Show", "1925", "Prince Albert", "bill", "Pakistan", "spider", "pappy", "elypshire", "Bull Moose Party", "Genoa", "Sh Ontars Sister", "syria", "jamaica", "Jessica Simpson", "ethel Cecile Rosalie Allen", "Earthquake", "Campania", "Charlie Chan", "chiba", "louis armstrong", "king Louis XVIII", "Anne Boleyn", "playoff basketball", "Thailand", "Aron Ralston ( James Franco)", "Cannes Film Festival", "Lew Hoad", "London", "Cybill Shepherd", "hawk", "widow", "PHYSICS", "Wolfgang Amadeus Mozart", "Anne-Marie Duff", "Joan Rivers", "water and salt", "14", "phobias", "pears soap", "guitar", "Toby", "Argentina", "Kenny Everett", "Fenn Street School", "1804", "drawing letters in the air", "November 3, 2007", "John Bingham", "Marktown", "Bit Instant", "well over 1,000 pounds", "Jet Republic", "from TV news coverage,", "W.C. Handy", "Lamb of God", "legs", "1978"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5817708333333333}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6594", "mrqa_triviaqa-validation-32", "mrqa_triviaqa-validation-4901", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-5137", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-4996", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-7424", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-94", "mrqa_triviaqa-validation-2685", "mrqa_naturalquestions-validation-3323", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-5281", "mrqa_searchqa-validation-3244", "mrqa_searchqa-validation-10670"], "SR": 0.53125, "CSR": 0.5545343137254901, "retrieved_ids": ["mrqa_squad-train-66042", "mrqa_squad-train-74522", "mrqa_squad-train-5646", "mrqa_squad-train-43203", "mrqa_squad-train-46143", "mrqa_squad-train-53808", "mrqa_squad-train-55168", "mrqa_squad-train-11553", "mrqa_squad-train-3360", "mrqa_squad-train-6888", "mrqa_squad-train-40240", "mrqa_squad-train-43154", "mrqa_squad-train-64826", "mrqa_squad-train-16462", "mrqa_squad-train-58994", "mrqa_squad-train-82142", "mrqa_squad-train-61328", "mrqa_squad-train-85688", "mrqa_squad-train-50751", "mrqa_squad-train-42244", "mrqa_squad-train-82023", "mrqa_squad-train-62141", "mrqa_squad-train-1240", "mrqa_squad-train-33325", "mrqa_naturalquestions-validation-681", "mrqa_triviaqa-validation-2749", "mrqa_newsqa-validation-1103", "mrqa_searchqa-validation-15075", "mrqa_newsqa-validation-3096", "mrqa_triviaqa-validation-2063", "mrqa_searchqa-validation-12864", "mrqa_newsqa-validation-742", "mrqa_squad-validation-4572", "mrqa_triviaqa-validation-3208", "mrqa_newsqa-validation-831", "mrqa_triviaqa-validation-254", "mrqa_searchqa-validation-7004", "mrqa_newsqa-validation-47", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-7614", "mrqa_squad-validation-7845", "mrqa_triviaqa-validation-7330", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-7724", "mrqa_squad-validation-4539", "mrqa_naturalquestions-validation-6216", "mrqa_triviaqa-validation-4336", "mrqa_naturalquestions-validation-10354"], "EFR": 0.9666666666666667, "Overall": 0.7358026960784313}, {"timecode": 51, "before_eval_results": {"predictions": ["Carthage", "blue", "Robin Ellis", "mortadella", "binder", "helium", "between the 'Tarsal' bones of the hind-foot and the 'Phalanges' bones in the toe", "natural world and mysticism", "Dancing with the Stars", "South Pacific", "lauren woolley", "Bosnia and Herzegovina", "France", "Sparta", "\"Route\"", "squash", "Northwestern University", "Turkey", "yvonne", "China", "diffusion", "David Bowie", "Robben Island", "bukwus", "frankincense", "a zoom lens", "jane", "Rocky Marciano", "Benny Hill", "tinie Tempah", "Ruth Ellis", "Egypt", "the wren", "Eton College", "margot", "tabby", "Aug 24", "lace", "hindu", "Valentine Dyall", "Lesley Lawson (n\u00e9e Hornby; born 19 September 1949)", "Portugal", "Opus Dei", "Flying Pickets", "gaseous", "Kenya", "Benjamin Disraeli, 1st Earl of Beaconsfield", "Ted", "Andrew Mitchell", "reanne Evans", "blood", "ummat al - Islamiyah", "Rachel Kelly Tucker", "Honor\u00e9 Mirabeau", "Big Bad Wolf", "Daniel Radcliffe", "Christopher Whitelaw Pine", "President Obama", "Croatia playmaker", "Tom Hanks,", "Curly Lambeau", "wafers", "Rosa Parks", "a bed bug"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6140625}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-4257", "mrqa_triviaqa-validation-2964", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-2110", "mrqa_triviaqa-validation-5154", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-13", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-6925", "mrqa_naturalquestions-validation-1455", "mrqa_newsqa-validation-318", "mrqa_searchqa-validation-2456", "mrqa_searchqa-validation-11960"], "SR": 0.5625, "CSR": 0.5546875, "retrieved_ids": ["mrqa_squad-train-41717", "mrqa_squad-train-37271", "mrqa_squad-train-75584", "mrqa_squad-train-53403", "mrqa_squad-train-61354", "mrqa_squad-train-81210", "mrqa_squad-train-69609", "mrqa_squad-train-37842", "mrqa_squad-train-37544", "mrqa_squad-train-86430", "mrqa_squad-train-36598", "mrqa_squad-train-32015", "mrqa_squad-train-79191", "mrqa_squad-train-57194", "mrqa_squad-train-41970", "mrqa_squad-train-20783", "mrqa_squad-train-20785", "mrqa_squad-train-82856", "mrqa_squad-train-23395", "mrqa_squad-train-37486", "mrqa_squad-train-24886", "mrqa_squad-train-80912", "mrqa_squad-train-72348", "mrqa_squad-train-40866", "mrqa_searchqa-validation-11137", "mrqa_triviaqa-validation-6407", "mrqa_naturalquestions-validation-3066", "mrqa_squad-validation-1938", "mrqa_triviaqa-validation-4057", "mrqa_hotpotqa-validation-16", "mrqa_triviaqa-validation-3004", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-444", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-585", "mrqa_newsqa-validation-3825", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-1165", "mrqa_searchqa-validation-6234", "mrqa_squad-validation-384", "mrqa_hotpotqa-validation-1747", "mrqa_naturalquestions-validation-5550", "mrqa_triviaqa-validation-3782", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-978"], "EFR": 1.0, "Overall": 0.7424999999999999}, {"timecode": 52, "before_eval_results": {"predictions": ["Ringo Starr", "Apprendi v. New Jersey", "8th congressional district", "Erreway", "North Queensland", "George Clooney", "Pamelyn Wanda", "Christian Kern", "The Social Network", "$10.5 million", "2017", "Dutch", "2014", "rapper", "Missouri", "Rochdale", "50 best cities to live in.\"", "Virginia", "The Godfather Part II", "two", "Rigoletto", "Scunthorpe", "Talib Kweli", "motor", "American", "1 September 1864", "Eugene O'Neill", "Colonel Gaddafi", "Scottish singer and \"Britain's Got Talent\" winner Jai McDowall", "a wooden roller ride", "Sofia the First", "Sufism", "$700 million", "Carl Mears", "magnus", "The Saturdays", "Battle of White Plains", "North Carolina", "John Joseph Travolta", "ice hockey", "Hong Kong", "2006", "Pacific Place", "jazz", "sarod", "2009", "Northern Ireland", "1977", "Russian Ark", "Delacorte Press", "the voice of The Beast", "17th Century", "April 12, 2017", "the Star Wars film in 1977", "an ancient optical illusion toy", "nickel", "vickers-Armstrong", "Little Havana.", "gopi Podila", "school,", "a compliment", "because of the extensive pine forests that have covered the state", "Clay", "Peter Jackson"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6218463827838828}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.3076923076923077, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.20000000000000004, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-5562", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-1351", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-2044", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-4939", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-474", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3422", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-468", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-11977", "mrqa_searchqa-validation-5814", "mrqa_naturalquestions-validation-4846"], "SR": 0.546875, "CSR": 0.5545400943396226, "retrieved_ids": ["mrqa_squad-train-49774", "mrqa_squad-train-65532", "mrqa_squad-train-31033", "mrqa_squad-train-83130", "mrqa_squad-train-710", "mrqa_squad-train-58400", "mrqa_squad-train-53366", "mrqa_squad-train-39094", "mrqa_squad-train-27131", "mrqa_squad-train-11131", "mrqa_squad-train-14480", "mrqa_squad-train-21577", "mrqa_squad-train-80255", "mrqa_squad-train-49113", "mrqa_squad-train-71181", "mrqa_squad-train-36788", "mrqa_squad-train-26647", "mrqa_squad-train-60929", "mrqa_squad-train-18464", "mrqa_squad-train-14813", "mrqa_squad-train-55262", "mrqa_squad-train-9094", "mrqa_squad-train-31249", "mrqa_squad-train-1033", "mrqa_squad-validation-2000", "mrqa_squad-validation-4629", "mrqa_newsqa-validation-1529", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-866", "mrqa_newsqa-validation-3806", "mrqa_squad-validation-7700", "mrqa_searchqa-validation-4319", "mrqa_naturalquestions-validation-6555", "mrqa_squad-validation-4838", "mrqa_triviaqa-validation-4150", "mrqa_naturalquestions-validation-5017", "mrqa_searchqa-validation-5038", "mrqa_naturalquestions-validation-6865", "mrqa_squad-validation-7338", "mrqa_hotpotqa-validation-2887", "mrqa_naturalquestions-validation-5041", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-2202", "mrqa_naturalquestions-validation-8277", "mrqa_triviaqa-validation-1380", "mrqa_newsqa-validation-1331", "mrqa_searchqa-validation-6597", "mrqa_naturalquestions-validation-1415"], "EFR": 0.9655172413793104, "Overall": 0.7355739671437866}, {"timecode": 53, "before_eval_results": {"predictions": ["Mike Mills", "1998", "30.9%", "Kittie", "American", "People!", "34.9 kilometres", "The Vanguard Group", "American", "Ready to Die", "to steal the plans for the Death Star", "Danish", "York County", "Seventeen", "Wake Island", "Australian Defence Force", "June 11, 1973", "Arthur William Bell III", "Boston", "Erreway", "Tampa Bay Lightning", "CBS", "Boston, Massachusetts", "Elena Stefanik", "Jennifer Taylor", "Correcaminos UAT", "9Lives brand cat food", "Black Ravens", "September 10, 1993", "Flamingo Hotel", "42,972", "9,000", "Michael Seater", "Drunken Master II", "more than 100 countries", "Bassline", "E22", "Allies of World War I", "Geraldine Page", "Kristina Ceyton and Kristian Moliere", "\"Linda McCartney's Life in Photography\"", "Philip Billard Municipal Airport", "1964 to 1974", "Big Fucking German", "law", "Hamlet", "Bow River and the Elbow River", "Gillian Anderson", "segues", "united Ireland", "\"Queen In-hyun's Man\"", "a flash music video featuring an animated dancing banana was created", "Virgil Ogletree", "4 School of Public Health in the country", "Topiary", "quentin tarantino", "1961", "Luca di Montezemolo", "near the Somali coast", "blind, the victim of an acid attack by a spurned suitor.", "a captain", "a motorcycle", "Marky Mark", "cheese"], "metric_results": {"EM": 0.53125, "QA-F1": 0.640562996031746}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.8571428571428571, 0.8, 0.4, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-1745", "mrqa_hotpotqa-validation-3615", "mrqa_hotpotqa-validation-2025", "mrqa_hotpotqa-validation-71", "mrqa_hotpotqa-validation-1199", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-2826", "mrqa_hotpotqa-validation-2404", "mrqa_hotpotqa-validation-2840", "mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-1033", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-115", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1641", "mrqa_searchqa-validation-5501", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-16209"], "SR": 0.53125, "CSR": 0.5541087962962963, "retrieved_ids": ["mrqa_squad-train-54896", "mrqa_squad-train-50624", "mrqa_squad-train-71265", "mrqa_squad-train-18984", "mrqa_squad-train-84076", "mrqa_squad-train-11827", "mrqa_squad-train-43238", "mrqa_squad-train-14750", "mrqa_squad-train-54101", "mrqa_squad-train-66110", "mrqa_squad-train-2695", "mrqa_squad-train-25923", "mrqa_squad-train-56200", "mrqa_squad-train-55822", "mrqa_squad-train-21037", "mrqa_squad-train-15213", "mrqa_squad-train-50470", "mrqa_squad-train-61948", "mrqa_squad-train-81925", "mrqa_squad-train-16849", "mrqa_squad-train-41730", "mrqa_squad-train-27237", "mrqa_squad-train-57397", "mrqa_squad-train-69103", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3484", "mrqa_hotpotqa-validation-4655", "mrqa_newsqa-validation-1229", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-808", "mrqa_newsqa-validation-2027", "mrqa_triviaqa-validation-2478", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-3922", "mrqa_hotpotqa-validation-1534", "mrqa_searchqa-validation-12440", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-1529", "mrqa_squad-validation-10143", "mrqa_newsqa-validation-1330", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-2495", "mrqa_naturalquestions-validation-7390", "mrqa_newsqa-validation-3063", "mrqa_naturalquestions-validation-10057", "mrqa_searchqa-validation-2052", "mrqa_triviaqa-validation-2262"], "EFR": 1.0, "Overall": 0.7423842592592592}, {"timecode": 54, "before_eval_results": {"predictions": ["her brother, Brian", "speech", "Callability -- Some bonds", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "Waylon Jennings", "August 2, 1990", "Charlene Holt", "eight episode", "the courts", "the jungle by wolves", "an official document permitting a specific individual to operate one or more types of motorized vehicles", "18", "Jewel Akens", "New England", "Irsay", "Abid Ali Neemuchwala", "winter", "Roxette", "Cadillac", "Vincent Price", "Union", "the chest, back, shoulders, torso and / or legs", "Authority", "drizzle, rain, sleet", "1967", "Virginia", "due to Parker's pregnancy at the time of filming", "lakes or reservoirs at high altitudes", "merengue", "Times Square", "the 1960s.", "IBM", "Elvis Presley", "New England", "1998", "Karen Gillan", "part of the present Indian constitutive state of Meghalaya ( formerly Assam ), which includes the present districts of East Jaintia Hills district, headquarter Khliehriat,", "A rear - view mirror", "April 29, 2009", "flawed democracy", "2026", "William Chatterton Dix", "Donna Mills", "Selena Gomez", "Steve Russell", "1881", "an armed conflict without the consent of the U.S. Congress", "bassist Timothy B. Schmit", "Games played", "Sir Henry Bartle Frere", "Games", "Cambridge", "Oklahoma City", "choroid", "1982", "2015", "film", "22", "one", "economic opportunities.", "Ray Walston", "Ukrainian", "Napoleon", "\"Traumnovelle\" (\"Dream Story\")"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7370931483753762}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true], "QA-F1": [0.5, 0.0, 0.4, 0.8387096774193548, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.823529411764706, 0.42857142857142855, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6885245901639345, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6086956521739131, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5785", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-6211", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-2996", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-7780"], "SR": 0.59375, "CSR": 0.5548295454545454, "retrieved_ids": ["mrqa_squad-train-69682", "mrqa_squad-train-34058", "mrqa_squad-train-35783", "mrqa_squad-train-41402", "mrqa_squad-train-51381", "mrqa_squad-train-33512", "mrqa_squad-train-44890", "mrqa_squad-train-49078", "mrqa_squad-train-66592", "mrqa_squad-train-57965", "mrqa_squad-train-63675", "mrqa_squad-train-31214", "mrqa_squad-train-34578", "mrqa_squad-train-36060", "mrqa_squad-train-56659", "mrqa_squad-train-49354", "mrqa_squad-train-6621", "mrqa_squad-train-8629", "mrqa_squad-train-78273", "mrqa_squad-train-42498", "mrqa_squad-train-59160", "mrqa_squad-train-58160", "mrqa_squad-train-68948", "mrqa_squad-train-47315", "mrqa_naturalquestions-validation-8294", "mrqa_searchqa-validation-15033", "mrqa_triviaqa-validation-1746", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-221", "mrqa_searchqa-validation-8705", "mrqa_hotpotqa-validation-1754", "mrqa_naturalquestions-validation-8909", "mrqa_triviaqa-validation-2399", "mrqa_squad-validation-3985", "mrqa_naturalquestions-validation-5579", "mrqa_triviaqa-validation-4759", "mrqa_newsqa-validation-1988", "mrqa_triviaqa-validation-254", "mrqa_searchqa-validation-3322", "mrqa_squad-validation-1802", "mrqa_searchqa-validation-3970", "mrqa_searchqa-validation-7774", "mrqa_hotpotqa-validation-1475", "mrqa_searchqa-validation-4641", "mrqa_triviaqa-validation-5106", "mrqa_squad-validation-2368", "mrqa_triviaqa-validation-977", "mrqa_newsqa-validation-4030"], "EFR": 0.8461538461538461, "Overall": 0.7117591783216783}, {"timecode": 55, "before_eval_results": {"predictions": ["the 1960s.", "Charlton Heston", "without deviating from basic strategy", "Doreen Mantle", "Felicity Huffman", "March 18, 2005", "a solitary figure who is not understood by others, but is actually wise", "the forward reaction proceeds at the same rate as the reverse reaction", "28 July 1914", "Terry Kath", "In 1922", "2017", "Sylvester Stallone", "2008", "Ethiopia", "Scots law", "Abid Ali Neemuchwala", "Hodel", "online instant messenger", "James Fleet", "two", "the medial epicondyle of the humerus", "Border Collie", "Massachusetts", "all officeholders annually", "commercial at", "The Sun", "60 by West All - Stars", "August 22, 1980", "Jack Nicklaus", "2020", "General George Washington", "7.6 % Per Annum", "9.0 -- 9.1", "Part 2", "1966", "201", "2026", "1926", "October 20, 1977", "Cetshwayo", "50", "al - Mamlakah al - \u02bbArab\u012byah", "Garbi\u00f1e Muguruza", "23 %", "St. Louis Cardinals", "Rockwell", "for scientific observation", "Charlotte Hornets", "the lumbar cistern", "February 7, 2018", "muezzin", "Elizabeth Taylor", "Sweden", "fourth", "\"Queen In-hyun's Man\"", "James Franco", "step up attacks against innocent civilians.\"", "a delegation of American Muslim and Christian leaders", "completely changed the business of music,", "because he hears a different drummer", "the Manchus", "Chicago White Sox", "seven"], "metric_results": {"EM": 0.59375, "QA-F1": 0.690906302213868}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.9473684210526316, 0.6, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.4444444444444445, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-6040", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5034", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-2622", "mrqa_searchqa-validation-2367", "mrqa_searchqa-validation-12190", "mrqa_newsqa-validation-1458"], "SR": 0.59375, "CSR": 0.5555245535714286, "retrieved_ids": ["mrqa_squad-train-1990", "mrqa_squad-train-70035", "mrqa_squad-train-25996", "mrqa_squad-train-58143", "mrqa_squad-train-80364", "mrqa_squad-train-52906", "mrqa_squad-train-26329", "mrqa_squad-train-10056", "mrqa_squad-train-75875", "mrqa_squad-train-55857", "mrqa_squad-train-53513", "mrqa_squad-train-47396", "mrqa_squad-train-4493", "mrqa_squad-train-32027", "mrqa_squad-train-38619", "mrqa_squad-train-84276", "mrqa_squad-train-45213", "mrqa_squad-train-48793", "mrqa_squad-train-57672", "mrqa_squad-train-15657", "mrqa_squad-train-22430", "mrqa_squad-train-4800", "mrqa_squad-train-79738", "mrqa_squad-train-69269", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-2660", "mrqa_triviaqa-validation-7264", "mrqa_newsqa-validation-3415", "mrqa_searchqa-validation-15103", "mrqa_hotpotqa-validation-1747", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-7507", "mrqa_hotpotqa-validation-4101", "mrqa_newsqa-validation-213", "mrqa_naturalquestions-validation-86", "mrqa_triviaqa-validation-2926", "mrqa_hotpotqa-validation-474", "mrqa_naturalquestions-validation-2794", "mrqa_hotpotqa-validation-1706", "mrqa_searchqa-validation-9159", "mrqa_naturalquestions-validation-1282", "mrqa_searchqa-validation-9822", "mrqa_newsqa-validation-3315", "mrqa_naturalquestions-validation-6466", "mrqa_newsqa-validation-2900", "mrqa_triviaqa-validation-2667", "mrqa_squad-validation-7688"], "EFR": 1.0, "Overall": 0.7426674107142858}, {"timecode": 56, "before_eval_results": {"predictions": ["Clifford", "Sweden", "White House drama", "Adam Smith", "Luxembourg", "El Hiero", "Salvador dali", "clef", "tyne", "road race", "The Blues Brothers", "onion", "1984", "frottage", "penhaligon", "Kevin Painter", "Betsy", "Messenger orbiter", "goose bump", "duck-billed platypus", "Montr\u00e9al", "Jeffrey Archer", "Four Tops", "Velazquez", "wED", "rAC plc", "Charlie Chan", "Apocalypse Now", "taekwondo", "Ishmael", "jubilee line", "d'Artagnan", "buttercups", "the head", "jules Verne", "Chuck Hagel", "haute", "zephyr", "300", "motorcycle", "France", "James Garner", "marinated dried fruits", "Jay-Z", "bird", "paraphilia", "George IV", "Margaret Beckett", "Washington Post", "White Ferns", "the United States", "1836", "Austria - Hungary", "Sean O' Neal", "140 million", "The New Yorker", "In Pursuit", "Aung San Suu Kyi", "his brother to surrender.", "\"Walk -- Don't Run\" and \"Hawaii Five-O\"", "Mel Stuart", "South Park", "Tucson", "The Simpsons"], "metric_results": {"EM": 0.578125, "QA-F1": 0.659375}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-1659", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6437", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-6930", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-4810", "mrqa_newsqa-validation-2308", "mrqa_searchqa-validation-4275", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-11576"], "SR": 0.578125, "CSR": 0.555921052631579, "retrieved_ids": ["mrqa_squad-train-74148", "mrqa_squad-train-12510", "mrqa_squad-train-84443", "mrqa_squad-train-24040", "mrqa_squad-train-85542", "mrqa_squad-train-86068", "mrqa_squad-train-81553", "mrqa_squad-train-11066", "mrqa_squad-train-70106", "mrqa_squad-train-62265", "mrqa_squad-train-27975", "mrqa_squad-train-70048", "mrqa_squad-train-59865", "mrqa_squad-train-17871", "mrqa_squad-train-69666", "mrqa_squad-train-59796", "mrqa_squad-train-10873", "mrqa_squad-train-36724", "mrqa_squad-train-55791", "mrqa_squad-train-19830", "mrqa_squad-train-22012", "mrqa_squad-train-12389", "mrqa_squad-train-17272", "mrqa_squad-train-1513", "mrqa_newsqa-validation-3404", "mrqa_triviaqa-validation-4057", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-5900", "mrqa_searchqa-validation-15103", "mrqa_newsqa-validation-765", "mrqa_triviaqa-validation-7304", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-1372", "mrqa_squad-validation-6439", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-3919", "mrqa_triviaqa-validation-1403", "mrqa_naturalquestions-validation-4354", "mrqa_hotpotqa-validation-246", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-2926", "mrqa_searchqa-validation-6597", "mrqa_newsqa-validation-1021", "mrqa_triviaqa-validation-2902", "mrqa_naturalquestions-validation-2016", "mrqa_triviaqa-validation-6746", "mrqa_squad-validation-9764", "mrqa_squad-validation-6706"], "EFR": 1.0, "Overall": 0.7427467105263157}, {"timecode": 57, "before_eval_results": {"predictions": ["CBS", "Lord Nelson", "london", "Utah", "black light", "lacrosse", "Packers", "utrecht", "Operation Overlord", "pangloss", "Virginia", "loubert lawford", "yachts", "potatoes", "1215", "pullover", "diffusion", "wye", "jack London", "South Carolina", "Ellesmere Port", "Parsley", "french", "chile", "m11", "Lynda Baron", "Robert Guerrero", "alcatraz", "90%", "Sven Goran Eriksson", "thomas bacall", "f1", "A", "Jordan", "a written record", "Motown", "Sudan", "marble", "robbie", "Collective Noun", "Edinburgh", "Anschluss", "silk", "Irving Berlin", "medical", "Leo Tolstoy", "Austria", "oasis", "red bull", "avatars", "blue", "KU", "2003", "Magnavox Odyssey", "Clark", "to receive the benefits of the Morrill Acts of 1862 and 1890", "fifth", "the group's only goal is to kill members of the Zetas", "Japan", "Dr. Maria Siemionow,", "a triangle", "Harry Potter", "apricots", "Red Sea"], "metric_results": {"EM": 0.5, "QA-F1": 0.5720942982456141}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.9473684210526316, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-1194", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2532", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5877", "mrqa_naturalquestions-validation-8707", "mrqa_hotpotqa-validation-1514", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2021", "mrqa_newsqa-validation-2792", "mrqa_searchqa-validation-16366", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-8276"], "SR": 0.5, "CSR": 0.5549568965517242, "retrieved_ids": ["mrqa_squad-train-34625", "mrqa_squad-train-34969", "mrqa_squad-train-10360", "mrqa_squad-train-65461", "mrqa_squad-train-50559", "mrqa_squad-train-42940", "mrqa_squad-train-52950", "mrqa_squad-train-67837", "mrqa_squad-train-57381", "mrqa_squad-train-20000", "mrqa_squad-train-79174", "mrqa_squad-train-65044", "mrqa_squad-train-13534", "mrqa_squad-train-69504", "mrqa_squad-train-7690", "mrqa_squad-train-74181", "mrqa_squad-train-34557", "mrqa_squad-train-34245", "mrqa_squad-train-15888", "mrqa_squad-train-49388", "mrqa_squad-train-39180", "mrqa_squad-train-52547", "mrqa_squad-train-56778", "mrqa_squad-train-84203", "mrqa_searchqa-validation-5149", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4057", "mrqa_naturalquestions-validation-2206", "mrqa_triviaqa-validation-4150", "mrqa_hotpotqa-validation-2522", "mrqa_triviaqa-validation-3456", "mrqa_naturalquestions-validation-4359", "mrqa_newsqa-validation-3174", "mrqa_naturalquestions-validation-7212", "mrqa_newsqa-validation-2777", "mrqa_naturalquestions-validation-868", "mrqa_searchqa-validation-15477", "mrqa_naturalquestions-validation-6426", "mrqa_triviaqa-validation-6746", "mrqa_naturalquestions-validation-2102", "mrqa_triviaqa-validation-5816", "mrqa_triviaqa-validation-2303", "mrqa_newsqa-validation-1021", "mrqa_triviaqa-validation-4593", "mrqa_hotpotqa-validation-5644", "mrqa_searchqa-validation-2835", "mrqa_naturalquestions-validation-1953", "mrqa_searchqa-validation-2728"], "EFR": 0.9375, "Overall": 0.7300538793103448}, {"timecode": 58, "before_eval_results": {"predictions": ["Christian Louboutin", "apple", "Galapagos Islands", "\u201cacts of the greatest heroism or of the most conspicuous courage in circumstances of extreme danger\u201d", "Tonight", "onions", "bratislava", "Mariah Carey", "blancmange", "daily Herald", "four", "Ishmael", "Dick Francis", "larkin", "wynkyn de Worde", "the opossum", "Soviets", "UK Independence Party", "William Wallace", "charlie", "Monster M*A*S*H", "helene hanff", "Cum mortuis", "condor", "lead", "Canada", "Laos", "sports agent", "Puerto Rico", "John Huston", "posh", "cat", "bajan", "aurochs", "us", "michael Churchill", "de Gaulle", "mercury", "the Kamikaze", "jane dall", "clarinets", "Mary Poppins", "Motifs & Symbols", "Queensland", "b reliable character actor", "George Eastman", "United Nations of Football", "Kenya", "king dame dame sefton", "tuscany", "n Nissan", "Ptolemy", "Toto", "commemorating fealty and filial piety", "Heather Langen Kamp", "Operation Iceberg", "quarterly", "Rambosk", "Revolutionary Armed Forces of Colombia,", "a preliminary injunction", "the death-bed", "Pole vault", "the Maine", "Coleridge"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5876201923076922}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-5177", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-5071", "mrqa_triviaqa-validation-1383", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-1832", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-5207", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1708", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-4635", "mrqa_hotpotqa-validation-2639", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-384", "mrqa_searchqa-validation-10238", "mrqa_searchqa-validation-5746"], "SR": 0.515625, "CSR": 0.5542902542372881, "retrieved_ids": ["mrqa_squad-train-31245", "mrqa_squad-train-61760", "mrqa_squad-train-61618", "mrqa_squad-train-25382", "mrqa_squad-train-64584", "mrqa_squad-train-44008", "mrqa_squad-train-6583", "mrqa_squad-train-75587", "mrqa_squad-train-45126", "mrqa_squad-train-50870", "mrqa_squad-train-60121", "mrqa_squad-train-76332", "mrqa_squad-train-427", "mrqa_squad-train-49217", "mrqa_squad-train-12238", "mrqa_squad-train-85022", "mrqa_squad-train-40105", "mrqa_squad-train-32697", "mrqa_squad-train-69432", "mrqa_squad-train-55239", "mrqa_squad-train-77472", "mrqa_squad-train-31956", "mrqa_squad-train-24249", "mrqa_squad-train-33709", "mrqa_hotpotqa-validation-5562", "mrqa_naturalquestions-validation-86", "mrqa_newsqa-validation-1538", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2452", "mrqa_triviaqa-validation-1953", "mrqa_naturalquestions-validation-1682", "mrqa_searchqa-validation-2971", "mrqa_hotpotqa-validation-1173", "mrqa_newsqa-validation-3557", "mrqa_naturalquestions-validation-34", "mrqa_newsqa-validation-3588", "mrqa_naturalquestions-validation-8317", "mrqa_searchqa-validation-12440", "mrqa_naturalquestions-validation-4192", "mrqa_triviaqa-validation-765", "mrqa_naturalquestions-validation-5792", "mrqa_newsqa-validation-3961", "mrqa_hotpotqa-validation-2377", "mrqa_triviaqa-validation-3348", "mrqa_newsqa-validation-3446", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-7228", "mrqa_squad-validation-10143"], "EFR": 0.967741935483871, "Overall": 0.7359689379442318}, {"timecode": 59, "before_eval_results": {"predictions": ["Jesus", "Strato of Lampsacus", "Eliot Cutler", "goalkeeper", "David Weissman", "\"The Suite Life of Cody\"", "comedy", "November 29, 1895", "Goddess of Pop", "Nicole Kidman", "near Philip Billard Municipal Airport", "CenturyLink Field", "two years", "Walt Disney and Ub Iwerks", "Martin \"Marty\" McCann", "WB Television Network", "Gainsborough Trinity Football Club", "turns out to be a terrible date", "$7.3 billion", "head coach", "George I", "sixteen", "Rural Electrification Act of 1936", "2015", "Nicholas \"Nick\" Offerman", "Golden Globe Award for Best Actress \u2013 Motion Picture Comedy or Musical", "Jahseh Dwayne Onfroy", "Dire Straits", "wives and girlfriend of high-profile sportspersons", "MGM Grand Garden Special Events Center", "Best Alternative Music Album", "Pieter van Musschenbroek", "1979", "70 m and 90 m", "prime minister", "film and short novels", "Bulgarian-Canadian", "KXII", "James Bond", "Eastern College Athletic Conference", "Indian", "William Corcoran Eustis", "World Outgames", "Adelaide", "Saturday", "Waylon Albright \"Shooter\" Jennings", "Can't Be Tamed", "Bolton, England", "Stephen Hawking", "Naomi Wallace", "Saoirse Ronan", "Nacio Herb Brown", "a region in Greek mythology", "Todd Bridges", "lemon", "dungaree", "Jaipur", "in the southern Gaza city of Rafah,", "to the U.S. Consulate in Rio de Janeiro,", "CNN", "All's Well That ends Well", "ice cream", "Rock Island", "Captain James Cook"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7235194493006993}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.8, 0.4, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2372", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-1007", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5682", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-1451", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-3404", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-4188", "mrqa_searchqa-validation-6145", "mrqa_searchqa-validation-15613"], "SR": 0.59375, "CSR": 0.5549479166666667, "retrieved_ids": ["mrqa_squad-train-42417", "mrqa_squad-train-25944", "mrqa_squad-train-39581", "mrqa_squad-train-74734", "mrqa_squad-train-82560", "mrqa_squad-train-12372", "mrqa_squad-train-56952", "mrqa_squad-train-12799", "mrqa_squad-train-28772", "mrqa_squad-train-43178", "mrqa_squad-train-30657", "mrqa_squad-train-17063", "mrqa_squad-train-82568", "mrqa_squad-train-75114", "mrqa_squad-train-64422", "mrqa_squad-train-30409", "mrqa_squad-train-57889", "mrqa_squad-train-85052", "mrqa_squad-train-55371", "mrqa_squad-train-75892", "mrqa_squad-train-2259", "mrqa_squad-train-16374", "mrqa_squad-train-38275", "mrqa_squad-train-36187", "mrqa_newsqa-validation-3697", "mrqa_triviaqa-validation-7083", "mrqa_naturalquestions-validation-1285", "mrqa_newsqa-validation-2816", "mrqa_triviaqa-validation-7618", "mrqa_squad-validation-257", "mrqa_naturalquestions-validation-5017", "mrqa_squad-validation-5724", "mrqa_searchqa-validation-10856", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-1906", "mrqa_newsqa-validation-1392", "mrqa_naturalquestions-validation-868", "mrqa_newsqa-validation-1988", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-4668", "mrqa_naturalquestions-validation-3329", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2497", "mrqa_hotpotqa-validation-114", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3918", "mrqa_hotpotqa-validation-4319"], "EFR": 0.9615384615384616, "Overall": 0.7348597756410256}, {"timecode": 60, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1041", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-1368", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1463", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-1496", "mrqa_hotpotqa-validation-1706", "mrqa_hotpotqa-validation-1919", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2256", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-261", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2735", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2792", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3018", "mrqa_hotpotqa-validation-3020", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-3272", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-423", "mrqa_hotpotqa-validation-4253", "mrqa_hotpotqa-validation-430", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4459", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4536", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-503", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-5869", "mrqa_hotpotqa-validation-594", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-929", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-10091", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10411", "mrqa_naturalquestions-validation-1047", "mrqa_naturalquestions-validation-10513", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-1220", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1539", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-1870", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-2855", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4054", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-435", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4486", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4584", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-5211", "mrqa_naturalquestions-validation-5360", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6211", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6328", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6432", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6759", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-6886", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-6952", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7976", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8317", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9607", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9921", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1064", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-1688", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1746", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2026", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2250", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2368", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3526", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3822", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-623", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-693", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-903", "mrqa_searchqa-validation-10249", "mrqa_searchqa-validation-1030", "mrqa_searchqa-validation-10918", "mrqa_searchqa-validation-11406", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-12440", "mrqa_searchqa-validation-12493", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-13257", "mrqa_searchqa-validation-13456", "mrqa_searchqa-validation-1357", "mrqa_searchqa-validation-14104", "mrqa_searchqa-validation-14480", "mrqa_searchqa-validation-15508", "mrqa_searchqa-validation-15568", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1898", "mrqa_searchqa-validation-1999", "mrqa_searchqa-validation-2052", "mrqa_searchqa-validation-2143", "mrqa_searchqa-validation-217", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3597", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-515", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5631", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6304", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-709", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-792", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9185", "mrqa_searchqa-validation-9394", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9846", "mrqa_squad-validation-10000", "mrqa_squad-validation-10097", "mrqa_squad-validation-10135", "mrqa_squad-validation-10184", "mrqa_squad-validation-10326", "mrqa_squad-validation-10339", "mrqa_squad-validation-10496", "mrqa_squad-validation-1240", "mrqa_squad-validation-1269", "mrqa_squad-validation-1408", "mrqa_squad-validation-1708", "mrqa_squad-validation-1713", "mrqa_squad-validation-1765", "mrqa_squad-validation-1890", "mrqa_squad-validation-2019", "mrqa_squad-validation-2328", "mrqa_squad-validation-2456", "mrqa_squad-validation-2751", "mrqa_squad-validation-280", "mrqa_squad-validation-2886", "mrqa_squad-validation-2897", "mrqa_squad-validation-2943", "mrqa_squad-validation-2953", "mrqa_squad-validation-2959", "mrqa_squad-validation-3021", "mrqa_squad-validation-305", "mrqa_squad-validation-3184", "mrqa_squad-validation-3364", "mrqa_squad-validation-3406", "mrqa_squad-validation-3444", "mrqa_squad-validation-3551", "mrqa_squad-validation-3608", "mrqa_squad-validation-3796", "mrqa_squad-validation-3812", "mrqa_squad-validation-3909", "mrqa_squad-validation-402", "mrqa_squad-validation-4265", "mrqa_squad-validation-4298", "mrqa_squad-validation-4326", "mrqa_squad-validation-4450", "mrqa_squad-validation-4452", "mrqa_squad-validation-4583", "mrqa_squad-validation-4630", "mrqa_squad-validation-491", "mrqa_squad-validation-5004", "mrqa_squad-validation-5134", "mrqa_squad-validation-5180", "mrqa_squad-validation-5479", "mrqa_squad-validation-5692", "mrqa_squad-validation-5737", "mrqa_squad-validation-5781", "mrqa_squad-validation-5836", "mrqa_squad-validation-5852", "mrqa_squad-validation-6017", "mrqa_squad-validation-6089", "mrqa_squad-validation-6353", "mrqa_squad-validation-6494", "mrqa_squad-validation-6875", "mrqa_squad-validation-71", "mrqa_squad-validation-7205", "mrqa_squad-validation-7338", "mrqa_squad-validation-7434", "mrqa_squad-validation-7613", "mrqa_squad-validation-7781", "mrqa_squad-validation-7993", "mrqa_squad-validation-8134", "mrqa_squad-validation-8282", "mrqa_squad-validation-908", "mrqa_squad-validation-9173", "mrqa_squad-validation-9176", "mrqa_squad-validation-9193", "mrqa_squad-validation-9234", "mrqa_squad-validation-9367", "mrqa_squad-validation-9376", "mrqa_squad-validation-9461", "mrqa_squad-validation-9614", "mrqa_squad-validation-9666", "mrqa_squad-validation-9771", "mrqa_squad-validation-9900", "mrqa_squad-validation-9959", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1282", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1479", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1668", "mrqa_triviaqa-validation-1683", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2017", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-260", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2685", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2712", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2902", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-3187", "mrqa_triviaqa-validation-3211", "mrqa_triviaqa-validation-3301", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-3456", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-358", "mrqa_triviaqa-validation-3627", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4150", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4482", "mrqa_triviaqa-validation-4494", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-468", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5261", "mrqa_triviaqa-validation-5294", "mrqa_triviaqa-validation-5377", "mrqa_triviaqa-validation-5381", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-5622", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-6012", "mrqa_triviaqa-validation-6225", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6665", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6755", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-7038", "mrqa_triviaqa-validation-7112", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7374", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-7558", "mrqa_triviaqa-validation-7560", "mrqa_triviaqa-validation-7571", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-807", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-881", "mrqa_triviaqa-validation-899"], "OKR": 0.908203125, "KG": 0.48671875, "before_eval_results": {"predictions": ["Batman", "the Constitution of India gives a federal structure to the Republic of India, declaring it to be a `` Union of States ''", "Frank Oz", "786 -- 802", "Patris et Filii et Spiritus Sancti", "19 July 1990", "John Ernest Crawford", "Andy Warhol", "September 1972", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "in France", "BC Jean", "the BBC", "57 days", "961", "Jay Baruchel", "December 1886", "U.S. declared neutrality and worked to broker a peace", "Las Vegas", "municipal governments and public school boards", "Andrea Rom", "Greg Norman", "2014", "Coroebus of Elis", "giant planet", "Crepuscular animals", "Hank Williams", "after 5 years, it was earning $300,000,000 a year", "Missouri", "Abraham Lincoln's war goals", "10 : 30am", "David Ben - Gurion", "RMS Titanic", "Grey Wardens", "Angel Island Immigration Station", "Eight", "traditional dance", "Vasoepididymostomy", "the fourth quarter of the preceding year", "Basil Henson", "God forgave / God gratified", "Broken Hill and Sydney", "Reverse - Flash", "save, rescue, savior", "sedimentary rock", "Sir Ronald Ross", "NFL Scouting combine", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "post translational modification", "UTC \u2212 09 : 00", "near Camarillo, California", "goneril", "tomato", "Guru Nanak", "footballer", "mixed martial arts", "James Tinling", "Rima Fakih", "165-room", "David Bowie,", "The Peach Melba", "hot chestnuts", "Godiva", "Sri Lanka Freedom Party"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6653657106782107}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.28571428571428575, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9848", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-7489", "mrqa_triviaqa-validation-1154", "mrqa_hotpotqa-validation-4952", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-12527"], "SR": 0.5625, "CSR": 0.5550717213114754, "retrieved_ids": ["mrqa_squad-train-47281", "mrqa_squad-train-36716", "mrqa_squad-train-57577", "mrqa_squad-train-26824", "mrqa_squad-train-7397", "mrqa_squad-train-32597", "mrqa_squad-train-72539", "mrqa_squad-train-63771", "mrqa_squad-train-20147", "mrqa_squad-train-2923", "mrqa_squad-train-5590", "mrqa_squad-train-45800", "mrqa_squad-train-63289", "mrqa_squad-train-4732", "mrqa_squad-train-28819", "mrqa_squad-train-72799", "mrqa_squad-train-19593", "mrqa_squad-train-43016", "mrqa_squad-train-13629", "mrqa_squad-train-44583", "mrqa_squad-train-14686", "mrqa_squad-train-43387", "mrqa_squad-train-73646", "mrqa_squad-train-47177", "mrqa_searchqa-validation-10670", "mrqa_hotpotqa-validation-4234", "mrqa_searchqa-validation-11532", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-4451", "mrqa_newsqa-validation-2998", "mrqa_hotpotqa-validation-4842", "mrqa_squad-validation-7525", "mrqa_naturalquestions-validation-9639", "mrqa_searchqa-validation-2728", "mrqa_hotpotqa-validation-3714", "mrqa_naturalquestions-validation-3186", "mrqa_hotpotqa-validation-484", "mrqa_triviaqa-validation-4966", "mrqa_searchqa-validation-1621", "mrqa_newsqa-validation-2497", "mrqa_triviaqa-validation-765", "mrqa_newsqa-validation-1535", "mrqa_naturalquestions-validation-5939", "mrqa_searchqa-validation-10536", "mrqa_hotpotqa-validation-2792", "mrqa_naturalquestions-validation-8228", "mrqa_triviaqa-validation-2959", "mrqa_hotpotqa-validation-674"], "EFR": 0.9285714285714286, "Overall": 0.7323536299765808}, {"timecode": 61, "before_eval_results": {"predictions": ["the 1994 season", "the emperor Cuauhtemoc and Tenochtitlan", "Conrad Lewis", "Bart Millard", "Pangaea or Pangea", "111", "Kiss", "Justice Harlan", "Home run ( home base or scoring )", "Rose Stagg ( Valene Kane )", "Aaron Taylor - Johnson", "from the ancient Etruscan root autu - and has within it connotations of the passing of the year", "November 28, 1973", "T.J. Miller", "the pursuit of excessive wealth", "Malina Weissman", "Pasek & Paul", "the economy", "741 weeks", "serve as the chief Senate spokespeople for the political parties respectively holding the majority and the minority in the United States Senate, and manage and schedule the legislative and executive business of the Senate", "potential of hydrogen", "1957", "Space is the Place", "1999", "the beginning of the American colonies", "the concentration of a compound exceeds its solubility", "February 9, 2018", "chimera", "Andrew Lloyd Webber", "Dollree Mapp", "the 15th century", "electron donors", "Rich Mullins", "In a cell", "Beijing", "Mickey Mantle", "Shawn", "Kirsten Simone Vangsness", "the dress shop", "5 : 7 -- 8", "1603", "September 25, 1987", "the 1980s and'90s", "1939", "Randy Newman", "1956", "Ravi River", "prokaryotic", "# 4", "an active supporter of the League of Nations", "New York City", "shoes", "Sven Goran Eriksson", "horses", "Vanarama", "Odysseus", "40 million", "Dr. Maria Siemionow,", "Brian Smith.", "gun charges,", "Ronald Reagan", "titanium", "Hastings", "king urien"], "metric_results": {"EM": 0.65625, "QA-F1": 0.73879276790461}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5263157894736842, 0.0, 1.0, 0.5333333333333333, 1.0, 1.0, 0.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3628", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-7356", "mrqa_searchqa-validation-15585", "mrqa_searchqa-validation-13746", "mrqa_triviaqa-validation-3768"], "SR": 0.65625, "CSR": 0.556703629032258, "retrieved_ids": ["mrqa_squad-train-66851", "mrqa_squad-train-50022", "mrqa_squad-train-35961", "mrqa_squad-train-61382", "mrqa_squad-train-41270", "mrqa_squad-train-40225", "mrqa_squad-train-5293", "mrqa_squad-train-8120", "mrqa_squad-train-7865", "mrqa_squad-train-53321", "mrqa_squad-train-2686", "mrqa_squad-train-86340", "mrqa_squad-train-7249", "mrqa_squad-train-38886", "mrqa_squad-train-4963", "mrqa_squad-train-8075", "mrqa_squad-train-75970", "mrqa_squad-train-35918", "mrqa_squad-train-82246", "mrqa_squad-train-45751", "mrqa_squad-train-37696", "mrqa_squad-train-70257", "mrqa_squad-train-57770", "mrqa_squad-train-23844", "mrqa_newsqa-validation-384", "mrqa_triviaqa-validation-5342", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-1757", "mrqa_naturalquestions-validation-6903", "mrqa_triviaqa-validation-1314", "mrqa_searchqa-validation-2656", "mrqa_hotpotqa-validation-2018", "mrqa_triviaqa-validation-2508", "mrqa_triviaqa-validation-2202", "mrqa_newsqa-validation-1688", "mrqa_hotpotqa-validation-3216", "mrqa_triviaqa-validation-3187", "mrqa_searchqa-validation-3633", "mrqa_searchqa-validation-398", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-1576", "mrqa_triviaqa-validation-1076", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-10684", "mrqa_triviaqa-validation-4961", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-2714"], "EFR": 0.9090909090909091, "Overall": 0.7287839076246334}, {"timecode": 62, "before_eval_results": {"predictions": ["\"Gansbaai\"", "horse racing", "Italy", "honeybee", "adare", "linesider", "63 to 144 inches", "patcham", "squash", "jack London", "AFC Wimbledon", "Scotland", "Edward VIII", "Bugs Bunny", "Swaziland", "ambidevous", "bear Grylls", "japan", "wake", "silver", "y Yahoo!", "Klaus dolls", "honey", "Joanne Harris", "The Forbidden Club", "Kunigunde Mackamotski", "Moldova", "Chatsworth House", "India and Pakistan", "Bull Moose Party", "portico", "eagle", "Stockholm", "Laurent Planchon", "Hercules", "Real Madrid", "Tennessee whiskey", "Sir Matthew Pinsent", "Khomeini", "salsa", "Cuba", "John McEnroe", "kia", "Robert Stroud", "cat Stevens", "cuticle", "tyne", "oxygen", "mulberry", "trumpet", "Cockermouth", "more rural in its character and more readily adopted Latin as its common language", "October 1941", "Natya Shastra", "Amber Heard", "near Philip Billard Municipal Airport", "gull-wing doors", "July 8", "sexual harassment claims", "5,600", "tuna", "banzai", "\"A Tale of Possessors, Self-dispossessed.\"", "`` If These Dolls Could Talk ''"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6526041666666667}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-945", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-740", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-5993", "mrqa_naturalquestions-validation-4416", "mrqa_hotpotqa-validation-652", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-9158"], "SR": 0.59375, "CSR": 0.5572916666666667, "retrieved_ids": ["mrqa_squad-train-25753", "mrqa_squad-train-69827", "mrqa_squad-train-54555", "mrqa_squad-train-75001", "mrqa_squad-train-8125", "mrqa_squad-train-44499", "mrqa_squad-train-62107", "mrqa_squad-train-51488", "mrqa_squad-train-53171", "mrqa_squad-train-56953", "mrqa_squad-train-2515", "mrqa_squad-train-6611", "mrqa_squad-train-7922", "mrqa_squad-train-52954", "mrqa_squad-train-30944", "mrqa_squad-train-80038", "mrqa_squad-train-37021", "mrqa_squad-train-31327", "mrqa_squad-train-39023", "mrqa_squad-train-3344", "mrqa_squad-train-61046", "mrqa_squad-train-83362", "mrqa_squad-train-17477", "mrqa_squad-train-55032", "mrqa_triviaqa-validation-613", "mrqa_naturalquestions-validation-9093", "mrqa_hotpotqa-validation-5588", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4821", "mrqa_squad-validation-4883", "mrqa_squad-validation-8819", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-4122", "mrqa_naturalquestions-validation-6442", "mrqa_searchqa-validation-15995", "mrqa_naturalquestions-validation-10416", "mrqa_newsqa-validation-1123", "mrqa_hotpotqa-validation-5296", "mrqa_newsqa-validation-2027", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-6148", "mrqa_triviaqa-validation-2495", "mrqa_searchqa-validation-2656", "mrqa_searchqa-validation-1621", "mrqa_searchqa-validation-1770", "mrqa_squad-validation-7880"], "EFR": 0.9615384615384616, "Overall": 0.7393910256410257}, {"timecode": 63, "before_eval_results": {"predictions": ["mother.", "southern city of Naples", "\"CNN Heroes: An All-Star Tribute\"", "for the rest of the year", "\"It was never our intention to offend anyone,\"", "Bob Bogle", "\"We say to the people of Gaza, give more resistance and we will be with you in the field,", "his business dealings", "Saturday", "Samoans", "Haiti's capital, Port-au-Prince,", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "a man", "Amsterdam, in the Netherlands,", "a review of state government practices completed in 100 days.", "prostate cancer,", "90", "a birdie four at the last hole", "Rima Fakih", "JBS Swift Beef Company,", "37", "slayings of actress Sharon Tate and four others.", "33-year-old", "Christopher Savoie", "12-hour", "Judge Herman Thomas", "President Obama's race in 2008.", "oldest documented bikinis", "laundry service", "over the treatment of Muslims,", "twice.", "a song about freedom of speech.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "Friday,", "Gavin de Becker", "400 years ago", "U.S. Consulate in Rio de Janeiro,", "Apple employees", "heavy turbulence", "$3 billion", "Nkepile M abuse", "resources", "mother's memories of his mother.", "Lance Cpl. Maria Lauterbach", "then-Sen. Obama", "Technological Institute of Higher Learning of Monterrey,", "female soldier,", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "David Russ,", "Gary Coleman", "Chinese", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "stability, security, and predictability of British law and government", "six", "Israel", "mathematics", "Chuck Yeager", "Detroit", "the north bank of the North Esk", "singer", "a dinosaur", "the atmosphere", "The Thief knot", "Fort Kent, Maine"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6751422382996316}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.14634146341463414, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.10810810810810811, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.375]}}, "before_error_ids": ["mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2860", "mrqa_newsqa-validation-1820", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-3087", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-7224", "mrqa_triviaqa-validation-2413", "mrqa_hotpotqa-validation-2249", "mrqa_hotpotqa-validation-1540", "mrqa_searchqa-validation-9739", "mrqa_searchqa-validation-9458", "mrqa_naturalquestions-validation-6670"], "SR": 0.609375, "CSR": 0.55810546875, "retrieved_ids": ["mrqa_squad-train-69072", "mrqa_squad-train-81681", "mrqa_squad-train-9648", "mrqa_squad-train-25912", "mrqa_squad-train-20592", "mrqa_squad-train-57892", "mrqa_squad-train-67361", "mrqa_squad-train-46229", "mrqa_squad-train-83704", "mrqa_squad-train-3119", "mrqa_squad-train-37233", "mrqa_squad-train-37481", "mrqa_squad-train-10263", "mrqa_squad-train-85198", "mrqa_squad-train-19974", "mrqa_squad-train-80479", "mrqa_squad-train-19327", "mrqa_squad-train-25071", "mrqa_squad-train-13553", "mrqa_squad-train-77765", "mrqa_squad-train-78414", "mrqa_squad-train-38589", "mrqa_squad-train-29836", "mrqa_squad-train-46207", "mrqa_naturalquestions-validation-6237", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-3930", "mrqa_naturalquestions-validation-5155", "mrqa_newsqa-validation-1893", "mrqa_triviaqa-validation-7619", "mrqa_squad-validation-1765", "mrqa_naturalquestions-validation-8999", "mrqa_newsqa-validation-2731", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7414", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-8068", "mrqa_triviaqa-validation-945", "mrqa_newsqa-validation-418", "mrqa_searchqa-validation-172", "mrqa_naturalquestions-validation-767", "mrqa_searchqa-validation-9183", "mrqa_triviaqa-validation-774", "mrqa_hotpotqa-validation-1173", "mrqa_squad-validation-5262", "mrqa_newsqa-validation-3986", "mrqa_triviaqa-validation-6396"], "EFR": 1.0, "Overall": 0.7472460937500001}, {"timecode": 64, "before_eval_results": {"predictions": ["Slavic culture", "beer", "sweepstakes", "General McClellan", "Caleb Madison", "Shiraz", "Oz", "calcite", "Bologna", "potatoes", "Princeton", "China", "the knight", "Evian", "unicorns", "Jerusalem", "Andes", "Jim Jarmusch", "Martin Luther", "Miles Davis", "Tennessee", "Audrey Hepburn", "Falafel", "Aladdin", "history of Lake County, Indiana, and the Calumet region", "Derek Jeter", "Arthur C. Clarke", "Washington Redskins", "Vietnam War", "Carl Sagan", "Sydney", "Christian Louboutin", "the monk seal", "beer", "letters received or written", "milk with less than 1% fat", "Ginger Rogers", "Beijing", "plumeria", "Lafayette", "Marie Osmond", "The Pickwick Club", "pemmican", "comet", "Chuck Yeager", "Newton", "sheep", "Eragon", "Georgia", "French toast", "the Fifth Amendment", "Elvis Presley", "at the Louvre Museum in Paris", "the notion that an English p Larson may'have his nose up in the air ', upturned like the chicken's rear end", "bacall", "aws", "denarius", "18 November [O.S. 6 November] 1860", "White Horse", "the 1824 Constitution of Mexico", "Kurt Cobain", "Glasgow, Scotland", "Christopher Savoie", "London"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6571632258672699}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.7058823529411765, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9479", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-13851", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-6953", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-6692", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-1377", "mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-13122", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-3875", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-5831", "mrqa_triviaqa-validation-5387", "mrqa_hotpotqa-validation-4263", "mrqa_newsqa-validation-2011"], "SR": 0.59375, "CSR": 0.5586538461538462, "retrieved_ids": ["mrqa_squad-train-37219", "mrqa_squad-train-44460", "mrqa_squad-train-53006", "mrqa_squad-train-76672", "mrqa_squad-train-21473", "mrqa_squad-train-33575", "mrqa_squad-train-47531", "mrqa_squad-train-10767", "mrqa_squad-train-66510", "mrqa_squad-train-2274", "mrqa_squad-train-80123", "mrqa_squad-train-63836", "mrqa_squad-train-78976", "mrqa_squad-train-43570", "mrqa_squad-train-83326", "mrqa_squad-train-38528", "mrqa_squad-train-80891", "mrqa_squad-train-14765", "mrqa_squad-train-75006", "mrqa_squad-train-14846", "mrqa_squad-train-4156", "mrqa_squad-train-57721", "mrqa_squad-train-12294", "mrqa_squad-train-51763", "mrqa_triviaqa-validation-501", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1966", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-2262", "mrqa_naturalquestions-validation-10433", "mrqa_triviaqa-validation-4864", "mrqa_squad-validation-7876", "mrqa_squad-validation-5724", "mrqa_triviaqa-validation-2655", "mrqa_searchqa-validation-6150", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3435", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-1199", "mrqa_newsqa-validation-3290", "mrqa_naturalquestions-validation-3033", "mrqa_searchqa-validation-2141", "mrqa_triviaqa-validation-6437", "mrqa_naturalquestions-validation-6019", "mrqa_hotpotqa-validation-4311", "mrqa_naturalquestions-validation-681", "mrqa_triviaqa-validation-6833"], "EFR": 0.9230769230769231, "Overall": 0.731971153846154}, {"timecode": 65, "before_eval_results": {"predictions": ["Montana", "the Knight Ridder", "Casino Royale", "the Albrecht G Kessler", "the Apprentice", "Aeschylus", "the College of William & Mary", "Intelligence Quotient", "American author Robert A. Heinlein", "cracker barrel", "24", "Cowpoke", "Monty Python and the Holy Grail", "Ludwig van Beethoven", "Stalin", "In God We Trust", "Portland", "the People's Liberation Army (PLA) Air Force", "the Absalom", "Castle Rock", "Bollywood", "Marcia Brady", "the Habsburg", "Euphoria", "a Twinkie", "the altitude", "the Book of Laughter and forgetting", "Richard", "Anne of Cleves", "a lover of words", "Fred Rogers", "Princess Liliuokalani", "the Pituitary Gland", "the South African War", "the pulp", "Michelle Pfeiffer", "Aswan", "Billy Ray Cyrus", "a quiver", "The Body", "Impostor syndrome", "the melting of ice", "Davy Crockett", "Sagittarius", "a volcano", "zinc", "Dubliners", "Jules Verne", "Cuba", "the Taliban", "the Kennedy memorial", "Ali", "Otis Timson", "during prenatal development", "Exile", "Carmen Miranda", "Joan Crawford", "Harvey Birdman", "Chief of the Operations Staff of the Armed Forces High Command", "Erich Maria Remarque", "to never make the cut for a face-to-face interview with the president", "ensuring that all prescription drugs on the market are FDA approved", "15-year-old's", "Firoz Shah Tughlaq"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6251723530595813}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.25, 0.0, 0.5, 0.08695652173913043, 0.07407407407407408, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15873", "mrqa_searchqa-validation-5082", "mrqa_searchqa-validation-9446", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-5265", "mrqa_searchqa-validation-3800", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-5095", "mrqa_searchqa-validation-10927", "mrqa_searchqa-validation-9120", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-16276", "mrqa_searchqa-validation-3734", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-8840", "mrqa_searchqa-validation-6273", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-8703", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-2440", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5531", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1062", "mrqa_naturalquestions-validation-10509"], "SR": 0.515625, "CSR": 0.5580018939393939, "retrieved_ids": ["mrqa_squad-train-54183", "mrqa_squad-train-60759", "mrqa_squad-train-34964", "mrqa_squad-train-29690", "mrqa_squad-train-61024", "mrqa_squad-train-32581", "mrqa_squad-train-45802", "mrqa_squad-train-43971", "mrqa_squad-train-45571", "mrqa_squad-train-49223", "mrqa_squad-train-47638", "mrqa_squad-train-44126", "mrqa_squad-train-76408", "mrqa_squad-train-72314", "mrqa_squad-train-79627", "mrqa_squad-train-33955", "mrqa_squad-train-64886", "mrqa_squad-train-12210", "mrqa_squad-train-49497", "mrqa_squad-train-84700", "mrqa_squad-train-69875", "mrqa_squad-train-74166", "mrqa_squad-train-56060", "mrqa_squad-train-7225", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1412", "mrqa_squad-validation-1938", "mrqa_newsqa-validation-2103", "mrqa_squad-validation-2000", "mrqa_searchqa-validation-709", "mrqa_hotpotqa-validation-1273", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-2262", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-639", "mrqa_newsqa-validation-3726", "mrqa_triviaqa-validation-6642", "mrqa_squad-validation-3497", "mrqa_triviaqa-validation-3660", "mrqa_hotpotqa-validation-5014", "mrqa_triviaqa-validation-3361", "mrqa_searchqa-validation-15103", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-4996", "mrqa_newsqa-validation-872", "mrqa_naturalquestions-validation-4520", "mrqa_triviaqa-validation-1076", "mrqa_newsqa-validation-3394"], "EFR": 0.967741935483871, "Overall": 0.740773765884653}, {"timecode": 66, "before_eval_results": {"predictions": ["eight", "a endocrine organ", "December 20, 1951", "the ninth w\u0101", "Terry Kath", "inability to comprehend and formulate language", "hyperirritable spots in the fascia surrounding skeletal muscle", "when the Moon's ecliptic longitude and the Sun's Ecliptics longitude differ by 0 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0, respectively", "1546", "Lord Banquo", "January 1923", "Alex Drake", "a habitat", "potatoes", "archeological materials", "free floating", "the United States", "Help!", "30 October 1918", "Austria - Hungary", "Domhnall Gleeson", "13 to 22 June 2012", "T - Bone Walker", "Paul Baumer", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "Executive Residence of the White House Complex", "Article Two", "April 13, 2018", "Bush", "Yuzuru Hanyu", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "off the southernmost tip of the South American mainland", "appearing in the five - year time jump for her brother's wedding to Serena van der Woodsen", "The first recorded Christmas celebration was in Rome in 336", "January 1, 2016", "Leonardo da Vinci", "absolute temperature", "Thawne", "Philippe Petit", "Proposition 103", "in 2008", "inside the cell nucleus", "Julie Adams", "775 rooms", "Pakistan", "100 % owned by Xiu Li Dai and Yongge Dai", "Norman Whitfield", "Americans who served in the armed forces and as civilians during World War II", "Uplokayukta", "James Fleet", "the year AD 1 immediately follows the year 1 BC", "David Davis", "Dirty Dancing", "mumps", "Delphi Lawrence", "2 May 2015", "International Boxing Federation", "dress", "Pakistani officials, India", "British", "Jamaica", "cocoa", "Python", "not guilty"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7025683350263182}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.8, 0.8333333333333333, 0.8095238095238095, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9387755102040816, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 0.923076923076923, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-4419", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-47", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1259", "mrqa_searchqa-validation-10836"], "SR": 0.546875, "CSR": 0.5578358208955223, "retrieved_ids": ["mrqa_squad-train-30187", "mrqa_squad-train-26555", "mrqa_squad-train-85862", "mrqa_squad-train-28692", "mrqa_squad-train-23310", "mrqa_squad-train-8718", "mrqa_squad-train-32000", "mrqa_squad-train-16393", "mrqa_squad-train-71525", "mrqa_squad-train-65643", "mrqa_squad-train-14426", "mrqa_squad-train-57884", "mrqa_squad-train-50183", "mrqa_squad-train-40266", "mrqa_squad-train-29611", "mrqa_squad-train-62108", "mrqa_squad-train-39855", "mrqa_squad-train-54384", "mrqa_squad-train-38003", "mrqa_squad-train-5065", "mrqa_squad-train-20737", "mrqa_squad-train-38200", "mrqa_squad-train-44310", "mrqa_squad-train-34720", "mrqa_squad-validation-2000", "mrqa_hotpotqa-validation-1437", "mrqa_triviaqa-validation-7220", "mrqa_naturalquestions-validation-2069", "mrqa_naturalquestions-validation-6650", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-4032", "mrqa_naturalquestions-validation-2855", "mrqa_newsqa-validation-2444", "mrqa_hotpotqa-validation-1745", "mrqa_newsqa-validation-2471", "mrqa_squad-validation-9136", "mrqa_naturalquestions-validation-81", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-8909", "mrqa_newsqa-validation-744", "mrqa_hotpotqa-validation-227", "mrqa_naturalquestions-validation-9013", "mrqa_newsqa-validation-1514", "mrqa_hotpotqa-validation-3290", "mrqa_hotpotqa-validation-1667", "mrqa_triviaqa-validation-2110", "mrqa_searchqa-validation-10015", "mrqa_triviaqa-validation-3187"], "EFR": 0.9655172413793104, "Overall": 0.7402956124549666}, {"timecode": 67, "before_eval_results": {"predictions": ["Pyeongchang County, Gangwon Province, South Korea", "Padawan", "in a liquid solution", "April 1917", "London", "Utah", "1969", "by October 1986", "the referee", "English law applies in England and Wales", "interspecific hybridization and parthenogenesis", "Dana Castellaneta", "Reproductive system", "a list of island countries", "local authorities", "terrier", "Gibraltar", "September 1947", "7 July", "in the bone marrow", "Sophia Akuffo", "who the better fighters are relative to their weight", "the team", "Sarah Josepha Hale", "Ingrid Bergman", "Jessica Simpson", "on the microscope's stage", "the Old Testament", "Daren Maxwell Kagasoff", "Steveston Outdoor pool in Richmond, BC", "Ricardo Chavira", "Michigan", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "Friedman Billings Ramsey", "Miami Heat of the National Basketball Association ( NBA )", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys", "Toronto Islands", "lighter", "the final episode of the series", "Roger Nichols and Paul Williams", "Konakuppakatil Gopinathan Balakrishnan", "Ryan Evancic", "Border Collie", "Vesta", "1665 to 1666", "sugars and amino acids", "Menelaus", "on the continent of Antarctica", "California", "during Christmas season in the late 1970s", "December 1349", "Ipswich Town", "dreary", "British Airways", "Genderqueer", "14,673", "YouTube", "five", "Joel \"Taz\" DiGregorio", "military trials", "the Death Valley", "1972", "Ichabod Crane", "Thomas Jefferson"], "metric_results": {"EM": 0.5, "QA-F1": 0.6439815527504105}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.4444444444444445, 0.4, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5517241379310345, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.5, 0.0, 0.1111111111111111, 0.625, 1.0, 0.4444444444444445, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-1269", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-4558", "mrqa_triviaqa-validation-263", "mrqa_hotpotqa-validation-621", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-4207"], "SR": 0.5, "CSR": 0.5569852941176471, "retrieved_ids": ["mrqa_squad-train-21960", "mrqa_squad-train-61027", "mrqa_squad-train-81368", "mrqa_squad-train-37786", "mrqa_squad-train-3251", "mrqa_squad-train-69080", "mrqa_squad-train-3333", "mrqa_squad-train-28810", "mrqa_squad-train-45294", "mrqa_squad-train-11281", "mrqa_squad-train-36291", "mrqa_squad-train-75815", "mrqa_squad-train-75319", "mrqa_squad-train-4158", "mrqa_squad-train-74175", "mrqa_squad-train-83105", "mrqa_squad-train-92", "mrqa_squad-train-24113", "mrqa_squad-train-65368", "mrqa_squad-train-34702", "mrqa_squad-train-62522", "mrqa_squad-train-57749", "mrqa_squad-train-34797", "mrqa_squad-train-66427", "mrqa_newsqa-validation-4027", "mrqa_squad-validation-3812", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-9818", "mrqa_newsqa-validation-868", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-10416", "mrqa_hotpotqa-validation-1996", "mrqa_triviaqa-validation-3660", "mrqa_newsqa-validation-1825", "mrqa_triviaqa-validation-5993", "mrqa_searchqa-validation-7923", "mrqa_triviaqa-validation-7497", "mrqa_searchqa-validation-2761", "mrqa_newsqa-validation-3720", "mrqa_naturalquestions-validation-3323", "mrqa_searchqa-validation-13710", "mrqa_triviaqa-validation-4901", "mrqa_naturalquestions-validation-2196", "mrqa_squad-validation-8819", "mrqa_triviaqa-validation-5919", "mrqa_searchqa-validation-7004", "mrqa_naturalquestions-validation-2907"], "EFR": 0.96875, "Overall": 0.7407720588235295}, {"timecode": 68, "before_eval_results": {"predictions": ["Malaysia", "nomadic people", "5.4%", "Parkinson's Disease", "John Jay", "Warsaw", "Soviet", "Murfreesboro", "South Africa", "David", "Muhammad", "Melanesia", "Joe Namath", "high and dry", "a Doll", "the Baroque Modernity", "Cleopatra", "the International Space Station", "Iran", "Gaius", "the Charleston", "South Africa", "John Deere", "Thames", "Oxford", "William Wordsworth", "Witch of the West", "Tuscaloosa", "Germany", "Sabino Canyon", "Frasier", "David Bowie", "Sicily", "Herbert Hoover", "Zhou Enlai", "pep", "Lake Geneva", "Barbie", "The Mole", "HIV/AIDS", "Today Show", "Golden", "jaundice", "Bern", "bchamel", "Jackie Robinson", "Buzzbee", "Diane Arbus", "Willa Cather", "\"sustained\"", "the marathon", "Masha Skorobogatov", "Kyla Pratt", "Dumont d'Urville Station", "Union Gap", "Charlotte\\'s Web", "Cameroon", "Ding Sheng", "May 5, 2015", "Massapequa", "near Grand Ronde, Oregon.", "1937,", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "acid phosphate"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5878720238095239}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 0.2666666666666667, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3218", "mrqa_searchqa-validation-2956", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-13698", "mrqa_searchqa-validation-11405", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-8634", "mrqa_searchqa-validation-2330", "mrqa_searchqa-validation-6130", "mrqa_searchqa-validation-1918", "mrqa_searchqa-validation-16229", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-5611", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-5307", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-9364", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3166", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-893", "mrqa_triviaqa-validation-3820"], "SR": 0.515625, "CSR": 0.5563858695652174, "retrieved_ids": ["mrqa_squad-train-34481", "mrqa_squad-train-35008", "mrqa_squad-train-12764", "mrqa_squad-train-26176", "mrqa_squad-train-35406", "mrqa_squad-train-75678", "mrqa_squad-train-64370", "mrqa_squad-train-42924", "mrqa_squad-train-30054", "mrqa_squad-train-25521", "mrqa_squad-train-26161", "mrqa_squad-train-40644", "mrqa_squad-train-60869", "mrqa_squad-train-4626", "mrqa_squad-train-73228", "mrqa_squad-train-61740", "mrqa_squad-train-52562", "mrqa_squad-train-45043", "mrqa_squad-train-5751", "mrqa_squad-train-68062", "mrqa_squad-train-45873", "mrqa_squad-train-38129", "mrqa_squad-train-51682", "mrqa_squad-train-46454", "mrqa_newsqa-validation-744", "mrqa_squad-validation-2595", "mrqa_triviaqa-validation-3073", "mrqa_searchqa-validation-9739", "mrqa_triviaqa-validation-3522", "mrqa_naturalquestions-validation-3170", "mrqa_newsqa-validation-1004", "mrqa_triviaqa-validation-2843", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-232", "mrqa_triviaqa-validation-7083", "mrqa_searchqa-validation-4032", "mrqa_naturalquestions-validation-7549", "mrqa_newsqa-validation-4041", "mrqa_hotpotqa-validation-3721", "mrqa_triviaqa-validation-23", "mrqa_naturalquestions-validation-3628", "mrqa_squad-validation-2145", "mrqa_searchqa-validation-8872", "mrqa_naturalquestions-validation-9239", "mrqa_newsqa-validation-3259", "mrqa_triviaqa-validation-4315", "mrqa_hotpotqa-validation-5154", "mrqa_naturalquestions-validation-10039"], "EFR": 1.0, "Overall": 0.7469021739130435}, {"timecode": 69, "before_eval_results": {"predictions": ["an aqueduct", "quark", "Christopher Reeve", "Bucharest", "Macbeth", "John Jacob Astor", "Trans casting", "Penn Station", "The Sun Also Rises", "Cherokee", "Ferrari", "banquet", "Burgundy", "Joe Hill", "Revelation", "Kentucky", "Supernatural", "Jean Foucault", "Montana", "Deep brain stimulation", "krukhit", "Amazon", "Park Hill Oklahoma", "Anne Hathaway", "Model A", "Greece", "Vietnam", "Tintern", "Canuck", "Daniel Day-Lewis", "Sir Isaac Newton", "Blue Ridge Mountain range", "Frdric Chopin", "Susan B. Anthony", "Dexter", "the quokka", "the Washington Bullets", "aws", "Batman", "Knocked Up", "the first chimp in space", "Sir John Soane", "jazz", "South Carolina", "Han Solo", "Georg Brandis", "proscenium", "Zapata", "cape", "Barry Goldwater", "Guinness", "Portugal. The Man", "1983", "singers Laura Williams and Sally Dworsky", "wales", "red", "Greek", "Best Animated Feature", "1937", "Stephen James Ireland", "a group of college students of Pakistani background.", "Copts", "\"an eye for an eye,\"", "Retina display"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6510416666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-14157", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-6228", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-10046", "mrqa_searchqa-validation-6956", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-16813", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-14736", "mrqa_searchqa-validation-10184", "mrqa_searchqa-validation-9078", "mrqa_searchqa-validation-2333", "mrqa_searchqa-validation-6185", "mrqa_searchqa-validation-5427", "mrqa_naturalquestions-validation-6349", "mrqa_triviaqa-validation-3274", "mrqa_triviaqa-validation-7182", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-987", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-2435"], "SR": 0.546875, "CSR": 0.55625, "retrieved_ids": ["mrqa_squad-train-22836", "mrqa_squad-train-41761", "mrqa_squad-train-67244", "mrqa_squad-train-774", "mrqa_squad-train-23930", "mrqa_squad-train-82815", "mrqa_squad-train-74895", "mrqa_squad-train-67618", "mrqa_squad-train-38534", "mrqa_squad-train-66058", "mrqa_squad-train-21643", "mrqa_squad-train-12240", "mrqa_squad-train-6244", "mrqa_squad-train-80033", "mrqa_squad-train-31240", "mrqa_squad-train-82841", "mrqa_squad-train-43680", "mrqa_squad-train-53297", "mrqa_squad-train-58482", "mrqa_squad-train-14395", "mrqa_squad-train-67429", "mrqa_squad-train-55433", "mrqa_squad-train-36976", "mrqa_squad-train-85793", "mrqa_searchqa-validation-10614", "mrqa_newsqa-validation-2591", "mrqa_searchqa-validation-2971", "mrqa_hotpotqa-validation-3819", "mrqa_searchqa-validation-3817", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-5155", "mrqa_hotpotqa-validation-2639", "mrqa_squad-validation-6706", "mrqa_naturalquestions-validation-276", "mrqa_hotpotqa-validation-2567", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-3658", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-2017", "mrqa_newsqa-validation-616", "mrqa_triviaqa-validation-7690", "mrqa_newsqa-validation-2660", "mrqa_hotpotqa-validation-1996", "mrqa_newsqa-validation-2732", "mrqa_searchqa-validation-5082", "mrqa_hotpotqa-validation-2244", "mrqa_naturalquestions-validation-8596", "mrqa_triviaqa-validation-6380"], "EFR": 0.9655172413793104, "Overall": 0.7399784482758621}, {"timecode": 70, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1910", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2888", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-3355", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-39", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_hotpotqa-validation-978", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1120", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4890", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6279", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-963", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12391", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14334", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3809", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7527", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-2467", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2943", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3832", "mrqa_squad-validation-3852", "mrqa_squad-validation-386", "mrqa_squad-validation-3919", "mrqa_squad-validation-3946", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-3994", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4467", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5493", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6638", "mrqa_squad-validation-6875", "mrqa_squad-validation-6957", "mrqa_squad-validation-7064", "mrqa_squad-validation-739", "mrqa_squad-validation-7549", "mrqa_squad-validation-7688", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-7917", "mrqa_squad-validation-8309", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-893", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-143", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1802", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2000", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2420", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2936", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2940", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3079", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4328", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5861", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-595", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6549", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917"], "OKR": 0.89453125, "KG": 0.5078125, "before_eval_results": {"predictions": ["Kathy Najimy", "2006 -- 06", "The Divergent Series : Ascendant", "Mel Tillis", "2026", "Clare Torry", "Andrew Lloyd Webber", "at the TV studio in the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Health or vitality", "Stephen Graham", "6 - 6", "1955", "Kevin McKidd", "Parthenogenesis", "fertilization", "Yente", "Judy Garland", "in the stems and roots of certain vascular plants", "from a Czech word, robota", "in skeletal muscle and the brain", "Nazi Germany and Fascist Italy", "Gunpei Yokoi", "David Motl", "a simple majority", "February 16, 2010", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "scrolls dating back to the 12th century", "Buddhist", "Kiss", "the eighth series of the UK version of The X Factor", "Trace Adkins", "postero - medially towards the optic chiasm", "to manage the characteristics of the beer's head", "United States, the United Kingdom", "1957", "James Intveld", "15 February 1998", "Christopher Allen Lloyd", "100,000", "January 2004", "Bartolomeu Dias", "Isabela Moner", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "a numeric scale used to specify the acidity or basicity of an aqueous solution", "Fall 1998", "the Qianlong Emperor", "Guwahati", "74", "South Africa", "Rufus and Chaka Khan", "eight", "morocco", "jamaica", "mead", "Tomorrowland", "Tallahassee City Commission", "January 18, 1977", "pesos", "Martin Buber, Emanuel Levinas, or Primo Levi", "123 pounds of cocaine and 4.5 pounds of heroin", "In Memoriam", "Mercury", "Oz", "UNICEF"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7035555763680763}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.16666666666666666, 0.888888888888889, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.9, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307692, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.9600000000000001, 0.14285714285714288, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-347", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-305", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-527", "mrqa_triviaqa-validation-4646", "mrqa_newsqa-validation-1309"], "SR": 0.578125, "CSR": 0.5565580985915493, "retrieved_ids": ["mrqa_squad-train-6164", "mrqa_squad-train-53720", "mrqa_squad-train-12101", "mrqa_squad-train-48822", "mrqa_squad-train-57819", "mrqa_squad-train-64650", "mrqa_squad-train-31164", "mrqa_squad-train-51839", "mrqa_squad-train-35340", "mrqa_squad-train-23311", "mrqa_squad-train-35014", "mrqa_squad-train-14428", "mrqa_squad-train-44081", "mrqa_squad-train-12898", "mrqa_squad-train-33148", "mrqa_squad-train-65704", "mrqa_squad-train-43378", "mrqa_squad-train-54381", "mrqa_squad-train-86387", "mrqa_squad-train-20872", "mrqa_squad-train-59575", "mrqa_squad-train-46043", "mrqa_squad-train-13670", "mrqa_squad-train-43823", "mrqa_naturalquestions-validation-6718", "mrqa_hotpotqa-validation-5531", "mrqa_naturalquestions-validation-5304", "mrqa_newsqa-validation-2275", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-6519", "mrqa_searchqa-validation-12646", "mrqa_naturalquestions-validation-4359", "mrqa_searchqa-validation-3530", "mrqa_naturalquestions-validation-10209", "mrqa_hotpotqa-validation-4655", "mrqa_squad-validation-8904", "mrqa_triviaqa-validation-4517", "mrqa_searchqa-validation-12440", "mrqa_triviaqa-validation-613", "mrqa_newsqa-validation-616", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-2143", "mrqa_searchqa-validation-6087", "mrqa_newsqa-validation-2183", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-51"], "EFR": 0.9629629629629629, "Overall": 0.7437479623109025}, {"timecode": 71, "before_eval_results": {"predictions": ["Easter Island", "George Balanchine", "Roussimoff", "the Cretaceous Period", "Austen", "Leonardo DiCaprio", "the Basque language", "Cherry Jones", "Happy Feet", "a guardian angel", "the United States Naval Academy", "the Tame", "Law & Order: Special Victims Unit", "the Caucasus Mountains", "June Carter Cash", "Cape Town", "a reef", "David Glasgow Farragut", "1:24 a.m.", "odd job", "a Skull", "Marie Osmond", "Scrabble", "suckers", "Roman Catholicism", "London", "Fert", "Halliburton Oil Well Cementing Company", "the eye", "Boston", "anamosa", "Spelling Bee", "poetry", "the Battle of Fort Donelson", "the Nautilus", "(Henry) Givens", "Sucrose", "Cheshire", "Cuba", "The Prince and the Pauper", "Thomas Paine", "Abraham Lincoln", "Lord North", "Charles I", "Aunt Jemima", "Diane Arbus", "Palitana", "Paul Redhead", "Utah", "Humulin", "Kublai Khan", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "Kimberlin Brown", "Henry Selick", "Caviar", "July 20", "argentina", "the vicar of Wantage", "Marc Bolan", "Polish-Jewish", "apartment building in Cologne, Germany,", "in the Iraq's autonomous region of Kurdish.", "$40 and a loaf of bread.", "argentina"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5784350198412698}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-13029", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-11798", "mrqa_searchqa-validation-9571", "mrqa_searchqa-validation-5919", "mrqa_searchqa-validation-4309", "mrqa_searchqa-validation-4000", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-15548", "mrqa_searchqa-validation-3904", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-16607", "mrqa_searchqa-validation-6350", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-15590", "mrqa_naturalquestions-validation-1680", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-3746", "mrqa_hotpotqa-validation-3593", "mrqa_hotpotqa-validation-2493", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-3002", "mrqa_triviaqa-validation-1782"], "SR": 0.484375, "CSR": 0.5555555555555556, "retrieved_ids": ["mrqa_squad-train-69347", "mrqa_squad-train-28037", "mrqa_squad-train-41628", "mrqa_squad-train-57474", "mrqa_squad-train-39853", "mrqa_squad-train-75406", "mrqa_squad-train-53456", "mrqa_squad-train-72798", "mrqa_squad-train-33510", "mrqa_squad-train-80507", "mrqa_squad-train-23456", "mrqa_squad-train-49800", "mrqa_squad-train-16032", "mrqa_squad-train-71385", "mrqa_squad-train-55415", "mrqa_squad-train-18809", "mrqa_squad-train-86332", "mrqa_squad-train-2838", "mrqa_squad-train-59902", "mrqa_squad-train-26279", "mrqa_squad-train-193", "mrqa_squad-train-70673", "mrqa_squad-train-50882", "mrqa_squad-train-82386", "mrqa_searchqa-validation-4715", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-5588", "mrqa_triviaqa-validation-4901", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-2024", "mrqa_naturalquestions-validation-2452", "mrqa_searchqa-validation-1279", "mrqa_naturalquestions-validation-347", "mrqa_hotpotqa-validation-3428", "mrqa_newsqa-validation-181", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-3782", "mrqa_triviaqa-validation-7039", "mrqa_squad-validation-3270", "mrqa_squad-validation-10321", "mrqa_triviaqa-validation-4317", "mrqa_naturalquestions-validation-7750", "mrqa_hotpotqa-validation-3020", "mrqa_searchqa-validation-6252", "mrqa_triviaqa-validation-4577", "mrqa_naturalquestions-validation-10610"], "EFR": 0.9393939393939394, "Overall": 0.738833648989899}, {"timecode": 72, "before_eval_results": {"predictions": ["Austria", "peninsulas", "I Am Woman", "Brasilia", "Applebee\\'s", "the American Colonies", "Backgammon", "Steely Dan", "Artemis", "Hobart", "Colorado Springs", "Cheap trick", "poached eggs", "Islam", "Cerberus", "General Lee", "Tobago", "Brigadoon", "Columbus", "Elijah Muhammad", "Spain", "Federico Fellini", "Fenway Park", "the Hermitage picture", "The Princess Diaries", "fluoridation", "Herman Melville", "Korea", "John Henry", "Babe Ruth", "Barbara Boxer", "Chicago", "Wallace & Gromit: The Curse of the Were-Rabbit", "sesame", "Nike", "Jack Nicholson", "nitrogen", "the Omaha tribe", "dogs", "Gauguin", "Francis Scott Key", "Mexico", "the Peashooter", "Joe Pozzuoli", "Recycling fraud", "the Massachusetts Bay Colony", "\"The pie was eaten by Bob\"", "box office", "Alfred Hitchcock", "the Basques", "Ambrose Bierce", "The president", "2.45 billion years ago", "Jennifer Morrison", "london", "meteoroids", "Argentina", "Edinburgh", "Campbellsville", "mathematician and physicist", "San Simeon, California,", "\"Fiveteen years ago Wednesday, at a house adjacent to the park, Kurt Cobain's dead body was discovered by an electrician.", "Brian Smith,", "Ballon d'Or"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6995104949874686}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4210526315789474, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-4160", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-1079", "mrqa_searchqa-validation-1805", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-10948", "mrqa_searchqa-validation-5372", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-775", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-10907", "mrqa_searchqa-validation-6294", "mrqa_searchqa-validation-3922", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-8257", "mrqa_naturalquestions-validation-2889", "mrqa_triviaqa-validation-6507", "mrqa_triviaqa-validation-5187", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4856", "mrqa_newsqa-validation-1962"], "SR": 0.59375, "CSR": 0.5560787671232876, "retrieved_ids": ["mrqa_squad-train-79466", "mrqa_squad-train-27865", "mrqa_squad-train-82908", "mrqa_squad-train-43698", "mrqa_squad-train-66593", "mrqa_squad-train-78836", "mrqa_squad-train-51654", "mrqa_squad-train-60948", "mrqa_squad-train-17982", "mrqa_squad-train-3171", "mrqa_squad-train-78645", "mrqa_squad-train-2396", "mrqa_squad-train-17604", "mrqa_squad-train-66903", "mrqa_squad-train-19832", "mrqa_squad-train-51490", "mrqa_squad-train-74400", "mrqa_squad-train-35855", "mrqa_squad-train-32430", "mrqa_squad-train-20788", "mrqa_squad-train-84853", "mrqa_squad-train-41255", "mrqa_squad-train-42487", "mrqa_squad-train-72166", "mrqa_naturalquestions-validation-922", "mrqa_newsqa-validation-522", "mrqa_naturalquestions-validation-4870", "mrqa_newsqa-validation-3437", "mrqa_triviaqa-validation-5578", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-7067", "mrqa_newsqa-validation-3781", "mrqa_squad-validation-6759", "mrqa_naturalquestions-validation-8599", "mrqa_triviaqa-validation-6328", "mrqa_searchqa-validation-4394", "mrqa_squad-validation-8229", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-2194", "mrqa_searchqa-validation-11532", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3491", "mrqa_searchqa-validation-4319", "mrqa_searchqa-validation-11977", "mrqa_triviaqa-validation-2151", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-104", "mrqa_squad-validation-1827"], "EFR": 1.0, "Overall": 0.7510595034246575}, {"timecode": 73, "before_eval_results": {"predictions": ["barcarole", "Sinclair Lewis", "Hilary Swank", "Sun Lust Pictures", "Sacred Wonders", "Israel", "Lundy", "Van Morrison", "Carbon", "Stuart Bingham", "Frank Darabont", "\"first in war, first in peace, and first in the hearts of his countrymen.\"", "Adam Smith", "espresso", "organizational theory", "Volkswagen", "Sir John Major", "Oldham", "new Netherland", "jabba the Hutt", "Andy Murray", "Crystal Gayle", "Zachary Taylor", "Azerbaijan", "Chechnya", "John Buchan", "green", "city centre section of modern day Chester", "Hippety Hopper", "a bodice or corset", "Kenya", "a pumpkin", "denmark", "Sicily", "france", "Magic Circle", "Julie Andrews Edwards", "Pancho Villa", "Nigeria", "Leeds", "Palm Sunday", "Cologne", "Oliver!", "nipponese", "Ra\u00fal Castro", "indiopia", "d Qatar", "(Willa) Escher", "Mexico", "n Carolina", "Friends", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "Tom Brady", "December 11, 2014", "Forrest Gump", "Michael Sheen", "Mel Blanc", "Arnold Drummond", "dining scene", "Abhisit Vejjajiva", "Jackie Moon", "Maria Callas", "Desperate Housewives", "his advocacy of young earth creationism and intelligent design."], "metric_results": {"EM": 0.609375, "QA-F1": 0.6610203598484848}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636]}}, "before_error_ids": ["mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-2845", "mrqa_triviaqa-validation-1197", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-957", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5976", "mrqa_triviaqa-validation-925", "mrqa_triviaqa-validation-3753", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-2414", "mrqa_triviaqa-validation-6862", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-4028", "mrqa_hotpotqa-validation-1306", "mrqa_newsqa-validation-1828", "mrqa_searchqa-validation-16053", "mrqa_hotpotqa-validation-3469"], "SR": 0.609375, "CSR": 0.5567989864864865, "retrieved_ids": ["mrqa_squad-train-60847", "mrqa_squad-train-5791", "mrqa_squad-train-34320", "mrqa_squad-train-74365", "mrqa_squad-train-63733", "mrqa_squad-train-10845", "mrqa_squad-train-32104", "mrqa_squad-train-18391", "mrqa_squad-train-60002", "mrqa_squad-train-29338", "mrqa_squad-train-84222", "mrqa_squad-train-62416", "mrqa_squad-train-40863", "mrqa_squad-train-79205", "mrqa_squad-train-1290", "mrqa_squad-train-74402", "mrqa_squad-train-31170", "mrqa_squad-train-22687", "mrqa_squad-train-27971", "mrqa_squad-train-71902", "mrqa_squad-train-70984", "mrqa_squad-train-6467", "mrqa_squad-train-37779", "mrqa_squad-train-55433", "mrqa_squad-validation-606", "mrqa_newsqa-validation-664", "mrqa_hotpotqa-validation-788", "mrqa_searchqa-validation-9458", "mrqa_hotpotqa-validation-1033", "mrqa_searchqa-validation-5372", "mrqa_newsqa-validation-2733", "mrqa_searchqa-validation-16176", "mrqa_naturalquestions-validation-8983", "mrqa_triviaqa-validation-1314", "mrqa_searchqa-validation-1999", "mrqa_newsqa-validation-1749", "mrqa_naturalquestions-validation-10249", "mrqa_newsqa-validation-3437", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-14104", "mrqa_squad-validation-4298", "mrqa_squad-validation-151", "mrqa_naturalquestions-validation-4863", "mrqa_newsqa-validation-3035", "mrqa_searchqa-validation-1377", "mrqa_naturalquestions-validation-8254", "mrqa_hotpotqa-validation-2195", "mrqa_squad-validation-5435"], "EFR": 0.96, "Overall": 0.7432035472972973}, {"timecode": 74, "before_eval_results": {"predictions": ["james bardon", "james bond", "the Royal Navy", "high-speed car crash", "apples", "orange-juice yellow", "Jersey Boys", "spain", "outflow current", "Viola", "hay fever", "gin", "British Columbia", "rent doesn't include additional costs such as insurance or business rates", "whooping cough", "Peter Stuyvesant", "apples", "India and Pakistan", "labyrinth", "Chiricahua Apache", "atrioventricular node ( AV node )", "blucher", "Pius XII", "mole salamander", "3", "george i", "Lincolnshire", "Zimbabwe", "rolling hillsides", "orange, lemon, lime, grape and strawberry", "al-Minufiyah", "David Bowie", "Silent Spring", "bath and Wells", "eile de Becque", "Michael Sheen", "The Archers", "Tottenham Court Road", "mountmorency", "californianus", "12", "poultry Lane", "Pinocchio", "cenozoic", "Jimmy Carter", "Jamie Oliver", "hub", "willy Russell", "Petula Clark", "New Democracy", "The Blue Boy", "Border Collie", "Kristy Swanson", "fourth season", "Charles and fellow cleric George Whitefield", "Hermione Baddeley", "Floyd Casey Stadium", "Bowie", "Iraqi economy.", "Robert Kimmitt.", "the Mammoth Cave", "recessive", "Charles Dickens", "Fayetteville, North Carolina"], "metric_results": {"EM": 0.5, "QA-F1": 0.5984375}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-6107", "mrqa_triviaqa-validation-4228", "mrqa_triviaqa-validation-2233", "mrqa_triviaqa-validation-3482", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-169", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-2699", "mrqa_triviaqa-validation-7353", "mrqa_triviaqa-validation-2180", "mrqa_triviaqa-validation-6797", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4832", "mrqa_triviaqa-validation-4470", "mrqa_triviaqa-validation-1344", "mrqa_triviaqa-validation-6876", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-5738", "mrqa_naturalquestions-validation-8404", "mrqa_hotpotqa-validation-2801", "mrqa_newsqa-validation-1032", "mrqa_searchqa-validation-4464", "mrqa_hotpotqa-validation-3787"], "SR": 0.5, "CSR": 0.5560416666666667, "retrieved_ids": ["mrqa_squad-train-10252", "mrqa_squad-train-60308", "mrqa_squad-train-56960", "mrqa_squad-train-68625", "mrqa_squad-train-16375", "mrqa_squad-train-632", "mrqa_squad-train-20280", "mrqa_squad-train-33198", "mrqa_squad-train-3481", "mrqa_squad-train-55839", "mrqa_squad-train-39999", "mrqa_squad-train-79180", "mrqa_squad-train-1035", "mrqa_squad-train-56721", "mrqa_squad-train-48946", "mrqa_squad-train-52652", "mrqa_squad-train-58287", "mrqa_squad-train-49755", "mrqa_squad-train-14789", "mrqa_squad-train-31880", "mrqa_squad-train-17980", "mrqa_squad-train-25968", "mrqa_squad-train-80218", "mrqa_squad-train-61380", "mrqa_naturalquestions-validation-868", "mrqa_searchqa-validation-2231", "mrqa_squad-validation-7364", "mrqa_newsqa-validation-1524", "mrqa_searchqa-validation-4169", "mrqa_triviaqa-validation-102", "mrqa_newsqa-validation-1456", "mrqa_squad-validation-9458", "mrqa_newsqa-validation-748", "mrqa_hotpotqa-validation-474", "mrqa_triviaqa-validation-4966", "mrqa_searchqa-validation-13900", "mrqa_naturalquestions-validation-4348", "mrqa_squad-validation-5887", "mrqa_naturalquestions-validation-8965", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-6833", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-3658", "mrqa_newsqa-validation-3435", "mrqa_triviaqa-validation-6885", "mrqa_newsqa-validation-1805", "mrqa_searchqa-validation-8760", "mrqa_triviaqa-validation-3563"], "EFR": 0.9375, "Overall": 0.7385520833333333}, {"timecode": 75, "before_eval_results": {"predictions": ["new Zealand", "1961", "tomato", "Moriarty", "Duke Orsino", "james vi", "Francis Carr", "Budapest", "Gillette", "oil", "mediterranean", "Bash", "city of michael", "hallux", "swallow sidecar Company", "Chicago", "Brett Favre", "netherlands", "Gryffindor", "hallmarks", "John Buchan", "Pyrenees", "17", "joshua", "canada", "Elysium", "algebra", "Eddie Murphy", "Crete", "denmark", "Copenhagen", "vena cava", "joshua szymborska", "orca", "Christopher Nolan", "purple rain", "chess", "Ireland", "Diana Vickers", "Feb. 14", "Damian Green", "kryptos", "Bagel", "france", "South Dakota", "Alexander Dubcek", "Denver", "Chicago Cubs", "st. Louis", "Iberia", "Rosetta", "in the basic curriculum", "the Saudi Arab kingdom", "the nucleus", "August 14, 1848", "1892", "Merck & Co.", "1,500", "Shenzhen in southern China.", "Iran", "cola", "Washington, D.C.", "sedimentary rock", "golf"], "metric_results": {"EM": 0.5625, "QA-F1": 0.609375}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-5653", "mrqa_triviaqa-validation-2012", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-6528", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-156", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-3745", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-366", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4506", "mrqa_searchqa-validation-10770", "mrqa_searchqa-validation-2183"], "SR": 0.5625, "CSR": 0.556126644736842, "retrieved_ids": ["mrqa_squad-train-83314", "mrqa_squad-train-36500", "mrqa_squad-train-81303", "mrqa_squad-train-82491", "mrqa_squad-train-57059", "mrqa_squad-train-2439", "mrqa_squad-train-34118", "mrqa_squad-train-24330", "mrqa_squad-train-76323", "mrqa_squad-train-10136", "mrqa_squad-train-54893", "mrqa_squad-train-67228", "mrqa_squad-train-16462", "mrqa_squad-train-86509", "mrqa_squad-train-79380", "mrqa_squad-train-66239", "mrqa_squad-train-71913", "mrqa_squad-train-67386", "mrqa_squad-train-2483", "mrqa_squad-train-85839", "mrqa_squad-train-20922", "mrqa_squad-train-34131", "mrqa_squad-train-1478", "mrqa_squad-train-60591", "mrqa_naturalquestions-validation-836", "mrqa_newsqa-validation-2142", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-6532", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-2163", "mrqa_newsqa-validation-3982", "mrqa_triviaqa-validation-3456", "mrqa_naturalquestions-validation-5926", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-11388", "mrqa_naturalquestions-validation-1327", "mrqa_squad-validation-386", "mrqa_naturalquestions-validation-9848", "mrqa_newsqa-validation-1836", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-243", "mrqa_newsqa-validation-1879", "mrqa_naturalquestions-validation-4520", "mrqa_newsqa-validation-1007", "mrqa_searchqa-validation-14981", "mrqa_naturalquestions-validation-2299", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-3087"], "EFR": 0.9285714285714286, "Overall": 0.7367833646616541}, {"timecode": 76, "before_eval_results": {"predictions": ["English author Rudyard Kipling", "Andrew Garfield", "California, Utah and Arizona", "epidemiology", "William Chatterton Dix", "1924", "September 27, 2017", "Alabama", "Scheria", "Sanchez Navarro", "Thomas Jefferson", "August 2, 1990", "Joe Pizzulo and Leeza Miller", "Julie Adams", "Ian Hart", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "a Native American nation from the Great Plains", "capillaries", "the French CYCLADES project directed by Louis Pouzin", "April 1979", "Tbilisi", "a security feature for `` card not present '' payment card transactions instituted to reduce the incidence of credit card fraud", "Atticus Finch", "four", "Liam Cunningham", "2013", "ummat al - Islamiyah", "# 4", "1980", "2017 season", "W. Edwards Deming", "Saphira", "passwords", "Galveston hurricane", "the final years of the Third Republic", "Ajay Tyagi", "arthur davall\u00e9e", "Paul Revere", "Julius Caesar", "Aristotle", "1990", "Zeus", "two degrees of freedom", "April 10, 2018", "Lee County, Florida, United States", "mid November", "Kevin Spacey", "Fa Ze YouTubers", "two installments", "the British", "excessive", "lingerie", "\u00ef\u00bf\u00bd", "emperor of rome", "Karolina Dean", "four hundred", "Caesars Entertainment Corporation", "North Korea intends to launch a long-range missile in the near future,", "the sins of the members of the church,", "Marcus Schrenker,", "horse-master", "a capella", "give a heads up", "Charice"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5818977591036415}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.5714285714285715, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-4115", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-7881", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-860", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-5761", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-8474", "mrqa_searchqa-validation-1590"], "SR": 0.53125, "CSR": 0.5558035714285714, "retrieved_ids": ["mrqa_squad-train-25035", "mrqa_squad-train-30891", "mrqa_squad-train-27186", "mrqa_squad-train-32938", "mrqa_squad-train-76505", "mrqa_squad-train-28047", "mrqa_squad-train-8143", "mrqa_squad-train-39384", "mrqa_squad-train-79471", "mrqa_squad-train-53189", "mrqa_squad-train-20546", "mrqa_squad-train-73241", "mrqa_squad-train-59095", "mrqa_squad-train-77401", "mrqa_squad-train-74365", "mrqa_squad-train-81566", "mrqa_squad-train-69485", "mrqa_squad-train-2459", "mrqa_squad-train-38287", "mrqa_squad-train-17403", "mrqa_squad-train-53296", "mrqa_squad-train-35618", "mrqa_squad-train-9189", "mrqa_squad-train-52757", "mrqa_newsqa-validation-3732", "mrqa_searchqa-validation-9822", "mrqa_naturalquestions-validation-6886", "mrqa_triviaqa-validation-4784", "mrqa_newsqa-validation-3280", "mrqa_hotpotqa-validation-132", "mrqa_naturalquestions-validation-2558", "mrqa_newsqa-validation-3933", "mrqa_naturalquestions-validation-2889", "mrqa_hotpotqa-validation-906", "mrqa_squad-validation-10217", "mrqa_searchqa-validation-13029", "mrqa_searchqa-validation-15055", "mrqa_searchqa-validation-2835", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2491", "mrqa_naturalquestions-validation-4197", "mrqa_naturalquestions-validation-3323", "mrqa_newsqa-validation-1196", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-3752", "mrqa_naturalquestions-validation-9602", "mrqa_newsqa-validation-190", "mrqa_naturalquestions-validation-5070"], "EFR": 0.9333333333333333, "Overall": 0.7376711309523809}, {"timecode": 77, "before_eval_results": {"predictions": ["The Senate & Senators Flashcards", "Istanbul", "Cana of Galilee", "take It", "The Marriage of Figaro", "Valerie bertinelli", "glitter", "Bayer", "picture book for children", "Karl Rove", "Russians", "Ireland", "THE 14th president", "Portland", "Florida Keys", "Doctor John Dolittle", "fish", "transmission", "hot air balloons", "vacuum tubes", "The Bridges of Madison County", "Italy", "ghee", "LOUIS XIV.", "ice cream", "Louis XIV", "hyaena hyaena hyeena hyena", "Alien", "Kennedy assassination conspiracy", "Indira Gandhi", "rodents", "Stephen Decatur", "Patti LaBelle", "the Plowman", "Molly Brown", "birds", "hurricanes", "The Wall Street Journal", "fragging", "Tinactin", "Virgin Atlantic", "Perrier", "Eastwick", "king Richard III", "trout", "Pakistan", "Minnesota", "San Francisco", "rabbit", "latte", "pistol", "Brazil, Turkey and Uzbekistan", "Nicole DuPort", "species", "deep purple", "davian man", "dada", "July 25 to August 4", "1755", "Eric Cartman", "1.2 million", "president Luca di Montezemolo", "Roger Federer", "allori"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6395833333333334}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-6923", "mrqa_searchqa-validation-692", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-2938", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-4400", "mrqa_searchqa-validation-15029", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-15498", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5989", "mrqa_searchqa-validation-15247", "mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-2685", "mrqa_searchqa-validation-11923", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-2858", "mrqa_naturalquestions-validation-9830", "mrqa_triviaqa-validation-3041", "mrqa_hotpotqa-validation-4011", "mrqa_newsqa-validation-1364", "mrqa_triviaqa-validation-5253"], "SR": 0.578125, "CSR": 0.5560897435897436, "retrieved_ids": ["mrqa_squad-train-17108", "mrqa_squad-train-16182", "mrqa_squad-train-71719", "mrqa_squad-train-61589", "mrqa_squad-train-9144", "mrqa_squad-train-60", "mrqa_squad-train-41137", "mrqa_squad-train-21129", "mrqa_squad-train-40619", "mrqa_squad-train-69326", "mrqa_squad-train-44898", "mrqa_squad-train-39089", "mrqa_squad-train-63597", "mrqa_squad-train-69224", "mrqa_squad-train-16668", "mrqa_squad-train-66948", "mrqa_squad-train-44460", "mrqa_squad-train-45985", "mrqa_squad-train-48723", "mrqa_squad-train-39144", "mrqa_squad-train-44506", "mrqa_squad-train-80468", "mrqa_squad-train-40101", "mrqa_squad-train-48498", "mrqa_triviaqa-validation-2003", "mrqa_hotpotqa-validation-2150", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-4874", "mrqa_squad-validation-1566", "mrqa_naturalquestions-validation-182", "mrqa_searchqa-validation-4724", "mrqa_searchqa-validation-11502", "mrqa_triviaqa-validation-4798", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-759", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-2146", "mrqa_newsqa-validation-1007", "mrqa_searchqa-validation-1857", "mrqa_squad-validation-6171", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-3951", "mrqa_naturalquestions-validation-9572", "mrqa_newsqa-validation-1012", "mrqa_triviaqa-validation-3532", "mrqa_searchqa-validation-3613", "mrqa_squad-validation-7700", "mrqa_triviaqa-validation-1955"], "EFR": 0.9629629629629629, "Overall": 0.7436542913105413}, {"timecode": 78, "before_eval_results": {"predictions": ["Tycho Brahe", "Little Miss Sunshine", "Philadelphia", "Peter Rabbit", "Tommy Franks", "Ur", "Jonny Quest", "Rwanda", "Fort Sumter", "Love Story", "Captains Courageous", "Bryan Adams", "Moses", "engineering", "Chaucer", "Toronto Blue Jays", "Wing Commander", "Isaac Asimov", "Sayonara", "the Orient Express", "Da Vinci Code blue", "Sir Walter Scott", "a face cord", "Louisiana", "The Maltese Falcon", "Douglas MacArthur", "Teflon", "eastern Kentucky", "PG", "occipital", "a spoon", "Little Red Riding Hood", "The Jonas Brothers", "Iceland", "the Popsicle", "Los Angeles", "vincent", "Chelsea Morning", "a comb", "Venice", "Paraguay", "Theodor Wilhelm Hoffmann", "debts", "the Cowardly Lion", "El Supremo", "Foot Locker", "Princess Kenobi", "artichoke", "all those discussed here", "Hammurabi", "Alkalinity", "the ninth w\u0101", "Matt Monro", "on the two tablets", "Mt kenya", "Elvis Presley", "Boston Legal", "all-time", "Channel 4", "Mark Neary Donohue Jr.", "the Sadr City and Adhamiya districts of Baghdad City,", "a share in the royalties", "Arizona", "American 3D computer-animated comedy"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7751488095238095}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-4423", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-6426", "mrqa_searchqa-validation-8259", "mrqa_searchqa-validation-15144", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-15756", "mrqa_searchqa-validation-11821", "mrqa_searchqa-validation-5218", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-12891", "mrqa_naturalquestions-validation-10310", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-4688", "mrqa_hotpotqa-validation-5344", "mrqa_newsqa-validation-939", "mrqa_hotpotqa-validation-2673"], "SR": 0.671875, "CSR": 0.5575553797468354, "retrieved_ids": ["mrqa_squad-train-42711", "mrqa_squad-train-82172", "mrqa_squad-train-7758", "mrqa_squad-train-25093", "mrqa_squad-train-52031", "mrqa_squad-train-57267", "mrqa_squad-train-387", "mrqa_squad-train-10991", "mrqa_squad-train-5213", "mrqa_squad-train-22504", "mrqa_squad-train-21186", "mrqa_squad-train-6335", "mrqa_squad-train-29580", "mrqa_squad-train-51803", "mrqa_squad-train-61169", "mrqa_squad-train-54425", "mrqa_squad-train-28123", "mrqa_squad-train-29569", "mrqa_squad-train-18834", "mrqa_squad-train-40117", "mrqa_squad-train-13455", "mrqa_squad-train-37152", "mrqa_squad-train-65717", "mrqa_squad-train-75663", "mrqa_newsqa-validation-2591", "mrqa_naturalquestions-validation-4561", "mrqa_hotpotqa-validation-1273", "mrqa_searchqa-validation-16276", "mrqa_naturalquestions-validation-8990", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3210", "mrqa_naturalquestions-validation-1912", "mrqa_triviaqa-validation-3717", "mrqa_hotpotqa-validation-1306", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-2310", "mrqa_naturalquestions-validation-10549", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-13003", "mrqa_searchqa-validation-1049", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-9451", "mrqa_squad-validation-7876", "mrqa_newsqa-validation-3803", "mrqa_naturalquestions-validation-3651", "mrqa_hotpotqa-validation-4047", "mrqa_searchqa-validation-12611", "mrqa_newsqa-validation-2471"], "EFR": 0.9047619047619048, "Overall": 0.732307206901748}, {"timecode": 79, "before_eval_results": {"predictions": ["Confederate", "Banquo", "Detroit", "Blossom", "W", "Ford", "Joseph Campbell", "curmudgeon", "Faith Hill", "Novel", "a broom", "Edinburgh", "engineering", "Cyprus", "savanna", "tandoor", "floatplane", "piano", "Sure", "oyster", "What's Eating Gilbert Grape", "Mrs. Barbara Bush", "North Carolina", "The Jungle Book", "eggs", "the capablanca", "the Sadler's Wells Ballet", "Pakistan", "the FBI's Hostage Rescue Team (HRT)", "Aaron Burr", "Johns Hopkins University", "Jason", "Mississippi River", "Damascus", "Oahu", "Devo", "biology", "stuffing", "Reading Railroad", "(George) Eliot", "the Cotton Bowl", "Shiloh", "ventriloquism", "Takana", "apples", "cedar", "Almond Joy", "Australia", "Sam Houston", "Caesar salad", "cable cars", "July 14, 1969", "on permanent display at the Louvre Museum in Paris", "July 1, 1923", "bleakriac", "Casualty", "The Boar", "Johnny Cash, Waylon Jennings, Willie Nelson, and Kris Kristofferson", "Sarajevo", "Annie Ida Jenny No\u00eb Haesendonck", "Mother's Day", "fifth successive season", "The son of Gabon's former president", "Wildcats"], "metric_results": {"EM": 0.578125, "QA-F1": 0.628886217948718}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-12299", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-15892", "mrqa_searchqa-validation-7340", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-3152", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-15134", "mrqa_searchqa-validation-8338", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-2648", "mrqa_searchqa-validation-13472", "mrqa_naturalquestions-validation-1446", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-6870", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-3155", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-3923"], "SR": 0.578125, "CSR": 0.5578125, "retrieved_ids": ["mrqa_squad-train-77279", "mrqa_squad-train-37503", "mrqa_squad-train-81889", "mrqa_squad-train-26900", "mrqa_squad-train-47271", "mrqa_squad-train-30585", "mrqa_squad-train-22164", "mrqa_squad-train-86009", "mrqa_squad-train-45143", "mrqa_squad-train-24697", "mrqa_squad-train-24639", "mrqa_squad-train-28679", "mrqa_squad-train-86069", "mrqa_squad-train-70091", "mrqa_squad-train-49617", "mrqa_squad-train-84471", "mrqa_squad-train-74997", "mrqa_squad-train-58950", "mrqa_squad-train-30640", "mrqa_squad-train-73566", "mrqa_squad-train-70342", "mrqa_squad-train-42563", "mrqa_squad-train-71405", "mrqa_squad-train-8800", "mrqa_searchqa-validation-1415", "mrqa_newsqa-validation-1855", "mrqa_searchqa-validation-7662", "mrqa_newsqa-validation-551", "mrqa_squad-validation-4452", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7172", "mrqa_triviaqa-validation-2630", "mrqa_searchqa-validation-9148", "mrqa_naturalquestions-validation-2482", "mrqa_triviaqa-validation-3079", "mrqa_newsqa-validation-1671", "mrqa_searchqa-validation-1999", "mrqa_triviaqa-validation-2693", "mrqa_newsqa-validation-4122", "mrqa_searchqa-validation-5586", "mrqa_searchqa-validation-4996", "mrqa_searchqa-validation-10372", "mrqa_triviaqa-validation-4643", "mrqa_naturalquestions-validation-10416", "mrqa_searchqa-validation-5746", "mrqa_newsqa-validation-2261", "mrqa_naturalquestions-validation-1423", "mrqa_hotpotqa-validation-2639"], "EFR": 1.0, "Overall": 0.7514062499999999}, {"timecode": 80, "UKR": 0.798828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2195", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5359", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8668", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-12646", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15580", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-3000", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3018", "mrqa_searchqa-validation-3137", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5886", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6252", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8631", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-8803", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9490", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-1125", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-3969", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4312", "mrqa_squad-validation-4326", "mrqa_squad-validation-4528", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5320", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6229", "mrqa_squad-validation-6243", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-9446", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-3348", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5668", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6522", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7052", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7140", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7727", "mrqa_triviaqa-validation-7773", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.873046875, "KG": 0.515625, "before_eval_results": {"predictions": ["china", "partridge", "long-term effects", "Czech Republic", "George IV", "Azerbaijan", "belgian", "Sisyphus", "Lateran Pact of 1929", "coffee house", "phnom Penh", "Moldova", "Taking of Pelham One Two Three", "egypt", "Frank McCourt", "Furby", "Arkansas", "Texas", "Norway", "get Smart", "will Blake", "cape horn", "Philipp Kohlschreiber", "Charlie Chan", "Galileo Galilei", "Great British Bake Off", "1914", "shekel", "George Sand", "Michael Caine", "Professor Brian Cox", "jack brabham", "Knutsford", "Coronation Street", "McDonnell Douglas", "tyne", "Missouri", "Emma Chambers", "Buckinghamshire", "Turkey", "domestic cat", "john rav Epstein", "nine", "One Direction", "Groucho Marx", "brazil", "Idris Elba", "India", "August 1925", "Sweeney Todd", "Rio Grande", "If waivers are requested outside the playing season, or before November 1, then the player shall be transferred to the team with the lowest points in the preceding season", "The first series premiered on BBC Two on 8 January 1999", "David Joseph Madden", "The Braes o' Bowhether", "Mary Astor", "al-Qaeda", "natural gas", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Madonna", "Hawaii", "Monaco", "P D James", "MacFarlane"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6618990384615384}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3779", "mrqa_triviaqa-validation-953", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-797", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3969", "mrqa_triviaqa-validation-7733", "mrqa_triviaqa-validation-1559", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-4756", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-1845", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2733", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-9986", "mrqa_hotpotqa-validation-2718", "mrqa_newsqa-validation-4110", "mrqa_searchqa-validation-12999"], "SR": 0.640625, "CSR": 0.5588348765432098, "retrieved_ids": ["mrqa_squad-train-62454", "mrqa_squad-train-21379", "mrqa_squad-train-27959", "mrqa_squad-train-71667", "mrqa_squad-train-20123", "mrqa_squad-train-5978", "mrqa_squad-train-15834", "mrqa_squad-train-27445", "mrqa_squad-train-538", "mrqa_squad-train-49646", "mrqa_squad-train-26822", "mrqa_squad-train-79963", "mrqa_squad-train-17673", "mrqa_squad-train-83878", "mrqa_squad-train-68341", "mrqa_squad-train-48738", "mrqa_squad-train-61361", "mrqa_squad-train-31085", "mrqa_squad-train-7486", "mrqa_squad-train-5441", "mrqa_squad-train-1341", "mrqa_squad-train-41318", "mrqa_squad-train-70630", "mrqa_squad-train-83777", "mrqa_searchqa-validation-7004", "mrqa_squad-validation-3998", "mrqa_triviaqa-validation-156", "mrqa_newsqa-validation-81", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-8765", "mrqa_triviaqa-validation-2189", "mrqa_naturalquestions-validation-9272", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1219", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9079", "mrqa_searchqa-validation-11621", "mrqa_searchqa-validation-4464", "mrqa_naturalquestions-validation-9002", "mrqa_triviaqa-validation-4374", "mrqa_hotpotqa-validation-1007", "mrqa_searchqa-validation-3734", "mrqa_newsqa-validation-1144", "mrqa_searchqa-validation-15194", "mrqa_naturalquestions-validation-9013", "mrqa_triviaqa-validation-77", "mrqa_naturalquestions-validation-3609"], "EFR": 0.9565217391304348, "Overall": 0.740571323134729}, {"timecode": 81, "before_eval_results": {"predictions": ["the main highway entrance at California State Route 1,", "at the end of an interrogative sentence", "Minneapolis Lakers", "John Dalton", "Alamodome and city of San Antonio", "A rear - view mirror ( or rearview mirror )", "Golden Gate Bridge", "RAF", "BC Jean and Toby Gad", "UNESCO / ILO", "September 2017", "Universal Pictures and Focus Features", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "September 29, 2017", "nine", "Tbilisi, Georgia", "April 1917", "1900", "Bryan Cranston", "Geothermal gradient", "around 10 : 30am", "frontal lobe", "British anti-invasion preparations of 1803 -- 05", "potential of hydrogen", "volcanic activity and then gradually moves away from the ridge", "held that `` a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "the biblical Book of Exodus", "As of January 17, 2018, 201 episodes", "The pia mater", "members of the gay ( LGBT ) community", "Burbank, California", "1986", "2018 Winter Olympics", "rapid destruction of the donor red blood cells by host antibodies", "1603", "English author Rudyard Kipling", "March 16, 2018", "Fusajiro Yamauchi", "the rez", "2013", "the breast or lower chest", "Phil Gallagher", "supported modern programming practices and enabled business applications to be developed with Flash", "2018", "Saint Peter", "1960", "August 19, 2016", "Madison, Wisconsin", "Washington, Jay and Franklin", "ABC", "Brevet Colonel Robert E. Lee", "glockenspiel", "alaskan", "Hercules", "Elbow", "Dundalk", "NCAA Division II", "the Airbus A330-200", "about 50", "\"I sort of had a fascination with John Dillinger", "Portugal", "gravity", "Lafayette C. Baker", "Yemen,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7400026396400048}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.15384615384615383, 1.0, 0.3636363636363636, 0.7710843373493976, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4210526315789474, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.26666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-8700", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-9477", "mrqa_naturalquestions-validation-2319", "mrqa_triviaqa-validation-4319", "mrqa_hotpotqa-validation-2398", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-4136", "mrqa_searchqa-validation-2328"], "SR": 0.640625, "CSR": 0.5598323170731707, "retrieved_ids": ["mrqa_squad-train-67140", "mrqa_squad-train-84658", "mrqa_squad-train-8533", "mrqa_squad-train-13095", "mrqa_squad-train-7336", "mrqa_squad-train-36979", "mrqa_squad-train-16669", "mrqa_squad-train-52084", "mrqa_squad-train-5040", "mrqa_squad-train-48445", "mrqa_squad-train-39509", "mrqa_squad-train-70854", "mrqa_squad-train-25633", "mrqa_squad-train-71533", "mrqa_squad-train-32773", "mrqa_squad-train-1961", "mrqa_squad-train-15803", "mrqa_squad-train-74057", "mrqa_squad-train-86051", "mrqa_squad-train-34342", "mrqa_squad-train-54673", "mrqa_squad-train-64681", "mrqa_squad-train-62719", "mrqa_squad-train-28238", "mrqa_naturalquestions-validation-3707", "mrqa_newsqa-validation-3290", "mrqa_searchqa-validation-15103", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-2855", "mrqa_triviaqa-validation-774", "mrqa_hotpotqa-validation-482", "mrqa_newsqa-validation-1994", "mrqa_searchqa-validation-2858", "mrqa_hotpotqa-validation-3928", "mrqa_naturalquestions-validation-923", "mrqa_searchqa-validation-8374", "mrqa_naturalquestions-validation-4354", "mrqa_searchqa-validation-4275", "mrqa_newsqa-validation-371", "mrqa_triviaqa-validation-2310", "mrqa_squad-validation-6655", "mrqa_newsqa-validation-2288", "mrqa_hotpotqa-validation-5889", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7457", "mrqa_naturalquestions-validation-2069", "mrqa_searchqa-validation-3875", "mrqa_squad-validation-2043"], "EFR": 0.9130434782608695, "Overall": 0.732075159066808}, {"timecode": 82, "before_eval_results": {"predictions": ["Jon Stewart", "henry i", "Ross Kemp", "jumanji", "Kirk Douglas", "William Shakespeare", "Christmas", "African violet", "Rod Stewart", "Gerald Ford", "bassoon", "pembrokeshire coast park", "gazzetta dello sport newspaper.", "South Africa", "sows", "The Persistence of Memory", "orangutans", "Time Machine", "Uranus", "Tacitus", "Lady Gaga", "Mecca", "cirrus uncinus", "kiev", "myxomatosis", "augusta bacall", "Philippines", "xerophyte", "blur", "getting to know you", "The Last King of Scotland", "jaws", "Pearson PLC", "John Steinbeck", "The Bulletin", "violin", "Ross Bagdasarian", "Mark Hamill", "Sam Smith", "Burma", "peasant", "cryonics", "j\u00f8rn Utzon", "Another Day in Paradise", "decorate", "vienna", "Department of Justice", "South Africa", "rapid eye movement", "Antonio Vivaldi", "Corfu", "Walter Brennan", "a solitary figure who is not understood by others, but is actually wise", "Continental drift", "Charles White Whittlesey", "December 31, 2015", "White Knights of the Ku Klux Klan", "6,000", "fill a million sandbags and place 700,000 around our city,\"", "Judge Herman Thomas", "a Nanosecond", "Mazurek Dbrowskiego", "@ Fontdeck", "1945 to 1951"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7351190476190477}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-1895", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-4361", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2050", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-4711", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-2044"], "SR": 0.671875, "CSR": 0.5611822289156627, "retrieved_ids": ["mrqa_squad-train-27679", "mrqa_squad-train-60998", "mrqa_squad-train-74126", "mrqa_squad-train-54881", "mrqa_squad-train-45406", "mrqa_squad-train-25381", "mrqa_squad-train-23006", "mrqa_squad-train-74310", "mrqa_squad-train-71958", "mrqa_squad-train-21901", "mrqa_squad-train-71402", "mrqa_squad-train-63861", "mrqa_squad-train-56764", "mrqa_squad-train-76011", "mrqa_squad-train-84063", "mrqa_squad-train-33837", "mrqa_squad-train-67098", "mrqa_squad-train-3996", "mrqa_squad-train-48470", "mrqa_squad-train-34546", "mrqa_squad-train-37358", "mrqa_squad-train-80413", "mrqa_squad-train-38277", "mrqa_squad-train-56486", "mrqa_hotpotqa-validation-4311", "mrqa_searchqa-validation-14981", "mrqa_triviaqa-validation-2689", "mrqa_searchqa-validation-4394", "mrqa_hotpotqa-validation-5117", "mrqa_naturalquestions-validation-9848", "mrqa_triviaqa-validation-263", "mrqa_naturalquestions-validation-8359", "mrqa_newsqa-validation-2595", "mrqa_hotpotqa-validation-3979", "mrqa_naturalquestions-validation-4544", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-2938", "mrqa_squad-validation-3687", "mrqa_searchqa-validation-14194", "mrqa_hotpotqa-validation-3405", "mrqa_newsqa-validation-2830", "mrqa_naturalquestions-validation-6237", "mrqa_newsqa-validation-2608", "mrqa_triviaqa-validation-1197", "mrqa_newsqa-validation-3620", "mrqa_hotpotqa-validation-3428", "mrqa_triviaqa-validation-2441"], "EFR": 0.8095238095238095, "Overall": 0.7116412076878944}, {"timecode": 83, "before_eval_results": {"predictions": ["Libya", "Syriza", "swimminglasses", "wrigley", "james Blake", "fred flinstone", "eight", "Charles Taylor", "Palm Sunday", "thailand", "The Wicker Man", "sac", "chess", "spider", "Peter Nichols", "peter duncan", "the Count Basie Orchestra", "glenn", "Xenophon", "emundsen", "abbeys", "Pensacola, Florida", "Ireland", "Michael Hordern", "g Gerald Durrell", "Ishmael", "swiss", "climates", "tanks", "james bresslaw", "Etruscan", "James Van Allen", "hillsborough", "Bulls Eye", "South Africa", "rubber sole", "Helen Gurley Brown", "karnak", "The Jungle Book", "joshua", "Massachusetts", "Josh Brolin", "Hamlet", "helen Glover", "paul", "dirigible", "michael parker", "rock follies", "australia", "ann darrow", "Sheila Kaye-Smith", "1996", "beef and pork", "16 June", "Squam Lake", "3D computer-animated comedy", "1902", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "The Impeccable", "Justicialist Party, or PJ by its Spanish acronym,", "the Ming dynasty", "Prince Albert", "a crossword clue", "al Qaeda."], "metric_results": {"EM": 0.5, "QA-F1": 0.5856136204481792}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-5095", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1983", "mrqa_triviaqa-validation-6393", "mrqa_triviaqa-validation-5621", "mrqa_triviaqa-validation-1765", "mrqa_triviaqa-validation-2611", "mrqa_triviaqa-validation-6733", "mrqa_triviaqa-validation-3922", "mrqa_triviaqa-validation-3264", "mrqa_triviaqa-validation-7240", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-352", "mrqa_triviaqa-validation-2857", "mrqa_triviaqa-validation-7713", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-4848", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-5596", "mrqa_newsqa-validation-3703", "mrqa_searchqa-validation-581", "mrqa_searchqa-validation-6285"], "SR": 0.5, "CSR": 0.5604538690476191, "retrieved_ids": ["mrqa_squad-train-15632", "mrqa_squad-train-81850", "mrqa_squad-train-76516", "mrqa_squad-train-38808", "mrqa_squad-train-82589", "mrqa_squad-train-24488", "mrqa_squad-train-28732", "mrqa_squad-train-1452", "mrqa_squad-train-53070", "mrqa_squad-train-39732", "mrqa_squad-train-43551", "mrqa_squad-train-18068", "mrqa_squad-train-655", "mrqa_squad-train-81080", "mrqa_squad-train-59613", "mrqa_squad-train-78451", "mrqa_squad-train-56012", "mrqa_squad-train-2204", "mrqa_squad-train-6886", "mrqa_squad-train-84314", "mrqa_squad-train-18267", "mrqa_squad-train-64644", "mrqa_squad-train-57832", "mrqa_squad-train-9360", "mrqa_searchqa-validation-7662", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-2099", "mrqa_newsqa-validation-3151", "mrqa_hotpotqa-validation-5499", "mrqa_searchqa-validation-7774", "mrqa_hotpotqa-validation-1679", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6018", "mrqa_naturalquestions-validation-3609", "mrqa_searchqa-validation-11137", "mrqa_triviaqa-validation-1917", "mrqa_searchqa-validation-6181", "mrqa_newsqa-validation-2011", "mrqa_naturalquestions-validation-6519", "mrqa_hotpotqa-validation-1900", "mrqa_searchqa-validation-8659", "mrqa_newsqa-validation-2449", "mrqa_naturalquestions-validation-2540", "mrqa_triviaqa-validation-2002", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-2562", "mrqa_hotpotqa-validation-2887", "mrqa_naturalquestions-validation-485"], "EFR": 0.90625, "Overall": 0.7308407738095238}, {"timecode": 84, "before_eval_results": {"predictions": ["ganges", "David Hilbert", "Halifax", "florida", "q", "Franklin Delano Roosevelt", "Buncefield Depot", "Cesium", "cappuccino", "turanga leela", "david cambay", "OK", "Brad Pitt", "florida", "william lamb", "Jupiter Mining Corporation", "phil hartman", "Nouakchott", "gibald", "casa di Giulietta", "weekly", "gail gail i", "noah", "Jezebel", "john budge", "queen Victoria and Prince Albert", "quentin tarantino", "Dick Whittington", "the comitium", "rowing", "goofy", "gin", "Supertramp", "leicestershire", "halogens", "Jackie Kennedy", "blue", "calcium carbonate", "toleware", "cuba", "Lorraine", "Nicola Adams", "yankees", "Andes", "Essex Eagles", "carry On quip", "American History X", "endometriosis", "music", "Brighton", "reneHiguita", "approximately 26,000 years", "Travis Tritt and Marty Stuart", "Thorleif Haug", "Leontes", "Stormzy", "Tottenham Hotspur", "\"To be casually talking about military action because we're getting frustrated seems to me somewhat dangerous,\"", "one count of attempted murder in the second degree in the October 12 attack", "Ma Khin Khin Leh,", "Nintendo", "Germaine Greer", "Jacob and Esau", "Surrey"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6405753968253969}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.2857142857142857, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3183", "mrqa_triviaqa-validation-3894", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-3125", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6485", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4587", "mrqa_naturalquestions-validation-2509", "mrqa_hotpotqa-validation-875", "mrqa_hotpotqa-validation-3265", "mrqa_newsqa-validation-729", "mrqa_newsqa-validation-2718", "mrqa_searchqa-validation-14852"], "SR": 0.578125, "CSR": 0.5606617647058824, "retrieved_ids": ["mrqa_squad-train-58518", "mrqa_squad-train-39441", "mrqa_squad-train-64083", "mrqa_squad-train-62205", "mrqa_squad-train-18700", "mrqa_squad-train-52547", "mrqa_squad-train-59006", "mrqa_squad-train-46481", "mrqa_squad-train-85340", "mrqa_squad-train-48958", "mrqa_squad-train-56503", "mrqa_squad-train-23724", "mrqa_squad-train-58718", "mrqa_squad-train-63459", "mrqa_squad-train-41522", "mrqa_squad-train-39318", "mrqa_squad-train-6071", "mrqa_squad-train-66448", "mrqa_squad-train-59198", "mrqa_squad-train-35887", "mrqa_squad-train-38952", "mrqa_squad-train-60914", "mrqa_squad-train-9126", "mrqa_squad-train-37940", "mrqa_searchqa-validation-4356", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-1456", "mrqa_triviaqa-validation-5923", "mrqa_searchqa-validation-15736", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-5464", "mrqa_searchqa-validation-4423", "mrqa_naturalquestions-validation-3285", "mrqa_hotpotqa-validation-3428", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-340", "mrqa_squad-validation-5724", "mrqa_searchqa-validation-16103", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-1049", "mrqa_hotpotqa-validation-5549", "mrqa_naturalquestions-validation-4674", "mrqa_newsqa-validation-469", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-2536"], "EFR": 0.8518518518518519, "Overall": 0.7200027233115469}, {"timecode": 85, "before_eval_results": {"predictions": ["eagle", "teacher", "Shaft", "semicubical parabola", "Jets", "plateau", "florida", "Back", "Rudyard Kipling", "eat porridge", "Life and Opinions of Tristram Shandy", "David Arroyo", "160", "earth", "pram", "lexis", "c Cyprus", "sheep", "Laos", "Toilet Lid Lock", "Andes Mountains", "George Sand", "10", "minder", "Shepherd Neame", "shoulder", "severn", "legs", "Brief Encounter", "Saturday Night and Sunday Morning", "afterlife", "around May 1", "1982", "bea", "Danish", "priesthood", "Pablo Escobar", "South Africa", "Microsoft", "Bolivia", "Napoleon Bonaparte", "secretary", "Apocalypse Now", "Judy Garland", "Amnesty International", "Wizard", "the Land of the Long White Cloud", "Asia Minor", "Renzo Piano", "50", "russia", "before the first year begins", "1,776 steps", "Hold On", "1919", "patosaurus", "La opera at the Teatro Metastasio", "Virgin America", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "Police", "Daredevil", "George Washington Carver", "panda", "California, Texas and Florida,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6623263888888888}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2922", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-3394", "mrqa_triviaqa-validation-421", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-4290", "mrqa_triviaqa-validation-193", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-3480", "mrqa_triviaqa-validation-3109", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-3040", "mrqa_triviaqa-validation-6628", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-5964", "mrqa_triviaqa-validation-672", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-3561", "mrqa_hotpotqa-validation-372", "mrqa_hotpotqa-validation-4899", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-237", "mrqa_newsqa-validation-2338"], "SR": 0.609375, "CSR": 0.5612281976744187, "retrieved_ids": ["mrqa_squad-train-27917", "mrqa_squad-train-20496", "mrqa_squad-train-31419", "mrqa_squad-train-14122", "mrqa_squad-train-26282", "mrqa_squad-train-8025", "mrqa_squad-train-55915", "mrqa_squad-train-13813", "mrqa_squad-train-75523", "mrqa_squad-train-86578", "mrqa_squad-train-51657", "mrqa_squad-train-11853", "mrqa_squad-train-58527", "mrqa_squad-train-18930", "mrqa_squad-train-37845", "mrqa_squad-train-38671", "mrqa_squad-train-56317", "mrqa_squad-train-58475", "mrqa_squad-train-74878", "mrqa_squad-train-58696", "mrqa_squad-train-68759", "mrqa_squad-train-42648", "mrqa_squad-train-460", "mrqa_squad-train-85486", "mrqa_hotpotqa-validation-1007", "mrqa_searchqa-validation-3203", "mrqa_searchqa-validation-16276", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-6352", "mrqa_hotpotqa-validation-5562", "mrqa_naturalquestions-validation-4824", "mrqa_hotpotqa-validation-4263", "mrqa_hotpotqa-validation-3265", "mrqa_triviaqa-validation-2441", "mrqa_naturalquestions-validation-2466", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-6581", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-2044", "mrqa_triviaqa-validation-3782", "mrqa_naturalquestions-validation-9737", "mrqa_newsqa-validation-4170", "mrqa_hotpotqa-validation-2398", "mrqa_newsqa-validation-469", "mrqa_triviaqa-validation-3926", "mrqa_naturalquestions-validation-3672", "mrqa_triviaqa-validation-6046", "mrqa_squad-validation-6171"], "EFR": 0.76, "Overall": 0.7017456395348838}, {"timecode": 86, "before_eval_results": {"predictions": ["\"This is not something that anybody can reasonably anticipate,\"", "money or other discreet aid", "41,", "Adidas", "kite boards", "suicide bombing", "iTunes", "september", "at a relative's house,", "shot in the head", "at a house party in Crandon, Wisconsin,", "Kenneth Cole", "$17,000", "137", "teeth", "urged NATO to take a more active role in countering the spread of the", "School-age girls", "Theoneste Bagosora,", "\"The Lost Symbol,\"", "Haiti's", "about 2,000", "German authorities", "his brother to surrender.", "Roy Foster", "Mogadishu", "\"face of the peace initiative has been attacked.\"", "16", "\"The Book\"", "fighting charges of Nazi war crimes", "Boys And Girls alone", "Buenos Aires and 255 British military personnel died.", "ALS6,", "public-television show.", "\" Number Ones\" and \"Horehound\"", "Joe Patane", "his parents", "an empty water bottle", "Juri Kibuishi,", "forged credit cards and identity theft", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "\"I don't think I'll be particularly extravagant.\"", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "five", "Facebook and Google,", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "NATO's International Security Assistance Force", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England this weekend.", "a U.S. military helicopter", "mental health", "\"The Cycle of Life,\"", "fining the computer chip giant a record  $1.45 billion", "Thomas Edison", "Randy", "Thomas Lennon", "1947", "bacall", "Mariette", "Boston Celtics", "Australian", "Northwestern Hawaiian Islands", "florida", "toasted", "the Seine", "Mary Tyler Moore Show"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6356639951342695}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.2222222222222222, 1.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.19512195121951217, 0.9166666666666666, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-3787", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-227", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-3913", "mrqa_naturalquestions-validation-10724", "mrqa_triviaqa-validation-884", "mrqa_hotpotqa-validation-4625", "mrqa_searchqa-validation-14248"], "SR": 0.578125, "CSR": 0.5614224137931034, "retrieved_ids": ["mrqa_squad-train-79165", "mrqa_squad-train-49779", "mrqa_squad-train-54650", "mrqa_squad-train-78587", "mrqa_squad-train-51461", "mrqa_squad-train-7419", "mrqa_squad-train-30707", "mrqa_squad-train-2444", "mrqa_squad-train-31007", "mrqa_squad-train-59380", "mrqa_squad-train-61155", "mrqa_squad-train-7073", "mrqa_squad-train-68792", "mrqa_squad-train-42374", "mrqa_squad-train-11451", "mrqa_squad-train-14369", "mrqa_squad-train-27851", "mrqa_squad-train-60871", "mrqa_squad-train-40746", "mrqa_squad-train-59060", "mrqa_squad-train-63251", "mrqa_squad-train-82063", "mrqa_squad-train-82333", "mrqa_squad-train-55250", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_naturalquestions-validation-9737", "mrqa_searchqa-validation-16569", "mrqa_hotpotqa-validation-4575", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-115", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-10927", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-9120", "mrqa_newsqa-validation-1805", "mrqa_searchqa-validation-12477", "mrqa_triviaqa-validation-4887", "mrqa_newsqa-validation-1144", "mrqa_squad-validation-3130", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1844", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-4006", "mrqa_triviaqa-validation-2655", "mrqa_triviaqa-validation-2730"], "EFR": 0.7777777777777778, "Overall": 0.7053400383141762}, {"timecode": 87, "before_eval_results": {"predictions": ["four", "yellow", "pertussis", "Vincent Motorcycle Company", "Harrison Ford", "Reservoir", "equator", "Wimbledon", "sugar baby love", "1981", "Bernardo Bertolucci", "The Seven Year Itch", "Dieppe raid", "Mediterranean", "spinach", "La Boh\u00e8me", "Nicky Morgan", "Midsomer Murders", "Muriel", "Abraham", "Aquaman", "American Civil War", "Christian Louboutin", "st Pauls", "domestic chicken", "orange blossom", "herpes zoster", "Queen Mary", "rupiah", "lisping Violet- Elizabeth Bott", "charles II", "Illinois", "danish", "landlord's game", "spores", "Christine Keeler", "Silver Hatch", "watchmaker", "quetzal", "clogs", "haw Hannibal Heyes and Kid Curry", "humbert humbert", "Edwina Currie", "Baton Rouge", "WarsawWarsaw", "2010", "Carole King", "drizzle", "Casualty", "Trimdon", "sleepless in seattle", "Telma Hopkins, Joyce Vincent Wilson and her sister Pamela Vincent on backing vocals", "1624", "Milira", "8 March 1723", "Austrian Volksbanks", "1848 to 1852", "50,", "the Kurdish militant group in Turkey", "Elena Kagan", "the abacus", "the Maine", "the Marquis de Lafayette", "Donny Osmond"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5719984442640693}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2178", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-2704", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-7166", "mrqa_triviaqa-validation-3669", "mrqa_triviaqa-validation-5437", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-1688", "mrqa_triviaqa-validation-2756", "mrqa_triviaqa-validation-985", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-1255", "mrqa_triviaqa-validation-7150", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-7175", "mrqa_triviaqa-validation-5775", "mrqa_naturalquestions-validation-2862", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-5487", "mrqa_newsqa-validation-3922", "mrqa_newsqa-validation-1506", "mrqa_naturalquestions-validation-5696"], "SR": 0.515625, "CSR": 0.5609019886363636, "retrieved_ids": ["mrqa_squad-train-953", "mrqa_squad-train-81603", "mrqa_squad-train-49597", "mrqa_squad-train-28320", "mrqa_squad-train-84613", "mrqa_squad-train-35743", "mrqa_squad-train-83617", "mrqa_squad-train-40769", "mrqa_squad-train-44702", "mrqa_squad-train-49499", "mrqa_squad-train-47543", "mrqa_squad-train-28597", "mrqa_squad-train-70986", "mrqa_squad-train-48175", "mrqa_squad-train-64467", "mrqa_squad-train-3680", "mrqa_squad-train-10744", "mrqa_squad-train-25299", "mrqa_squad-train-58197", "mrqa_squad-train-18742", "mrqa_squad-train-36654", "mrqa_squad-train-1474", "mrqa_squad-train-21989", "mrqa_squad-train-22697", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-1988", "mrqa_searchqa-validation-13813", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-601", "mrqa_naturalquestions-validation-1409", "mrqa_newsqa-validation-3096", "mrqa_squad-validation-5818", "mrqa_newsqa-validation-2622", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-3345", "mrqa_searchqa-validation-6923", "mrqa_squad-validation-3270", "mrqa_searchqa-validation-6228", "mrqa_naturalquestions-validation-9235", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-6107", "mrqa_squad-validation-8184", "mrqa_newsqa-validation-744", "mrqa_naturalquestions-validation-2006", "mrqa_searchqa-validation-550", "mrqa_hotpotqa-validation-5790", "mrqa_newsqa-validation-1309", "mrqa_searchqa-validation-4495"], "EFR": 0.8387096774193549, "Overall": 0.7174223332111437}, {"timecode": 88, "before_eval_results": {"predictions": ["fish", "luau", "Pat Paulsen", "Paddington Bear", "Antarctica", "gambling", "Mensheviks", "prada", "Euphoria Men Intense Calvin Klein cologne", "an axe", "Shake5peare", "baboon", "Chicken Little", "Bach", "Bangkok", "Eli Whitney's", "John Smith", "James Buchanan Eads", "A Bug's Life", "NASCAR", "quiver", "Queen of Hearts", "Henry Cabot Lodge, Jr.", "Benito Mussolini", "a sheepshank", "Robert Burns", "Ebony", "Jack Nicklaus", "brass", "Las Vegas", "fiber", "poppy", "portrait", "Lord of the Flies", "The Pursuit of Happyness", "Nickelback", "Succotash", "George Freeth", "the Falkland Islands", "acetone", "fudge", "adultery", "frankfurter", "Roanoke", "Blackbeard", "Lindsay Davenport", "Borden", "sulfur dioxide", "Amish TV", "dachshund", "Robert Frost", "Virginia Dare", "Ole Einar Bj\u00f8rndalen", "the first quarter of the 19th century", "George Washington", "Puerto Rico", "Lady Penelope", "1987", "Jacobite uprising", "east", "in a tenement in the Mumbai suburb of Chembur,", "Monday's", "Illinois Reform Commission", "t.S. Eliot"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6574425574425574}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.923076923076923, 1.0, 0.18181818181818182, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16628", "mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-14289", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-12850", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-14294", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14824", "mrqa_searchqa-validation-15658", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-2110", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-5235", "mrqa_searchqa-validation-5816", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-220", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3631", "mrqa_triviaqa-validation-813"], "SR": 0.578125, "CSR": 0.5610955056179776, "retrieved_ids": ["mrqa_squad-train-71209", "mrqa_squad-train-24740", "mrqa_squad-train-85178", "mrqa_squad-train-33051", "mrqa_squad-train-85632", "mrqa_squad-train-61110", "mrqa_squad-train-83662", "mrqa_squad-train-73868", "mrqa_squad-train-29201", "mrqa_squad-train-85321", "mrqa_squad-train-53551", "mrqa_squad-train-77482", "mrqa_squad-train-23487", "mrqa_squad-train-80724", "mrqa_squad-train-49899", "mrqa_squad-train-80874", "mrqa_squad-train-14629", "mrqa_squad-train-79748", "mrqa_squad-train-18617", "mrqa_squad-train-7253", "mrqa_squad-train-23825", "mrqa_squad-train-47238", "mrqa_squad-train-47216", "mrqa_squad-train-77038", "mrqa_triviaqa-validation-7458", "mrqa_triviaqa-validation-2975", "mrqa_naturalquestions-validation-1767", "mrqa_searchqa-validation-244", "mrqa_searchqa-validation-9364", "mrqa_triviaqa-validation-6881", "mrqa_naturalquestions-validation-707", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-6285", "mrqa_triviaqa-validation-2361", "mrqa_hotpotqa-validation-2905", "mrqa_triviaqa-validation-1402", "mrqa_hotpotqa-validation-1754", "mrqa_newsqa-validation-2718", "mrqa_naturalquestions-validation-321", "mrqa_triviaqa-validation-2460", "mrqa_searchqa-validation-12611", "mrqa_triviaqa-validation-115", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-3394", "mrqa_squad-validation-10316", "mrqa_newsqa-validation-1364", "mrqa_naturalquestions-validation-3672", "mrqa_newsqa-validation-2308"], "EFR": 0.8518518518518519, "Overall": 0.720089471493966}, {"timecode": 89, "before_eval_results": {"predictions": ["Bill Bryson", "Pink Panther", "Jordan", "Sweden", "sartre", "Motown", "Carl Johan", "Venus", "riyadh", "margot fonteyn", "Diane Keaton", "plutocracy", "domino", "ringway", "4 Extra", "women and men alike", "violin", "U2", "bacall", "Australia", "auk", "weir", "belgian", "soybean", "George Best", "Time Bandits", "Jean-Paul Gaultier", "Red Rock West", "at the origin", "Zagreb", "handley Page", "Marine One", "Zachary Taylor", "Hitler", "all Saints\u2019 Day", "Shaft", "brazil", "Louis Le Vau", "Scotland", "Tripoli", "jubilee line", "Abbey Theatre", "canada", "willow", "buttermoor", "Denver", "france", "Mel Blanc", "Lily Allen", "terrorist", "oats", "Wisconsin", "season seven", "Whig candidates William Henry Harrison ( the `` hero of Tippecanoe '' ) and John Tyler", "more than 230", "Serie B league", "Mark O'Connor", "Colombia.", "a federal judge in Mississippi", "his comments had been taken out of context.", "Wade E. Pickren", "the Untouchables", "rdere", "Tyne Daly"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7161458333333334}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1295", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-7107", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-7737", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-1687", "mrqa_newsqa-validation-3566", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-9329"], "SR": 0.65625, "CSR": 0.5621527777777777, "retrieved_ids": ["mrqa_squad-train-39333", "mrqa_squad-train-20162", "mrqa_squad-train-1933", "mrqa_squad-train-72903", "mrqa_squad-train-44626", "mrqa_squad-train-45084", "mrqa_squad-train-41757", "mrqa_squad-train-48835", "mrqa_squad-train-7284", "mrqa_squad-train-56716", "mrqa_squad-train-43185", "mrqa_squad-train-71538", "mrqa_squad-train-11612", "mrqa_squad-train-38866", "mrqa_squad-train-2528", "mrqa_squad-train-36537", "mrqa_squad-train-11176", "mrqa_squad-train-36960", "mrqa_squad-train-82745", "mrqa_squad-train-46855", "mrqa_squad-train-65876", "mrqa_squad-train-56205", "mrqa_squad-train-39727", "mrqa_squad-train-76223", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-1529", "mrqa_naturalquestions-validation-681", "mrqa_searchqa-validation-3322", "mrqa_newsqa-validation-2183", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-6986", "mrqa_naturalquestions-validation-8700", "mrqa_hotpotqa-validation-1199", "mrqa_searchqa-validation-8872", "mrqa_newsqa-validation-2635", "mrqa_naturalquestions-validation-5702", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-1948", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-2582", "mrqa_naturalquestions-validation-1953", "mrqa_newsqa-validation-1506", "mrqa_hotpotqa-validation-1679", "mrqa_naturalquestions-validation-5348", "mrqa_hotpotqa-validation-5549", "mrqa_newsqa-validation-2595"], "EFR": 0.7272727272727273, "Overall": 0.695385101010101}, {"timecode": 90, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1216", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1843", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1332", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-669", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10613", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11143", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12031", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16299", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3758", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4169", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4581", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-6445", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8276", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9896", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-3955", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5110", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7708", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1147", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4956", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5035", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.8828125, "KG": 0.5078125, "before_eval_results": {"predictions": ["Shooting Star", "colleen McCullough", "The Lion King", "Cyprus", "f. Lee Bailey", "sprite", "tunisia", "dove", "giraffe", "horse races", "stockton-on-Trent", "Venus", "top do was to dress and act as cool as Bond in his role", "tunisia", "colleen McCullough", "Egypt", "freddi", "drew carey", "Three Mile Island", "sp Sicily", "Sunset Boulevard", "Bombe", "Brussels", "arrows", "The Quatermass Experiment", "pasta harvest", "Frogmore Estate or Gardens", "Emmy", "caucausus", "88", "Cold Comfort Farm", "new year", "iceland", "David Hilbert", "mediterranean", "Declaration of Independence", "Marlon Brando", "fish", "Cleopatra", "go back to the times and work of Anthony Bennezeth; the gentle Quaker teacher who invented the peace movement as recorded in the standard text books on the history of the peace", "robert schumann", "Whisky Galore", "Grace Slick", "Michael Caine", "fonds de la Recherche Scientifique", "Robert Boyle", "1929", "The Lone Gunmen", "ue", "daily Herald", "pj harvey", "Brian Steele", "a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s ( brother, brotherly )", "Cherokee River", "Benedict of Nursia", "Wichita", "Jewish", "Friday,", "has been breeding tuataras for the past 35 years,", "a dove", "method acting", "Annie Proulx", "\"Nude, Green Leaves and Bust\""], "metric_results": {"EM": 0.640625, "QA-F1": 0.6736019736842105}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06666666666666668, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-5508", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-5925", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-6609", "mrqa_triviaqa-validation-5526", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5155", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-3038", "mrqa_triviaqa-validation-7391", "mrqa_triviaqa-validation-7098", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-1956", "mrqa_naturalquestions-validation-10147", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-508"], "SR": 0.640625, "CSR": 0.5630151098901099, "retrieved_ids": ["mrqa_squad-train-12485", "mrqa_squad-train-57528", "mrqa_squad-train-62871", "mrqa_squad-train-27940", "mrqa_squad-train-26693", "mrqa_squad-train-75046", "mrqa_squad-train-74451", "mrqa_squad-train-76535", "mrqa_squad-train-24963", "mrqa_squad-train-9889", "mrqa_squad-train-60108", "mrqa_squad-train-3097", "mrqa_squad-train-82513", "mrqa_squad-train-82539", "mrqa_squad-train-603", "mrqa_squad-train-21690", "mrqa_squad-train-57961", "mrqa_squad-train-68396", "mrqa_squad-train-30305", "mrqa_squad-train-72790", "mrqa_squad-train-59127", "mrqa_squad-train-35535", "mrqa_squad-train-45541", "mrqa_squad-train-32143", "mrqa_newsqa-validation-3087", "mrqa_squad-validation-6171", "mrqa_searchqa-validation-4032", "mrqa_newsqa-validation-1761", "mrqa_triviaqa-validation-5726", "mrqa_searchqa-validation-7059", "mrqa_naturalquestions-validation-4007", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-7773", "mrqa_searchqa-validation-4996", "mrqa_newsqa-validation-3484", "mrqa_searchqa-validation-7662", "mrqa_searchqa-validation-12996", "mrqa_naturalquestions-validation-1135", "mrqa_squad-validation-7357", "mrqa_hotpotqa-validation-2882", "mrqa_newsqa-validation-4118", "mrqa_triviaqa-validation-5500", "mrqa_newsqa-validation-169", "mrqa_searchqa-validation-8634", "mrqa_naturalquestions-validation-138", "mrqa_newsqa-validation-2328", "mrqa_searchqa-validation-2623", "mrqa_squad-validation-3811"], "EFR": 0.8260869565217391, "Overall": 0.7153204132823698}, {"timecode": 91, "before_eval_results": {"predictions": ["Evangelicalism\u2019s flagship magazine", "2011", "John McClane", "Marika Green", "Princeton University", "conservative", "Lombardy", "beat writers", "Elton John", "Newcastle upon Tyne, England", "Lev Ivanovich Yashin", "Blackwood Partners Management Corporation", "1958", "2007 Formula One season", "robot Overlords", "1776", "Ashland", "public house", "1944", "Austria", "Ron Cowen and Daniel Lipman", "The Soloist", "indoor", "Hannaford", "January 30, 1930", "Dr. Alberto Taquini", "Sammy Gravano", "Chris Weidman", "the north", "Harrison Ford", "C. H. Greenblatt", "Taoiseach of Ireland", "Dissection", "The Killer", "seacoast region", "Ken Rutherford and Pakistan by Javed Miandad", "Dorothy", "2017", "people working in film and the performing arts", "June 2, 2008", "The 8th Habit", "one", "London", "Australia", "\"Ready Player One\"", "1981 World Rowing Championships", "1911", "15,023", "North Atlantic Conference", "highland regions of Scotland", "Ken Howard", "March 2018", "Bay of Plenty, Taupo and Wellington", "62", "fly", "Perry Mason", "Oliver Goldsmith", "Afghan lawmakers", "James Whitehouse,", "the Somali border town of Afmado", "Ford", "Jason Bourne", "Aktion Club", "barter -- trading goods and services without exchanging money"], "metric_results": {"EM": 0.625, "QA-F1": 0.7009548611111112}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4948", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-4564", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-1703", "mrqa_naturalquestions-validation-1636", "mrqa_triviaqa-validation-1178", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-9994", "mrqa_newsqa-validation-719"], "SR": 0.625, "CSR": 0.5636888586956521, "retrieved_ids": ["mrqa_squad-train-68105", "mrqa_squad-train-27679", "mrqa_squad-train-32476", "mrqa_squad-train-80624", "mrqa_squad-train-16116", "mrqa_squad-train-59383", "mrqa_squad-train-26568", "mrqa_squad-train-72191", "mrqa_squad-train-49813", "mrqa_squad-train-44847", "mrqa_squad-train-53995", "mrqa_squad-train-70608", "mrqa_squad-train-83469", "mrqa_squad-train-16709", "mrqa_squad-train-37075", "mrqa_squad-train-66866", "mrqa_squad-train-10172", "mrqa_squad-train-65988", "mrqa_squad-train-56652", "mrqa_squad-train-77462", "mrqa_squad-train-73543", "mrqa_squad-train-640", "mrqa_squad-train-53931", "mrqa_squad-train-43366", "mrqa_naturalquestions-validation-3602", "mrqa_searchqa-validation-13247", "mrqa_triviaqa-validation-7175", "mrqa_searchqa-validation-16176", "mrqa_hotpotqa-validation-875", "mrqa_triviaqa-validation-3820", "mrqa_naturalquestions-validation-6234", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-5621", "mrqa_hotpotqa-validation-4120", "mrqa_triviaqa-validation-2096", "mrqa_searchqa-validation-2971", "mrqa_triviaqa-validation-4901", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-4863", "mrqa_hotpotqa-validation-5386", "mrqa_triviaqa-validation-5060", "mrqa_searchqa-validation-14608", "mrqa_newsqa-validation-729", "mrqa_triviaqa-validation-7264", "mrqa_searchqa-validation-12728", "mrqa_triviaqa-validation-7439", "mrqa_searchqa-validation-1053", "mrqa_newsqa-validation-722"], "EFR": 0.875, "Overall": 0.7252377717391305}, {"timecode": 92, "before_eval_results": {"predictions": ["Margery Williams", "the capital of French Indochina", "Terry Richardson", "one of the youngest publicly documented people to be identified as transgender", "the Female Socceroos", "Odense Boldklub", "SpongeBob SquarePants 4-D", "Oldham County", "Grammar, logic, and rhetoric", "the Wright brothers", "a research university with high research activity", "O.T. Genasis", "science fiction drama", "Speedway World Championship", "Citric acid", "200", "a moth", "Gerald Hatten Buss", "Delacorte Press", "close range combat", "twice", "Eli Roth", "South Australia", "Lincoln Riley", "December 13, 1920", "Mark Sinclair", "John McClane", "rural", "Orchard Central", "Art of Dying", "Book of Judges", "Ant Timpson, Ted Geoghegan and Tim League", "the Bahamian island of Great Exuma", "John Ford", "classical realism", "Marvel Comics", "7,500 and 40,000", "for crafting and voting on legislation", "Earvin \"Magic\" Johnson Jr.", "Nevada", "Yasir Hussain", "Victoria", "Nancy Dow", "the Long Island Rail Road", "Universal's volcano Bay", "25 December 2009", "the Hanna-Barbera show \"Birdman and the Galaxy Trio.\"", "Jango Fett", "\"Der Rosenkavalier\", \"Elektra\", \"Die Frau ohne Schatten\" and \"Salome\"", "Minnesota's 8th congressional district", "NBA 2K16", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth )", "Professor Eobard Thawne", "India", "one", "beetle", "australia", "umpire Jake Garner", "Kearny, New Jersey.", "computer problems", "pasta and meatballs", "Tennessee Williams", "Illinois", "bacall"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7208851911976911}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333326, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.33333333333333337, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4211", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-41", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4365", "mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1445", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-1176", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-4979", "mrqa_hotpotqa-validation-4735", "mrqa_naturalquestions-validation-2472", "mrqa_triviaqa-validation-6079", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-909", "mrqa_searchqa-validation-8642", "mrqa_triviaqa-validation-4848"], "SR": 0.578125, "CSR": 0.5638440860215054, "retrieved_ids": ["mrqa_squad-train-78507", "mrqa_squad-train-68153", "mrqa_squad-train-72988", "mrqa_squad-train-18076", "mrqa_squad-train-50396", "mrqa_squad-train-76869", "mrqa_squad-train-42563", "mrqa_squad-train-49736", "mrqa_squad-train-14780", "mrqa_squad-train-44366", "mrqa_squad-train-75599", "mrqa_squad-train-26920", "mrqa_squad-train-39137", "mrqa_squad-train-32671", "mrqa_squad-train-23366", "mrqa_squad-train-55440", "mrqa_squad-train-5900", "mrqa_squad-train-52829", "mrqa_squad-train-4032", "mrqa_squad-train-66556", "mrqa_squad-train-67858", "mrqa_squad-train-37210", "mrqa_squad-train-28738", "mrqa_squad-train-11864", "mrqa_triviaqa-validation-6828", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-5041", "mrqa_searchqa-validation-7620", "mrqa_naturalquestions-validation-8239", "mrqa_searchqa-validation-3082", "mrqa_hotpotqa-validation-2009", "mrqa_searchqa-validation-14852", "mrqa_triviaqa-validation-4361", "mrqa_naturalquestions-validation-1282", "mrqa_searchqa-validation-11821", "mrqa_newsqa-validation-3280", "mrqa_naturalquestions-validation-2418", "mrqa_newsqa-validation-169", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-1380", "mrqa_hotpotqa-validation-3059", "mrqa_searchqa-validation-13710", "mrqa_naturalquestions-validation-6886", "mrqa_triviaqa-validation-4992", "mrqa_searchqa-validation-1049", "mrqa_naturalquestions-validation-5564", "mrqa_newsqa-validation-4170", "mrqa_hotpotqa-validation-558"], "EFR": 0.6666666666666666, "Overall": 0.6836021505376344}, {"timecode": 93, "before_eval_results": {"predictions": ["YIVO", "Archbishop of Canterbury", "Samuel Beckett", "January 28, 2016", "The Catholic Church in Ireland", "close range combat", "Iran", "Kate Millett", "Timothy Matthew Howard", "Lucky", "Do Kyung-soo", "John Hunt", "Kongo", "William Finn", "Sam Raimi", "The final of 2011 Asian Cup", "a suburb", "\"personal earnings\" (such as salary and wages), \"business income\" and \"capital gains\"", "Kaneohe Bay", "August 17, 2017", "Montagues and Capulets", "Walter R\u00f6hrl", "left", "Vladimir Menshov", "film", "Denmark and Norway", "Love and Theft", "C. W. Grafton", "Valerie Stowe", "My Love from the Star", "143,372", "Jack St. Clair Kilby", "Cold Spring", "Afghanistan", "Operation Sculpin", "guitar feedback", "Flushed Away", "George Washington Bridge", "Reginald Engelbach", "Van Diemen's Land", "Tampa", "Sergeant First Class", "140 million", "SpongeBob SquarePants 4-D", "StubHub Center", "Argentinian", "the Americas and the entire South American temperate zone", "a large portion of rural Maine", "1998", "The More", "John F. Kennedy Jr.", "com TLD", "Andrew Lloyd Webber", "John Bull", "NASCAR", "The Avengers", "Rick Wakeman", "mental health", "Madhav Kumar Nepal", "hardship", "Berlin", "William Golding", "The Odd Couple", "Americans who served in the armed forces and as civilians during World War II"], "metric_results": {"EM": 0.734375, "QA-F1": 0.809146148989899}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.9090909090909091, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.22222222222222224, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5676", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4235", "mrqa_naturalquestions-validation-4844", "mrqa_newsqa-validation-1063"], "SR": 0.734375, "CSR": 0.5656582446808511, "retrieved_ids": ["mrqa_squad-train-5882", "mrqa_squad-train-39511", "mrqa_squad-train-5821", "mrqa_squad-train-14519", "mrqa_squad-train-29884", "mrqa_squad-train-15089", "mrqa_squad-train-43680", "mrqa_squad-train-6750", "mrqa_squad-train-39244", "mrqa_squad-train-80941", "mrqa_squad-train-55603", "mrqa_squad-train-53527", "mrqa_squad-train-57774", "mrqa_squad-train-4275", "mrqa_squad-train-20766", "mrqa_squad-train-39254", "mrqa_squad-train-70928", "mrqa_squad-train-28839", "mrqa_squad-train-46427", "mrqa_squad-train-21559", "mrqa_squad-train-12473", "mrqa_squad-train-73970", "mrqa_squad-train-1966", "mrqa_squad-train-54342", "mrqa_naturalquestions-validation-5055", "mrqa_hotpotqa-validation-246", "mrqa_triviaqa-validation-3926", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-1224", "mrqa_newsqa-validation-1506", "mrqa_naturalquestions-validation-6555", "mrqa_searchqa-validation-9458", "mrqa_searchqa-validation-8642", "mrqa_naturalquestions-validation-1372", "mrqa_newsqa-validation-1392", "mrqa_hotpotqa-validation-5464", "mrqa_newsqa-validation-686", "mrqa_searchqa-validation-5372", "mrqa_naturalquestions-validation-10724", "mrqa_searchqa-validation-963", "mrqa_triviaqa-validation-2357", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-1974", "mrqa_searchqa-validation-10907", "mrqa_searchqa-validation-9789", "mrqa_triviaqa-validation-1948", "mrqa_searchqa-validation-3875"], "EFR": 0.5882352941176471, "Overall": 0.6682787077596997}, {"timecode": 94, "before_eval_results": {"predictions": ["Nutbush", "alfa", "6", "golf", "Einstein", "Manchester", "southampton", "Bleak House", "george iv", "president of the United States", "york", "to make wrinkles", "Amy Tan", "The Great Gatsby", "Charlie Chan", "1664", "A Beautiful Mind", "marquess of Burlington", "Iain Duncan Smith", "silversmith", "george orwell", "Jim Peters", "nitrogen", "oldpatricktoe-end", "Delilah", "iberian heiresses to thrones", "cuckoo", "MD", "The Wicker Man", "yellow", "avonlea", "giacomo Meyerbeer", "Guardian", "John Huston", "Passenger Pigeon", "Anne Frank", "manchego", "brazil", "pi\u00f1a colada", "fauntleroy", "kachhi", "Petula Clark", "Jo Moore", "Flo Rida", "Comedy of Errors", "beer", "chemical origins of life", "Finland", "fructose", "dolma", "kempton park", "Cress", "Vesta's fire and the sun", "the Season 6 premiere", "Tiffany & Company", "2010 to 2012", "Nathan Bedford Forrest", "Friday,", "Six of the 15 lightning- strike deaths in the U.S. so far this year", "that bad times somewhere else in the U.S. may eventually come to affect them", "tanning", "Tulane", "Brody", "the Bactrian"], "metric_results": {"EM": 0.59375, "QA-F1": 0.63168211996337}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.1904761904761905, 1.0, 1.0, 0.0, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-661", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-1828", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-1409", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-7655", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-2276", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9903", "mrqa_hotpotqa-validation-2141", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1300", "mrqa_searchqa-validation-5376", "mrqa_naturalquestions-validation-8046"], "SR": 0.59375, "CSR": 0.565953947368421, "retrieved_ids": ["mrqa_squad-train-10192", "mrqa_squad-train-49171", "mrqa_squad-train-6761", "mrqa_squad-train-67601", "mrqa_squad-train-10882", "mrqa_squad-train-60480", "mrqa_squad-train-28878", "mrqa_squad-train-49746", "mrqa_squad-train-28244", "mrqa_squad-train-17632", "mrqa_squad-train-19173", "mrqa_squad-train-60643", "mrqa_squad-train-20881", "mrqa_squad-train-75434", "mrqa_squad-train-27924", "mrqa_squad-train-8739", "mrqa_squad-train-12437", "mrqa_squad-train-68798", "mrqa_squad-train-24814", "mrqa_squad-train-32640", "mrqa_squad-train-60499", "mrqa_squad-train-14680", "mrqa_squad-train-80780", "mrqa_squad-train-19343", "mrqa_searchqa-validation-9789", "mrqa_newsqa-validation-3210", "mrqa_squad-validation-7332", "mrqa_naturalquestions-validation-7242", "mrqa_searchqa-validation-2456", "mrqa_triviaqa-validation-5185", "mrqa_searchqa-validation-2049", "mrqa_triviaqa-validation-193", "mrqa_naturalquestions-validation-6324", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5265", "mrqa_squad-validation-5262", "mrqa_newsqa-validation-1488", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-741", "mrqa_naturalquestions-validation-8909", "mrqa_hotpotqa-validation-3785", "mrqa_searchqa-validation-13377", "mrqa_triviaqa-validation-4577", "mrqa_triviaqa-validation-5220", "mrqa_searchqa-validation-15142", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-6528", "mrqa_newsqa-validation-742"], "EFR": 0.6153846153846154, "Overall": 0.6737677125506073}, {"timecode": 95, "before_eval_results": {"predictions": ["the Korean War", "$1.5 million", "Fernando Caceres", "37", "opposition parties", "green-card warriors", "people give the United States abysmal approval ratings.", "Secretary of State", "my wife's name", "periods", "U.S. senators", "to put a lid on the marking of Ashura this year.", "Alexey Pajitnov,", "Spc. Megan Lynn Touma,", "regulators in the agency's Colorado office", "the west African nation", "Leo Frank,", "Sri Lanka", "Johannesburg", "last year's", "enormous suffering and massive displacement.", "heavy flannel or wool", "It is not something that has gotten lost,\"", "near Pakistan's border with Afghanistan", "walk", "an independent homeland since 1983.", "would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "longest domestic relay in Olympic history,", "Kim Jong Un", "Arthur E. Morgan III,", "billboards with an image of the burning World Trade Center", "Adidas", "Too many glass shards left by beer drinkers in the city center,", "E! News", "Cologne, Germany,", "she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "70,000", "The station", "one of Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "has said very little in public about the scandal, which has swept Western Europe this year,", "large accumulations of ice", "East Java", "part of a planned training exercise designed to help the prince learn to fly in combat situations.", "billions of dollars", "between June 20 and July 20.", "Rod Blagojevich,", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "Majid Movahedi,", "violating anti-trust laws.", "Jonas", "billions of dollars", "Windows Media Video ( WMV )", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "Joe Pizzulo and Leeza Miller", "Alan Turing", "paisley", "Tiny", "three", "The Apple iPod+HP", "Lithuanian", "Jeopardy!", "Ed Viesturs", "The Sun Also Rises", "Rob Reiner"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6420182418918632}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.972972972972973, 0.6666666666666666, 1.0, 0.4, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.35714285714285715, 1.0, 1.0, 0.1111111111111111, 0.09523809523809522, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-3562", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-1314", "mrqa_triviaqa-validation-4402", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-318", "mrqa_searchqa-validation-8089", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-10142"], "SR": 0.515625, "CSR": 0.5654296875, "retrieved_ids": ["mrqa_squad-train-67743", "mrqa_squad-train-68804", "mrqa_squad-train-67295", "mrqa_squad-train-32719", "mrqa_squad-train-8953", "mrqa_squad-train-60553", "mrqa_squad-train-42881", "mrqa_squad-train-53993", "mrqa_squad-train-32615", "mrqa_squad-train-21419", "mrqa_squad-train-71737", "mrqa_squad-train-72096", "mrqa_squad-train-8192", "mrqa_squad-train-75662", "mrqa_squad-train-43586", "mrqa_squad-train-46411", "mrqa_squad-train-68973", "mrqa_squad-train-39681", "mrqa_squad-train-39986", "mrqa_squad-train-75610", "mrqa_squad-train-32729", "mrqa_squad-train-33808", "mrqa_squad-train-7706", "mrqa_squad-train-62866", "mrqa_searchqa-validation-10770", "mrqa_triviaqa-validation-423", "mrqa_searchqa-validation-11576", "mrqa_newsqa-validation-3992", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1380", "mrqa_triviaqa-validation-2246", "mrqa_naturalquestions-validation-5034", "mrqa_triviaqa-validation-3906", "mrqa_squad-validation-3985", "mrqa_searchqa-validation-12396", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-1094", "mrqa_hotpotqa-validation-2377", "mrqa_triviaqa-validation-2459", "mrqa_newsqa-validation-1688", "mrqa_triviaqa-validation-4374", "mrqa_hotpotqa-validation-1742", "mrqa_naturalquestions-validation-8591", "mrqa_newsqa-validation-4059", "mrqa_searchqa-validation-5919", "mrqa_newsqa-validation-3437", "mrqa_searchqa-validation-1852", "mrqa_searchqa-validation-14446"], "EFR": 0.5161290322580645, "Overall": 0.6538117439516129}, {"timecode": 96, "before_eval_results": {"predictions": ["UNICEF", "poppy production", "security breach", "urged NATO to take a more active role in countering the spread of the", "public opinion", "Christopher Savoie", "prisoners at the South Dakota State Penitentiary", "Tuesday afternoon.", "Iowa,", "Dr. Jennifer Arnold and husband Bill Klein,", "Chinese", "Bloomberg", "more than 1.2 million", "the estate with its 18th-century sights, sounds, and scents.", "Keating Holland.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Flint, Michigan.", "FARC rebels.", "Mexico", "Larry King", "Alberto Espinoza Barron,", "spiral into economic disaster.", "Four", "The tall 34-year-old,", "burned over 65 percent of his body", "Brian Smith", "2-1", "a motor scooter", "Chuck Bass", "April 2010.", "\"Nothing But Love\"", "Mandi Hamlin", "people look at the content of the speech, not just the delivery.", "Yemen,", "Dodi Fayed,", "an American who entered the country illegally from China on Christmas Eve.", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "The boat, the designers said, could make life just like at home on a personal estate for its owner.", "Manny Pacquiao", "\"I knew it was a weight-loss show and I knew it were a dance show, but I didn't think I was going to learn so much about myself through the process,\"", "The minivan ran a red light and struck two vehicles at an intersection,", "President Thabo Mbeki", "Bright Automotive, a small carmaker from Anderson, Indiana,", "Jeffrey Jamaleldine", "Negotiators for Zelaya and Roberto Micheletti, the politician who was appointed president hours after Zelaya's June 28 removal, reached an agreement late Thursday", "Haiti,", "Atlanta's", "his business dealings", "hardship for terminally ill patients and their caregivers,", "nearly 28 years", "Hollywood", "I \u00d7 9", "ThonMaker", "The weekly Torah portion", "syria", "bacall", "bacall", "sexy Star", "March 31, 1944", "Dutch", "a dragon", "Marcus Garvey", "zodiac", "obsessive-compulsive"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6605478472547377}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.9166666666666666, 0.0, 1.0, 0.0, 0.9523809523809523, 0.2, 0.4444444444444445, 0.6666666666666666, 0.25806451612903225, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-905", "mrqa_newsqa-validation-1138", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-5312", "mrqa_searchqa-validation-2594", "mrqa_searchqa-validation-9869"], "SR": 0.5625, "CSR": 0.5653994845360825, "retrieved_ids": ["mrqa_squad-train-69688", "mrqa_squad-train-36414", "mrqa_squad-train-11407", "mrqa_squad-train-67470", "mrqa_squad-train-72447", "mrqa_squad-train-36254", "mrqa_squad-train-27788", "mrqa_squad-train-57209", "mrqa_squad-train-19864", "mrqa_squad-train-21366", "mrqa_squad-train-23948", "mrqa_squad-train-30476", "mrqa_squad-train-20991", "mrqa_squad-train-32628", "mrqa_squad-train-86545", "mrqa_squad-train-45291", "mrqa_squad-train-32686", "mrqa_squad-train-51729", "mrqa_squad-train-7986", "mrqa_squad-train-78698", "mrqa_squad-train-6294", "mrqa_squad-train-55448", "mrqa_squad-train-64248", "mrqa_squad-train-64130", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-3338", "mrqa_searchqa-validation-5427", "mrqa_triviaqa-validation-2495", "mrqa_triviaqa-validation-6107", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-1300", "mrqa_triviaqa-validation-2820", "mrqa_searchqa-validation-12391", "mrqa_squad-validation-4452", "mrqa_searchqa-validation-4495", "mrqa_searchqa-validation-5886", "mrqa_triviaqa-validation-2819", "mrqa_triviaqa-validation-519", "mrqa_hotpotqa-validation-3930", "mrqa_newsqa-validation-3459", "mrqa_triviaqa-validation-1380", "mrqa_newsqa-validation-1569", "mrqa_triviaqa-validation-4663", "mrqa_searchqa-validation-483", "mrqa_triviaqa-validation-7175", "mrqa_newsqa-validation-1360", "mrqa_naturalquestions-validation-4054", "mrqa_newsqa-validation-2903"], "EFR": 0.5714285714285714, "Overall": 0.6648656111929307}, {"timecode": 97, "before_eval_results": {"predictions": ["Johannes Gutenberg", "In the 2020 edition, the all - star teams will be replaced by qualifying teams", "in the books of Exodus and Deuteronomy", "Thomas Jefferson", "about the level of the third lumbar vertebra, or L3, at birth", "King Dasharatha", "Pakistan", "for the red - bed country of its watershed", "the United States, its NATO allies and others", "Rashida Jones", "cut off close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "warm and is considered to be the most comfortable climatic conditions of the year", "Peter Andrew Beardsley MBE", "Season two", "Terry Reid", "1260 cubic centimeters ( cm ) for men and 1130 cm for women", "May 3, 2005", "As of September 18, 2012, the chain operates 639 stores in 43 states", "the Rashidun Caliphs", "British Columbia, Canada", "Koine Greek : apokalypsis", "Pyeongchang County, Gangwon Province, South Korea", "A spiral galaxy like the Milky Way", "a very long forward pass in American football, made in desperation, with only a small chance of success and time running out on the clock", "1943", "Tokyo for the 2020 Summer Olympics", "Lituya Bay in Alaska", "the Cow Palace, before they moved to their present home, the SAP Center at San Jose in 1993", "Panzerkampfwagen VIII Maus", "July 2, 1776", "accomplish the objectives of the organization", "Domhnall Gleeson", "Simon Callow", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house which would destroy a colony of burrowing owls who live on the site", "Laura Jane Haddock", "In May 2016 Canada officially removed its objector status to UNDRIP, almost a decade after it was adopted by the General Assembly", "Bacon", "It was published in 1994 and written by Francesca Simon and illustrated by Tony Ross", "Matthew Broderick", "senators", "origins of replication, in the genome", "the fourth quarter of the preceding year", "April 2016", "Michael Schumacher", "Massachusetts", "Latitude", "2010", "post translational modification", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "petition for a writ of certiorari", "Jules Shear", "guinea", "island", "the head and neck", "Romeo", "De La Soul", "Delilah Rene", "July", "conviction of Peru's ex-president is a warning to those who deny human rights.", "$81,8709", "the Missouri", "jade", "Frank Sinatra", "Long troop deployments"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6423816365222615}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.15384615384615385, 0.2857142857142857, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.07407407407407407, 0.0, 1.0, 1.0, 0.0, 0.8148148148148148, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.08108108108108107, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06060606060606061, 1.0, 0.18181818181818182, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-6687", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-1339", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-7950", "mrqa_triviaqa-validation-473", "mrqa_triviaqa-validation-2425", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2892"], "SR": 0.5625, "CSR": 0.5653698979591837, "retrieved_ids": ["mrqa_squad-train-47422", "mrqa_squad-train-77910", "mrqa_squad-train-42128", "mrqa_squad-train-39874", "mrqa_squad-train-13550", "mrqa_squad-train-8744", "mrqa_squad-train-19740", "mrqa_squad-train-8646", "mrqa_squad-train-42925", "mrqa_squad-train-35781", "mrqa_squad-train-37246", "mrqa_squad-train-78576", "mrqa_squad-train-72563", "mrqa_squad-train-84186", "mrqa_squad-train-31827", "mrqa_squad-train-4493", "mrqa_squad-train-57806", "mrqa_squad-train-9016", "mrqa_squad-train-53888", "mrqa_squad-train-80821", "mrqa_squad-train-23409", "mrqa_squad-train-43574", "mrqa_squad-train-11082", "mrqa_squad-train-66793", "mrqa_naturalquestions-validation-1327", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-3428", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-6610", "mrqa_naturalquestions-validation-1187", "mrqa_searchqa-validation-13247", "mrqa_triviaqa-validation-7038", "mrqa_naturalquestions-validation-4561", "mrqa_newsqa-validation-3063", "mrqa_searchqa-validation-14519", "mrqa_triviaqa-validation-1955", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-5775", "mrqa_hotpotqa-validation-411", "mrqa_naturalquestions-validation-681", "mrqa_searchqa-validation-6285", "mrqa_naturalquestions-validation-2385", "mrqa_searchqa-validation-5949", "mrqa_searchqa-validation-15280"], "EFR": 0.32142857142857145, "Overall": 0.614859693877551}, {"timecode": 98, "before_eval_results": {"predictions": ["hair", "Deimos", "Lana Turner", "a Polaroid picture", "Oklahoma City", "June Carter Cash", "owl", "Colleen", "fat", "poison ivy", "Denny McLain", "road", "Edith Wharton", "Liberia", "Rockabilly", "Buckingham Palace", "AARP", "Arturo Toscanini", "Bangladesh", "Saturn", "Nancy Pelosi", "Unison", "Dr. Pepper", "misery", "the Black Swallower", "coal mining", "Iowa", "kidnapping", "John Paul II", "a photocopy", "Syria", "Sylvia", "a plies", "the Bean Sidhe", "Japan", "Zephyr Teachout", "a ballistic missile submarine", "Ambrose Bierce", "Walt Whitman", "frequency", "Macbeth", "Colorado", "vice presidential running mate", "Tommy Franks", "Botswana", "Mousehunt", "the Dow Jones", "Winston Churchill", "Vietnam", "a tuba", "a Croque Madam", "Kyla Pratt", "Wisconsin", "March 11, 2018", "Top Cat", "goose bumps", "Adrian Cronauer", "1 August 1971", "Australia", "Bronwyn Bishop", "Jacob,", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "Joe Jackson", "About 200"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7441907051282051}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8500", "mrqa_searchqa-validation-1461", "mrqa_searchqa-validation-11742", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15889", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-14445", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-10204", "mrqa_searchqa-validation-13577", "mrqa_searchqa-validation-13824", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-7781", "mrqa_searchqa-validation-1625", "mrqa_searchqa-validation-2884", "mrqa_triviaqa-validation-312", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-1955"], "SR": 0.703125, "CSR": 0.5667613636363636, "retrieved_ids": ["mrqa_squad-train-76528", "mrqa_squad-train-71684", "mrqa_squad-train-24695", "mrqa_squad-train-71639", "mrqa_squad-train-48174", "mrqa_squad-train-76771", "mrqa_squad-train-54130", "mrqa_squad-train-47879", "mrqa_squad-train-12683", "mrqa_squad-train-63372", "mrqa_squad-train-66545", "mrqa_squad-train-13090", "mrqa_squad-train-30869", "mrqa_squad-train-28013", "mrqa_squad-train-47337", "mrqa_squad-train-64300", "mrqa_squad-train-1637", "mrqa_squad-train-58943", "mrqa_squad-train-16181", "mrqa_squad-train-86430", "mrqa_squad-train-85686", "mrqa_squad-train-60500", "mrqa_squad-train-38239", "mrqa_squad-train-1857", "mrqa_squad-validation-6439", "mrqa_naturalquestions-validation-3598", "mrqa_newsqa-validation-1413", "mrqa_triviaqa-validation-5185", "mrqa_searchqa-validation-8582", "mrqa_naturalquestions-validation-4206", "mrqa_newsqa-validation-3833", "mrqa_hotpotqa-validation-1864", "mrqa_naturalquestions-validation-707", "mrqa_triviaqa-validation-1917", "mrqa_searchqa-validation-8760", "mrqa_hotpotqa-validation-2986", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-1265", "mrqa_searchqa-validation-3618", "mrqa_newsqa-validation-4122", "mrqa_naturalquestions-validation-3297", "mrqa_triviaqa-validation-5632", "mrqa_naturalquestions-validation-6857", "mrqa_searchqa-validation-15498", "mrqa_newsqa-validation-2477", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-421"], "EFR": 0.2631578947368421, "Overall": 0.6034838516746411}, {"timecode": 99, "UKR": 0.794921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1078", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1650", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-1866", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-2023", "mrqa_hotpotqa-validation-2067", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-276", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3141", "mrqa_hotpotqa-validation-3214", "mrqa_hotpotqa-validation-3329", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3783", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-3878", "mrqa_hotpotqa-validation-3905", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-4120", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4590", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5578", "mrqa_hotpotqa-validation-5594", "mrqa_hotpotqa-validation-5595", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5620", "mrqa_hotpotqa-validation-5647", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-84", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1987", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2225", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2659", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3568", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-3668", "mrqa_naturalquestions-validation-3805", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4553", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-6372", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6678", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7127", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-793", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-7958", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8120", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-8216", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-8764", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-9239", "mrqa_naturalquestions-validation-926", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-1254", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1517", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2951", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3146", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-325", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-3588", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-395", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-459", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-70", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-777", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-823", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-1049", "mrqa_searchqa-validation-10670", "mrqa_searchqa-validation-10675", "mrqa_searchqa-validation-10795", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-11271", "mrqa_searchqa-validation-11530", "mrqa_searchqa-validation-11570", "mrqa_searchqa-validation-11965", "mrqa_searchqa-validation-12252", "mrqa_searchqa-validation-12568", "mrqa_searchqa-validation-12594", "mrqa_searchqa-validation-1279", "mrqa_searchqa-validation-12962", "mrqa_searchqa-validation-12999", "mrqa_searchqa-validation-13041", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-13115", "mrqa_searchqa-validation-13120", "mrqa_searchqa-validation-13232", "mrqa_searchqa-validation-13273", "mrqa_searchqa-validation-13478", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-14655", "mrqa_searchqa-validation-15686", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-16021", "mrqa_searchqa-validation-16176", "mrqa_searchqa-validation-16209", "mrqa_searchqa-validation-16308", "mrqa_searchqa-validation-16378", "mrqa_searchqa-validation-16569", "mrqa_searchqa-validation-1827", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2038", "mrqa_searchqa-validation-2304", "mrqa_searchqa-validation-2368", "mrqa_searchqa-validation-2467", "mrqa_searchqa-validation-2884", "mrqa_searchqa-validation-3013", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-3518", "mrqa_searchqa-validation-3573", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-4089", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-5149", "mrqa_searchqa-validation-5177", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-5812", "mrqa_searchqa-validation-5911", "mrqa_searchqa-validation-5922", "mrqa_searchqa-validation-5943", "mrqa_searchqa-validation-663", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7213", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7419", "mrqa_searchqa-validation-7829", "mrqa_searchqa-validation-7871", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8465", "mrqa_searchqa-validation-8638", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-8985", "mrqa_searchqa-validation-9249", "mrqa_searchqa-validation-935", "mrqa_searchqa-validation-9372", "mrqa_searchqa-validation-9696", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-9789", "mrqa_searchqa-validation-9853", "mrqa_searchqa-validation-9902", "mrqa_searchqa-validation-9910", "mrqa_squad-validation-10369", "mrqa_squad-validation-10477", "mrqa_squad-validation-115", "mrqa_squad-validation-1156", "mrqa_squad-validation-127", "mrqa_squad-validation-1371", "mrqa_squad-validation-2328", "mrqa_squad-validation-259", "mrqa_squad-validation-2691", "mrqa_squad-validation-280", "mrqa_squad-validation-2959", "mrqa_squad-validation-3052", "mrqa_squad-validation-3124", "mrqa_squad-validation-3144", "mrqa_squad-validation-3230", "mrqa_squad-validation-3241", "mrqa_squad-validation-335", "mrqa_squad-validation-34", "mrqa_squad-validation-3406", "mrqa_squad-validation-3608", "mrqa_squad-validation-3703", "mrqa_squad-validation-3919", "mrqa_squad-validation-4066", "mrqa_squad-validation-415", "mrqa_squad-validation-4326", "mrqa_squad-validation-494", "mrqa_squad-validation-4986", "mrqa_squad-validation-5422", "mrqa_squad-validation-5604", "mrqa_squad-validation-5726", "mrqa_squad-validation-5781", "mrqa_squad-validation-5960", "mrqa_squad-validation-6169", "mrqa_squad-validation-6502", "mrqa_squad-validation-6875", "mrqa_squad-validation-7064", "mrqa_squad-validation-7549", "mrqa_squad-validation-7717", "mrqa_squad-validation-7751", "mrqa_squad-validation-8754", "mrqa_squad-validation-8904", "mrqa_squad-validation-8958", "mrqa_squad-validation-959", "mrqa_squad-validation-9716", "mrqa_triviaqa-validation-1019", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1166", "mrqa_triviaqa-validation-12", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1239", "mrqa_triviaqa-validation-1290", "mrqa_triviaqa-validation-1512", "mrqa_triviaqa-validation-1576", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-1806", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-189", "mrqa_triviaqa-validation-1917", "mrqa_triviaqa-validation-2002", "mrqa_triviaqa-validation-2004", "mrqa_triviaqa-validation-2036", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2140", "mrqa_triviaqa-validation-2170", "mrqa_triviaqa-validation-2194", "mrqa_triviaqa-validation-2303", "mrqa_triviaqa-validation-2328", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-2504", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-2565", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-2730", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-2939", "mrqa_triviaqa-validation-2975", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3043", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3115", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3347", "mrqa_triviaqa-validation-341", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3495", "mrqa_triviaqa-validation-3522", "mrqa_triviaqa-validation-3525", "mrqa_triviaqa-validation-3747", "mrqa_triviaqa-validation-3768", "mrqa_triviaqa-validation-384", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-3967", "mrqa_triviaqa-validation-426", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4410", "mrqa_triviaqa-validation-4447", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4711", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-483", "mrqa_triviaqa-validation-4848", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5032", "mrqa_triviaqa-validation-5141", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-5312", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-5695", "mrqa_triviaqa-validation-5763", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5823", "mrqa_triviaqa-validation-5853", "mrqa_triviaqa-validation-5855", "mrqa_triviaqa-validation-5867", "mrqa_triviaqa-validation-5897", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5952", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-61", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6475", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6618", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6732", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6833", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7103", "mrqa_triviaqa-validation-7190", "mrqa_triviaqa-validation-7327", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7497", "mrqa_triviaqa-validation-7688", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-971"], "OKR": 0.896484375, "KG": 0.52421875, "before_eval_results": {"predictions": ["New Orleans", "Gulliver's Travels; Of Mice and Men", "Minnesota", "the Rosetta Stone", "Japan", "crumpets", "Lord Bill Astor", "peripheral", "Henry Wadsworth Longfellow", "Canton", "Hormel Foods", "syllable", "Theodore", "Beaufort", "Roger Williams", "Niels Bohr", "the sun", "Moby Dick", "monsters", "the Big Z Memorial Surf Off", "Scorpio", "a cat", "Finding Nemo", "the International Space Station", "Shakira", "Candice Bergen", "a shark", "Ireland", "George J. Mitchell", "Henry Wadsworth Longfellow", "Gauguin", "Mary, Queen of Scots", "bamboo", "Animal Crackers", "Crete", "Frank Sinatra", "George Armstrong Custer", "Barney Stinson", "March 18", "Marlee Matlin", "Ben- Hur: A Tale of the Christ", "Hideo Nomo", "Dan Rather", "KLM", "food combining", "ducti", "elephants", "Arkansas", "Bank of America", "a piccolo", "the tuba", "Jason Marsden", "1998", "Garfield Sobers", "bavaria", "Jimmy Carter", "blue", "Detroit, Michigan", "the Troubles", "ARY Films", "Sri Lanka", "Ali Bongo", "The commission, led by former U.S. Attorney Patrick Collins,", "Muhammad"], "metric_results": {"EM": 0.6875, "QA-F1": 0.767001488095238}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true], "QA-F1": [0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4116", "mrqa_searchqa-validation-11687", "mrqa_searchqa-validation-16851", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-9926", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-11534", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-3988", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-1713", "mrqa_searchqa-validation-9182", "mrqa_searchqa-validation-7727", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-3439", "mrqa_newsqa-validation-3926"], "SR": 0.6875, "CSR": 0.56796875, "retrieved_ids": ["mrqa_squad-train-54055", "mrqa_squad-train-52623", "mrqa_squad-train-27716", "mrqa_squad-train-35773", "mrqa_squad-train-24598", "mrqa_squad-train-72707", "mrqa_squad-train-30320", "mrqa_squad-train-77280", "mrqa_squad-train-1126", "mrqa_squad-train-54415", "mrqa_squad-train-70598", "mrqa_squad-train-34311", "mrqa_squad-train-78832", "mrqa_squad-train-79756", "mrqa_squad-train-33058", "mrqa_squad-train-55244", "mrqa_squad-train-78031", "mrqa_squad-train-47512", "mrqa_squad-train-80788", "mrqa_squad-train-50972", "mrqa_squad-train-77571", "mrqa_squad-train-65480", "mrqa_squad-train-22900", "mrqa_squad-train-59943", "mrqa_newsqa-validation-748", "mrqa_naturalquestions-validation-1423", "mrqa_triviaqa-validation-4073", "mrqa_searchqa-validation-15033", "mrqa_newsqa-validation-964", "mrqa_naturalquestions-validation-4007", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-3469", "mrqa_searchqa-validation-16229", "mrqa_naturalquestions-validation-3672", "mrqa_searchqa-validation-16021", "mrqa_hotpotqa-validation-318", "mrqa_newsqa-validation-3476", "mrqa_squad-validation-4539", "mrqa_searchqa-validation-354", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-7004", "mrqa_newsqa-validation-759", "mrqa_naturalquestions-validation-6019", "mrqa_newsqa-validation-1032", "mrqa_searchqa-validation-11923", "mrqa_newsqa-validation-2338", "mrqa_naturalquestions-validation-8257", "mrqa_triviaqa-validation-2611"], "EFR": 0.4, "Overall": 0.63671875}]}