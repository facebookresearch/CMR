{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=100.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=100_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4200, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["April 20", "Paul Whiteman", "2005", "Islamism", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "the center of the curving path", "eight", "28", "global", "Fresno", "Amazonia: Man and Culture in a Counterfeit Paradise", "88%", "broken wing and leg", "Emmy Awards", "silt up the lake", "The TEU specifically excludes certain regions, for example the Faroe Islands, from the jurisdiction of European Union law", "legitimate medical purpose", "week 7", "Kromme Rijn", "Robert Boyle", "five", "Paul Samuelson", "The First Doctor encounters himself in the story The Space Museum", "until the age of 16", "80%", "Five", "threatened \"Old Briton\" with severe consequences", "less than 200,000", "Anheuser-Busch InBev", "Charles River", "Veni redemptor gentium", "northwestern Canada", "intracellular pathogenesis", "1998", "seven months old", "The Service Module was discarded", "July", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Article 17(3)", "Doctor Who \u2013 The Ultimate Adventure", "Eric Roberts", "electricity could be used to locate submarines", "1910\u20131940", "the owner", "2.666 million", "September 1969", "Apollo Program Director", "dreams", "Charles I", "more than 1,100 tree species", "complete the modules to earn Chartered Teacher Status", "true larvae", "apicomplexan-related diseases", "Rev. Paul T. Stallsworth", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "2009", "Daniel Diermeier", "PNU and ODM camps", "136", "12 to 15 million", "flax", "February 1, 2016", "2001", "lipophilic alkaloid toxins"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8867121848739495}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6614", "mrqa_squad-validation-4170", "mrqa_squad-validation-6426", "mrqa_squad-validation-8037", "mrqa_squad-validation-7774", "mrqa_squad-validation-3885", "mrqa_squad-validation-1492", "mrqa_squad-validation-5788", "mrqa_squad-validation-4343"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["Carolina Panthers", "1530", "Gallifrey", "ca. 2 million", "intractable problems", "effective planning", "pyrenoid and thylakoids", "Canterbury", "1873", "A bridge", "clergyman", "Commission v Italy", "sports tourism", "G", "eating both fish larvae and small crustaceans", "slow to complete division", "Astra's", "T. T. Tsui Gallery", "1905", "theatres", "those who proceed to secondary school or vocational training", "1521", "Search the Collections", "CD8", "3 January 1521", "a bill", "the University of Aberdeen", "2014", "Missy", "US$10 a week", "Super Bowl City", "Dudley Simpson", "esoteric", "temperature and light", "4000 years", "new entrance building", "Levi's Stadium", "tutor", "electricity", "2007", "Los Angeles", "zeta function", "adviser", "over $40 million", "Sunday Service of the Methodists in North America", "Lead fusible plugs", "occupational stress", "Synthetic aperture radar", "European Parliament and the Council of the European Union", "the coronation of Queen Elizabeth II", "O(n2)", "the Main Quadrangles", "35", "quadratic time", "antisemitic", "over-fishing and long-term environmental changes", "the Onon River and the Burkhan Khaldun mountain", "Hassan al-Turabi", "robbery", "Arlen Specter", "it is Oprah's daughters", "Venus Williams", "prisoners", "a globose pome"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7448529411764706}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8691", "mrqa_squad-validation-3958", "mrqa_squad-validation-4648", "mrqa_squad-validation-8864", "mrqa_squad-validation-9454", "mrqa_squad-validation-7818", "mrqa_squad-validation-1272", "mrqa_squad-validation-2122", "mrqa_squad-validation-1530", "mrqa_squad-validation-9977", "mrqa_squad-validation-1818", "mrqa_squad-validation-2525", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808", "mrqa_triviaqa-validation-5325"], "SR": 0.71875, "CSR": 0.7890625, "EFR": 0.8888888888888888, "Overall": 0.8389756944444444}, {"timecode": 2, "before_eval_results": {"predictions": ["it developed into a major part of the Internet backbone", "The best-known legend", "Egyptians", "quantity surveyor", "the \"missile gap\"", "Tanaghrisson", "1852", "1564", "Concentrated O2", "adenosine triphosphate", "300 men", "11.5 inches (292.1 mm)", "1964", "the infected corpses over the city walls of Kaffa", "CD4 co-receptor", "the Lutheran and Reformed states in Germany and Scandinavia", "Chinggis Khaan International Airport", "modern buildings as well as structures dating from the 15th\u201318th centuries", "the warmest months from May through September, while the driest months are from November through April", "NBC", "Three", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Gymnosperms", "to punish Christians by God", "Word and Image department", "Nafzger", "MPEG-4", "BSkyB", "chromalveolates", "embroidery", "three hundred years", "26", "Duran Duran", "The Mallee and upper Wimmera are Victoria's warmest regions with hot winds blowing from nearby semi-deserts", "long distance services", "the chloroplasts of C4 plants", "the violence that subsequently engulfed the country", "primality", "divergent boundaries", "the Chancel Chapel", "Kurt H. Debus", "the construction of military roads to the area by Braddock and Forbes", "In bays where they occur in very high numbers", "soap opera Dallas", "Ted Heath", "late 14th-century", "high-voltage", "A contract", "Arabic numerals", "pamphlets on Islam", "draftsman", "1993\u201394", "the collider is finally ready for an attempt to circulate a beam of protons the whole way around the 17-mile tunnel", "France", "20", "the U.N. General Assembly", "the Koreans edge into second place in Asian qualifying Group 2 to finish ahead of Saudi Arabia on goal difference and seal their place in the finals", "the results by a chaplain about 1:45 p.m., per jail policy", "56", "school", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory", "AbdulMutallab was in the bathroom for about 15 to 20 minutes, \"pushed the plunger on the bomb and prepared to die,\"", "coca wine", "Dissection"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7436701574569222}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5454545454545454, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0909090909090909, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4847", "mrqa_squad-validation-3088", "mrqa_squad-validation-3480", "mrqa_squad-validation-8905", "mrqa_squad-validation-4772", "mrqa_squad-validation-3270", "mrqa_squad-validation-7211", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565", "mrqa_squad-validation-2906", "mrqa_squad-validation-2899", "mrqa_squad-validation-4322", "mrqa_squad-validation-9872", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1201"], "SR": 0.671875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 3, "before_eval_results": {"predictions": ["\"vanguard of change and Islamic reform\"", "less than a year", "dampening the fire", "187 feet (57 m)", "Zagreus", "Swynnerton Plan", "extra-legal", "Harvey Martin", "December 1895", "lion, leopard, buffalo, rhinoceros, and elephant", "Establishing \"natural borders\"", "drama series", "17 years", "sold", "income inequality", "pastor", "1534", "mesoglea", "20%", "Isaac Newton", "Miocene", "\"citizenship\"", "13", "force model", "Super Bowl City", "three", "Georgia", "Fort Caroline", "Horace Walpole", "Afrikaans", "adaptive immune system", "24", "applications such as on-line betting, financial applications", "United Kingdom", "issues related to the substance of the statement", "2007", "Luther's anti-Jewish works", "short-tempered and even harsher", "First Minister", "two catechisms", "orientalism and tropicality", "John Dobson", "Super Bowl 50", "Beryl", "prima scriptura", "the Mongol Empire", "LOVE Radio", "\u201cLady\u201d or a \u201cWoman\u201d", "Robinsons, Unicorn Bitter", "\"The Mullen \u00ef\u00bf\u00bd\"", "cutis anserina", "Bogota", "Artemis", "Malaxis paludosa", "concealpcion", "pilot", "\"Cup of tea!\"", "Mexico", "9", "Dorset", "The Daily Mirror", "Kat ( Jessica Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Paul Lynde", "adenosine diphosphate"], "metric_results": {"EM": 0.6875, "QA-F1": 0.721474358974359}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-10466", "mrqa_squad-validation-3139", "mrqa_squad-validation-2486", "mrqa_squad-validation-9540", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-5104"], "SR": 0.6875, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 4, "before_eval_results": {"predictions": ["four days", "the International Fr\u00e9d\u00e9ric Chopin Piano Competition", "Nuda", "non-tertiary", "21", "The Christmas Invasion", "vertebrates", "Miocene", "Robert Underwood Johnson", "Cuba", "the Horniman Museum", "the ability to pursue valued goals", "University of Erfurt", "curved space", "Johann Sebastian Bach", "1524\u201325", "Spain's loss to Britain of Florida (Spain had ceded this to Britain in exchange for the return of Havana, Cuba)", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "2009", "British", "2005", "Germany and Austria", "self-starting design", "two tumen (20,000 soldiers)", "international metropolitan region", "Paul Revere", "projects sponsored by the National Science Foundation (NSF) beginning in 1985", "Go-Ahead", "football", "second-largest", "biologist", "upper sixth", "arrested", "Pakistan", "1185", "Central business districts", "Hisao Yamada", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "13th century", "organisms", "co-NP", "between 1.4 and 5.8 \u00b0C above 1990 levels", "cornwall", "stand by Me", "agie", "cornwall", "cornwall", "cornwall", "leopold II", "oak leaf", "cornwall", "cornwall", "cornwall", "cornwall", "cornwall", "The Killers", "cornwall", "Russell Crowe", "cornwall", "cornwall", "Skat", "cornwall", "Christopher Nolan", "gun"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6227678571428572}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.06666666666666667, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-6981", "mrqa_squad-validation-1621", "mrqa_squad-validation-10145", "mrqa_squad-validation-7554", "mrqa_squad-validation-1360", "mrqa_squad-validation-6046", "mrqa_squad-validation-3119", "mrqa_squad-validation-4848", "mrqa_squad-validation-5374", "mrqa_squad-validation-6279", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-9913", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-2062", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-13775", "mrqa_triviaqa-validation-1816"], "SR": 0.59375, "CSR": 0.70625, "EFR": 1.0, "Overall": 0.853125}, {"timecode": 5, "before_eval_results": {"predictions": ["the wedding banquet", "respiration", "a deficit", "$216,000", "City of Malindi", "Apollo Applications Program", "trial division", "from the mid-sixties through to the present day", "An attorney", "eight", "The Book of Discipline", "Olivier Messiaen", "1759-60", "The Rankine cycle", "deadly explosives", "first half of the eighteenth century", "5,560", "Tricia Marwick", "the main contractor", "\"ctenes", "third most abundant chemical element", "adjustable spring-loaded valve", "agriculture", "metamorphosed", "David Suzuki", "if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "June 1979", "4.95 mL", "December 12", "second half of the 20th Century", "Ten", "the edge railed rack and pinion Middleton Railway", "John Elway", "1598", "The Eleventh Doctor", "Tugh Temur", "War of Currents", "dampening the fire", "eight years", "the Dalai Lama", "\"Battlefield helicopter crews routinely practice landing in fields and confined spaces away from their airfields", "corruption", "the Dalai Lama", "deaths", "2-0", "Chesley \"Sully\" Sullenberger", "Stanford", "issued his first military orders", "July", "women", "McChrystal", "the Bronx", "Shanghai", "Fernando Gonzalez", "sportswear", "Linda Klein", "killing up to 280,000 people", "the sins of the members of the church", "humans", "a simple majority vote", "George Best", "June 26, 2018", "Mahler Symphonies", "March 19, 2017"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7952845982142858}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0625, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.8, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7831", "mrqa_squad-validation-3559", "mrqa_squad-validation-3555", "mrqa_squad-validation-3179", "mrqa_squad-validation-383", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-6817"], "SR": 0.734375, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["John Mayow", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "1 July 1851", "1562", "32.9%", "the architect's client and the main contractor", "2009", "Katy\u0144 Museum", "Derek Wolfe", "silicates", "methotrexate or azathioprine", "over 100,000", "3.55 inches", "Bukhara", "Beyonc\u00e9", "its safaris", "Huntington Boulevard", "Northern Pride Festival", "eight", "Henry Laurens", "Roone Arledge", "the Florida legislature", "autoimmune", "Diarmaid MacCulloch", "permafrost", "University Athletic Association", "idolatry", "Protestant clergy to marry", "a suite of network protocols", "German", "21 to 11", "eight", "\"Yes, I committed the act of which you accuse me.", "1130", "15", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "November 25, 2002", "the center", "fovea centralis", "Milcom", "Will", "MGM Resorts International", "Bobby Darin", "Gene MacLellan", "Matt Monro", "Andhra Pradesh and Odisha", "2013", "Wisconsin", "Jonathan Breck", "Neil Young", "milling", "hot enough that light in the form of either glowing or a flame is produced", "de jure racial segregation was ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution", "hot summers and mild winters", "Sir Alex Ferguson", "Bachendri Pal", "April 2011", "1979", "Manet", "Justin Spitzer", "Dr. Paul Appelbaum", "Choir-laid", "Chronic Obstructive Pulmonary disease", "Jim Inhofe"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7083260489510489}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.5, 1.0, 0.18181818181818185, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1166", "mrqa_squad-validation-291", "mrqa_squad-validation-8400", "mrqa_squad-validation-6223", "mrqa_squad-validation-4673", "mrqa_squad-validation-2416", "mrqa_squad-validation-978", "mrqa_squad-validation-6913", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-4865", "mrqa_newsqa-validation-130", "mrqa_searchqa-validation-10794", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-9370"], "SR": 0.640625, "CSR": 0.7008928571428572, "EFR": 1.0, "Overall": 0.8504464285714286}, {"timecode": 7, "before_eval_results": {"predictions": ["The Hoppings", "Chicago", "declared an alliance with Sultan Muhammad, brought this land to life and gave assistance and support to the Muslims", "Inherited wealth", "\"There is a world of difference between his belief in salvation and a racial ideology.", "StubHub Center", "WMO Executive Council and UNEP Governing Council", "trial division", "1999", "Gian Lorenzo Bernini", "densely occupied", "dendritic cells, keratinocytes and macrophages", "Newcastle Mela", "ambiguity", "1665", "The Day of the Doctor", "punts", "1.6 kilometres", "\"winds up\" the debate", "2010", "around 100,000", "thermal expansion", "John Sutcliffe", "Owen Daniels", "incitement to terrorism", "Brookhaven", "A construction project", "Roy", "17 February 1546", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people), human inequality can be addressed", "September 30, 1960", "Brian Steele", "the status line", "it showed such a disregard for the life and safety of others", "Callable bonds", "left coronary artery", "five", "Havana Harbor", "three", "1932", "Walter Mondale", "Mitch Murray", "Marie Fredriksson ( vocals ) and Per Gessle ( vocals and guitar )", "December 1, 2009", "virtual reality simulator", "Noah Schnapp", "Antonio Banderas", "ratio of the length s of the arc by the radius r of the circle", "Kanawha River", "Julie Stichbury, as Primrose Larkin ( 1991 ) ( 6 episodes )   Abigail Rokison", "in consistency and content", "William J. Bell", "Spanish explorers", "September 2017", "Krypton", "1990", "Russia", "(butterfly)", "Murcia", "India", "Wolfgang Amadeus Mozart", "Armin Meiwes", "Mot\u00f6rhead", "Bill Clinton"], "metric_results": {"EM": 0.625, "QA-F1": 0.6719953606442577}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6294", "mrqa_squad-validation-2608", "mrqa_squad-validation-8910", "mrqa_squad-validation-5189", "mrqa_squad-validation-6402", "mrqa_squad-validation-4385", "mrqa_squad-validation-6205", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-3182", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-5845"], "SR": 0.625, "CSR": 0.69140625, "EFR": 1.0, "Overall": 0.845703125}, {"timecode": 8, "before_eval_results": {"predictions": ["Prague", "extremely high humidity", "Gary Kubiak", "Sports Night", "2nd century BCE", "taxation", "destroy the antichrist", "collective bargaining, political influence, or corruption", "the Ilkhanate", "a second Gleichschaltung", "1864", "NL and NC", "fifty", "Albert Einstein", "discarded", "state or government schools", "ash tree", "Commission v France", "purposed to remove random noise and camera shake without destroying historical legitimacy", "1689", "adaptive immune system", "case law by the Court of Justice", "Von Miller", "mad scientist", "Dendritic cells", "Allston Science Complex", "writing a five volume book in his native Greek \u03a0\u03b5\u03c1\u03af \u03cd\u03bb\u03b7\u03c2 \u03b9\u03b1\u03c4\u03c1\u03b9\u03ba\u03ae\u03c2 in the 1st century AD", "wars", "in all land - living organisms, both alive and dead", "creating complementary economic and political units for different ethnic groups", "Hirschman", "a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Sergeant Himmelstoss", "four", "bohrium", "a major fall in stock prices", "Buddhism", "March 31 to April 8, 2018", "Blind carbon copy to tertiary recipients who receive the message", "start fires, hunt, and bury their dead", "Ray Charles", "the story's themes of moral dilemma and choosing between the easy and the right decision", "a yellow background instead of a white one", "Rose Stagg", "continues the pre-existing appropriations at the same levels as the previous fiscal year", "Arnold Schoenberg", "Tiffany Adams Coyne", "Trace Adkins", "Guy Burnet as Theo, DJ Khaled's music producer", "Southampton ( 1902, then in the Southern League )", "July 23, 2016", "Auburn Tigers football team", "Katherine Kiernan Maria", "1943", "Rumplestiltskin", "Kryptonite", "molecular cloning, RNA sequencing, polymerase chain reaction ( PCR ), or genome analysis", "Santiago", "Japan", "Lance Cpl. Maria Lauterbach", "Johnny Depp", "the actinide berkelium", "Peter Rovit", "an isosceles triangle"], "metric_results": {"EM": 0.5, "QA-F1": 0.6265155849955125}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9166666666666666, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.3333333333333333, 0.0, 1.0, 0.8823529411764706, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.5, 0.5, 0.6, 1.0, 0.0851063829787234, 0.0, 0.0, 0.6896551724137931, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 1.0, 0.10256410256410255, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-976", "mrqa_squad-validation-4634", "mrqa_squad-validation-4029", "mrqa_squad-validation-3113", "mrqa_squad-validation-6678", "mrqa_squad-validation-3946", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-2351", "mrqa_newsqa-validation-2518", "mrqa_searchqa-validation-10924", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-2789"], "SR": 0.5, "CSR": 0.6701388888888888, "EFR": 0.9375, "Overall": 0.8038194444444444}, {"timecode": 9, "before_eval_results": {"predictions": ["gold", "War of Currents", "12", "Antoine Lavoisier", "the United Kingdom, Australia, Canada and the United States", "well before Braddock's departure for North America.", "Philip Webb and William Morris", "forming a 'A National Gallery of British Art'", "the Middle East", "the government of the United Kingdom was controlled by the Conservative Party, while Scotland itself elected relatively few Conservative MPs", "exceeds any given number", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "the Channel Islands", "enhanced greenhouse effect", "they were nomads", "southern Chinese manufacturers and merchants", "Owen Jones", "the Eleventh Doctor", "the BBC National Orchestra of Wales", "bilaterians", "the traditional Chinese autocratic-bureaucratic system", "August 1992", "ABC", "France's claim to the region was superior to that of the British, since Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.", "New South Wales", "November 17, 2017", "Daren Maxwell Kagasoff", "the Royal Air Force ( RAF )", "the Sunni Muslim family", "3,000 metres ( 9,800 ft )", "to form a higher alkane", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "James Watson and Francis Crick", "September 29, 2017", "11 p.m. to 3 a.m", "Kingdom of Strathclyde was originally a part of the Hen Ogledd, its people speaking a Brythonic language distinct from Scottish Gaelic and the English derived from Lothian", "200,564", "Lake Michigan", "orbit", "7000301604928199000", "inversely proportional to the wave frequency, so gamma rays have very short wavelengths that are fractions of the size of atoms, whereas wavelengths on the opposite end of the spectrum can be as long as the universe", "Spanish", "the roofs of the choir side - aisles", "the president", "different parts of the globe", "statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "on - and off - premises sales in one form or another on Sundays at some restricted time", "4,840 community hospitals, which are defined as nonfederal, short - term general, or specialty hospitals", "Louis XVIII", "spinal spinal nerve roots arise from the cord as they get closer to the head", "T'Pau", "the London Symphony Orchestra and London Philharmonic", "Julia Ormond", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "George Strait", "Jack Osbourne and Cheryl Burke", "the fallopian tube", "study insects and their relationship to humans, other organisms, and the environment", "195029 June 1994", "Dana Andrews", "North Korea's reclusive leader Kim Jong-Il", "Hakeemullah Mehsud", "Bob Dylan", "a mythical half-human and half-eagle creature"], "metric_results": {"EM": 0.5, "QA-F1": 0.6075948391573391}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7027027027027027, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8181818181818182, 1.0, 1.0, 1.0, 0.08, 0.0, 0.5, 1.0, 0.0, 0.18518518518518515, 0.6666666666666666, 0.14285714285714285, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.16666666666666669, 0.5, 0.8, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3585", "mrqa_squad-validation-9334", "mrqa_squad-validation-8589", "mrqa_squad-validation-7771", "mrqa_squad-validation-8410", "mrqa_squad-validation-10232", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-3868", "mrqa_hotpotqa-validation-940", "mrqa_hotpotqa-validation-1398", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-16618"], "SR": 0.5, "CSR": 0.653125, "EFR": 1.0, "Overall": 0.8265625}, {"timecode": 10, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-940", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8859", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-996", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-16618", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10035", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10145", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10301", "mrqa_squad-validation-10359", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-10415", "mrqa_squad-validation-10466", "mrqa_squad-validation-1082", "mrqa_squad-validation-1109", "mrqa_squad-validation-1131", "mrqa_squad-validation-1151", "mrqa_squad-validation-1166", "mrqa_squad-validation-1180", "mrqa_squad-validation-1180", "mrqa_squad-validation-1195", "mrqa_squad-validation-121", "mrqa_squad-validation-1217", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1395", "mrqa_squad-validation-1468", "mrqa_squad-validation-1488", "mrqa_squad-validation-1492", "mrqa_squad-validation-1621", "mrqa_squad-validation-1630", "mrqa_squad-validation-1645", "mrqa_squad-validation-170", "mrqa_squad-validation-1719", "mrqa_squad-validation-1732", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1962", "mrqa_squad-validation-1971", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2082", "mrqa_squad-validation-2122", "mrqa_squad-validation-2159", "mrqa_squad-validation-217", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2361", "mrqa_squad-validation-2412", "mrqa_squad-validation-2416", "mrqa_squad-validation-2418", "mrqa_squad-validation-2457", "mrqa_squad-validation-2486", "mrqa_squad-validation-2548", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2608", "mrqa_squad-validation-2633", "mrqa_squad-validation-2667", "mrqa_squad-validation-2678", "mrqa_squad-validation-2843", "mrqa_squad-validation-2861", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2899", "mrqa_squad-validation-291", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-2985", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3088", "mrqa_squad-validation-3104", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3119", "mrqa_squad-validation-3162", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3209", "mrqa_squad-validation-3215", "mrqa_squad-validation-3241", "mrqa_squad-validation-3307", "mrqa_squad-validation-3355", "mrqa_squad-validation-3362", "mrqa_squad-validation-3407", "mrqa_squad-validation-3411", "mrqa_squad-validation-3413", "mrqa_squad-validation-3451", "mrqa_squad-validation-348", "mrqa_squad-validation-349", "mrqa_squad-validation-3522", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3559", "mrqa_squad-validation-3569", "mrqa_squad-validation-3585", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3822", "mrqa_squad-validation-383", "mrqa_squad-validation-3830", "mrqa_squad-validation-3841", "mrqa_squad-validation-3855", "mrqa_squad-validation-3882", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-3946", "mrqa_squad-validation-4008", "mrqa_squad-validation-4028", "mrqa_squad-validation-4046", "mrqa_squad-validation-4121", "mrqa_squad-validation-4172", "mrqa_squad-validation-423", "mrqa_squad-validation-4296", "mrqa_squad-validation-433", "mrqa_squad-validation-4331", "mrqa_squad-validation-4376", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4418", "mrqa_squad-validation-4430", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-4463", "mrqa_squad-validation-4495", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4648", "mrqa_squad-validation-4673", "mrqa_squad-validation-4704", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-477", "mrqa_squad-validation-4772", "mrqa_squad-validation-4803", "mrqa_squad-validation-4807", "mrqa_squad-validation-4841", "mrqa_squad-validation-4848", "mrqa_squad-validation-4936", "mrqa_squad-validation-4983", "mrqa_squad-validation-5023", "mrqa_squad-validation-5063", "mrqa_squad-validation-5083", "mrqa_squad-validation-513", "mrqa_squad-validation-513", "mrqa_squad-validation-5136", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5247", "mrqa_squad-validation-5250", "mrqa_squad-validation-5254", "mrqa_squad-validation-5265", "mrqa_squad-validation-5295", "mrqa_squad-validation-5307", "mrqa_squad-validation-5318", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5418", "mrqa_squad-validation-5445", "mrqa_squad-validation-5485", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5563", "mrqa_squad-validation-5564", "mrqa_squad-validation-5566", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5608", "mrqa_squad-validation-5653", "mrqa_squad-validation-5664", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5766", "mrqa_squad-validation-5782", "mrqa_squad-validation-5788", "mrqa_squad-validation-5821", "mrqa_squad-validation-5843", "mrqa_squad-validation-5852", "mrqa_squad-validation-5951", "mrqa_squad-validation-6046", "mrqa_squad-validation-6049", "mrqa_squad-validation-6067", "mrqa_squad-validation-6073", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6294", "mrqa_squad-validation-6310", "mrqa_squad-validation-638", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-653", "mrqa_squad-validation-6531", "mrqa_squad-validation-6535", "mrqa_squad-validation-6548", "mrqa_squad-validation-6567", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-6678", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6840", "mrqa_squad-validation-6841", "mrqa_squad-validation-6868", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6917", "mrqa_squad-validation-6959", "mrqa_squad-validation-6962", "mrqa_squad-validation-6981", "mrqa_squad-validation-703", "mrqa_squad-validation-7030", "mrqa_squad-validation-7142", "mrqa_squad-validation-7175", "mrqa_squad-validation-7211", "mrqa_squad-validation-7252", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-743", "mrqa_squad-validation-7435", "mrqa_squad-validation-7456", "mrqa_squad-validation-7554", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7632", "mrqa_squad-validation-7633", "mrqa_squad-validation-7678", "mrqa_squad-validation-7699", "mrqa_squad-validation-7704", "mrqa_squad-validation-7709", "mrqa_squad-validation-7716", "mrqa_squad-validation-7747", "mrqa_squad-validation-7766", "mrqa_squad-validation-7771", "mrqa_squad-validation-7774", "mrqa_squad-validation-7775", "mrqa_squad-validation-7800", "mrqa_squad-validation-7818", "mrqa_squad-validation-7831", "mrqa_squad-validation-7846", "mrqa_squad-validation-7863", "mrqa_squad-validation-7888", "mrqa_squad-validation-7899", "mrqa_squad-validation-795", "mrqa_squad-validation-7965", "mrqa_squad-validation-797", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8071", "mrqa_squad-validation-8163", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8365", "mrqa_squad-validation-8372", "mrqa_squad-validation-8410", "mrqa_squad-validation-8414", "mrqa_squad-validation-8441", "mrqa_squad-validation-8486", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8589", "mrqa_squad-validation-859", "mrqa_squad-validation-8598", "mrqa_squad-validation-8600", "mrqa_squad-validation-8666", "mrqa_squad-validation-8670", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8907", "mrqa_squad-validation-9020", "mrqa_squad-validation-9030", "mrqa_squad-validation-9050", "mrqa_squad-validation-9116", "mrqa_squad-validation-9121", "mrqa_squad-validation-9151", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9379", "mrqa_squad-validation-9401", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9454", "mrqa_squad-validation-9465", "mrqa_squad-validation-9484", "mrqa_squad-validation-9540", "mrqa_squad-validation-9590", "mrqa_squad-validation-9608", "mrqa_squad-validation-9689", "mrqa_squad-validation-9738", "mrqa_squad-validation-976", "mrqa_squad-validation-9760", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9954", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6711"], "OKR": 0.921875, "KG": 0.43359375, "before_eval_results": {"predictions": ["2009", "Huntington Boulevard", "from the mid-sixties through to the present day", "William Hartnell and Patrick Troughton", "Distributed Adaptive Message Block Switching", "1331", "Confucianism", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "military and economic", "a military coup d'\u00e9tat", "the A1", "proteolysis", "a form of starch", "Eisleben", "break off the cathode, pass out of the tube, and physically strike him", "late night talk shows", "13\u20137", "around half", "the comprehensive institutions of the Great Yuan", "linear", "Chuck Howley", "sea level change", "Annette", "Tim McGraw and Kenny Chesney", "a man who could assume the form of a great black bear", "Casino promotions such as complimentary matchplay vouchers or 2 : 1 blackjack payouts allow the player to acquire an advantage without deviating from basic strategy", "the sperm plasma then fuses with the egg's plasma membrane, the sperm head disconnects from its flagellum and the egg travels down the Fallopian tube to reach the uterus", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "a sociological perspective", "6 January 793", "Colon Street", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Sam Waterston", "Robert Irsay", "Edgar Lungu", "UNESCO / ILO", "five", "Achal Kumar Jyoti", "the President", "Homer Banks, Carl Hampton and Raymond Jackson", "The management team", "British R&B girl group Eternal", "Boston Bruins", "Spike stops her, telling her that the only way to go forward is to just keep living her life", "six", "Yuzuru Hanyu", "a receptor or enzyme is distinct from the active site", "the geologist James Hutton", "a song by British - American rock band Fleetwood Mac from their fourteenth studio album Tango in the Night ( 1987 )", "31 October 1972", "December 2, 1942", "September 8, 2017", "Hollywood Masonic Temple", "Kevin Corrigan", "Gaget, Gauthier & Co. workshop", "Renhe Sports Management Ltd", "Tangled", "to establish radio communication with each other by touching them together or bringing them into close proximity, usually no more than a few centimetres", "Kenneth Hood", "a hard rock/blues rock band, they have also been considered a heavy metal band, although they have always dubbed their music simply \"rock and roll\".", "Scotland", "she was a young skater and desperately wanted to make her mother proud.", "the Missouri Compromise of 1850", "Peyton Place"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5510614329058623}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.782608695652174, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.07692307692307693, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.375, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0909090909090909, 1.0, 1.0, 0.2, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.21428571428571427, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7832", "mrqa_squad-validation-8160", "mrqa_squad-validation-4849", "mrqa_squad-validation-9831", "mrqa_squad-validation-5357", "mrqa_squad-validation-1388", "mrqa_squad-validation-434", "mrqa_squad-validation-625", "mrqa_squad-validation-8747", "mrqa_squad-validation-8530", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-2277", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-15757"], "SR": 0.46875, "CSR": 0.6363636363636364, "EFR": 1.0, "Overall": 0.7479758522727272}, {"timecode": 11, "before_eval_results": {"predictions": ["The availability of the Bible in vernacular languages", "detention", "Western Xia", "a majority of all MEPs (not just those present) to block or suggest changes", "carbon related", "co-chair", "Katharina", "three", "August 10, 1948", "the holy catholic (or universal) church", "30\u201360%", "Louis Paul Cailletet", "Class II MHC molecules", "crust and lithosphere", "orange", "electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons", "stroke", "13 June 1525", "polynomial-time reductions", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Choebes", "1947", "Spanish explorers", "Western Australia", "October 19, 2005", "April 10, 2018", "October 1, 2015", "number of games where the player played, in whole or in part", "cat in the hat", "nobiliary particle indicating a noble patrilineality or as a simple preposition that approximately means of or from in the case of commoners", "Tom Brady", "China in modern times", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Kida", "1960", "Bengal tiger", "Deathly Hallows", "flood barrier", "De Wayne Warren", "triacylglycerol", "1970", "autu", "Tom Brady", "Margaery Tyrell", "Isle of Sheppey in England", "Lake Powell", "Clarence L. Tinker", "Bed and breakfast", "Kent Robbins", "Garbi\u00f1e Muguruza", "Keith Richards", "Hercules", "June 12, 2018", "Bart Howard", "111", "Leonard Bernstein", "The Hague", "historic buildings, arts, and published works", "Lisa", "Arnoldo Rueda Medina", "tax incentives", "George Wallace", "Florida", "CNN"], "metric_results": {"EM": 0.5, "QA-F1": 0.5973024371461872}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.9743589743589743, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5714285714285715, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-8581", "mrqa_squad-validation-3474", "mrqa_squad-validation-4952", "mrqa_squad-validation-10483", "mrqa_squad-validation-1720", "mrqa_squad-validation-3385", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-386", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1549", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-13057"], "SR": 0.5, "CSR": 0.625, "EFR": 1.0, "Overall": 0.745703125}, {"timecode": 12, "before_eval_results": {"predictions": ["savanna or desert", "Downtown San Bernardino", "novelist Philip Roth, Canadian-born Pulitzer Prize and Nobel Prize for Literature winning writer Saul Bellow, political philosopher, literary critic and author of the New York Times bestseller \"The Good War\" author Studs Terkel", "three", "Graz, Austria", "Sky Digital", "until 1796", "Einstein", "Gary Kubiak", "soy farmers", "The Sinclair Broadcast Group", "mercuric oxide (HgO)", "Guglielmo Marconi", "Luther's education", "wages and profits", "the Germanic conquest of central, western, and southern Europe (west of and including Italy) was complete, excluding only Muslim Iberia", "the judicial branch", "biostratigraphers", "1999", "Miami Heat", "the nasal septum", "a warrior, Mage, or rogue coming from an elven, human, or Dwarven background", "Baez", "1799", "interphase", "The purse, which is fixed in United States dollars, was $2 million in 2011, with a winner's share of $315,600", "a tree species ( that generally grows in the elevation range of 3,000 to 4,200 metres ( 9,800 to 13,800 ft ) in the Himalayas )", "Federer", "James Corden", "Pasek & Paul", "March 29, 2018", "1956", "Mary Elizabeth Ellis", "Gwendoline Christie", "Orangeville, Ontario, Canada", "Guy Pemberton ( born 3 April 1955 ; died 2017 ) ( Sara Coward )", "It is small - sized, light, but quite expensive at the same time", "Afghanistan", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Joe, who knows that Stumpy holds an old grudge against Joe's wealthy and powerful brother", "James Rodr\u00edguez", "8ft", "various submucosal membrane sites", "1985, 2016, 2018", "any vessel approaching British waters", "April 17, 1982", "Acid rain", "April 26, 2005", "crossbar", "Thespis", "Ra\u00fal Eduardo Esparza", "By functions", "Thebes", "a constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "Consular Report of Birth Abroad for children born to U.S. citizens ( who are also eligible for citizenship ), including births on military bases in foreign territory", "True or false", "The Forefoot", "southwestern", "Planet Terror", "between South America and Africa", "one", "Easter", "The snowflake curve, a geometric pattern repeated at smaller scales, has fractional dimensions", "the ruble"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6561199795574796}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.07142857142857142, 0.0, 1.0, 0.0, 0.29629629629629634, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.06666666666666667, 0.5, 0.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2644", "mrqa_squad-validation-8032", "mrqa_squad-validation-9895", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2644", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-559", "mrqa_newsqa-validation-2782", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-10635"], "SR": 0.59375, "CSR": 0.6225961538461539, "EFR": 0.9615384615384616, "Overall": 0.7375300480769231}, {"timecode": 13, "before_eval_results": {"predictions": ["return home", "socialist realism", "to spearhead the regeneration of the North-East", "sleep deprivation", "July 1977", "Anderson", "quantum electrodynamics (or QED)", "provisional elder", "antigenic variation", "kinematic measurements", "worker, capitalist/business owner, landlord", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Licensed Local Pastor", "feigned retreat to break enemy formations and to lure small enemy groups away from the larger group and defended position for ambush and counterattack", "both Kenia and Kegnia", "24 March 1879", "Rob Van Winkle", "Hermine", "Edie Falco", "Leonard Bernstein", "Empire of the Sun", "James Polk", "Nicaragua", "Afghanistan", "Cambodia", "Aladdin", "Uncle Henry", "Lady and the Tramp", "a magic wand", "China", "solemn", "the University of Hanoi", "Agliff", "Alexander Ulyanov", "Beatrix Potter", "Louvre", "James Buchanan", "Volvic", "Andrea del Sarto", "Nevada (from [S Sierra] Nevada \"Snowcapped [mountain range]\") New Mexico (Calqued from...", "Christopher Columbus", "the Balfour Declaration", "a contingency fee", "a tortoise", "zucchetto", "Shinto", "The Simpsons", "Albert", "\"Beauty is truth, truth beauty,\"", "silk cord", "The Last Pharaoh", "Philadelphia", "Robert Downey Jr.", "The Fresh Prince of Bel-Air", "her abusive husband", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Tweety Bird", "Daniel Defoe", "Atlantic Ocean", "Battle of Britain and the Battle of Malta", "to the U.S. Consulate in Rio de Janeiro", "the club's board", "three", "volatile crime scene"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6681102462352463}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7407407407407407, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10088", "mrqa_squad-validation-8065", "mrqa_squad-validation-6255", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-13069", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-4663", "mrqa_searchqa-validation-4983", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-12856", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-11170", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-4717", "mrqa_newsqa-validation-4188", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-2199"], "SR": 0.59375, "CSR": 0.6205357142857143, "EFR": 0.9615384615384616, "Overall": 0.7371179601648352}, {"timecode": 14, "before_eval_results": {"predictions": ["Huguenots", "1281", "Tower Theatre", "the Mi'kmaq and the Abenaki", "basic design typical of Eastern bloc countries", "Palestine", "a pair of retractable tentacles fringed with tentilla (\"little tentacles\") that are covered with colloblasts, sticky cells that capture prey", "drawn by the convenience of the railroad and worried about flooding, moved to the new community.", "Ireland", "18 April 1521", "Melus of Bari", "Warszawa", "in the Migration period", "2012", "35", "Trump", "Today's Jeopardy", "Blue Nile", "Solomon", "Betsey Sedgwick", "Eragon", "Rawhide", "a miniature p puppy", "Judy Garland", "Paul Hornung", "silk", "Donna Summer", "Fantastic Four", "John Mahoney", "Murder by Death", "Dave Brubeck", "Dalton", "Washington", "Rita Hayworth", "People", "Franklin D. Roosevelt", "rice", "udon", "Sirhan Sirhan", "the Moon", "solstice", "FORKS", "Mountain Dew", "Omaha", "Van Halen", "Mrs. Jordan", "the Erie Canal", "The ____ Family", "Baltic Sea", "senators", "Magic Johnson", "The 10 Best Duomo (Cathedral of Santa Maria dei Fiori)", "hog", "a syllable", "N\u0289m\u0289n\u0289 )", "the 1980 BBC adaptation of Pride and Prejudice starring Elizabeth Garvie and David Rintoul, adapted from the novel by Fay Weldon.", "Imola Circuit", "sheep", "provides its services in the Japanese market.", "Berea College", "held a nomination paper for a parliamentary seat on November 25 and appeared headed for a power showdown with Musharraf before she was assassinated Thursday.", "Sharon Bialek", "The European Council", "the Caribbean"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5552320075757575}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4166666666666667, 0.8181818181818181, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2943", "mrqa_squad-validation-4621", "mrqa_squad-validation-4585", "mrqa_squad-validation-1062", "mrqa_squad-validation-9348", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-9432", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-8666", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-534", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-10408", "mrqa_triviaqa-validation-1936", "mrqa_hotpotqa-validation-5184", "mrqa_newsqa-validation-846", "mrqa_triviaqa-validation-2317"], "SR": 0.484375, "CSR": 0.6114583333333333, "EFR": 1.0, "Overall": 0.7429947916666666}, {"timecode": 15, "before_eval_results": {"predictions": ["easier and more efficient than anywhere else", "arrested", "Francis Marion", "the death of Elisabeth Sladen in early 2011", "3.6%", "nearly 42,000", "14", "the United States Census Bureau", "Asia", "19 April 1943", "actions-oriented", "rapidly evolve and adapt", "present-day Upstate New York and the Ohio Country", "the Roman Republic", "Phil Woolas", "South Africa", "Will Carling", "the Chatham House Rule", "change", "America Online Inc.", "Aramis", "bees", "The Firm", "The Streets", "violin", "bronze", "the gun", "the Titanic", "an hernia", "the gluteus maximus", "Donkey Kong", "Georgia", "Massachusetts", "impressionist landscape, figure, circus genre, ballet", "La Boh\u00e8me", "John Quincy Adams", "oliban", "Ethel Skinner", "Queen Victoria", "My Fair Lady", "Germany", "lignin", "Adolphe Adam", "cathead", "Barcelona", "the coelacanth", "mono", "cats", "Love Never Dies", "Elizabeth I", "a Harmolodic Life", "crossword clue", "the ISS", "the Marcy Brothers", "1770 BC", "costume party", "terrorist activity", "Sydney", "Chancellor Angela Merkel", "CNN", "Mike Wallace", "the Professor", "Burundi", "The Howard Stern"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5843344155844156}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false], "QA-F1": [0.7272727272727273, 0.0, 0.0, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1830", "mrqa_squad-validation-6702", "mrqa_squad-validation-3118", "mrqa_squad-validation-7872", "mrqa_squad-validation-10108", "mrqa_squad-validation-5893", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-871", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-7024", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3616", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8359", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-8487"], "SR": 0.484375, "CSR": 0.603515625, "EFR": 1.0, "Overall": 0.74140625}, {"timecode": 16, "before_eval_results": {"predictions": ["computational complexity theory", "\"apostate\" leaders of Muslim states", "tensions over slavery and the power of bishops in the denomination", "mannerist architecture", "489", "Grainger Town area", "the Great Fire of London", "Theory of the Earth", "Ward", "Newton", "bilaterians", "one of the daughters of former King of Thebes, Oedipus", "the forces of Andrew Moray and William Wallace", "lowered the river's base level ( its lowest point )", "between two and 30 eggs", "biannually", "Arthur Chung", "18", "9 February 2018", "Yuzuru Hanyu", "marley & Me", "26.617 \u00b0 N 81.617", "it is a 2017 American biographical war drama film written and directed by Jason Hall, in his directorial debut, based on the 2013 non-fiction book of the same name by David Finkel", "Duck", "McKim Marriott", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "31", "Austin, Texas", "postero - medially towards the optic chiasm", "the Twelvers", "vasoconstriction of most blood vessels", "attached to another chromosome", "Rockwell", "Allhallowtide", "Sylvester Stallone", "In Time", "Rodney Crowell", "`` 0 '' trunk code", "in eukaryotic cells", "Melissa Disney", "Darren McGavin", "UNESCO / ILO", "Thaddeus Rowe Luckinbill", "ummat al - Islamiyah", "William DeVaughn", "a Spanish surname", "Brevet Colonel Robert E. Lee, lieutenant colonel of the 2nd U.S. Cavalry Regiment", "The Annunciation", "Charles Sherrington", "preteen", "seven", "novella", "Southwest Florida International Airport ( RSW ), located southeast of the city", "21 June 2007", "nine", "California", "England", "R&M", "sorcy Jackson & The Olympians", "root out terrorists within its borders.", "Rudolf Nureyev", "43", "Kurdish militant group in Turkey", "Mohamed Alanssi"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6246132543926661}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.27272727272727276, 0.0, 0.5, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5882352941176471, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5348", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-3649", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-4917", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-814"], "SR": 0.546875, "CSR": 0.6001838235294117, "EFR": 0.9655172413793104, "Overall": 0.7338433379817444}, {"timecode": 17, "before_eval_results": {"predictions": ["in Sydney", "Doritos", "Hans Vredeman de Vries", "$20.4 billion, or $109 billion in 2010 dollars", "Lake \u00dcberlingen", "Finsteraarhorn", "seven", "a Time Lord who desires to rule the universe", "the Lippe", "the American Revolutionary War", "25", "Chuck Noland", "Madeline Reeves", "March 31, 2013", "pulmonary heart disease ( cor pulmonale )", "before the first year begins", "the much - decorated Adam Schumann ( Miles Teller )", "The president", "UNESCO / ILO", "asexually", "The Republic of Tecala", "in 2002", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Travis Tritt", "March 16, 2018", "Cee - Lo", "The Sun", "a living prokaryotic cell ( or organelle )", "1966", "Charlotte Hornets", "Julie Adams", "April 29, 2009", "J. Presper Eckert and John William Mauchly", "201", "1983", "May 17, 2018", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T )", "MacFarlane", "regulate the employment and working conditions of civil servants", "Hercules", "Angel Island ( California )", "Rufus and Chaka Khan", "moist temperate climates", "before they kill him", "7.6 mm", "a song, written solely by Gaye", "1955", "B.F. Skinner", "2020", "continues the pre-existing appropriations at the same levels as the previous fiscal year ( or with minor modifications ) for a set amount of time", "during World War II", "the Friday of Sorrows", "Orangeville, Ontario, Canada", "a semicubical parabola", "Switzerland", "a conspirator, with John Wilkes Booth, in the assassination of U.S. President Abraham Lincoln", "St Augustine's Abbey", "two Russian bombers have landed at a Venezuelan airfield", "the insurgency", "Marilyn Monroe", "a cheerleader", "NBA 2K16", "Baltimore", "September 25, 2017"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5732900895400896}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.72, 0.5714285714285715, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.4444444444444445, 1.0, 0.0, 0.2, 0.25, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5925925925925926, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.24, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2998", "mrqa_squad-validation-3953", "mrqa_squad-validation-7700", "mrqa_squad-validation-7288", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-7409", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-7892", "mrqa_triviaqa-validation-2922", "mrqa_hotpotqa-validation-538", "mrqa_newsqa-validation-3489", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-10032", "mrqa_hotpotqa-validation-4735"], "SR": 0.453125, "CSR": 0.5920138888888888, "EFR": 0.9714285714285714, "Overall": 0.7333916170634921}, {"timecode": 18, "before_eval_results": {"predictions": ["1904", "Cardinal Cajetan Luther stated that he did not consider the papacy part of the biblical Church", "Michael Heckenberger and colleagues of the University of Florida", "declined significantly", "quantum", "prices", "Miller", "Super Bowl XX", "gold", "p \u2212 1!", "2017", "to absorb menstrual flow", "Sets heart in mediastinum and limits its motion", "Danish - Norwegian patronymic surname meaning `` son of Anders ''", "12 to 36 months old", "fresh water", "her abusive husband", "HTTP / 1.1", "the south coast of eastern New Guinea,", "2009", "December 2, 1942", "John Cooper Clarke", "1792", "Amitabh Bachchan, Akshay Kumar, Bobby Deol, Divya Khosla Kumar, Sandali Sinha and Nagma", "Annette Strean", "the Bee Gees", "September 25, 1987", "Lord's", "the President of Representatives", "the little girl ( Addy Miller )", "Ritchie Cordell", "9.0 -- 9.1 ( M )", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "2010", "James Corden", "the development of electronic computers in the 1950s", "the internal reproductive anatomy ( such as the uterus in females )", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "Exodus 20 : 1 -- 17", "two senators, regardless of its population, serving staggered terms of six years", "beta decay", "Rachel Sarah Bilson", "the NIRA", "March 5, 2014", "Haikou on the Hainan Island", "19 June 2018", "stromal connective tissue", "Sunday night", "Stephen Curry of Davidson", "Susie's father, Ben Willis", "Rufus and Chaka Khan", "Jodie Foster", "Rory McIlroy", "Aristotle", "Cubs", "1919", "Lithuanian", "President Obama's race in 2008", "Lou and Wilson", "treasury bail bonds", "malignancies", "Pete Seeger", "Kiss Me, Kate", "Alchetron"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6030267239210058}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.75, 1.0, 0.2222222222222222, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.5106382978723404, 1.0, 1.0, 0.0, 0.2222222222222222, 0.9090909090909091, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.10810810810810811, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2268", "mrqa_squad-validation-10388", "mrqa_squad-validation-845", "mrqa_squad-validation-5", "mrqa_squad-validation-9213", "mrqa_naturalquestions-validation-2406", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2904", "mrqa_triviaqa-validation-1259", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3307", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-10336", "mrqa_triviaqa-validation-6808"], "SR": 0.46875, "CSR": 0.5855263157894737, "EFR": 0.9117647058823529, "Overall": 0.7201613293343654}, {"timecode": 19, "before_eval_results": {"predictions": ["the phagosomal membrane", "well before Braddock's departure for North America", "General Hospital", "destroyed", "at night", "an assertive teacher who is prepared to impose their will upon a class", "generally antagonistic", "whether or not to plead guilty", "the resultant", "Valley Falls", "1992", "Sam Raimi", "the Town of North Hempstead, Nassau County, New York", "political thriller", "Timothy McVeigh", "Germaine", "1986 to 2013", "the NYPD's 83rd Precinct", "Azeroth", "October 5, 1930", "2001", "musical research", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Michael Sheen", "The rule of three", "the Mayor of the City of New York", "841", "My Boss, My Hero", "Roy Spencer", "Bardney", "tragedy", "The Everglades", "Saint Louis County", "1891", "Eielson Air Force Base in Alaska", "Serhiy Paradzhanov", "Germany", "eight", "Carson City", "Lindsey Islands", "the 2008 presidential election", "October 12, 1962", "Eli Roth", "Paige O'Hara", "Northern Ireland", "St. Louis, Missouri", "the 78th PGA Championship", "100 million", "every aspect of public and private life", "Burnley", "Russian Empire", "between 11 or 13 and 18", "the onset and progression of Alzheimer's disease", "Thai : \u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30, phayanchana )", "7 June 2005", "Imola", "Sufjan Stevens", "producing rock music with a country influence", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "the Diamonds", "Ethel Merman", "$1.5 million", "Sen. Barack Obama", "Rwanda has invaded Congo various times to launch an offensive against Hutus allegedly linked to the genocide."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6741815476190476}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.26666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.1111111111111111]}}, "before_error_ids": ["mrqa_squad-validation-8687", "mrqa_squad-validation-6024", "mrqa_squad-validation-1944", "mrqa_squad-validation-1551", "mrqa_squad-validation-10408", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-5485", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2629", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-12758", "mrqa_newsqa-validation-3659"], "SR": 0.546875, "CSR": 0.58359375, "EFR": 1.0, "Overall": 0.737421875}, {"timecode": 20, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1939", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4867", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-346", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-4917", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8487", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9432", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10151", "mrqa_squad-validation-10192", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10381", "mrqa_squad-validation-10388", "mrqa_squad-validation-10399", "mrqa_squad-validation-10408", "mrqa_squad-validation-1062", "mrqa_squad-validation-1082", "mrqa_squad-validation-1131", "mrqa_squad-validation-1180", "mrqa_squad-validation-1248", "mrqa_squad-validation-1272", "mrqa_squad-validation-1334", "mrqa_squad-validation-1348", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-150", "mrqa_squad-validation-1530", "mrqa_squad-validation-1551", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1939", "mrqa_squad-validation-2003", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-217", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2361", "mrqa_squad-validation-2416", "mrqa_squad-validation-2481", "mrqa_squad-validation-2511", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2667", "mrqa_squad-validation-2772", "mrqa_squad-validation-2861", "mrqa_squad-validation-2881", "mrqa_squad-validation-2906", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3148", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3270", "mrqa_squad-validation-3355", "mrqa_squad-validation-3385", "mrqa_squad-validation-3411", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3830", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4159", "mrqa_squad-validation-4186", "mrqa_squad-validation-423", "mrqa_squad-validation-4331", "mrqa_squad-validation-434", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4430", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4681", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-501", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5198", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5265", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-552", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5640", "mrqa_squad-validation-5653", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5849", "mrqa_squad-validation-5893", "mrqa_squad-validation-5986", "mrqa_squad-validation-6024", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6294", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-6728", "mrqa_squad-validation-680", "mrqa_squad-validation-6841", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6917", "mrqa_squad-validation-7029", "mrqa_squad-validation-7164", "mrqa_squad-validation-7175", "mrqa_squad-validation-7302", "mrqa_squad-validation-7362", "mrqa_squad-validation-7366", "mrqa_squad-validation-7435", "mrqa_squad-validation-744", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7709", "mrqa_squad-validation-7740", "mrqa_squad-validation-7766", "mrqa_squad-validation-7775", "mrqa_squad-validation-7818", "mrqa_squad-validation-7821", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8144", "mrqa_squad-validation-8163", "mrqa_squad-validation-828", "mrqa_squad-validation-8336", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8372", "mrqa_squad-validation-848", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8600", "mrqa_squad-validation-8687", "mrqa_squad-validation-8747", "mrqa_squad-validation-8759", "mrqa_squad-validation-8807", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-9050", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9348", "mrqa_squad-validation-9401", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9830", "mrqa_squad-validation-9831", "mrqa_squad-validation-9893", "mrqa_squad-validation-9930", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4825", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7542", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-871"], "OKR": 0.841796875, "KG": 0.45078125, "before_eval_results": {"predictions": ["one way streets", "since at least the mid-14th century", "Marlee Matlin", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight until it was flying like a conventional plane", "Classic", "88", "826", "leishmaniasis", "Romulus, My Father", "Theodore Roosevelt Mason", "1996", "1964", "Kinnairdy Castle", "James Edward Franco", "(Thai: \u0e44\u0e17\u0e22\u0e41\u0e2d\u0e23\u0e4c\u0e40\u0e2d\u0e40\u0e4a\u0e35\u0e22 \u0e40\u05ad\u0e01\u0e0b\u0e4c )", "December 24, 1973", "(20 June 1794, Konstanz \u2013 8 June 1845, Freiburg im Breisgau)", "James Dean", "Rob Reiner", "Kansas", "Westfield Tea Tree Plaza", "26,000", "Alemannic", "1992", "Darci Kistler", "the EN World web site", "(25 March 1948 \u2212 27 December 2013)", "Pigman's Bar-B- Que", "Columbus, Ohio", "Northern", "Glendale, Arizona", "The Tower of London", "John Sullivan", "churros", "Franklin, Indiana", "Ub Iwerks", "mentalfloss.com", "Omega SA", "Flashback", "(1900, Copeville, Texas \u2013 1955)", "Kiernan Brennan Shipka", "1993", "Democratic Republic of the Congo", "Brett Ryan Eldredge", "The pronghorn", "Walcha", "French", "FBI", "BMW X6", "David Michael Bautista Jr.", "the Cherokee River", "15 mi", "Taylor Swift", "control purposes", "the final play of the 2017 / 18 Divisional Round game against the New Orleans Saints", "Douglas Croft", "Melbourne, Victoria, Australia", "Felipe Massa", "The Washington Post", "(144953)", "Viva Las Vegas", "(Nyctophilus)", "Democritus", "Tennessee"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5335836038961039}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.2857142857142857, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1459", "mrqa_squad-validation-6653", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4802", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3402", "mrqa_searchqa-validation-15983", "mrqa_triviaqa-validation-694"], "SR": 0.453125, "CSR": 0.5773809523809523, "EFR": 1.0, "Overall": 0.7169605654761905}, {"timecode": 21, "before_eval_results": {"predictions": ["between 1000 and 1900", "Imperialism", "student-teacher relationships", "2%", "Ted Ginn Jr.", "Harrods", "Coptic Cathedral", "Philadelphia", "Disco", "mixed martial arts", "Kentucky River", "Claudius", "1995", "public", "December 21, 1956", "Fitzroya cupressoides", "Diane Kruger and L\u00e9a Seydoux", "Nanna Popham Britton", "Sada Carolyn Thompson", "Lawrence of Arabia", "Nelson Mandela", "Dissection", "Iranian-German", "The A41 road", "Forbes", "Bronwyn Kathleen Bishop", "the Circle-Vision attraction The Timekeeper", "Jena Malone", "water", "Hong Kong First Division League", "Four Weddings and a Funeral", "from 2011 to 2014", "July 16, 1971", "influenced by the music genres of electronic rock, electropop and R&B", "Theodore Anthony Nugent", "Reverend Lovejoy", "Geet", "October 5, 1930", "Pamelyn Wanda Ferdin", "The Ninth Gate", "Comeng and Clyde Engineering", "neuro-orthopaedic", "Saint Motel", "About 200", "War Is the Answer", "Eleanor of Aquitaine", "Trilochanpala", "Rashida Jones", "The Division of Cook", "Peter Townsend", "Blue Origin", "Taylor Swift", "18 December 1975", "convergent plate boundary", "it failed to enforce its rule", "Kiri Te Kanawa", "Christchurch", "30-minute", "the area was sealed off, so they did not know casualty figures", "John-Boy Walton", "the melved brass bass", "the diphthong / a\u028a /", "once in about 24 hours", "Nigel Lythgoe, Mia Michaels, and Adam Shankman"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6534733495670996}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.22222222222222224, 0.4, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.7499999999999999, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-9904", "mrqa_squad-validation-1873", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-4466", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-99", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4172", "mrqa_naturalquestions-validation-10448", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5447", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-7852"], "SR": 0.515625, "CSR": 0.5745738636363636, "EFR": 0.967741935483871, "Overall": 0.7099475348240469}, {"timecode": 22, "before_eval_results": {"predictions": ["the actioner Zorro", "Zagreus", "January 1985", "Computational complexity theory", "to plan the physical proceedings, and to integrate those proceedings with the other parts", "post-World War I", "Esteban Ocon", "New Orleans Saints", "Clarence Nash", "The Dressmaker", "1,800", "from August 14, 1848", "Urijah Faber", "Mary-Kay Wilmers", "baeocystin", "The authorship of Titus Andronicus", "Vaisakhi List", "Ars Nova Theater", "Monticello", "H. Sawin Millett Jr. (born October 8, 1937) is a Maine politician", "\"Menace II Society\"", "16 March 1987", "the luxury Holden Calais (VF) nameplate", "Louis \"Louie\" Zamperini", "1975", "PlayStation 3", "The Andes or Andean Mountains", "the Knight Company", "Parlophone Records", "Nicolas Winding Refn", "South America", "the Kingdom of Morocco", "Gerard Marenghi", "1958", "126,202", "31 July 1975", "orange", "Roseann O'Donnell", "There Is Only the Fight", "SAVE", "29,000", "the extraterrestrial hypothesis", "Larnelle Steward Harris", "4,613", "a morir so\u00f1ando or orange Creamsicle", "Ramzan Kadyrov", "Albert", "Tetrahydrogestrinone", "5,042", "Captain B.J. Hunnicutt", "Victoria, Duchess of Kent", "The Keeping Hours", "the Gilbert building", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Elena Anaya", "Rome", "The Archers", "a raven", "a particular health ailment or beauty concern", "during a world tour in the mid-1990s", "Daniel Radcliffe", "Milla Jovovich", "a sword", "a spoiled brat"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6796033902691511}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6010", "mrqa_squad-validation-6956", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-460", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-5396", "mrqa_triviaqa-validation-1447", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-11933"], "SR": 0.578125, "CSR": 0.5747282608695652, "EFR": 1.0, "Overall": 0.716430027173913}, {"timecode": 23, "before_eval_results": {"predictions": ["the growth of mass production", "Edmonton, Canada", "Ollie Treiz", "Western Xia", "Richard Trevithick", "coughing and sneezing", "9", "his writings about the outdoors", "The Backstreet Boys", "November of that year", "Hawaii", "Tool", "1919", "May 5, 2015", "Arthur Miller", "Edmonton, Alberta", "Brent Robert Barry (born December 31, 1971)", "1989 until 1994", "The LA Galaxy", "Nicholas Donabet Kristof", "film and short novels", "Flushed Away", "first baseman and third baseman", "River Welland", "The Process", "200,167", "Bohemia", "\"The Brothers\" (2001)", "Armin Meiwes", "1933", "Dirk Werner Nowitzki", "Carl Michael Edwards II", "In a Better World", "the 45th Infantry Division", "Iron Man 3", "Julie 2", "Conservatorio Verdi in Milan", "6'5\"", "a terrible date", "June 26, 1970", "Lawrenceburg, Indiana", "14", "\"About a Boy\"", "1982", "1978", "A black cat", "Denmark", "1999", "The Ryukyuan people (\u7409\u7403\u6c11\u65cf, Ry\u016bky\u016b minzoku, Okinawan: \"Ruuchuu minzuku\")", "Peter Thiel", "Summerlin", "Cersei Lannister", "Kida", "the disk", "Mahinda Rajapaksa", "iron", "naples", "Alias Smith and Jones", "British broadcaster Channel 4 has been criticized for creating a new television show which looks at how children as young as eight would cope without their parents for two weeks.", "\"pushed the plunger on the bomb and prepared to die,\"", "2.5 million", "1,000,000 milligrams", "keta", "naples"], "metric_results": {"EM": 0.53125, "QA-F1": 0.628969885651629}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6, 0.8571428571428571, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.125, 0.0, 1.0, 0.0, 1.0, 0.05555555555555555, 0.10526315789473685, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2154", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-1564", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-2293", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-865", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-3074"], "SR": 0.53125, "CSR": 0.5729166666666667, "EFR": 1.0, "Overall": 0.7160677083333333}, {"timecode": 24, "before_eval_results": {"predictions": ["1965", "A job where there are many workers willing to work a large amount of time (high supply) competing for a job that few require (low demand)", "A tundra", "article 30", "The mermaid", "Kabul in the eastern Afghan province of Logar", "a fair and independent manner and ratify successful efforts.", "Aac, 8, Hope, 7, Noah, 5, Phoebe Joy, 3, Lydia Beth, 2, Annie, also 2,", "Kurt Cobain", "seven", "two years", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.\"", "piano", "Charlotte Gainsbourg and Willem Dafoe", "The Charlie Daniels Band", "the United States can learn much from Turkey's many complex issues and identities, but also handled tough issues with great skill.", "curfew", "Missouri", "stealing the personal credit information of thousands of unsuspecting American and European consumers", "five victims by helicopter", "30-minute", "San Diego County", "Iran", "cancer", "baseball bat", "U.S. Navy", "$199", "A new classified directive to coalition forces in Afghanistan puts restrictions on nighttime raids of Afghan homes and compounds", "sportswear", "\"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "eight", "the FAA received no reports from pilots in the air of any sightings", "1941", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "NATO fighters", "Jaime Andrade", "Somali President Sheikh Sharif Sheikh Ahmed", "\"I wasn't sure whether I was going to return to 'E! News' this week or after the new year.", "that school staff and security should patrol campuses, especially violence-prone areas, during and after school events.", "Orbiting Carbon Observatory", "German Chancellor Angela Merkel", "Donald Trump", "Ryder Russell", "Scarlett Keeling", "Maersk Alabama", "Joe Jackson", "Asashoryu", "the District of Columbia National Guard", "Interior Ministry", "Anil Kapoor", "$3 billion", "question people if there's reason to suspect they're in the United States illegally", "a young girl", "Elizabeth Dean Lail", "1926", "cover", "Wyoming", "Kenny Everett", "Bandai", "Richa Sharma", "The Frost Place Advanced Seminar", "Stringer Bell", "Valentina Tereshkova", "Baccarat"], "metric_results": {"EM": 0.5, "QA-F1": 0.586987675824992}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.16, 1.0, 1.0, 1.0, 0.30769230769230765, 0.0, 0.0, 0.5, 1.0, 1.0, 0.06060606060606061, 1.0, 0.0, 1.0, 0.058823529411764705, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.8, 0.10256410256410256, 0.125, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7407", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3010", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6214", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-14187"], "SR": 0.5, "CSR": 0.5700000000000001, "EFR": 0.96875, "Overall": 0.7092343750000001}, {"timecode": 25, "before_eval_results": {"predictions": ["phycobilisomes, and contain chlorophyll b", "WatchESPN", "independent prescribing authority", "10,000", "it infringed on democratic freedoms", "U.N. High Commissioner for Refugees", "a house party in Crandon, Wisconsin,", "chairman of the House Budget Committee,", "a basement, like Kerstin, and those who lived above ground, apparently unaware of the abuse of their mother and siblings,", "is a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "U.S.", "company Polo", "three empty vodka bottles, the inquest heard.", "Nkepile M abuse", "a Royal Air Force helicopter", "Samuel Herr, 26, and Juri Kibuishi,", "McDonald's", "the administration's progress, while we also encourage him to 'evolve faster' on supporting full marriage equality,\"", "1981", "Jewish", "al Fayed's", "Teresa Hairston", "The pilot, whose name has not yet been released,", "18", "the festivities run from mid-November until the holidays end.", "\"Sesame Street's\" Grover, how to make gnocchi with Mario Batali, and the ins and outs of prettying up your home with any number of programs on HGTV.", "Jason Chaffetz", "debris", "an encounter 14 years ago that, if true, constitutes an unreported sexual assault.", "Hansa (Malmborgsgatan 6) and Triangeln (Sodra Forstadsgatan 41)", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "21", "the heart of an urban center like Los Angeles.", "they would not be making any further comments, citing the investigation.", "that these shorter-range missiles can be \"rolled out on a dime,\" but the U.S. intelligence community sees no \"readily observable\" indication of an imminent long-range missile launch.", "the 3rd District of Utah.", "Sen. Debbie Stabenow (D- Michigan)", "\"peregruzka\"", "one", "Lee Probert.", "1998.", "23", "American", "\"We know this can be done,\"", "Monday.", "Frank Ricci,", "Old Trafford", "Guinea, Myanmar, Sudan and Venezuela.", "the Carrousel du Louvre,", "homicide", "Les Bleus", "Madison", "Gibraltar, a British Overseas Territory, located at the southern tip of the Iberian Peninsula, is the subject of an irredentist territorial claim by Spain", "need to repent in time", "John McCarthy", "The Rime of the Ancient Mariner", "dolphins", "Atlas ICBM", "Lionel Eugene Hollins", "Duchess Eleanor of Aquitaine", "John Irving", "diabetes", "Vienna"], "metric_results": {"EM": 0.328125, "QA-F1": 0.43739557011615837}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true], "QA-F1": [0.33333333333333337, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.8, 0.0, 1.0, 0.5, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.17647058823529413, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.5, 0.15384615384615385, 1.0, 1.0, 0.1, 0.45454545454545453, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-6354", "mrqa_squad-validation-8392", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-981", "mrqa_newsqa-validation-2745", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3726", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2673", "mrqa_triviaqa-validation-3799", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-5238"], "SR": 0.328125, "CSR": 0.5606971153846154, "EFR": 1.0, "Overall": 0.7136237980769231}, {"timecode": 26, "before_eval_results": {"predictions": ["Eero Saarinen", "according to a multiple access scheme", "prime", "Climate fluctuations during the last 34 million years", "Chicago Cubs", "The Salopian", "a string, or the vibrating air column in the case of a brass instrument,", "Norman Hartnell", "canoeing", "Millbank in London", "Poland", "Adam Ant", "sea levels gradually rose, the waters of continental shelves were colonized for the first time by large marine reptiles and reef-building corals of modern aspect", "New Democracy", "Missouri", "six", "mushrooms", "Turkey", "1948", "Pooh", "London Borough of Walford in the East End of London", "Civil Law (B.C.L.)", "Laurence Olivier,", "orange, yellow, red and white", "Leonard Bernstein", "is", "even numbers", "naples", "blood", "aircraft", "Passion fruit", "jumper", "'Zulu'", "caridean shrimp", "Yemen", "Welles", "George IV", "Barry Briggs", "Joseph Smith", "Devon, Dorset, Norfolk, Northamptonshire, Oxfordshire, Suffolk, Surrey, Warwickshire, West Sussex", "Ukrainian steppes", "meat", "Beaujolais Nouveau", "rowing", "$1", "(28 July 1925 - 14 April 1988)", "A consummate actor in both film", "'Lord Nelson'", "Biblical", "Argentina", "bees", "Sinclair Lewis", "Phosphorus pentoxide", "( son of Bindusara )", "Danish", "music engraver and publisher", "Cheshire County, New Hampshire", "Atomic Kitten", "Phillip A. Myers.", "11th year in a row", "Opryland.", "Micah Sanders", "an American singer-songwriter,", "Alzheimer's disease"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4979166666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-8918", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-1872", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-397", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-2740", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4118", "mrqa_naturalquestions-validation-946", "mrqa_hotpotqa-validation-5706", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-347", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-1554"], "SR": 0.453125, "CSR": 0.556712962962963, "EFR": 0.9428571428571428, "Overall": 0.7013983961640211}, {"timecode": 27, "before_eval_results": {"predictions": ["water-cooled", "Genghis Khan Mausoleum", "the most cost efficient bidder", "February 1", "Kylie Jenner's first child", "53", "st ives", "2005", "Elizabeth Dean Lail", "Anthony Caruso", "Peter Finch", "Erastus Utoni", "Guant\u00e1namo or GTMO ( / \u02c8\u0261\u026atmo\u028a / )", "775", "north", "23 %", "\u0292wa d\u0259 viv\u0281", "Roanoke", "self - titled album", "Central Germany", "King Saud University", "Sherwood Forest", "on the southeastern coast", "Orlando", "epidemiology", "Kyrie Irving", "brain region", "Paul Hogan", "represent the 50 states of the United States of America", "1996", "Emma Watson", "in capillaries", "September 6, 2019", "two", "Shenzi", "August 6", "3000 BC", "Isthmus of Corinth", "1976", "on the vaginal floor", "October 2008", "Broken Hill and Sydney", "Latin liberalia studia", "The Hunger Games : Mockingjay -- Part 1 ( 2014 )", "1773", "Escherichia coli", "8", "on many space probes and on manned lunar missions", "Carol Worthington", "when viewed from different points on Earth", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "2014", "st ives", "dorset", "pressure", "Frederick Martin \"Fred\" Mac Murray", "Franz Ferdinand", "extreme nationalist, and nativist", "(the Democratic VP candidate delivers a big speech next Wednesday)", "Jeanne Tripplehorn", "How I Met Your Mother", "Portuguese", "grow old", "caruso"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5519237825829841}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4615384615384615, 0.33333333333333337, 0.0, 0.0, 0.2666666666666667, 1.0, 0.15999999999999998, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.06896551724137931, 0.4347826086956522, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.5, 0.0, 0.5454545454545454, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-449", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-1180", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-3827", "mrqa_hotpotqa-validation-5286", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-435", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-7864"], "SR": 0.421875, "CSR": 0.5518973214285714, "EFR": 0.918918918918919, "Overall": 0.6956476230694981}, {"timecode": 28, "before_eval_results": {"predictions": ["the father of the house when in his home", "Welsh", "data link", "blue", "Mead", "Vietnam", "John Peel,", "Moscow", "insects", "Estonia", "Siberia", "malaria", "on the grounds of a hospital that treated injured war veterans,", "Kent", "Arthur, Prince of Wales", "Israel", "butterflies", "it suddenly burst into flames while landing in the full view of the crowd", "Philippines", "terrorist groups have a violence target and an influence target.", "the number thirteen", "Aquae Sulis", "Eric Coates", "Wildeve", "to make wrinkles in one's face", "Brothers In Arms", "Mexico State (Toluca)", "Aberystwyth", "Eric Morley", "fertility", "Frank Spillane", "Pablo Picasso", "Cole Porter", "Alberto Salazar", "Washington Post", "Niger", "Lone Gunmen", "David Nixon", "piano", "TRITON", "Addis Ababa", "pascal", "heart", "Nova Scotia", "\u201clone wolf\u201d attacks that left one dead before Christmas raised alarm over the threat posed by French Islamists returning from Syria and Iraq.", "Len Hutton", "Nigeria", "Dead Sea", "40", "Gibraltar", "1929", "penultima", "President Gerald Ford", "Spanish missionaries", "William Wyler", "rash", "1887", "Portal", "on various military check posts in Pakistan's border with Afghanistan Saturday night and early Sunday morning,", "a plaque", "Phillip A. Myers.", "Priscilla Presley", "Cold Mountain", "a detective in the successful action film Gone in 60"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6541666666666667}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false], "QA-F1": [0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-6573", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-836", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-3924", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-388", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-1846", "mrqa_searchqa-validation-2066"], "SR": 0.59375, "CSR": 0.5533405172413793, "EFR": 0.9615384615384616, "Overall": 0.7044601707559681}, {"timecode": 29, "before_eval_results": {"predictions": ["Albert C. Outler", "African Union chairman Jakaya Kikwete", "Drogo", "new jersey", "pemberley", "new South Wales", "chipmunk", "Amnesty International", "scud", "new jersey", "Labrador Retriever", "Rio Grande", "st petersburg", "ecclesiastical communities", "new jersey", "m\u00e9manson", "Brooklyn", "Octavian", "Bible", "new jersey", "m\u00e9 petersburg", "Gryffindor", "archer", "The French Connection", "Toonami", "Stanley", "Rapa Nui,", "copenhagen", "baku", "m\u00e9ronymic", "Oliver!", "Hinduism", "soles", "Alanis Morissette", "high sewing", "Herbert Henry Asquith,", "m\u00e9opotamia", "index fingers", "stenbrohult", "tlachtli", "tractors", "50", "function", "Sh Ontars Sister", "purple", "Fenn Street School", "new jersey", "arthur", "mike", "Marx", "geography", "succussu", "March 31, 2017", "1987", "reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "Dutch", "1981", "Humberside", "was being discriminated against on the basis of nationality.", "soeoth, pictured here with his wife, says he was injected with drugs by ICE agents against his will.", "copenhagen", "Jefferson the Virginian", "tap", "microwave"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4842447916666667}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.625, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8383", "mrqa_triviaqa-validation-7292", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-2279", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4925", "mrqa_triviaqa-validation-453", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-5667", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-2522", "mrqa_hotpotqa-validation-2997", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2618", "mrqa_searchqa-validation-10920"], "SR": 0.421875, "CSR": 0.5489583333333333, "EFR": 0.972972972972973, "Overall": 0.7058706362612612}, {"timecode": 30, "UKR": 0.7265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1132", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-394", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-478", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-998", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2364", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-913", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5986", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-9310", "mrqa_squad-validation-10141", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-1131", "mrqa_squad-validation-1272", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-1551", "mrqa_squad-validation-1639", "mrqa_squad-validation-1641", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1732", "mrqa_squad-validation-1830", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2849", "mrqa_squad-validation-2881", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3205", "mrqa_squad-validation-3270", "mrqa_squad-validation-3385", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3841", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4166", "mrqa_squad-validation-423", "mrqa_squad-validation-4385", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4655", "mrqa_squad-validation-4673", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4806", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5198", "mrqa_squad-validation-5234", "mrqa_squad-validation-5265", "mrqa_squad-validation-5331", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5590", "mrqa_squad-validation-5640", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5986", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6614", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-668", "mrqa_squad-validation-6727", "mrqa_squad-validation-6894", "mrqa_squad-validation-6956", "mrqa_squad-validation-7029", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-7435", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7740", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8144", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-839", "mrqa_squad-validation-848", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8600", "mrqa_squad-validation-8646", "mrqa_squad-validation-8656", "mrqa_squad-validation-8687", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-915", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-939", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9813", "mrqa_squad-validation-9878", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1021", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3512", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4717", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4858", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-884"], "OKR": 0.87109375, "KG": 0.45703125, "before_eval_results": {"predictions": ["cellular respiration", "centre-left Australian Labor Party", "the most cost efficient bidder", "Prussian", "850 m", "Armani, Esprit", "media executive and the current chair of Cox Enterprises", "capital of the Socialist Republic of Vietnam", "Stephen Mangan", "saloon-keeper", "KWPW (107.9 FM, \"Power 108\")", "Mari Jiwe McCabe", "Adult Swim", "Venice", "Tropical Storm Ann", "1974", "plasma", "March 17, 2015", "American", "1958", "China Airlines", "Wayne County", "political office of Keeper of the Great Seal of Scotland", "Republican President Richard Nixon", "Greek-American", "Farhan Akhtar", "1986", "Beauty and the Beast", "137th", "half of the Nobel Prize in Physics", "Nayvadius DeMun Wilburn", "1956", "Elizabeth Nethersole", "hiphop", "47,818", "Salisbury", "Lakshmibai", "Tony Aloupis", "sarod", "Anne Arundel County", "Austria Wien", "the local midnight", "Robert A. Iger", "Netherlands", "Dr. Alberto Taquini", "2 March 1972", "Terry the Tomboy", "Gracie Mansion", "Parlophone Records", "R-8 Human Rhythm Composer", "\"Obergruppenf\u00fchrer\"", "World War I", "1983", "Linda Davis", "Sunni Muslim family", "mona Lisa", "eight", "highlands", "an open window that fits neatly around him", "staff sergeant", "\"The techniques they used were all authorized, but the manner in which they applied them was overly aggressive and too persistent.\"", "trans fats", "Anna Mary Robertson", "indiazza Della Signoria Florence"], "metric_results": {"EM": 0.5, "QA-F1": 0.5797123015873016}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.4, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2885", "mrqa_hotpotqa-validation-3524", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-1370", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4313", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-2529", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-276", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4709", "mrqa_newsqa-validation-3858", "mrqa_newsqa-validation-3819", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-15972"], "SR": 0.5, "CSR": 0.5473790322580645, "EFR": 1.0, "Overall": 0.720413306451613}, {"timecode": 31, "before_eval_results": {"predictions": ["The Dutch health authorities regarded the treatment unnecessary,", "phagocytes", "Finland", "Gulf of Aden", "Natty Bumppo", "San Francisco", "Amsterdam", "cellulose", "moles", "mrs henderson presents", "Sicily", "Howard Keel", "Lilo & Stitch", "Charlie henderson presents", "Sweet Home Alabama", "alopecia or hair loss", "Percy Sledge - When A Man Loves A Woman", "mrs henderson presents", "Poem Hunter", "mrs henderson presents", "1780s", "80\u2019s pop band", "George Fox", "Croatian", "Manchester City", "mrs henderson presents", "beer", "Isaac, Patriarch of the Bible", "henderson", "fidelio", "ABBA", "Some Like It Hot", "mercury", "Marcus Antonius", "fumage", "Enrico Caruso", "the invasion of Russia was one of Hitler's", "proton", "nitric acid", "mrs henderson", "flesh", "Mille Miglia race", "tiger", "rhododendron", "Uranus", "mrs henderson presents 13.7 kilometer course through the city center", "mrs henderson", "mousetrap", "stenographer", "caliper", "arts", "Adolf Hitler", "the player to the dealer's right", "2020", "Black Mesa Research Facility", "4,613", "Swiss Super League", "Benj Pasek and Justin Paul", "Mark Sanford,", "the sight of celebrity pontificating about the plight of the environment", "innovative, exciting skyscrapers", "headed to Broadway in 'Chicago'", "The Planets", "mrs henderson"], "metric_results": {"EM": 0.34375, "QA-F1": 0.46008184523809526}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.4, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6, 0.2857142857142857, 0.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.5, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4462", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-3257", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-7201", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-6667", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-7145", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-1690", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3", "mrqa_searchqa-validation-15722", "mrqa_searchqa-validation-199", "mrqa_searchqa-validation-10166"], "SR": 0.34375, "CSR": 0.541015625, "EFR": 0.9761904761904762, "Overall": 0.7143787202380952}, {"timecode": 32, "before_eval_results": {"predictions": ["Torchwood", "Thomas Sowell", "lilo", "liverpool", "pangram", "October 31", "Wyoming", "Leicester", "snakes", "kangaroos", "Lisieux", "1929", "hypopituitarism", "February", "trumpet", "Gloucestershire", "Jupiter Mining Corporation", "Harold Bierman, Jr.", "come Find Yourself", "Yulia Tymochenko", "france", "Adriatic Sea", "Jon Bon Jovi", "fife", "Goran Ivanisevic", "Francis Drake", "Wikipedia", "Baku", "Truro", "The Terrier", "tundras tundra", "Madness", "Barings Bank", "Anne Boleyn", "Mrs. Peacock", "Ken Norton", "Yann Martel", "cabbage", "John Denver", "Fleet Street", "Claire Goose", "divisor", "south africa", "sian Williams", "france", "Edward III", "Bill Bryson", "American Tobacco Company", "\"If\u2013\u201d", "afrikaans", "Norman Mailer", "gravel", "between 1923 and 1925", "member states", "capillaries", "classical", "Laban Movement Analysis", "Thomas Arundell", "\"There were no reports of ground strikes or interference with aircraft in flight,", "on the bench", "Bright Automotive", "Will & Grace", "prehensile", "between Great Britain and France"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5639236111111111}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.08, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_triviaqa-validation-1725", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-7073", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-1770", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-10495", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-3994", "mrqa_searchqa-validation-852"], "SR": 0.53125, "CSR": 0.540719696969697, "EFR": 1.0, "Overall": 0.7190814393939394}, {"timecode": 33, "before_eval_results": {"predictions": ["the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "1580s", "call for the Dead", "France", "Ted Heath", "wuthering Heights", "Portugal", "Everton", "vice-admiral", "Sweeney Todd", "carrie", "benfica", "six", "1984", "11", "Buzz Aldrin", "eddie williams", "Archie Shuttleworth", "eddie williams", "The IT Crowd", "The Cream of Manchester", "africa", "pokemon", "eddie", "eisbach", "Ruth Rendell", "wales", "The Call", "eddie", "brouilly", "John Constable", "sheep", "willow", "carmen Miranda", "nottingham", "will make you jump jump", "1882", "venus williams", "diabetes", "seppanyaki", "sea otter", "dot-com", "Tunisia", "george Roberts", "otalgia", "scar", "Vladimir Putin", "croquet", "low-cost", "The Shard", "weight", "wigan Warriors", "Parashara ( c. 400 -- c. 500 AD )", "minimum viable", "The United States Secretary of State", "Jos\u00e9 Bispo Clementino dos Santos", "The Big East Conference", "Umberto II of Italy", "Spc. Megan Lynn Touma,", "suppress the memories and to live as normal a life as possible", "40", "ID", "The Tonight Show", "copper"], "metric_results": {"EM": 0.4375, "QA-F1": 0.48846726190476186}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true], "QA-F1": [0.47619047619047616, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.6666666666666665, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-27", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-748", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-1809", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-4146", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-11614"], "SR": 0.4375, "CSR": 0.5376838235294117, "EFR": 1.0, "Overall": 0.7184742647058824}, {"timecode": 34, "before_eval_results": {"predictions": ["Central business districts", "1973", "Brittany", "st Joseph's Day", "kairo", "September", "Japanese", "baul sartre", "james stewart", "Dr. Martin Luther King Jr.", "petticoat", "indus", "Puerto Rico", "360\u00b0", "Charles Taylor", "bearded", "Ireland", "The Savoy", "niki lauda", "karelia", "kurguelen", "japan", "Massachusetts", "boutros", "The Rolling Stones", "Uranus", "mole", "Aleister Crowley", "Greek", "Spain", "mumbai", "kitsune", "frottage", "collage", "spring", "eriksson", "Maerten Tromp", "Angus Deayton", "jessica edgeworth", "Emily Davison", "jotun", "penny", "princess Diana", "ethel Skinner", "jann Haworth", "ghee", "cleopatra kirkus", "rambling", "commitment", "anastasia Dobromyslova", "s\u00e8vres", "procol harum", "Victory gardens", "artes liberales", "Mark 3 : 13 -- 19", "AT&T", "2003", "Dachshunds", "\"totaled,\"", "Tehran plans to use its program to build nuclear weapons.\"", "\"deeply intimate portrait will provide viewers with a raw and honest look inside a musical dynasty.\"", "Nashville", "\"reshit\"", "anemia"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5074244281045752}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3529411764705882, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-805", "mrqa_triviaqa-validation-7088", "mrqa_triviaqa-validation-7015", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-3914", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-2249", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-6010", "mrqa_naturalquestions-validation-4837", "mrqa_hotpotqa-validation-2621", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-16252"], "SR": 0.453125, "CSR": 0.5352678571428571, "EFR": 0.9714285714285714, "Overall": 0.7122767857142857}, {"timecode": 35, "before_eval_results": {"predictions": ["Seven Days to the River Rhine", "Seerhein", "2,579", "to collect menstrual flow", "New Jersey Devils of the National Hockey League ( NHL )", "the Near East", "on BBC One", "jessica Newton", "Hook", "Brian Steele", "nearby objects show a larger parallax than farther objects when observed from different positions", "four", "Leonard Bernstein", "The Portuguese", "9 February 2018", "1970s", "the long - running Harry Potter film series", "the 18th century", "her abusive husband", "Canterbury Tales", "in case of passing a constitutional amendment bill and voted in favour of the bill with more than 50 % of the total members of a house", "federal republic", "July 14, 1969", "Frank Langella", "Tennesseeitans", "a container often made of papier - m\u00e2ch\u00e9, pottery, or cloth", "July 1, 2005", "Castleford", "Action Jackson", "New England Patriots", "the world's first collected descriptions of what builds nations'wealth", "Patrick Warburton", "when the cell is undergoing the metaphase of cell division", "`` One Son ''", "Mara Jade", "revenge and karma", "Kyla Coleman", "Nathan Hale", "Rachel Kelly Tucker", "far lesser degree by blood capillaries extending to the outer layers of the dermis", "during World War II", "S\u00e9rgio Mendes", "on an inward spiral where it would eventually cross the event horizon", "David Tennant", "Kelly Reno", "Brooke Wexler", "$1.84 billion", "a patronymic surname", "Leonard Nimoy", "Billy Hill", "2005", "Pangaea", "Kent", "petula Clark", "elbow", "Tufts University", "2002", "May 4, 2004", "work together to stabilize Somalia and cooperate in security and military operations.", "Communist Party of Nepal (Unified Marxist-Leninist)", "Robert Barnett", "macGyver", "Daniel Boone", "chicago"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6836917462201038}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.1904761904761905, 1.0, 0.0, 0.13793103448275862, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-10614", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2030"], "SR": 0.609375, "CSR": 0.5373263888888888, "EFR": 0.92, "Overall": 0.7024027777777777}, {"timecode": 36, "before_eval_results": {"predictions": ["$5,000,000", "The Handmaid's Tale", "Austrian", "Archbishop of Canterbury", "1912", "102,984", "Emmanuel Ofosu Yeboah", "Clarence Nash", "Bulgarian", "Macomb County in the U.S. state of Michigan", "Dusty Dvoracek", "vice-president", "1972", "Disney California Adventure", "Indiana", "Travis County, Texas", "The 1996 PGA Championship", "orange", "Regionalliga Nord", "six different constructors taking the first six positions", "ragby", "life insurance", "Ukrainian", "vacation in Cape Cod", "actress and model", "BBC Focus", "George Clooney", "Joe Scarborough", "Tottenham ( ) or Spurs", "the attack on Pearl Harbor", "Alemannic", "Vienna", "Jesper Myrfors", "Paper", "Amway", "Ogallala Aquifer", "What's Up", "News Corp and 21st Century Fox", "a bet", "12", "Rockbridge County", "sexual activity", "English", "Heathrow", "Carlos Santana", "two Nobel Peace Prizes", "Dutch", "a role-playing game or wargame campaign", "2013 Cannes Film Festival", "Aiden English", "a young getaway driver and music lover who must work for a kingpin", "Bill Belichick", "Achal Kumar Jyoti", "a set of connected behaviors, rights, obligations, beliefs, and norms as conceptualized by people in a social situation", "jazz", "17 pink \"double-word\" squares", "plac\u0113b\u014d", "a federal judge in Mississippi", "when the economy turns unfriendly,", "\"It was perfect work, ready to go for the stimulus package,\"", "wounds", "pinnipeds", "Brazil", "Harry S Truman"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5848261981074481}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.3076923076923077, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.8571428571428571, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3066", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3145", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-2776", "mrqa_newsqa-validation-2449", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-13135"], "SR": 0.484375, "CSR": 0.5358952702702703, "EFR": 1.0, "Overall": 0.7181165540540541}, {"timecode": 37, "before_eval_results": {"predictions": ["Robert Lane and Benjamin Vail", "pjaro carpintero", "20 feet", "cannibalism", "the College of William and Mary", "China", "bling-bling", "Biggie Smalls", "Marsha Hunt", "chinook", "Feodor Ivanovich", "Sarah Hughes", "Sonnet to Orpheus", "Caesar salad", "a case", "David Berkowitz", "jeopardy/2762_Qs.txt at master", "New Mexico", "a 3500 lb. sculpture of one of these spheres used by early astronomers to represent the circles of the heavens", "licorice stick", "A Moon for the Misbegotten", "Donovan's first big hit when he went psychedelic", "a place name", "Dublin", "mathematical", "George II", "Suzuki Grand Vitara", "Yogi Bear", "a Lebanese President", "Judas Iscariot", "Christopher Reeve", "Stripes", "Little Red Riding Hood", "550 nm", "Daryl Hall and John Oates", "Cherokee", "The cause of the French people", "Gettysburg National Military Park", "Cortland Niccum", "Jackie Kennedy", "U.S. President Grover Cleveland", "Orange County", "Dorian Gray", "Arkansas", "Aphrodite", "the Board", "Faust", "Aaron Burr", "violin", "ethanol", "Adam Smith", "famous figures as Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "the church at Philippi, one of the earliest churches to be founded in Europe", "Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus", "red", "Cole Porter", "a non-speaking character", "New York City", "Westminster system", "Phil Collins", "nuclear", "Egypt", "Jackson sitting in Renaissance-era clothes and holding a book", "from boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5177678571428572}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.4, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 0.08000000000000002, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-12628", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12401", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-2181", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-7563", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-8404", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-10312", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-14370", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-2068", "mrqa_searchqa-validation-6339", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-7414", "mrqa_hotpotqa-validation-4081"], "SR": 0.421875, "CSR": 0.5328947368421053, "EFR": 0.972972972972973, "Overall": 0.7121110419630157}, {"timecode": 38, "before_eval_results": {"predictions": ["1954", "Zion", "cloves", "Carla Gugino", "the United States", "St Martin-in-the-Fields", "a physician or surgeon", "Greenpeace", "coelacanth", "the Ice Cream Day", "a canton", "a Palestinian city", "Caracas", "Giza", "Breakfast at Tiffany's", "a horse", "Faneuil Hall", "Babe", "shrimp", "balsa", "Awar Sadat", "Sonagchi", "a sea lion", "The Saints", "Diana", "Tasmania", "Taj Mittal", "cobalt", "Louisa May Alcott", "the wild outskirts of...", "Spider-Man 3", "grease", "glucose", "a fracas", "Nintendo", "Van Helsing", "Hugh Grant", "the Great Wall", "hand", "Hormel Foods Corporation", "Cressida", "Clara Barton", "Kauai", "the esophagus", "Joseph", "Otsego County", "Fred", "Colombia", "Thomas Paine", "Venezuela", "canticle", "Donna", "The claims process starts at noon Eastern Time and ends 24 hours later", "a fully centralized service with individual user accounts focused on one - on - one conversations", "Apollon", "Chicago", "Andrew Lloyd Webber", "1776", "Des McAleenan", "Princess Jessica", "Hutus and Tutsis", "her father", "Monday and Tuesday", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5830233134920635}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5833333333333334]}}, "before_error_ids": ["mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-1238", "mrqa_searchqa-validation-9939", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-2019", "mrqa_searchqa-validation-3861", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-6570", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-1716", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1534", "mrqa_searchqa-validation-14173", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-6886", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-4264", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-4021"], "SR": 0.53125, "CSR": 0.5328525641025641, "EFR": 1.0, "Overall": 0.7175080128205128}, {"timecode": 39, "before_eval_results": {"predictions": ["movements of nature", "retirement", "Hill Street Blues", "tuberculosis", "Ross Perot", "murdered", "fuchsia", "fracture", "Lance Armstrong", "Kung Fu", "tennis elbow", "sienna", "Hindu", "the Village Voice", "Nacho Libre", "\"Strawberry Fields Forever\"", "Cygnus", "( Christopher) Columbus", "nougat", "a Scotch egg", "the Manhattan Project", "the Eiffel Tower", "Roger Federer", "a sculpere", "a yolk", "a cheddar", "the Anglo-Iranian (formerly Anglo-Persian) Oil Company", "Florida", "The Virgin Spring", "(Ed) Dostoyevsky", "the Old West", "Queen Victoria", "Atlanta", "a dice", "Zorro", "assume", "Jack Sprat", "offbeat", "uranium", "ismene", "(o) Santa Barbara", "Gannett", "( William) Shakespeare", "George Sand", "Columbia Glacier", "Dick Gephardt", "a valedictorian", "Lord Louis Mountbatten", "Fine Arts", "\"Old Hickory\"", "Robert Peary", "down to the ground", "Hellenism", "James W. Marshall", "cressida", "linseed", "(John) McClane", "Wojtek", "Montreal, Quebec, Canada", "Broad Sands Bay", "\"I think the Camry gets a bad rap for being the'microwave oven'", "58 minutes", "\"What she's doing is putting a personal and human face on the issue", "$31,000"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5359375}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-2577", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-12788", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-262", "mrqa_searchqa-validation-3491", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-4826", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-1888", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-31", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-1152"], "SR": 0.46875, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7171875}, {"timecode": 40, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2920", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3586", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-514", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-13508", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-2066", "mrqa_searchqa-validation-222", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4829", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9908", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10088", "mrqa_squad-validation-10388", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1248", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2608", "mrqa_squad-validation-2831", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3331", "mrqa_squad-validation-349", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4046", "mrqa_squad-validation-4155", "mrqa_squad-validation-4331", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4634", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5248", "mrqa_squad-validation-5307", "mrqa_squad-validation-5389", "mrqa_squad-validation-5469", "mrqa_squad-validation-5506", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5728", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6024", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6224", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6702", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7211", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-801", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-9140", "mrqa_squad-validation-915", "mrqa_squad-validation-9285", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9525", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.82421875, "KG": 0.49765625, "before_eval_results": {"predictions": ["toward the center of the curving path", "The Liberty Bell", "Hinduism", "a turtle", "Donkey", "Jane Eyre", "Aiden", "William Shakespeare", "France", "Montmartre", "The Dying Swan", "Elvis Presley", "a protractor", "voter registration", "The Kite Runner", "white granite", "Islamabad", "horseshoe", "Stephen Crane", "trespass", "Jack Dempsey", "beheading", "Val Kilmer", "Pakistan", "Milwaukee", "a kiwi", "Pop-Tarts", "sugar", "Enrico Fermi", "Tiger Woods", "Madding Crowd", "helping federal, state, and local officials provide the general public with advanced notification of", "Grace Kelly", "a monkey", "Bilbo", "Oliver Wendell Holmes", "the Constitution", "Proverbs", "a photon", "Maria Montessori", "orchid", "asparagus", "the Sun", "Michelangelo", "Spain", "ale", "Superman", "a clause in the sentence", "Brazil", "Puget Sound", "phylum", "Yahya Khan", "Qianlong", "1038", "Bubba", "Easter Parade", "Thom Yorke", "beer", "Keeper of the Privy Seal of Scotland", "Martin O'Malley", "1983", "The cause of the child's death will be listed as homicide by undetermined means", "five", "Edgehill"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7439918154761904}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-7659", "mrqa_searchqa-validation-12159", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-9067", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-7222", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-4248", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-9569", "mrqa_hotpotqa-validation-3112", "mrqa_newsqa-validation-2627"], "SR": 0.6875, "CSR": 0.5350609756097561, "EFR": 1.0, "Overall": 0.7155278201219513}, {"timecode": 41, "before_eval_results": {"predictions": ["metals", "2004", "the FBI decide to make Gracie the new `` face '' of the FBI", "Karen Gillan", "Moira Kelly", "Nick Kroll", "Albert Einstein", "Miami Heat", "in Poems : Series 1", "transceivers", "the east African coast", "a large quantity", "James Madison", "Tom Brady", "asphyxia", "1975", "Thomas Edison", "DNA replication begins at specific locations", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "on Flag Day in 1954", "Erica Rivera", "Afghanistan", "Gettysburg College", "the Geography of Oklahoma", "thia Weil", "an epithelial surface by way of a duct", "an oxidant, usually atmospheric oxygen", "1 mile ( 1.6 km )", "1871", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "1970", "solids", "Presley Smith", "a combination of genetics and the male hormone dihydrotestosterone", "Norma's estranged son", "the types of instruments that are used in data collection", "Tami Lynn", "Christianity", "radians", "1931", "the fifth UK album release by the band", "July 31, 2010", "24", "August 8, 1945", "Phillip Ratcliffe", "31 March 1909", "increase consistency in United States federal sentencing", "the meridian", "Buffalo Springfield", "the times sign or the dimension sign", "word - level or section titles ( A-heads ) within the chapters", "the first Plantagenets", "lighting", "Mar del Sur", "Julie Taymor", "an organ", "Tony Aloupis", "American Civil Liberties Union", "Chevron", "Enchautegui's death", "Pancho Gonzales", "William Henry Harrison", "the AOU Checklist of North", "Kwame Nkrumah"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6226976778998213}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.16666666666666669, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1818181818181818, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.7368421052631579, 0.6666666666666666, 1.0, 0.0, 0.06451612903225806, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9237", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-7595", "mrqa_newsqa-validation-3427", "mrqa_searchqa-validation-2827", "mrqa_hotpotqa-validation-1806"], "SR": 0.53125, "CSR": 0.5349702380952381, "EFR": 0.9333333333333333, "Overall": 0.7021763392857143}, {"timecode": 42, "before_eval_results": {"predictions": ["high voltage", "a mozzarella", "Texas", "Song of Solomon", "Zohan", "Denmark", "Battlestar Galactica", "Sainte-Marie", "sheep", "Mary I", "\"Macbeth\" and Donizetti's \"Lucia di Lammermoor.\"", "a blackbird", "Patty Duke", "Hoosiers", "Judas", "3,000th", "grow old along with me", "savanna", "\"Lucky\" Luciano\"", "a freight ton", "Kellogg's", "Fall Guy", "a wolf", "Elizabeth Barret Browning", "Me and My Shadows", "crocodiles", "greece", "Slovenia", "a baboon", "Morris West", "Outta nowhere", "El burlador de Sevilla", "Hawaii", "Empire", "the League of Nations", "Sally Ride", "bulletproof", "Tilbury speech", "75% on percentile scale", "a fragrance for women 2006", "the Civil War", "a moon", "Meatballs", "Holden Caulfield", "the Caucasus mountains, southwestern", "Firebird", "14", "Lecompton", "Midnight Cowboy", "Rosetta Stone", "Louisiana", "Malayalam", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "a scuffle with the Beast Folk", "Arabian Gulf", "Ross Kemp", "Marine One", "Juan Manuel Mata Garc\u00eda (]", "Vietnam War", "Taylor Swift", "Bialek", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "on-loan David Beckham", "Alexandros Grigoropoulos, whose death"], "metric_results": {"EM": 0.5, "QA-F1": 0.5958810286935287}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15384615384615385, 0.8, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-9124", "mrqa_searchqa-validation-15998", "mrqa_searchqa-validation-8140", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-8429", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-6562", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-11902", "mrqa_searchqa-validation-1970", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-448", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-1406", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-1411", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-115"], "SR": 0.5, "CSR": 0.534156976744186, "EFR": 1.0, "Overall": 0.7153470203488372}, {"timecode": 43, "before_eval_results": {"predictions": ["Southeastern U.S.", "gin", "Leicester", "the mile run", "the Jets", "scurvy", "Japan", "Razor", "falconry", "Niger", "Norman Brookes", "Billy Crystal", "Jaipur", "Goran Ivanisevic", "36", "Benard", "Morocco", "Henry Hudson", "bridge", "Noah", "Much Ado About Nothing", "Felix", "Louis XV", "Elizabeth Mainwaring", "Australia", "Robert A. Heinlein", "Old Ironsides", "Aug. 24, 1572", "fertilization", "painting", "Massachusetts", "Sherlock Holmes", "Delaware", "Olivia Smith", "Costa Concordia", "elliptical", "orange juice", "the Doors", "Mona Lisa", "graphite", "Bash Street", "Mercury", "Ireland", "Gandalf", "Andile Smith", "Michael Curtiz", "Minder", "a star", "a turkey", "Eva Marie", "Eeyore", "Saint Alphonsa", "Ku - Klip", "March 15, 1945", "Portsea", "1861", "McLaren", "\" Maria\"", "Diego Milito", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Zyrtec", "children", "diameter", "Leo Frank"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5807291666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-5367", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-2978", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7617", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-5446", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2751", "mrqa_searchqa-validation-14791"], "SR": 0.515625, "CSR": 0.5337357954545454, "EFR": 1.0, "Overall": 0.715262784090909}, {"timecode": 44, "before_eval_results": {"predictions": ["the Old Rhine Bridge at Constance (0 km) to Hoek van Holland (1036.20 km)", "Tim McGraw", "bought the land from the local Lenape to be on good terms with the Native Americans and ensure peace for his colony", "Marcus Atilius Regulus", "when the forward reaction proceeds at the same rate as the reverse reaction", "Lucknow", "Kristy Swanson", "Hendersonville, North Carolina", "a logarithmic scale used to specify the acidity or basicity of an aqueous solution", "Abraham", "Kelly Osbourne", "1773", "the final episode of the series", "a god of the Ammonites", "the Sui", "just after the Super Bowl", "1980s", "31 October 1972", "The Italian Agostino Bassi", "graduation with a Bachelor of Medicine, Bachelor of surgery degree", "Germany", "Real Madrid", "LED illuminated display", "BC Jean", "about 24 hours", "an evaluation by an individual", "2015", "UVA", "Tommy Shaw", "2018", "Thomas Alva Edison", "B.R. Ambedkar", "the Indian Hockey Federation ( IHF )", "Niles", "Judy Collins", "Oklahoma", "Grand Inquisition", "T'Pau", "Angel Island Immigration Station", "Johannes Gutenberg of Mainz", "Terry Reid", "the name of a work gang", "Johannes Gutenberg", "Domhnall Gleeson", "Russia", "the Pandavas", "2020 National Football League ( NFL ) season", "1994 season", "is one of the parables of Jesus which appears in Luke Luke 18 : 1 - 8", "the church at Philippi", "Chuck Noland", "The Green Mile", "Cal Ripken, Jr.", "1905", "Workers' Party", "Revolver", "140 million", "contraband", "space shuttle Discovery", "750", "lew springsteen", "nantucket", "Microsoft", "love Never Dies"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6745973609254858}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true], "QA-F1": [0.375, 0.0, 0.07692307692307693, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 1.0, 0.2857142857142857, 0.5714285714285715, 1.0, 0.05405405405405406, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9113", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-7676", "mrqa_newsqa-validation-695", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-3760"], "SR": 0.546875, "CSR": 0.5340277777777778, "EFR": 0.8620689655172413, "Overall": 0.6877349736590038}, {"timecode": 45, "before_eval_results": {"predictions": ["generally westward", "the United States", "Felipe Massa", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Tibetans", "six bodies", "President Bush", "Brad Blauser, center", "Sunday", "President Obama", "at a depth of about 1,300 meters in the Mediterranean Sea", "one", "raping and murdering", "Ryder Russell", "4", "father, Osama bin Laden", "the club's board", "insect stings", "U.N. resolutions", "peter Docter", "Fullerton, California", "Chinese flags and stripped down to their underwear", "$2.6 million", "Rev. Alberto Cutie", "ceo Herbert Hainer", "Chris Robinson", "269,000", "surrounding areas of the bustling capital faced further inundation at the next high tide.", "Dharamsala, India", "the state's attorney", "delivered three machine guns and two silencers", "\"deep sorrow\"", "two", "Spaniard", "Steve Williams", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "Oxbow, a town of about 238 people", "motor scooter", "Joan Rivers", "6,000", "gossip girl", "the FDA's Office of Antimicrobial Products", "Diversity", "650", "Yemen", "South Africa", "Daniel Radcliffe", "President Sheikh Sharif Sheikh Ahmed", "in cities throughout Canada", "Florida's Everglades,", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "Tyrion Jaime", "June 8, 2009", "'Q'", "wolf", "polio", "mathematics", "the outdoors", "Mel Blanc", "giving mouth", "Colorado", "the CIA", "Billy Bob Thornton"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5497127259994907}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.4, 1.0, 0.5454545454545454, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.4, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9261", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-3308", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-5370", "mrqa_triviaqa-validation-3086", "mrqa_hotpotqa-validation-4796", "mrqa_searchqa-validation-6261"], "SR": 0.46875, "CSR": 0.5326086956521738, "EFR": 1.0, "Overall": 0.7150373641304347}, {"timecode": 46, "before_eval_results": {"predictions": ["every four years", "part husky or other Nordic breed, and possibly part terrier", "oversee the local church", "enterocytes of the duodenal lining", "warmth", "a limited period of time", "prophase I of meiosis", "federal government", "alveolar process", "Katharine Hepburn", "Thorleif Haug", "multiple", "England, Northern Ireland, Scotland and Wales", "Norman Whitfield and Barrett Strong", "to solve its problem of lack of food self - sufficiency", "January 2004", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "276", "the French CYCLADES project directed by Louis Pouzin", "when each of the variables is a perfect monotone function of the other", "Kansas City Chiefs", "Middlesex County", "especially in Western cultures", "Husrev Pasha", "in 2001", "Squamish, British Columbia, Canada", "Mankombu Sambasivan Swaminathan", "players must choose, in advance, whether they wish to collect a jackpot prize in cash or annuity", "1962", "prophets and beloved religious leaders", "Best Picture, Best Director for Fincher, Best Actor for Pitt and Best Supporting Actress", "interstate communications by radio, television, wire, satellite, and cable", "Patris et Filii et Spiritus Sancti", "William Chatterton Dix", "1987", "Stefanie Scott", "Gamora", "in the fovea centralis", "Ephesus", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Ferraro", "seven", "Uralic languages", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired, assumes", "The word first came into common usage in the late 18th century", "British and French Canadian fur traders", "Lori McKenna", "2017", "October 27, 1904", "in the mid - to late 1920s", "Major Robert Smith", "Runic", "Backgammon", "Malcolm Bradbury", "Atlanta Braves", "All-Star Game and All-NBA Team", "Holberg's comedy \"Den V\u00e6gelsindede\"", "Pakistan", "Malcolm X", "February 12", "Charles Dickens", "the Capulets & the Montagues", "plutonium-238", "24"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5696277624144166}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.7272727272727273, 0.6666666666666666, 0.30769230769230765, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.1904761904761905, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8148148148148148, 0.0, 0.7499999999999999, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.6122448979591837, 0.125, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-1692", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-5895"], "SR": 0.4375, "CSR": 0.5305851063829787, "EFR": 0.9444444444444444, "Overall": 0.7035215351654845}, {"timecode": 47, "before_eval_results": {"predictions": ["Max Martin", "1995 Mitsubishi Eclipse - Shot at by Johnny Tran", "2005", "Lewis Carroll", "the government - owned corporation of Puerto Rico responsible for electricity generation, power distribution, and power transmission on the island", "Austria - Hungary", "Mace Coronel", "libretto", "Theodore Roosevelt", "Harry", "1800", "dress shop", "As of January 17, 2018, 201 episodes", "Miller", "Instagram's own account", "Experimental neuropsychology", "2015", "Selena Gomez", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W \ufeff / \ufefd 14.69278 \u00b0 N 17.44667 \u00b0 W", "Dido", "during initial entry training", "Eddie Murphy", "1997", "Bonnie Aarons", "1960", "1979", "Part XI", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "The Risin'Sun '' by Texas Alexander ( 1928 )", "halogenated paraffin hydrocarbons that contain only carbon, chlorine, and fluorine, produced as volatile derivative of methane, ethane, and propane", "the 1980s", "MacFarlane", "18 by Frankie Laine's `` I Believe '' in 1953", "1898", "a set of components that included charting, advanced UI, and data services ( Flex Data Services )", "Majandra Delfino", "the members of the actual club with the parading permit as well as the brass band", "The Confederate States Army ( C.S.A. )", "three", "MFSK", "Zachary John Quinto", "Sanchez Navarro", "Have I Told You Lately", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "John Travolta", "Scheria", "1 October 2006", "Auburn Tigers", "Eda Reiss Merin", "Michael Clarke Duncan", "states that the `` Senate may propose or concur with Amendments as on other Bills", "geheime Staatspolizei", "Belgium", "heartburn", "Maryland", "Vanessa Hudgens", "two", "Johannesburg", "a sort of robot living inside.", "hardship for terminally ill patients and their caregivers", "The Odyssey", "Jeff Goldblum", "Prego", "marillion"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6306764549724534}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.5454545454545454, 0.0, 0.08695652173913042, 0.2727272727272727, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.6666666666666666, 0.0, 0.0, 0.9500000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6451612903225806, 0.08333333333333333, 0.0, 1.0, 1.0, 0.4615384615384615, 0.5, 0.07407407407407408, 0.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10381", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6490", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-89", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-2606", "mrqa_newsqa-validation-886"], "SR": 0.515625, "CSR": 0.5302734375, "EFR": 0.9032258064516129, "Overall": 0.6952154737903226}, {"timecode": 48, "before_eval_results": {"predictions": ["Daylight Saving Time", "Cygnus", "Constantine", "Lautrec", "Nigeria's", "Hawaii", "Jeremiah", "Paddock", "Don Knotts", "cinnamon Life", "Kbenhavn", "Porgy and Bess", "Dutchman", "Deimos", "Cheers", "Robert Frost", "Thelma Dickinson", "Underground Chapel", "cyclorama", "Laila Ali", "Fuchsia excorticata", "the First Folio", "San DiegoTijuana", "Fat man", "snuggle", "Led Zeppelin", "the Book of Kells", "grandmother", "nuclear fission", "a Heisman", "Gulf of Tonkin", "Stephen Vincent Bent", "coelacanth", "Prague", "the Federal Reserve", "alliterative combustibles", "Afghanistan", "Cheetah", "Ambrose Bierce", "the American Lung Association", "croquet", "Aphrodite", "New age", "Chico Rodriquez", "Budapest", "John Mahoney", "Python reticulatus", "Nit-A-Nee", "Georges Pompidou,", "Beverly Cleary", "Afghanistan", "2004", "a recognized group of people who jointly oversee the activities of an organization", "since 3, 1, and 4 are the first three significant digits of \u03c0", "10", "The Young Men's Christian Association", "girl named   Alice falling through a rabbit hole into a fantasy world populated by peculiar, anthropomorphic creatures.", "the theory of direct scattering and inverse scattering", "43rd", "South America", "more than 9,500 energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "$150 billion", "fill a million sandbags and place 700,000 around our city.", "Angel Cabrera"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5992647058823528}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-9339", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-4404", "mrqa_searchqa-validation-12001", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-5114", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-8359", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-12165", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-3066", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4738", "mrqa_hotpotqa-validation-1316", "mrqa_newsqa-validation-1791"], "SR": 0.515625, "CSR": 0.5299744897959184, "EFR": 1.0, "Overall": 0.7145105229591837}, {"timecode": 49, "before_eval_results": {"predictions": ["William Wyler", "Redford's adopted home state of Utah", "pelvic floor", "Kanawha Rivers", "amino acids glycine and arginine", "Brian Steele", "1937", "1920s", "during the united monarchy of Israel and Judah", "2005", "New York City", "Beorn", "Jonathan Cheban", "1992", "Montreal", "18", "Kristy Swanson", "230 million kilometres", "Camping World Stadium in Orlando", "the Bactrian", "sorrow regarding the environment", "the New York Yankees", "Woodrow Wilson", "Phosphorus pentoxide", "Brooklyn", "Secretary of Commerce Herbert Hoover", "the Gemara", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "Tom Brady", "Around 1200", "Walter Egan", "The Crossing", "a legendary figure of both pre-Islamic and Islamic mythology", "the Federated States of Micronesia", "origins of replication, in the genome", "people raced modified cars on dry lake beds northeast of Los Angeles under the rules of the Southern California Timing Association ( SCTA ), among other groups", "13 to 22 June 2012", "Jackie Robinson", "Coton in the Elms", "the American Civil War", "Pradyumna", "1979", "James Madison", "UMBC", "Roger Federer", "17 - year - old", "Cathy Dennis and Rob Davis", "5,534", "the semilunar pulmonary valve", "electron donors", "a mirror in automobiles and other vehicles, designed to allow the driver to see rearward through the vehicle's rear window ( rear windshield )", "Geroge W. Bush", "the USS Constitution", "Lucas McCain", "Christian Maelen", "2014", "1984", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "the Listeria monocytogenes bacteria", "United States", "Sagamore Hill", "hyperthyroidism", "a mammal", "UVB rays"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6858612547820397}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 0.6976744186046512, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.18181818181818182, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-7213", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-8591", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-558", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-14922", "mrqa_searchqa-validation-10011", "mrqa_triviaqa-validation-7608"], "SR": 0.609375, "CSR": 0.5315624999999999, "EFR": 1.0, "Overall": 0.7148281249999999}, {"timecode": 50, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-561", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9724", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3331", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.779296875, "KG": 0.46328125, "before_eval_results": {"predictions": ["alla capella", "caterpillar", "Zeus", "8", "Lady Gaga", "John Alec Entwistle", "Esmeralda's Barn night  club", "a fearful man, all in coarse gray with a great iron on his leg", "Paris", "December 18, 1958", "Cumberland sausage", "Gloucestershire", "Queen Victoria and Prince Albert", "car ferry", "Amanda Barrie", "Sharjah", "Madagascar", "Persian Gulf", "(Miss) Havisham", "oxygen", "Macbeth", "Gentlemen Prefer Blondes", "the Russian leader", "Netherlands", "hose", "Erie Canal", "John Key", "Subway's", "geodetics", "Dylan Thomas", "Jeff Bridges", "the Tower of London", "game of bridge", "quarter", "cirrus uncinus", "Manchester, England", "the first world war", "cheese", "Klaus dolls", "Argentina", "Colin Montgomerie", "James Valentine", "the Count Basie Orchestra", "the Be Leviathan", "Sir John Houblon", "Top Gun", "elbow", "General Paulus", "isobar", "Neapolitan", "Danish", "July 14, 1969", "1923", "Massillon, Ohio", "1937", "LA Galaxy", "2017", "The BBC", "Frank Ricci,", "\"Three Little Beers,\"", "sweatshirt", "Susan B. Anthony dollar", "elephant", "in the lawless southern provinces and especially in the Taliban stronghold of Helmand"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5316220238095238}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6534", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-4700", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-7490", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-5086", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1896", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-4693", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-6275", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-3522", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2793", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-12831"], "SR": 0.453125, "CSR": 0.5300245098039216, "EFR": 0.9714285714285714, "Overall": 0.6882593662464986}, {"timecode": 51, "before_eval_results": {"predictions": ["Samson", "smith", "Passenger Pigeon", "catalyst", "Sir Edwin Landseer", "Hitler", "Albert Einstein", "smith", "scales", "Judy Garland", "dave wALLiams", "tepuis", "into one of the Vikings nine realms", "lyle", "horse", "hypopituitarism", "lew", "bees", "Utrecht", "limestone", "lewla-rh", "lyle", "france", "mantle", "tribal", "algae", "shines", "Algeria", "Churchill Downs", "smith", "Leonard Bernstein", "Vladimir Putin", "gansbaai", "nahuatl", "20", "Ken Platt", "krakowski", "adios", "denmark", "smith", "Muppet Christmas Carol", "wiziwig", "jump", "apples", "Old Sparky", "lyle", "22", "Vancouver Island", "Dublin", "Babylonian Empire", "Continental Marines", "Joe Spano", "Gary Grimes", "in the liver and kidneys", "CBS News", "Herman's Hermits", "the United States Congress", "theocracy", "clothes", "south africa", "Brunei", "a can", "peanut butter cup", "1983"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5243303571428573}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-1635", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-7762", "mrqa_naturalquestions-validation-692", "mrqa_hotpotqa-validation-4406", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-4091", "mrqa_searchqa-validation-3853"], "SR": 0.453125, "CSR": 0.5285456730769231, "EFR": 1.0, "Overall": 0.6936778846153847}, {"timecode": 52, "before_eval_results": {"predictions": ["horse", "australia", "sweden", "wallbanger", "jennifer", "snark", "Prussian 2nd Army", "silversmith", "Joshua Tree National Park", "carpathia", "Superman", "Letchworth", "velvet", "4", "nightmare", "Crackerjack", "white Ferns", "Utah", "carburetors", "Rosalynde", "Labrador Retriever", "what", "delaware", "criminals", "jennifer wieck", "squeeze", "Apocalypse Now", "Boojum", "st Moritz", "dave hockney", "Scafell Pike", "Edgar Allan Poe", "stanley", "Tony Blackburn", "Donna Summer", "united Kingdom", "kiki", "wolf", "Titanic", "stanley purdy", "piano", "smithman", "Malawi", "australia", "Roy Rogers", "mrs baryshnikov", "bristol", "Ruth Rendell", "Smiths", "ontario", "Ohio", "Miami Heat", "Joanna Page", "1961 during the Cold War", "1902", "FCI Danbury", "25 June 1971", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "sweden", "a \"happy ending\" to the case.", "universal and equal suffrage", "christopher lauer", "john smith", "Colonel"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-4558", "mrqa_triviaqa-validation-455", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2767", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-4518", "mrqa_triviaqa-validation-6609", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3898", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-1934", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-11787"], "SR": 0.53125, "CSR": 0.5285966981132075, "EFR": 1.0, "Overall": 0.6936880896226415}, {"timecode": 53, "before_eval_results": {"predictions": ["pigment", "Jack Ruby", "Google", "hugh Dowding", "squash", "tony", "Jim Smith,", "injecting a 7 percent solution intravenously three times a day", "david Bowie", "hemp", "canoeist", "Louren\u00e7o Marques", "Christian Louboutin", "Ironside", "the need to toss logs across narrow chasms to cross them", "James Dean", "Mars", "once", "about Eve", "chicken", "George Orwell", "noel ed Hogan", "homeless", "Derbyshire", "andes d\u2019Avignon", "polynesian", "france", "lothbrok", "Charlie Brown", "Ruth Rendell", "noel", "jodie Foster", "hot Chocolate", "the Republic of Namibia", "1921", "france", "the Clash", "praseodymium", "Bruce Alexander", "slap", "spinal cord", "noel", "Arthur Hailey", "hedgehog", "mrs henderson", "Little arrows", "a nerve cell cluster  or a group of nerve cell bodies located in the autonomic nervous system", "noel", "orrest", "27", "orchid", "104 colonists and Discovery", "aiding the war effort", "anterograde amnesia and dissociation", "Cesar Millan", "2004 Paris Motor Show", "Jiu-Jitsu", "cell phones", "mashaal", "drug cartels", "france", "steel", "place Betting", "2012"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5055147058823529}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-3401", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-2371", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-31", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1350", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-3119", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-294", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5551", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4525", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4442", "mrqa_hotpotqa-validation-3655", "mrqa_newsqa-validation-3515", "mrqa_searchqa-validation-16488", "mrqa_searchqa-validation-14449"], "SR": 0.46875, "CSR": 0.5274884259259259, "EFR": 0.9705882352941176, "Overall": 0.6875840822440087}, {"timecode": 54, "before_eval_results": {"predictions": ["Big Mamie", "dancer", "Canadian", "Sunyani West District", "six", "basketball", "Adelaide", "Alfred Preis", "Forest of Bowland", "Chengdu Aircraft Corporation", "Tom Rob Smith", "Switzerland", "The Keeping Hours", "Jeffrey Adam \"Duff\" Goldman", "Ang Lee", "Mineola", "67,038", "Kentucky Wildcats", "Eileen Atkins", "Parliamentarians (\" roundsheads\") and Royalists (\"Cavaliers\")", "Harry Potter series", "Haitian Revolution", "alcoholic drinks", "Frederick Dewey Smith", "John Meston", "York County", "Cleveland Cavaliers", "Scottish rock band U2", "Hillsborough County", "CN Too", "San Diego County Fair", "the fictional city of Quahog, Rhode Island", "mathematician and physicist", "October 4, 1970", "George Raft", "korea", "1944", "the British Army", "311", "Jennifer Taylor", "Cartoon Network", "Columbia Records", "British", "6,241", "October 17, 2017", "Les Clark", "Marilyn Martin", "Hawaii", "Gregg Popovich", "1979", "Vincent Landay", "Lauren Tom", "~ 0.058 - 0.072 mm", "Spektor", "spain", "fort Sumter", "vanilla", "Alan Graham", "four", "Louvre", "alvaro eureka", "Bending", "will", "the Capitoline Wolf"], "metric_results": {"EM": 0.5, "QA-F1": 0.6248511904761904}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.8000000000000002, 0.3333333333333333, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-796", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-3161", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-1310", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5383", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2609", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-1776"], "SR": 0.5, "CSR": 0.5269886363636364, "EFR": 0.96875, "Overall": 0.6871164772727273}, {"timecode": 55, "before_eval_results": {"predictions": ["2016 U.S. Senate election", "1952", "private Roman Catholic university located in Great Falls, Montana within the Diocese of Great Falls\u2013Billings", "voice-work", "400 MW", "Kerry Marie Butler", "Esp\u00edrito Santo Financial Group", "September 5, 2017", "Tuesday, January 24, 2012", "Javan leopard", "Salman Rushdie", "Tom Rob", "Big Bad Wolf", "Rockland County", "Personal History", "Australian-American", "The Holston River", "1943", "1996", "Tamil Nadu", "Tallahassee City Commission", "A Boltzmann machine", "three", "Hechingen in Swabia", "Floyd Mutrux and Colin Escott", "A Little Princess", "\"\", and the 2013 Marvel One- Shot short film of the same name", "2008", "from July 2, 1967 to August 21, 1995", "Kennedy Road", "Mark Neveldine and Brian Taylor", "the fictional city of Quahog, Rhode Island", "Spiro Agnew", "Dr. Ralph Stanley", "three", "Noel Gallagher", "Thorgan Ganael Francis Hazard", "1692", "9", "the twelfth title", "Saudi Arabian", "26,000", "Tallaght, South Dublin", "Indian state of Gujarat", "General Manager", "\"punk rock\"", "Matt Lucas", "The 1990\u201391 UNLV Runnin' Rebels", "President Bill Clinton", "John D Rockefeller", "Barack Obama's", "pigs", "dystopian science fiction action thriller", "The flag of Vietnam, or `` red flag with a gold star ''", "pulsar", "The North Sea", "the first eight seasons", "\"totaled,\"", "Steven Gerrard", "ketamine", "the Caspian tern", "The Southern Hemisphere", "Diebold", "Andr\u00e9s Iniesta"], "metric_results": {"EM": 0.546875, "QA-F1": 0.643145743145743}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, true], "QA-F1": [0.6666666666666665, 1.0, 0.13333333333333333, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.4444444444444444, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-5615", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-2859", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-574", "mrqa_hotpotqa-validation-5228", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5846", "mrqa_triviaqa-validation-4662", "mrqa_triviaqa-validation-3487", "mrqa_newsqa-validation-456", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-6593"], "SR": 0.546875, "CSR": 0.52734375, "EFR": 1.0, "Overall": 0.6934375}, {"timecode": 56, "before_eval_results": {"predictions": ["35,000", "Vishal Bhardwaj", "Macau", "the Russian film industry", "no. 3", "The Government of Ireland", "Washington Street", "Eielson Air Force Base", "Clube Atl\u00e9tico Mineiro", "Steve Carell", "Michael Fred Phelps II", "Richard Strauss", "a married World War II nurse", "Iynx", "David Starkey", "Edward III", "Anne Perry", "Ready to Die", "DI Humphrey Goodman", "\"Darconville\u2019s Cat\" (1981)", "An invoice, bill or tab", "October 16, 2015", "Dan Brandon Bilzerian", "The Andes", "Kolkata", "STS-51-C", "Rankin County Republican Chairman Gregg Harper", "The Ones Who Walk Away from Omelas", "Minnesota's 8th congressional district", "\"Three Colours\" films", "November 27, 2002", "Hank Azaria", "UFC Fight Pass", "The Sun", "Noel Paul Stookey", "Monty Python's Flying Circus", "Ready Player One", "1,925", "7 October 1978", "June 2, 2008", "Ravenna", "John Meston", "the Battelle Energy Alliance", "a co-op of grape growers", "The Spiderwick Chronicles", "goalkeeper", "George Orwell", "Croatian", "Labour Party", "9 or 10 October 1813 \u2013 27 January 1901", "Warsaw, Poland", "Texhoma", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "September 2017", "Ken Norton", "doggeria", "Anne", "British", "Al Nisr Al Saudi", "5 1/2-year-old", "the element between iron & nickel", "that track", "The bassoon", "time in exchange for detailed public disclosure of an invention"], "metric_results": {"EM": 0.5, "QA-F1": 0.614405657748049}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8333333333333333, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.8, 0.4, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4347826086956522, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-5057", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3953", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-7526", "mrqa_newsqa-validation-3410", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-12021", "mrqa_searchqa-validation-6632"], "SR": 0.5, "CSR": 0.5268640350877193, "EFR": 1.0, "Overall": 0.6933415570175439}, {"timecode": 57, "before_eval_results": {"predictions": ["1961", "Ballon d'Or", "Elsie Audrey Mossom", "the first trans-Pacific flight", "House of Hohenstaufen", "BraveStarr", "the White Knights of the Ku Klux Klan", "Benjam\u00edn Arellano F\u00e9lix", "leo spelaea", "Tony Award", "Blood and Soil", "China", "Esperanza Spalding", "1854", "Art Deco-style", "Starachowice", "Debbie Harry", "England", "Mineola", "Tokyo's Narita International Airport", "Melbourne's City Centre", "926 East McLemore Avenue", "bioelectromagnetics", "German", "casting, job opportunities, and career advice", "ten episodes", "2012", "Ford Mustang", "Paul Hindemith", "Euripides", "second segment", "Clitheroe F.C.", "Peter Wooldridge Townsend", "Paper", "November 27, 2002", "Western Kentucky University", "1837", "2007", "al-Qaeda", "Humberside Airport", "around 3,500,000", "striker", "Nia Kay", "11 November 1918", "1891", "Bank of China Tower", "Lerotholi Polytechnic FC", "first", "the twelfth and thirteenth centuries", "Lee Travis", "Richard Allen Street", "latitude 90 \u00b0 North", "1.5 times the Schwarzschild radius", "A photoelectric, or optical smoke detector", "Japan", "Apollo", "leeds", "bragging about his sex life", "people are going to look at the content", "Fernando Torres", "Augello Cook", "The Fairmont Hotel (San)", "April 1814", "Sonja Henie"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6353794642857142}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.8, 0.8, 0.5, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.7499999999999999, 1.0, 1.0, 1.0, 0.1111111111111111, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-4762", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2122", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-3499", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2325", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-6491"], "SR": 0.46875, "CSR": 0.5258620689655172, "EFR": 1.0, "Overall": 0.6931411637931035}, {"timecode": 58, "before_eval_results": {"predictions": ["Donald Duck", "an open window that fits neatly around him", "Gyanendra", "Susan Atkins", "Palestinian prisoners", "sought Cain's help finding a job", "mammogram are known to be uncomfortable,\"", "a round", "is \"very interested\" in buying the studios,", "10 municipal police officers", "his past and his future", "\"Empire of the Sun,\"", "Carol Browner", "uncomfortable,\"", "Haeftling,", "55-year-old", "capital murder and three counts of attempted murder", "shock, quickly followed by speculation about what was going to happen next,\"", "eight Indians whom the rebels accused of collaborating with the Colombian government,", "a 16th grand Slam title.", "curfew", "\"Nude, Green Leaves and Bust\"", "demolishing American third seed Venus Williams in the final of the Sony Ericsson Open in Miami on Saturday.", "The Glasgow, Scotland concert", "antihistamine and an epinephrine auto-injector", "Alwin Landry's supply vessel Damon Bankston", "Afghanistan", "a ban on inflatable or portable signs and banners on public property", "Why do genocides and mass atrocities happen?", "prevents girls from attending school, requires veils for women and beards for men, and bans music and television.", "humans", "Vancouver, British Columbia", "an independent homeland", "two years ago.", "is a businessman, team owner, radio-show host and author.", "Sen. Barack Obama", "25 years in the fashion business for the New Yorker, as well as a quarter century as an advocate for social activism.", "fake his own death by crashing his private plane into a Florida swamp.", "\"I get positive feedback because everybody around me likes Obama,\"", "one-of-a-kind navy dress with red lining by the American-born Lintner,", "The Rev. Alberto Cutie", "citizenship", "housing, business and infrastructure repairs", "the earthquake's devastation.", "a mammoth's fossil", "\"We are here to cooperate with anyone and everyone that will help us find the guilty party and return Lisa home safely,\"", "creation of an Islamic emirate in Gaza,", "in the north and west of the country,", "Elisabeth's father", "2-1", "Somalia's piracy problem was fueled by environmental and political events.", "Francisco Pizarro", "Owen Vaccaro", "2003", "French manufacturer Renault", "cheese", "Alan Freed", "Woolsthorpe-by-Colsterworth", "War Is the Answer", "Director-General", "frozen in glaciers and icecaps", "Ms.", "achieve", "prevent both freezing and melting"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4806690045563885}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.4444444444444445, 0.0, 1.0, 0.4, 0.0, 0.21052631578947367, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 0.5, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.09999999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.3529411764705882, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2182", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2472", "mrqa_triviaqa-validation-4416", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-4517", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-1521"], "SR": 0.40625, "CSR": 0.5238347457627119, "EFR": 0.9736842105263158, "Overall": 0.6874725412578055}, {"timecode": 59, "before_eval_results": {"predictions": ["2nd Lt. Holley Wimunc.", "Sunday", "Burrell Edward Mohler Sr.", "A witness", "U.N. Security Council", "Charles Charles", "Kim Jong Il,", "second child", "on the bench", "leftist Workers' Party.", "Robert", "Christiane Amanpour", "Narayanthi Royal", "Mexico", "five", "going somewhere very special, far away,", "Charles 11 to 14 --", "CNN/Opinion Research Corporation", "Asashoryu,", "Saadi,", "Dr. Jennifer Arnold and husband Bill Klein,", "at the Lindsey oil refinery,", "Sunday.", "future relations between the Middle East and Washington", "prostate cancer,", "Naples", "at least 300", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "2002", "Spc. Megan Lynn Touma,", "how preachy and awkward cancer movies can get.\"", "some truly mind-blowing structures", "strife in Somalia,", "Miami Beach,", "Charles Darwin", "Charles H.W. Bush,", "President Obama.", "Charles Charles Stiles,", "the approximately eight hours we spent carving in the middle of our Mountain View, California, campus.\"", "baseball bat", "in his native Philippines", "Charles Charles, California", "Jaime Andrade", "Alfredo Astiz,", "Ralph Lauren", "The station", "The father of Haleigh Cummings,", "Adriano", "$8.8 million", "Tuesday", "Alan Graham", "Canada south of the Arctic", "Glenn Close", "Joseph M. Scriven", "Pete Sampras", "Manchester", "Robert Plant", "John Rich", "Cannes Film Festival", "Charles", "3", "Rooster Cogburn", "Korea", "1936"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5513708513708513}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true], "QA-F1": [0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.5714285714285715, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.8, 0.8, 0.0, 1.0, 0.0, 0.09090909090909091, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-1872", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-5522", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-53", "mrqa_searchqa-validation-11013"], "SR": 0.421875, "CSR": 0.5221354166666667, "EFR": 1.0, "Overall": 0.6923958333333334}, {"timecode": 60, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1899", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-382", "mrqa_hotpotqa-validation-4012", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-892", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-266", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2558", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1143", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5687", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6157", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7783", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.806640625, "KG": 0.46953125, "before_eval_results": {"predictions": ["2017", "1648 - 51 war", "Thomas Edison's assistants, Fred Ott", "season four", "Master Christopher Jones", "William Shakespeare", "the status line", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "The Maginot Line", "1937", "British rock band", "Hirschman", "Ronnie Dyson", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "approximately 11 %", "Guy Berryman", "Serbia by its ally, Austria - Hungary", "Sharyans Resources", "Babe Ruth", "Seattle, Washington", "420", "Wylie Draper", "western Cuba", "DNA was a repeating set of identical nucleotides", "Magnavox Odyssey", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "Sunday night", "tolled ( quota ) highways", "close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "innermost in the eye while the photoreceptive cells lie beyond", "Saturday", "Glenn Close", "reservoirs at high altitudes", "aiding the war effort", "into smaller pulmonary arteries that spread throughout the lungs", "Nepal", "the symbol \u00d7", "Ace", "Phillip Paley", "the tax rate paid by a small business", "liver and kidneys", "Lee County, Florida, United States", "tenderness of meat", "Stephen Graham", "Florida", "1971 -- 1979", "Jeff Barry and Andy Kim", "infection, irritation, or allergies", "ABC", "14 June 1940", "Maidstone, Kent", "6/36 = 1/6)", "hydrogen", "caliber", "Edward James Olmos", "July 21, 1981", "Toyota Hilux", "Omar bin Laden,", "a man's lifeless, naked body", "jazz", "Falkland Islands", "a cement pond", "Seventy-Six Trombones", "Fifty Shades of Grey"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5775549450549451}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.0, 0.3076923076923077, 0.0, 0.2666666666666667, 0.2666666666666667, 0.15384615384615383, 1.0, 0.0, 1.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-5460", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-6052", "mrqa_hotpotqa-validation-4674", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-15624", "mrqa_searchqa-validation-893"], "SR": 0.484375, "CSR": 0.521516393442623, "EFR": 0.9696969696969697, "Overall": 0.6995707976279186}, {"timecode": 61, "before_eval_results": {"predictions": ["17 - year - old", "prevents any movement between the fused vertebrae", "Dido", "international aid", "at least US $2 trillion by GDP in nominal or PPP terms", "January 15, 2007", "1,366.33 m ( 4,483 ft )", "Roger Federer", "Battle of Antietam", "1933", "it `` never had any meaning other than the obvious one '' and is about the `` loss of innocence in children ''", "the person compelled to pay for reformist programs", "John Travolta", "August 5, 1937", "Rory McIlroy", "Jonathan Breck", "the 4th century", "Smith Jerrod", "`` Acid rain ''", "2017", "Mulberry Street", "Pete Seeger", "The pacemaking signal generated in the sinoatrial node travels through the right atrium to the atrioventricular node, along the Bundle of His and through bundle branches to cause contraction of the heart muscle", "1994", "Wyatt and Dylan Walters", "10 May 1940", "Phillipa Soo", "the Kansas City Chiefs", "Judith Cynthia Aline Keppel", "Swedien and Jones", "the customer's account", "California's Del Norte Coast, Jedediah Smith, and Prairie Creek Redwoods State Parks", "The film follows a child with Treacher Collins syndrome trying to fit in", "metaphase", "Sally Field", "Eric Clapton", "the most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "Lagaan", "Colman", "Karen Gillan", "The major holiday ( Christmas, Easter, New Year's Day, Pentecost ) in Romania and Moldova", "Beijing, China", "`` - s ''", "Havana Harbor", "1824", "2017", "Qutab Ud - Din - Aibak", "Prince Henry", "the inferior thoracic border -- made up of the diaphragm", "USCS or USC", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "floating ribs", "Cowboy Builders", "Romania's", "Helensvale", "Kathryn Jean Martin \"Kathy\" Sullivan AM", "25 million", "a better environment.", "torture and indefinite detention", "Tuesday's iPhone 4S news,", "sleeping sickness", "Jacob Marley", "John Hersey", "22 September 2015"], "metric_results": {"EM": 0.5, "QA-F1": 0.6122658061069255}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.11764705882352942, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.13793103448275862, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.125, 0.9, 1.0, 1.0, 1.0, 0.13333333333333333, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 0.0, 0.2758620689655173, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8382", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-9867", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6596", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-6794", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-2249", "mrqa_searchqa-validation-14622"], "SR": 0.5, "CSR": 0.5211693548387097, "EFR": 0.96875, "Overall": 0.6993119959677421}, {"timecode": 62, "before_eval_results": {"predictions": ["Christian Dior", "cranberry", "fort boyard", "fibula", "fort boyard", "a beached whale", "the Mississippi River", "Kevin Costner", "Lil Jon", "the Boers", "the Colosseum", "Goldeneye", "fish stock", "tarmes", "Sir Winston Leonard Spencer-Churchill", "salford in Greater Manchester", "the Lincoln Tunnel", "Pinta", "Louisiana", "the Kid", "Rembrandt", "Canada", "Tony Banks", "fort boyard", "the emergency reserve", "Prague Castle", "the engineer of Byzantium", "electricity", "Harry Dean Stanton", "fort boyard", "Beck", "the Indian School", "a hunting bison", "the oboe", "Montpelier", "fort boyard", "a pound of Antonio's flesh", "Bahrain", "the Last Temptation of Christ", "Hamlet", "Heroes", "Russia", "beer", "the Hawks", "Carnival", "gold", "Samson", "Dustin Hoffman", "Nittany", "Sicily", "Socrates", "more than 1,000", "the large area needed for effective gas exchange", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "Amalthea", "fort boyard", "fort boyard", "Esperanza Spalding", "2000", "Sargent Shriver", "Daytime Emmy Lifetime Achievement Award", "The Impeccable", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "gleneagles"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5552927835894084}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.8695652173913043, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5263157894736842, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-13095", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-1906", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-13940", "mrqa_searchqa-validation-5976", "mrqa_searchqa-validation-1751", "mrqa_searchqa-validation-6371", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-481", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-16438", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-8676", "mrqa_searchqa-validation-6211", "mrqa_searchqa-validation-14898", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-8062", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-15398", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6028", "mrqa_triviaqa-validation-7643", "mrqa_hotpotqa-validation-5895", "mrqa_newsqa-validation-3961"], "SR": 0.453125, "CSR": 0.5200892857142857, "EFR": 0.9428571428571428, "Overall": 0.6939174107142858}, {"timecode": 63, "before_eval_results": {"predictions": ["the Voting Rights Act", "a very small concealed carry semi- auto initially chambered for the.380 ACP", "disabilities", "Harry Potter and the Chamber of Secrets", "General Sherman and the Navajos", "Shampoos", "the Bronze Age", "the People's Party", "the Tower of London", "Daniel", "Smoking", "the mall", "Alaska", "Cosmopolitan", "David Beckham", "Minoan", "Japan", "rodeo", "a crowbar", "Vietnam", "Stanford", "D major", "the Star-nations and Galactic Empires", "Alaska", "Wallis Warfield Simpson", "a centipede", "Cyrillic", "Sisters Rosensweig", "Brooklyn", "Penn State", "Easter Island", "Nasser", "the Hell", "Stephen Hawking", "Labour Day", "Mozambique", "landfills", "The Silence of the Lambs", "15", "Capone", "Paul McCartney", "Anne Murray", "Tennessee", "Buenos", "contagious", "Jane Austen", "wheat", "vinegar", "peanuts", "ethics", "Constantinople", "The Paris Sisters", "Rent", "Miami Heat of the National Basketball Association ( NBA )", "gluten", "embroidered cloth", "Friday", "2004", "Pacific War", "Tsavo East National Park", "the Dominican Republic", "almost 100", "insurers have the VIN [vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "left fielder"], "metric_results": {"EM": 0.578125, "QA-F1": 0.664807347670251}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.9032258064516129, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-3380", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-7830", "mrqa_searchqa-validation-2172", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-7562", "mrqa_searchqa-validation-7064", "mrqa_searchqa-validation-6957", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-15049", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-7378", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-4454", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-4005", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2362", "mrqa_hotpotqa-validation-798"], "SR": 0.578125, "CSR": 0.52099609375, "EFR": 1.0, "Overall": 0.70552734375}, {"timecode": 64, "before_eval_results": {"predictions": ["Malawi", "Ethiopia", "a sharpening stone", "the short-beaked echidna and the duck-billed platypus", "British Airways", "Yoshi", "1929", "The Blades", "the Kelly Gang", "Dante Alighieri", "repechage", "uranium", "Wildcats", "d'Artagnan", "Abraham Lincoln", "The Merchant of Venice", "inches", "Paul Rudd", "Tanzania", "Julian", "Christian Louboutin", "saxophone", "I Have Thoughts Of Killing You", "October", "lincolnshire", "Muriel Spark", "Turkey", "kvetching", "the Dutch", "Lome", "Juno", "Christian Dior", "Pat Houston", "King Henry III", "Dutch", "Jack Lemmon", "Trainspotting", "Australia", "the north-west corner of the central business district", "Diptera", "India and Pakistan", "obi", "Broccoli", "Heisenberg", "1976", "Cyclopes", "phrenology", "Full Metal jacket", "Tokyo", "California", "Windermere", "third", "the fictional elite conservative Vermont boarding school Welton Academy", "a Norman occupational surname ( meaning tailor ) in France", "Michael Jordan", "11 or 13 and 18", "Archduke Franz Ferdinand of Austria", "Chile", "the single-engine Cessna 206 went down,", "helping on the sandbags lines as the community raced to fill 1 million of them.", "Rachel Carson", "laundry", "Toni Morrison", "Copts"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6243594028520498}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.9090909090909091, 0.3636363636363636, 1.0, 1.0, 0.3529411764705882, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-4208", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-1712", "mrqa_triviaqa-validation-1543", "mrqa_triviaqa-validation-7398", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6308", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8858", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3827", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-2435"], "SR": 0.5625, "CSR": 0.5216346153846154, "EFR": 0.9642857142857143, "Overall": 0.698512190934066}, {"timecode": 65, "before_eval_results": {"predictions": ["armstrong barrie", "Temple of the Dog", "netheron", "Tina Turner", "The Four Seasons", "barry Taylor", "1986", "left port", "commercial", "manson", "Uganda", "Ash", "equal temperature", "satirical cartoons", "Alaska", "iron", "sepp Blatter", "c\u00e9vennes", "Bagram", "Mnemosyne", "Brighton", "The Pillow Book", "arthur", "Armenia", "John Denver", "smack", "Massachusetts", "cable", "wellington", "The Apprentice", "The Altamont Speedway Free Festival", "jumpin Jack Flash", "brixham", "billy ray cyrus", "Ghana", "capella", "Nelson Mandela", "wellington", "Illinois", "sailing", "a bone", "two-thirds", "netherly", "CBS", "noah beery, Jr.", "well-regarded academically", "Sarajevo", "cast", "Beyonce", "Carl Johan", "MI5", "Games played", "After releasing Xander from the obligation to be Sweet's `` bride ''", "By petition for a writ of certiorari", "Little Dixie", "American Way", "a polyglot", "fluoroquinolone", "Alina Cho", "eight.\"", "Batman", "Passover", "Johann Strauss II", "John Lennon and George Harrison,"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6404246794871794}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-3329", "mrqa_triviaqa-validation-3599", "mrqa_triviaqa-validation-2184", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-7203", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-2918", "mrqa_triviaqa-validation-5878", "mrqa_triviaqa-validation-7618", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-7950", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-1804", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-14825"], "SR": 0.578125, "CSR": 0.5224905303030303, "EFR": 0.9629629629629629, "Overall": 0.6984188236531986}, {"timecode": 66, "before_eval_results": {"predictions": ["sleepless in seattle", "Margaret Beckett", "Portugal", "typhoid fever", "Sheryl Crow", "Melvil Dewey", "japan", "Pancho Villa", "the Wild Bunch", "mikhail gorbachev", "South Korea", "prince bride", "tiptoe through the Tulips", "landerstra\u00dfe", "Lesley Garrett", "e\u00earoeis", "c\u00e9vennes", "Les Invalides", "1861", "parson brown", "prince arthur", "qWERTY", "Jesse", "aslan", "26", "the narwhal", "a wishbone", "photographer", "Charlie Chan", "taekwondo", "phosphorus", "prince london", "lead", "Mercury", "trenches", "Dorset", "Antonio Vivaldi", "Groucho Marx", "lacrosse", "Queen Elizabeth I", "alusoji Fasuba", "1804", "london", "clydebank", "the Cheshire Cat", "Bette Davis", "a charcoal powered grill", "Chrysler", "Digital Audio Broadcasting", "qatar", "Carl Sagan and his wife and co-writer, Ann Druyan", "2010", "U.S. Bank Stadium in Minneapolis, Minnesota", "a percentage of ethanol in the blood in units of mass of alcohol per volume of blood or mass ofalcohol per mass of blood", "the FAI Junior Cup", "the Knight Company", "Miami-Dade County", "World leaders", "January 24, 2006.", "giving birth to baby daughter Jada,", "analog watch", "Ovid", "a serious or other-than", "Latin"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5735958860958861}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.3636363636363636, 0.0, 0.0, 0.7567567567567568, 1.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4413", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-7527", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-4502", "mrqa_triviaqa-validation-458", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-5123", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-1834", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-4523", "mrqa_hotpotqa-validation-1030", "mrqa_newsqa-validation-801", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-8081"], "SR": 0.515625, "CSR": 0.5223880597014925, "EFR": 0.9354838709677419, "Overall": 0.6929025111338469}, {"timecode": 67, "before_eval_results": {"predictions": ["Nacre", "Mahatma Gandhi", "Battlestar Galactica", "a sperm whale", "South Africa", "Brett Favre", "Perilis", "Texas", "angioplasty", "anxiety", "a phaser", "Mary Pickford", "a termite", "Sayonara", "a cat", "india", "Silver", "Mars", "the Battle of Verdun", "Peril", "andes", "India", "bbc", "Houston Rockets", "veal", "apartheid", "Boston", "Peril", "Van Helsing", "\"elbow\"", "pesos", "Shop", "Bligh", "Urban Outfitters", "burt robbins", "Andrew Wyeth", "smallpox", "jimmy", "Al Jolson", "Risk", "Peril", "Don Quixote", "Richmond", "midnight", "The Age of Innocence", "Students for a Democratic Society", "Bering Sea", "the Caucasus", "Coretta Scott King", "tritonic", "With a Little Help from My Friends", "the Brewster family", "After World War II", "word or name formed as an abbreviation from the initial components in a phrase or a word, usually individual letters ( as in NATO or laser ) and sometimes syllables", "Brighton", "Microsoft", "Fleet Street", "1966", "Juilliard School", "City of Starachowice", "producing rock music with a country influence.", "Mombasa, Kenya,", "Thaksin Shinawatra,", "peninsulas"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6296875}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3288", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-7736", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4010", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-5960", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-14069", "mrqa_searchqa-validation-10385", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-3274", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-4308", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2674", "mrqa_naturalquestions-validation-2064"], "SR": 0.5625, "CSR": 0.5229779411764706, "EFR": 0.9642857142857143, "Overall": 0.698780856092437}, {"timecode": 68, "before_eval_results": {"predictions": ["Davenport", "food", "(Ulysses) Grant", "gonads", "the High Plains", "the gap", "The Bodyguard", "Kansas", "motor neurons", "Kirk Lazarus", "a isoton", "(disorder)", "electrons", "the Communist Party", "( Alfred) Binet", "the wife of Bath", "the Atlantic Ocean", "(disorder)", "James Buchanan", "Hinduism", "a Miasa French Countryside Ruby Stemware", "Wayne Brady", "Leon Trotsky", "Arkansas", "embryos", "Cuba", "a packet", "aircraft", "Physicians and Surgeons", "Thomas Nast", "3", "freezing", "Picabo Street", "the National Security Agency", "Kiss Me, Kate", "cereal", "Hercules", "(disorder)", "Rod Laver", "Eczema", "Ivy Dickens", "Selma, Alabama", "vote", "Herbert Hoover", "Joe Hill", "IHOP", "Bubba's Book Club", "amyotrophic lateral sclerosis", "Marcel Proust", "William Pitt the Younger", "Independence Day", "Daya Jethalal Gada", "Kevin Sumlin", "Charles Carson", "Albert Einstein", "constellation of Orion", "a dodo", "1986", "James Gandolfini", "China Airlines", "two and a half hours.", "39,", "Buenos Aires.", "Citizens for a Sound Economy"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5489583333333333}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-30", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-15032", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-5152", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-4081", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-3663", "mrqa_searchqa-validation-6721", "mrqa_searchqa-validation-12045", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-9373", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-8996", "mrqa_searchqa-validation-2684", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-15053", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_triviaqa-validation-7252", "mrqa_hotpotqa-validation-1474", "mrqa_newsqa-validation-3854"], "SR": 0.46875, "CSR": 0.5221920289855073, "EFR": 1.0, "Overall": 0.7057665307971015}, {"timecode": 69, "before_eval_results": {"predictions": ["Manitoba", "Santa Monica, California", "60 Minutes", "a fruitcake", "midnight", "Socrates", "Al Gore", "the Louvre", "(Henry) II", "(Erwin) Rommel", "Baton Rouge", "Langston Hughes", "furyula", "Green Bay", "Cleveland", "a slave", "Amsterdam", "( Stevie) Wonder", "Mary Mallon", "an inch", "an enormous bite force,", "Tokyo", "Tennessee Williams", "the United Arab Emirates", "Lurch", "a chancellor", "Big Ben", "Old Yeller", "a proverbs", "2001: A Space Odyssey", "the Big Bang to Black Holes", "Prince", "(Abraham)", "Duncan", "(Yitzhak) Rabin", "Brisbane", "( Jason) Bourne", "the Holocaust", "a globe", "Iowa", "Mephistopheles", "Samsonite", "a Marine", "Vietnam", "Switzerland", "Punxsutawney", "Sports Illustrated", "Venus", "Shia LaBeouf", "Annapolis", "Princess Anne", "( Hungarian : Magyarorsz\u00e1g z\u00e1szlaja )", "January 15, 2007", "Introverted Feeling ( Fi ) and Extroverted Intuition ( Ne )", "a King of the Anglo-Saxons", "Trainspotting", "Stella McCartney", "1902", "Anne of Green Gables", "Daniil Shafran", "a Columbian mammoth", "Pixar", "help the convicts find calmness in a prison culture lively with violence and chaos.", "then-Sen. Obama"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6749999999999999}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.5, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.2, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-234", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-4292", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-14167", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-3164", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13160", "mrqa_searchqa-validation-7011", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-6872", "mrqa_hotpotqa-validation-3564", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3695"], "SR": 0.609375, "CSR": 0.5234375, "EFR": 1.0, "Overall": 0.706015625}, {"timecode": 70, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4260", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5757", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1840", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10156", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-1971", "mrqa_squad-validation-2082", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3215", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4299", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-513", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6067", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6917", "mrqa_squad-validation-6960", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9401", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.849609375, "KG": 0.51640625, "before_eval_results": {"predictions": ["Doc Holliday", "Fritos", "the pale", "fowls", "a fruitcake", "Carrie Underwood", "Subclue 2", "Christa McAuliffe", "Kilimanjaro", "Misbegotten", "a pumpkin", "Jumbo", "Stoke-on-Trent", "Jalisco", "Adolf Hitler", "Portland", "imagism", "Rudolf Diesel", "a palace", "the brig", "the Spanish Republic", "Ruth", "sugar", "Cheaper by the Dozen", "Hans Christian Andersen", "San Francisco", "Betsey Johnson", "Wanted", "the flute", "BORE", "Peel", "the Homestead Act", "Ellis Island", "The Daily Eye Hemorrhage", "plantain", "Marie Curie", "Middle Dutch", "Aladdin", "rain", "the Samson", "Toy Story", "the Boston Tea Party", "George Sand", "Jim Jarmusch", "Afghanistan", "Minos", "Salamander", "Charles", "the rose hips", "Led Zeppelin", "Germenicia", "Michigan", "Texas A&M Aggies", "Alex Burrall, Jason Weaver and Wylie Draper", "Leroy", "The Kentucky Derby", "Iwo Jima", "Charles L. Clifford", "Vince Staples", "Sam Bettley", "Al-Shabaab,", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Lucky Dube", "Florida"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7093341503267974}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4747", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-2880", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-15363", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-16761", "mrqa_searchqa-validation-6077", "mrqa_searchqa-validation-8455", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-1531", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-585"], "SR": 0.65625, "CSR": 0.5253080985915493, "EFR": 1.0, "Overall": 0.7263116197183098}, {"timecode": 71, "before_eval_results": {"predictions": ["The Beatles", "The Andy Griffith Show", "inconcept", "the Pocono Mountains", "trip", "watermelon", "a kart", "tanks", "Simon & Garfunkel", "(Albert) Pujols", "the Andean bear", "ordinal", "nebulae", "George W. Bush", "Eastwick", "The Who", "Cy Young", "A trilogy set in Alagaesia", "a mime instructor", "conga drums", "a fern", "Nellie Bly", "IBM", "Pizza", "A Lesson from Aloes", "menudo-mdo", "debts", "a cleft palate or cleaved", "Nicole Kidman", "Aristophanes", "Wimbledon", "Buddha", "a restrictive type", "The Jungle Book", "a turquoise bead", "Papua New Guinea", "A Not-So- Hypo-Urbane Study of True Grit", "Halo 3", "Boz", "Sayonara", "a sense of touch", "a crescent", "The Moment of Truth", "aardvark", "Harpy", "Henry Fielding", "James A. Garfield", "a waterfowl", "The Dream Factory", "a Chinese orbital carrier rocket", "Henry Cavendish", "six degrees of freedom", "Olivia", "May 31, 2012", "Syria", "Ida Noddack", "(Bokm\u00e5l) or  (Nynorsk)", "World War II", "Peter Kay's Car Share", "Oneida Community", "Zac Efron", "the punishment for the player", "the murders of his father and brother.\"", "mantle"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6611979166666666}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11101", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-10741", "mrqa_searchqa-validation-2234", "mrqa_searchqa-validation-10201", "mrqa_searchqa-validation-1831", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-12940", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-2850", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-5808", "mrqa_hotpotqa-validation-2656", "mrqa_newsqa-validation-2382"], "SR": 0.609375, "CSR": 0.5264756944444444, "EFR": 1.0, "Overall": 0.7265451388888889}, {"timecode": 72, "before_eval_results": {"predictions": ["133", "the United States", "Comoros Islands", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "$2 billion", "\"Slumdog Millionaire\"", "17", "London's O2 arena,", "Joel \"Taz\" DiGregorio, keyboardist and original member of The Devil Went Down to Georgia.\"", "second child", "in her home", "a fake pilot's license", "north-south", "a tracheotomy", "64,", "Muslim festival", "reduce their carbon footprint by 132 tons.", "Republicans, for the most part, have stuck with Bush on the war.", "a court seeking an emergency stay to stop the judge's order in its tracks.", "a satellite", "Mokotedi Mpshe,", "cross-country skiers", "Jeff Klein", "Saturn", "the reality he has seen is \"terrifying.\"", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "Haeftling,", "severe flooding", "Consumer Reports", "Now Zad in Helmand province, Afghanistan.", "Tens of thousands of new voters", "second", "through the weekend,", "John McCain", "South Africa's president", "the abduction of minors", "three", "Fernando Caceres", "Piers Morgan", "the L'Aquila earthquake,", "David Russ", "country directors", "on vacation until Tuesday", "\"We must eliminate the perceived stigma, shame and dishonor of asking for help,\"", "the last few months,", "Filippo Inzaghi", "BMW", "Pakistan", "the bombers", "Mom", "yokozuna", "great king", "SIP ( Session Initiation Protocol )", "Ant & Dec", "Robert A. Heinlein", "an ostrich", "friends", "IT", "St. Louis Cardinals", "south-east of Thornhill, in Dumfries and Galloway, south-west Scotland", "Popcorn", "8", "a white elephant", "Viscount Cranborne"], "metric_results": {"EM": 0.5, "QA-F1": 0.575298427795031}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08695652173913042, 0.25, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.125, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1373", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1128", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-6294", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-2653", "mrqa_searchqa-validation-8732"], "SR": 0.5, "CSR": 0.5261130136986301, "EFR": 0.96875, "Overall": 0.720222602739726}, {"timecode": 73, "before_eval_results": {"predictions": ["$40 and a bread.", "Justicialist Party,", "Jiverly Wong,", "Efraim Kam,", "11,", "Ralph Cifaretto on the HBO series \"The Sopranos,\"", "average of 25 percent", "April 2", "Robert Mugabe", "\"When you come upon a line of slow moving traffic, a Prius driver will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "OneLegacy,", "an upper respiratory infection,\"", "10:30 p.m. October 3,", "Arabic, French and English,", "U.N. Office on Drugs and Crime,", "\"Maude\" and the sardonic Dorothy on \"The Golden Girls,\"", "Bill,", "beloved and admired and dreamed about for nearly a hundred years.", "ancient Jewish tradition", "\"The Lost Symbol,\"", "J.G. Ballard,", "little blue booties.", "17", "2008.", "12 hours in jail.", "Now Zad in Helmand province, Afghanistan.", "Russian concerns that the defensive shield could be used for offensive aims.", "American", "Los Alamitos Joint Forces Training Base", "Al-Shabaab,", "safer surroundings.", "Flint, Michigan,", "an upper respiratory infection,\"", "Ronald F. Ferguson", "Sri Lanka", "\"CNN Heroes: An All-Star Tribute\"", "\"medical escorts\" for deportation since 2003.", "nearly $162 billion in war funding", "scientific reasons.", "innovative, exciting skyscrapers", "concentration camps,", "Pakistani territory", "Benazir Bhutto, who was assassinated Thursday in Rawalpindi,", "Pew Research Center", "self-styled revolutionary Symbionese Liberation Army -- perhaps best known for kidnapping Patricia Hearst --", "38,", "\"stressed and tired force\"", "speed attempts", "Leo Frank,", "At least 38", "Iran", "Port Said to the southern terminus of Port Tewfik at the city of Suez", "Philadelphia", "September 2017", "John Tucker Mugabi Sentamu", "Tony Cozier", "Sheffield", "Dulwich", "Thomas Richard \"Tom\" Coughlin", "Guarani (endonym \"ava\u00f1e'\u1ebd\"", "Vulcan", "Beloved", "a flapjack", "Hungarian"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5674896284271284}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, false], "QA-F1": [0.7499999999999999, 0.3636363636363636, 1.0, 0.0, 1.0, 0.2857142857142857, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.28571428571428575, 0.25, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2763", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-10147", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-6111", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-2170", "mrqa_searchqa-validation-1383"], "SR": 0.453125, "CSR": 0.5251266891891893, "EFR": 1.0, "Overall": 0.7262753378378379}, {"timecode": 74, "before_eval_results": {"predictions": ["South Korea's", "Bill Klein,", "16", "the program was made with the parents' full consent.", "five", "Marie-Therese Walter.", "Sixteen", "1979", "federal help", "consumer confidence", "$81,8709", "Climatecare,", "Haleigh Cummings,", "state senators", "August 19, 2007.", "Ghana in Egypt.", "6,000", "1-0", "Brian Smith", "fight against terror will respect America's values.", "to provide security as needed.\"", "Sunday,", "city of romance, of incredible architecture and history.", "1,500 Marines", "At the weekend, the captain appealed urgently to be rescued, fearing the crew could be harmed or killed,", "Bill Haas", "Silicon Valley.", "people switched from the very bad category to the pretty bad category,", "at least nine people", "Royal Navy servicemen", "Cash for Clunkers", "Yemen,", "the Dalai Lama", "800,000 people", "carving a pumpkin", "Russia", "only one", "Democratic VP candidate", "Former U.S. soldier Steven Green", "Cologne, Germany,", "President George H.W. Bush", "Kurdish militant group in Turkey", "6-4", "Karen Floyd", "St Petersburg and Moscow,", "flights", "1-1 draw", "South African teenager Caster Semenya won the women's 800 meters gold medal at the World Athletics Championships in Berlin,", "Alaska or Hawaii.", "30", "repeal of the military's \"don't ask, don't tell\" policy", "Dollree Mapp", "Matthias Schleiden and Theodor Schwann", "the American Civil War", "giraffe", "hiking and climbing", "the International Monetary Fund", "Dallas/Fort Worth Metroplex", "Robert Matthew Hurley", "Ashanti Region", "Michael Kors", "Katherine Heigl", "Lenin", "henry"], "metric_results": {"EM": 0.515625, "QA-F1": 0.60654833105491}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-1743", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-258", "mrqa_naturalquestions-validation-3705", "mrqa_triviaqa-validation-280", "mrqa_triviaqa-validation-2282", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-5300", "mrqa_triviaqa-validation-6077"], "SR": 0.515625, "CSR": 0.525, "EFR": 0.9354838709677419, "Overall": 0.7133467741935483}, {"timecode": 75, "before_eval_results": {"predictions": ["The Wiltshire adventurer and TV personality Bear Grylls", "vereinsthaler", "Adam Faith", "the Elbe", "New South Wales", "Donald Sutherland", "Stockholm", "Leo", "The King's majesty", "a power outage Sunday,", "Scott Glenn", "Pluto", "Deep Blue", "green", "18 February", "squid and other fish", "the failure of the duke of Monmouth\u2019s rebellion", "Isaac", "New Israel Shekel", "serbia", "The Lost Weekend", "Althorp", "the conquest of Peru", "June", "Persuasion", "Brugh", "the AllStars", "power", "\"The Armies of the Night\"", "fishes", "floating", "Florence", "Touchstone Gold test", "pascal", "\"gruppetto\"", "Go West", "11", "Israel", "football", "murder, madness and seduction", "English", "Gentlemen Prefer Blondes", "dogs", "Kenya", "Conrad Murray", "Shuttle Launch", "Amelia Earhart", "Spinach", "John Gorman", "Benedict", "cirrocumulus", "Ariana Clarice Richards", "Turner Layton", "The Parlement de Bretagne", "Campbellsville University", "Daniel Craig", "Vanilla Air Inc.", "HPV (human papillomavirus)", "\"This is not something that anybody can reasonably anticipate,\"", "21 percent suggesting that", "the vacuum effect", "Kansas State University", "Parkinson\\'s disease", "Sammy Gravano"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5699652777777777}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5621", "mrqa_triviaqa-validation-5929", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-771", "mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-4874", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-5132", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-1221", "mrqa_triviaqa-validation-6354", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-3838", "mrqa_naturalquestions-validation-7021", "mrqa_hotpotqa-validation-684", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-10356", "mrqa_hotpotqa-validation-4564"], "SR": 0.515625, "CSR": 0.524876644736842, "EFR": 1.0, "Overall": 0.7262253289473684}, {"timecode": 76, "before_eval_results": {"predictions": ["Steve Jobs", "The French", "Islamic", "jail.", "two women", "seven", "carry haerta Rios, also known as \"La Burra\" or \"El Junior,\"", "suicide car bombing", "the Illinois Reform Commission", "vice-chairman of Hussein's Revolutionary Command Council.", "4 meters (13 feet) high", "The recent violence -- which has included attacks on pipelines and hostage-taking --", "183", "80,", "a bank", "The Ventures", "back at work.", "Mexico", "The Rev. Alberto Cutie", "California, Texas and Florida,", "2-1", "Iran of trying to build nuclear bombs,", "\"We will support efforts by the Afghan government to open the door to those Taliban", "dancing", "at the Bridgestone Invitational in Ohio", "Jenny Sanford", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Sheikh Sharif Sheikh Ahmed", "54-year-old", "safety issues", "bipartisan", "Haiti", "Mashhad", "10", "Buenos Aires.", "Coast Guard", "Natalie Wood's", "oaxaca", "The Belgian, a former world No. 1 but seeded 14th for this event", "Venus Williams", "toppled cranes", "\"the most dangerous precedent in this country, violating all of our due process rights.\"", "15,000", "the governor's office", "on vacation", "The father of Haleigh Cummings,", "\"We never thought that you could do this.\"", "E. coli", "Michoacan state,", "1,500", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "Jyoti Basu", "60", "Homer Banks, Carl Hampton and Raymond Jackson", "leeds", "edward ii", "seattle", "strings", "bfim Costa Santos", "1958", "(William Tecumseh Sherman", "buren", "air pressure", "eucritta"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5374593420619236}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 1.0, 0.0, 1.0, 0.625, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.08695652173913043, 0.5, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-4796", "mrqa_hotpotqa-validation-1902", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12792", "mrqa_triviaqa-validation-1454"], "SR": 0.46875, "CSR": 0.5241477272727273, "EFR": 0.9705882352941176, "Overall": 0.7201971925133689}, {"timecode": 77, "before_eval_results": {"predictions": ["Bill Haas", "plastic surgery", "more than 200.", "Stephen Worgu", "American Civil Liberties Union", "Ronald Cummings", "Dr. Death in Germany", "Ralph Lauren", "Marion Davies", "along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "Columbia", "one", "Jenny Sanford,", "Buenos Aires", "hundreds", "more than 78,000 parents", "St. Francis De Sales Catholic Church", "The Israeli Navy", "Buddhism", "in Seoul,", "President Obama", "Kim Jong Il", "strangeulation and asphyxiation and had two broken bones in his neck,", "Dennis Ray Gerwing", "more than $2 billion in disaster assistance", "hanging a noose in a campus library,", "Austria,", "Michael Jackson", "Brazil's", "Pixar's", "Pixar's", "1,500", "at the Annual Caddy Awards dinner in Shanghai,", "Nechirvan Barzani,", "Halloween is no exception.", "their own,", "Transport Workers Union leaders", "Amanda Knox's aunt", "in the Muslim north of Sudan", "in Melbourne", "cancer", "school officials made some readjustments to inauguration security as a precaution and did not", "the ship", "338", "finance", "Haeftling,", "bribing other wrestlers to lose bouts,", "in the northwestern province of Antioquia,", "outfit from designer", "350 U.S. soldiers", "\"Taz\" DiGregorio,", "active absorption", "Robin", "17 -- 15", "world Translation of the Holy Scriptures", "the Sulu Sea", "British", "The satirical News Network", "Valhalla Highlands Historic District", "boxer", "push", "rain", "John Stewart", "Neon"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49856150793650794}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.2666666666666667, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2633", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2997", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-699", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12492", "mrqa_searchqa-validation-4358"], "SR": 0.40625, "CSR": 0.522636217948718, "EFR": 1.0, "Overall": 0.7257772435897436}, {"timecode": 78, "before_eval_results": {"predictions": ["AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro on Sunday.", "Aung San Suu Kyi", "1957,", "Fort Bragg in North Carolina.", "their emergency plans and consider additional security measures in light of Wednesday's shooting,", "South Africa", "Al-Shabaab,", "-- you know -- black is beautiful,\"", "assassination of President Mohamed Anwar al-Sadat at the hands of four military officers during an annual parade celebrating the anniversary of Egypt's 1973 war with Israel.", "Diego Milito's", "11 countries,", "Robert Barnett,", "More than 22 million", "a senior at Stetson University studying computer science.", "along the equator between South America and Africa.", "did not go into further detail about her heart condition or the medical procedure.", "200", "Phillip A. Myers.", "Congress", "bipartisan", "South Carolina Republican Party Chairwoman Karen Floyd", "1912.", "Steven Green", "The plane, an Airbus A320-214,", "Casey Anthony,", "Lifeway's 100-plus stores nationwide", "clothes that are consistent and accessible.", "some deaths", "park bench facing Lake Washington", "The singer's personal security guard, Andrew Morris,", "e-mail", "\"The Real Housewives of Atlanta\"", "an independent homeland", "Air traffic delays began to clear up Tuesday evening after computer problems left travelers across the United States waiting in airports,", "egypt, Iowa,", "Barbara Streisand's", "the death of a pregnant soldier", "raping", "five", "Some have complained that his wins are too routine, and purists grouse that he does not poses the quality of \"hinkaku,\"", "Bob Johnson", "\u00a320 million ($41.1 million)", "the sanctuary is now the largest natural refuge of its kind in the United States.", "natural disasters", "his mother,", "red varnished cover with the word \"Album\" inscribed on it in gold lettering,", "the House.", "peace with Israel", "German authorities", "following in Arizona's footsteps would take states in the wrong direction.", "Fourteen", "song's central theme of loss", "antimeridian", "instructions", "The Undertones", "eberhard Hasche", "Fenn Street School", "American actor and former fashion model", "the Vietnam War", "1967", "Ukraine", "capitals", "Francis Steegmuller", "genome"], "metric_results": {"EM": 0.5, "QA-F1": 0.5911124725968475}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 0.375, 1.0, 1.0, 0.7499999999999999, 0.16, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.22222222222222224, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727272, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-889", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-584", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-4123", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-9852", "mrqa_triviaqa-validation-6200", "mrqa_hotpotqa-validation-5128", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-10570"], "SR": 0.5, "CSR": 0.5223496835443038, "EFR": 0.96875, "Overall": 0.7194699367088608}, {"timecode": 79, "before_eval_results": {"predictions": ["his wife, Jane", "20 years from the filing date subject to the payment of maintenance fees", "The eighth and final season", "Rigg", "March 2016", "one of the most common words in scripture", "the name announcement of Kylie Jenner's first child", "2003", "small orange collection boxes distributed to millions of trick - or - treaters", "seven", "February 9, 2018", "Coroebus of Elis", "Jonathan Breck", "George Harrison, his former bandmate from the Beatles", "Howard Ellsworth Rollins Jr", "Ancy Lostoma duodenale", "January 17, 1899", "Coordinated Universal Time ( UTC \u2212 09 : 00 )", "Thespis ( / \u02c8\u03b8\u025bsp\u026as / ; Greek : \u0398\u03ad\u03c3\u03c0\u03b9\u03c2 ; fl. 6th century BC )", "in the three - domain system of taxonomy designed by Carl Woese, an American microbiologist and biophysicist", "Wednesday, September 21, 2016", "Robber baron", "three", "Tbilisi, Georgia", "off the rez", "1895", "Harold Godwinson", "Sir Ronald Ross", "May 29, 2018", "1979", "'s fourth season", "ancient Athens", "Russia", "the motion of the continents", "United Nations", "November 2016", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "Real Madrid", "Angel Benitez", "by capillary action", "1830", "Louis Mountbatten, 1st Earl Mountbatter of Burma remained Governor - General of India for some time after", "foreign investors", "Bhupendranath Dutt", "June 22, 1942", "775 rooms", "In 1984", "senators", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Paul Lynde", "electric potential generated by muscle cells when these cells are electrically or neurologically activated", "a treaty of Waitangi", "James Garfield", "The Gambia", "England", "Sverdlovsk", "47,818,", "seven", "American Samoa.", "to close their shops during daily prayers,", "Versailles A", "Westminster College", "Bob Dylan", "Mount Pelee"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6908437164391448}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.5882352941176471, 0.6666666666666666, 1.0, 1.0, 0.0, 0.7999999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.4, 1.0, 0.7272727272727273, 0.2222222222222222, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5217391304347825, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-7848", "mrqa_triviaqa-validation-5704", "mrqa_newsqa-validation-1318", "mrqa_searchqa-validation-13149", "mrqa_searchqa-validation-13773"], "SR": 0.546875, "CSR": 0.52265625, "EFR": 0.8620689655172413, "Overall": 0.6981950431034483}, {"timecode": 80, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11088", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.81640625, "KG": 0.4859375, "before_eval_results": {"predictions": ["the 13th century", "14 December 1972 UTC", "China", "1919", "Abraham Gottlob Werner", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "the 1960s", "July 2014", "ase", "at a given temperature", "Skat ( German pronunciation : ( \u02c8ska\u02d0t )", "Claudia Grace Wells", "Bill Russell", "the East India Company", "the Reverse - Flash", "natural killer cells", "Elijah Wood", "four", "1923", "at the 1964 Republican National Convention in San Francisco, California", "2018", "Lilian Bellamy", "Kansas City Chiefs", "The Vamps", "1997", "William Wyler", "Ray Harroun", "one person", "NIRA", "Santa Fe, New Mexico, USA", "Kaley Christine Cuoco", "Marshall Sahlins", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) or Kozunak", "in the United Kingdom", "the Gupta Empire", "the oculus, or `` eye point ''", "James Madison", "Matt Monro", "Burton upon Trent", "Algeria", "June 1992", "prokaryotic cell ( or organelle )", "hosted by Brazil", "the English", "the May Revolution of 1810", "Jason Flemyng", "blue", "an expression of unknown origin", "New Zealand to New Guinea", "1960", "Kaiser Chiefs,", "Donna Jo Napoli", "Claire Goose", "\"The 8th Habit\"", "road movie", "Earvin \"Magic\" Johnson Jr.", "President Sheikh Sharif Sheikh Ahmed", "a Burmese python", "posting a $1,725 bail,", "Yitzhak Rabin", "eSI", "1936", "carbon"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6939191017316018}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5454545454545454, 0.4, 1.0, 0.5714285714285715, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.0, 1.0, 0.8, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-5995", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2802", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-1713", "mrqa_searchqa-validation-11202"], "SR": 0.59375, "CSR": 0.523533950617284, "EFR": 1.0, "Overall": 0.7093161651234567}, {"timecode": 81, "before_eval_results": {"predictions": ["William the Conqueror", "1963", "Alastair Cook", "The President of Zambia", "Ray Charles", "in Ephesus in AD 95 -- 110", "the recommended.HTML filename extension", "in the 1970s", "Dan Stevens", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "Wyoming was to the south, Idaho is to the west and southwest, and three Canadian provinces, British Columbia, Alberta, and Saskatchewan, are to the north", "Tatsumi", "1969", "Ethiopia", "1937", "artes liberales", "Debbie Reynolds", "MSC Crociere S. p.A.", "2020", "2007", "Hodel", "the coffee shop Monk's", "Siddharth Arora / Vibhav Roy as Ishaan Anirudh Sinha", "1991", "1900", "the World Trade Center Transportation Hub", "Munich, Bavaria", "spacewar!", "October 30, 2017", "Paul Lynde", "regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Frank Oz", "Sam Waterston", "Saturday evenings", "Johnny Darrell", "the 7th century at Rendlesham in East Anglia", "2018", "Jesse Triplett", "Ben Rosenbaum", "2013", "two", "Chris Martin", "the class that teaches students defensive techniques to defend against the Dark Arts, and to be protected from Dark creatures", "Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluated", "Jackie Robinson", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "Brazil, Turkey and Uzbekistan", "Times Square in New York City west to Lincoln Park in San Francisco", "two", "The pia mater", "United Nations Peacekeeping Operations", "the Kinks", "inishtrahull Island", "the Indus Valley", "Interstate 95", "ancient Troad region of western Anatolia (in modern-day Turkey)", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "$1.45 billion", "American Lindsey Vonn", "\"Watchmen\"", "Ptolemy", "Dixie", "San Salvador Island", "Snickers candy bars"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6320973084425959}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.08695652173913045, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.37499999999999994, 1.0, 1.0, 1.0, 1.0, 0.972972972972973, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.7999999999999999, 0.4, 0.631578947368421, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4210526315789474, 0.0, 0.8, 0.0, 1.0, 1.0, 0.8, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-1642", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-1861", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3156", "mrqa_searchqa-validation-470", "mrqa_triviaqa-validation-7778"], "SR": 0.46875, "CSR": 0.5228658536585367, "EFR": 0.9411764705882353, "Overall": 0.6974178398493543}, {"timecode": 82, "before_eval_results": {"predictions": ["the Four Seasons", "Rebecca Wright", "in the First Folio in 1623", "Stephen Graham", "in 2010", "Tulsa, Oklahoma", "beta decay", "Nickelback", "1998", "John Adams", "Ford", "NCIS Special Agent in Charge", "Edward Furlong", "Pierre Monet", "in the summer of 1929", "Dottie West", "in northern China", "`` Blood is the New Black ''", "in 1977", "arm", "Professor Eobard Thawne", "Jack Nicklaus", "the trunk", "St. John's, Newfoundland and Labrador", "by the early - to - mid fourth century", "a young husband and wife", "May 29, 2018", "Anglican", "September 27, 2017", "April 10, 2018", "Daniel Suarez", "states that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "January 15, 2007", "powers in the Eastern Bloc ( the Soviet Union and its satellite states )", "glial cells", "Eulsa Unwilling Treaty or Japan -- Korea Protectorate Treaty", "Bangladesh -- India border", "Ole Einar Bj\u00f8rndalen", "Jenny Slate", "XLVIII", "Prince William, Duke of Cambridge", "Nicklaus", "in 1904", "Peter Greene", "Matt Monro", "a little warmth", "ADP and P", "in November 28, 1973", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "in Rome in 336", "September 30", "Jane Austen", "Jon Stewart", "alfa Romeo", "five months", "Wanda", "April 24, 1934", "40 years", "49,", "$2 billion", "Sex and the City", "Coffee", "Bryan Adams", "Oldham, in Greater Manchester, England"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6453582089166766}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.9090909090909091, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8648648648648648, 1.0, 0.33333333333333337, 0.0, 0.18181818181818182, 0.3333333333333333, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.967741935483871, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9687", "mrqa_triviaqa-validation-6153", "mrqa_triviaqa-validation-3677", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2444", "mrqa_triviaqa-validation-6822"], "SR": 0.46875, "CSR": 0.5222138554216867, "EFR": 0.9411764705882353, "Overall": 0.6972874402019844}, {"timecode": 83, "before_eval_results": {"predictions": ["Ferdinand", "1964", "Hindi", "eight", "Flula Borg", "Comanche County, Oklahoma", "Swiss", "1872", "Congo River", "4,613", "George Washington Bridge", "Harry Robbins \"Bob\" Haldeman", "Guardians of the Galaxy Vol.  2", "2.1 million", "6'5\" and 190 pounds", "526", "North West England", "casting, job opportunities, and career advice", "Saint Elgiva", "Scotland", "2006", "Carrefour", "infinite sum of terms", "Daphnis et Chlo\u00e9", "Wayne Rooney", "second half of the third season", "Johnson & Johnson", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "the Blue Ridge Parkway", "The Backstreet Boys", "45th", "Foxborough, Massachusetts", "Wichita", "animation", "Sir John Major", "sandstone", "Reginald Engelbach", "Dutch", "BC Dz\u016bkija", "Anne", "Derek Jacobi", "Rebirth", "\"Big Mamie\"", "Albert Bridge, London", "Maxwell Atoms with Cartoon Network", "1903", "King of France", "Alemannic", "Larry Alphonso Johnson Jr.", "Marlon St\u00f6ckinger", "Taylor Swift", "October 27, 2016", "tenderness of meat", "Tigris and Euphrates rivers", "Mozambique Channel", "the Astor family", "Tashkent", "Afghanistan,", "police", "Buckhorn Mountain,", "Speed Racer", "Yogi Bear", "sakura", "to function like an endocrine organ, and dysregulation of the gut flora has been correlated with a host of inflammatory and autoimmune conditions"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7077537593984963}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4826", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1512", "mrqa_hotpotqa-validation-245", "mrqa_naturalquestions-validation-1728", "mrqa_triviaqa-validation-5311", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-3680", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5496", "mrqa_naturalquestions-validation-7393"], "SR": 0.640625, "CSR": 0.5236235119047619, "EFR": 1.0, "Overall": 0.7093340773809523}, {"timecode": 84, "before_eval_results": {"predictions": ["201", "Atlanta", "around 1872", "1978", "Hodel", "Central Germany", "Kida", "the Outfield", "William Shakespeare's As You Like It", "a central place in Christian eschatology", "McFerrin, Robin Williams, and Bill Irwin", "artes liberales", "the sinoatrial node", "plasma pressure to be controlled via pulse - width modulation of the pump voltage", "the original timeline is eventually restored", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "December 11, 2014", "A vanishing point", "Janie Crawford", "UTC \u2212 09 : 00", "mining", "Scottish post-punk band Orange Juice", "blood plasma and lymph in the `` intravascular compartment ''", "nearly 92 %", "Missouri River", "Apple shortened the name to `` OS X '' in 2012 and then changed it to `` fiat '' in 2016", "Jason Marsden", "The Chipettes", "Linda Davis", "Fats Waller", "Thespis", "Neal Dahlen", "May 19, 2017", "at some point", "about 375 miles ( 600 km ) south of Newfoundland", "the status line", "Samantha Jo `` Mandy '' Moore", "Dr. Lexie Grey", "Nodar Kumaritashvili", "the TLC - All That Theme Song", "Welch, West Virginia", "Nicki Minaj", "Steve Russell", "Danielle Rose Russell", "the church at Philippi", "data ( D )", "to solve its problem of lack of food self - sufficiency", "Boutique hotel", "The Lykan", "four", "Geothermal gradient", "soft contact lenses", "roger daltrey", "fire insurance", "Seoul, South Korea", "Saturday", "Al Capone", "14 years", "Syria and Iraq", "Russia", "the Dustbin", "the Equator", "Claudius Caesar", "There's no chance"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6779337165027954}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false], "QA-F1": [0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.7692307692307692, 0.25, 0.5, 1.0, 1.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-40", "mrqa_naturalquestions-validation-1767", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-5726", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5444", "mrqa_newsqa-validation-199", "mrqa_searchqa-validation-16205", "mrqa_newsqa-validation-2213"], "SR": 0.546875, "CSR": 0.5238970588235294, "EFR": 0.9655172413793104, "Overall": 0.702492235040568}, {"timecode": 85, "before_eval_results": {"predictions": ["the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "The uvea", "25 June 1932", "Felicity Huffman", "to control gene expression and mediate the replication of DNA during the cell cycle", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Roger Nichols and Paul Williams", "peninsular", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "letter series", "eleven separate regions of the Old and New World", "As of January 17, 2018, 201 episodes", "Derrick Henry", "Monk's", "B.R. Ambedkar", "the septum", "Cecil Lockhart", "in the Executive Residence of the White House Complex", "David Motl", "invertebrates", "as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "the C - peptide is cleaved", "the New Croton Reservoir in Westchester and Putnam counties", "Professor Eobard Thawne", "Freddie Highmore", "USA Today", "1939", "over 74", "during prenatal development", "Zedekiah", "A footling breech", "Robert Gillespie Adamson IV", "Waylon Jennings", "rear - view mirror", "the brainstem", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Doug Diemoz", "about $1.09 trillion", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "Frank Zappa", "1956", "Camping World Stadium in Orlando, Florida", "Louis XV", "the final episode of the series", "Jonathan Breck", "Jody Rosen", "British Kennel Club", "126 by Wilt Chamberlain from October 19, 1961 -- January 19, 1963", "James Chadwick", "eretria", "George Strait", "Swiss", "a trademark", "Las Vegas", "Hindi", "Town of Brookhaven", "January 2004", "space shuttle Discovery,", "dismissed all charges", "British author J.G. Ballard,", "the frittata", "a crabitat", "a parking meter", "Matlock"], "metric_results": {"EM": 0.625, "QA-F1": 0.6927900802900804}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.0, 0.5185185185185185, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6485", "mrqa_naturalquestions-validation-6066", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-2970", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-3940", "mrqa_naturalquestions-validation-8061", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-1284", "mrqa_hotpotqa-validation-5848", "mrqa_newsqa-validation-3278", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-10904"], "SR": 0.625, "CSR": 0.5250726744186047, "EFR": 0.9583333333333334, "Overall": 0.7012905765503876}, {"timecode": 86, "before_eval_results": {"predictions": ["San Francisco", "a Polaroid picture", "Mikhail Baryshnikov", "a filibuster", "Christo", "Berlin", "(Cyrus Hall McCormick", "the Ziegfeld Girl", "a ballpoint pen", "the tabernacle", "an insect life cycle", "Tequila", "the monovision", "Einstein", "a American infantryman", "a roller coaster", "President George H.W. Bush", "Elvis Presley", "a flushing toilet", "monovision", "Gogol", "the Hudson River", "Banquo", "zodiac", "Texas", "a pardon", "an alligator", "General Mills", "the comb", "Ferdinand Magellan", "Austria", "a drop of Roses lime", "\"The Drowsy Chaperone\"", "the owl", "the Greyhounds", "a clock", "John Molson", "An Crannchur Nisinta", "Alston", "Tanzania", "Colorado", "Christopher Columbus", "the Caribbean Sea", "Slovakia", "polygons", "Frederic Remington", "the biography written by daughter Julie", "Evan Almighty", "the toothache", "64", "a pillar of salt", "Johannes Gutenberg", "203", "4 January 2011", "Neighbours", "curb-roof", "a Lion", "Norway", "Big Machine Records", "The Campbell Soup Company", "Stoke City.", "identity documents", "Los Angeles", "Venezuela"], "metric_results": {"EM": 0.5, "QA-F1": 0.6005456349206348}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-10854", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-7345", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-4790", "mrqa_searchqa-validation-632", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-13536", "mrqa_searchqa-validation-5092", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6649", "mrqa_newsqa-validation-1516"], "SR": 0.5, "CSR": 0.5247844827586207, "EFR": 1.0, "Overall": 0.7095662715517241}, {"timecode": 87, "before_eval_results": {"predictions": ["Shopgirl", "Easter Island", "True", "The Bridge on the River Kwai", "Edwin Starr", "Virgin Atlantic", "a hard yellow cheese", "Indira Gandhi", "a pew", "Alfred Hitchcock", "the circulatory system", "the London Bridge", "Ho Chi Minh", "Vesuvius", "the Himalayas", "Kodachrome", "A Doctor's Love Affair", "Paul Simon", "Pakistan", "A Prairie Home Companion", "the bridge", "Smiley", "Thames", "Taipei", "Melba", "the Caspian Sea", "Babe Ruth", "the Edo era", "Stromboli", "apples", "a bat cookies", "bay", "the Canterbury Tales", "Japan", "Ben & Jerry", "a Sweater Girl", "Krakauer", "al-Ashraf Khalil", "Sweden", "John Glenn", "the Andes", "an ink", "Mitt Romney", "Goofy", "Peter Jackson", "an inch", "Neptune", "Hawaii", "a whale shark", "Simon Cowell", "the Northern Mockingbird", "origins of replication, in the genome", "27 January -- 16 April 1898", "57 days", "Mr. Humphries", "James Mason", "Tinie Tempah", "Arthurdale", "Ghana's", "Labour", "citizenship", "1,500", "African President Thabo Mbeki,", "Louis XV"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7447916666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3533", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-7940", "mrqa_searchqa-validation-9282", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-8024", "mrqa_searchqa-validation-3772", "mrqa_searchqa-validation-2075", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-6778", "mrqa_searchqa-validation-6947", "mrqa_naturalquestions-validation-1277", "mrqa_hotpotqa-validation-4402", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-3391"], "SR": 0.65625, "CSR": 0.5262784090909092, "EFR": 1.0, "Overall": 0.7098650568181818}, {"timecode": 88, "before_eval_results": {"predictions": ["Charles Dickens", "a fish", "a gravestone", "Grant Wood", "(Jeff) Probst", "All's Well That Ends Well", "the Black Sea", "Eggs Benedict", "a lovebird", "Agatha Christie", "a church", "Tracy Tynan", "the Mossad", "a backstroke", "Swahili", "a defibrillator", "Katrina", "a crash sensor", "a proscenium arch", "a tuna", "Pocahontas", "sinuses", "an African Cichlids", "Jane Eyre", "Kandahar", "(Nolan) Ryan", "miso", "(William) Henry Harrison", "clay", "the Jutland", "a space adventure film", "Fourteen Points", "Misery", "(Cora) Munro", "Jackson", "the Osmonds", "a guitar", "(Roberta) Flack", "Belgium", "(John) Miller", "Chicago", "an actuary", "a latke", "Montana", "a dulcimer", "a ventriloquist", "lead", "the apocrypha", "a discus", "Ayahuasca", "Top Gun", "a chimera", "H ions", "Nick Sager", "horse racing", "(Lucien Laurent)", "Mauna Kea", "Deftones", "The Design Inference", "(Andr\u00e9) Rieu", "Form Design Center.", "3-0", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "Ronald Lyle \" Ron\" Goldman"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6599702380952381}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.9047619047619047, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-13978", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-15586", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-16336", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-12604", "mrqa_searchqa-validation-2486", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-382", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-13957", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-2402", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-5153", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-692", "mrqa_hotpotqa-validation-2410"], "SR": 0.609375, "CSR": 0.5272120786516854, "EFR": 1.0, "Overall": 0.7100517907303371}, {"timecode": 89, "before_eval_results": {"predictions": ["ash and rubble", "Pierre Mamboundou", "African National Congress Deputy President Kgalema Motlanthe,", "Dan Parris, 25, and Rob Lehr,", "that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.", "18th", "The monarchy", "throwing three punches", "Dave Bego,", "near the equator,", "some U.S. senators", "\"we have more work to do,\"", "a one-shot victory in the Bob Hope Classic", "that she was lured to a dorm and assaulted in a bathroom stall.", "Citizens", "death", "businessman", "his business dealings", "a team of eight surgeons", "Meira Kumar", "The exact cause of IBS remains unknown,", "a nearby day care center whose children are predominantly African-American.", "The Mexican military", "The FBI's Baltimore field office", "UK", "a floating National Historic Landmark,", "Adidas,", "can be traced, in part, to a \"stressed and tired force\" made vulnerable by multiple deployments,", "Zac Efron", "he wants a \"happy ending\" to the case.", "3 to 17", "4.6 million", "Rima Fakih", "the Airbus A330-200", "the Internet", "228", "London's O2 arena,", "a more detailed necropsy.", "London Heathrow's Terminal 5.", "heroin", "Bob Dole,", "Section 60.", "Muqtada al-Sadr,", "Leaders of more than 30 Latin American and Caribbean nations", "At least 38 people", "two years", "heavy turbulence", "alongside", "Florida", "tax", "Jose Miguel Vivanco,", "The genome", "2010", "the Jurchen Aisin Gioro clan in Manchuria", "the Sea of Azov", "William WymarkJacobs", "serbia", "wine", "Tsavo East National Park", "\"Lucky\"", "bananas", "Ron Markstein's Toonopedia", "Midas", "Home Alone"], "metric_results": {"EM": 0.5, "QA-F1": 0.5769747483518579}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.5714285714285715, 1.0, 0.125, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-1036", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-1968", "mrqa_hotpotqa-validation-3474", "mrqa_searchqa-validation-15665"], "SR": 0.5, "CSR": 0.5269097222222222, "EFR": 1.0, "Overall": 0.7099913194444445}, {"timecode": 90, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-11678", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.8671875, "KG": 0.52734375, "before_eval_results": {"predictions": ["1974", "bat", "\"Antz\", \"Dr. Dolittle\", and \"The Santa Clause\"", "\"Switzerland of England\"", "Ford Island", "Food and Agriculture Organization", "Beno\u00eet Jacquot", "Lincoln", "World War II", "Les Temps modernes", "Agra", "1986", "\"King of Cool\"", "841", "Missouri", "1.23 million", "32", "Clarence Nash", "2015", "James Franco", "1970", "Sam Kinison", "1918", "Robert Gibson", "3730", "Washington State Cougars", "1939", "Forbes", "Univision", "English Wesleyan minister and biographer", "1865", "Dallas", "237 square miles", "Tennessee", "Orlando", "Oklahoma City", "Suspiria", "\"The School Boys\"", "Michele Bachmann", "2007", "Japan", "fixed-roof", "Singapore", "Balloon Street, Manchester", "\"The Cleveland Show\"", "Vanarama National League", "Anne Erin \"Annie\" Clark", "Atlantic", "Samantha Spiro", "Fort Hood, Texas", "Hong Kong", "Clarence Darrow", "Cristeta Comerford", "Instagram's own account", "1979", "Adam Smith", "geese", "Bill Klein,", "last summer.", "speed attempts", "Wyeth", "a calico", "biddy", "to the U.S. Holocaust Memorial Museum"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7103174603174603}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.33333333333333337, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5341", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-2900", "mrqa_hotpotqa-validation-2970", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-7442", "mrqa_triviaqa-validation-6410", "mrqa_searchqa-validation-3639", "mrqa_newsqa-validation-2420"], "SR": 0.578125, "CSR": 0.5274725274725275, "EFR": 0.9259259259259259, "Overall": 0.7184140656796907}, {"timecode": 91, "before_eval_results": {"predictions": ["( Leo) Tolstoy", "Montana", "Tigger", "the Huguenots", "Super Mario Bros.", "Lexington", "a ray", "mint", "Roxanne", "Florida State University", "Roald Dahl", "ER", "Buffalo Bill Cody", "a pager", "Hawaii", "Sir Isaac Newton", "Radiohead", "Cain", "Lignite", "the Vietnam War", "algebra", "aragon", "paul mcc McCartney", "The Bumsteads", "Drumline", "Donnie Wahlberg", "cytokinesis", "the Unabomber", "Tom Petty", "Harry Potter", "The Sixth Sense", "New Wave", "gin", "Santa", "fashion", "Billy the Kid", "the Stone Age", "(Cecil) Rhodes", "(James) Garner", "the Inn", "Michelle Pfeiffer", "take", "Jacob Garner", "Phelps", "Papua New Guinea", "the monsoon winds", "a prism", "sesame", "a quart", "hock", "Late Night", "UNICEF's global programing", "defense against rain rather than sun", "Vijaya Mulay", "Peru", "Istanbul", "caspian", "Dante", "George Raft", "Grace O'Malley", "Zed", "Communist", "empty water bottle", "new wave rock band The Fixx"], "metric_results": {"EM": 0.625, "QA-F1": 0.6979910714285714}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9909", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-12917", "mrqa_searchqa-validation-5080", "mrqa_searchqa-validation-14637", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-12260", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-2465", "mrqa_searchqa-validation-15001", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-3015", "mrqa_searchqa-validation-11934", "mrqa_searchqa-validation-3638", "mrqa_searchqa-validation-6925", "mrqa_searchqa-validation-13368", "mrqa_searchqa-validation-11404", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-3479", "mrqa_hotpotqa-validation-2012"], "SR": 0.625, "CSR": 0.5285326086956521, "EFR": 0.9583333333333334, "Overall": 0.7251075634057971}, {"timecode": 92, "before_eval_results": {"predictions": ["John Foster Dulles", "aluminum", "\" look, Momno cavities!\"", "The New York Times", "the PATRIOT Act", "hock", "John Madden", "Hemingway", "a rubaiyat", "a baseball commissioner", "Malaysia", "a yam", "Mariner 1", "the Chunnel", "President Lincoln's second inaugural address", "Smithfield", "Taxi Driver", "Poland", "a tomb", "a bagel bite", "the Tabernacle", "the U.S. Open", "the Galapagos", "Boston", "the USS Nautilus", "the Phoenicians", "parez", "Italy's second-oldest university", "Athens", "Eternity", "Amanda Plummer", "Pennsylvania", "Cotton Mather", "the Berlin Wall", "Suzanne Valadon", "Sexuality", "Elephants", "an axe", "Erwin Rommel", "France", "wheat", "Vermont", "Mending Wall", "Thomas Jefferson", "copper", "Wrigley", "rum", "a Towel", "Brian Curtis Wimes", "steel", "Cormac McCarthy", "Eydie Gorm\u00e9", "A turlough", "6,259", "Akhenaten", "Funchal", "Ruth Rendell", "Anthony Lynn", "The 2016 United States Senate election", "Martin Scorsese", "African National Congress Deputy President Kgalema Motlanthe,", "Angela Merkel", "in September,", "Somali-based"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5928819444444444}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-13482", "mrqa_searchqa-validation-5947", "mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-16916", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4976", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-4760", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-1937", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-12309", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-13222", "mrqa_naturalquestions-validation-335", "mrqa_triviaqa-validation-3919", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-4189"], "SR": 0.53125, "CSR": 0.5285618279569892, "EFR": 0.9666666666666667, "Overall": 0.7267800739247312}, {"timecode": 93, "before_eval_results": {"predictions": ["Twister", "Romeo & Juliet", "doughboy", "Georgetown", "the Dalmatian", "a cricket", "a knight", "Frontier", "a mummies", "Lot", "a hull", "aluminum", "Kevin Smith", "Jabez Stone", "major", "the Monitor", "Louis Pasteur", "the Bailiwick", "the Hudson Bay", "tetra", "Edgar Allan Poe", "Thomas Edison", "Manhattan", "(Robert) Lambeau", "T.E. Lawrence", "Blake Lively", "ape", "Eliot Spitzer", "licorice stick", "the union", "a fox", "Poe", "a star", "impeachment", "Santo Domingo", "a Dagger", "\" Nativity Story\"", "Nightingale", "The Inside Story", "George Moscone", "butterflies", "a Raincoat", "the devil's food cake", "Goodyear", "corpulent", "Edinburgh", "a crumpet", "trailgator bars", "the Great Smoky Mountains", "Walter Scott", "Punjabi", "October 1986", "in the duodenum", "Anthony Hopkins", "six", "Dubai", "horse racing", "Abdul Razzak Yaqoob", "England", "Colonel Gaddafi", "first-ever Test series triumph in Australia", "Haiti's capital, Port-au-Prince,", "if Gadhafi suffered the wound in crossfire or at close-range --", "Roberto Micheletti,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6405381944444444}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.8, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4195", "mrqa_searchqa-validation-13621", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-15297", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-16029", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-7204", "mrqa_searchqa-validation-3704", "mrqa_searchqa-validation-3150", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-5066", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-6686", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-5257", "mrqa_searchqa-validation-7782", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-221", "mrqa_hotpotqa-validation-3442", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-2385"], "SR": 0.5625, "CSR": 0.5289228723404256, "EFR": 1.0, "Overall": 0.733518949468085}, {"timecode": 94, "before_eval_results": {"predictions": ["John Mayer", "(Benjamin) Franklin", "the Amstel River", "Constantinople", "Georgie Porgie", "Superior", "Adam", "Puerto Rico", "hearth", "Quebec", "Belshazzar", "the Cincinnati Reds", "Once", "Shelley", "China", "Douglass", "Sitka", "the Amazons", "(Claude) Debussy", "a prince", "Bojangles", "Aunt Jemima", "Frank Sinatra", "a saucer", "surfing", "KLM", "(Frederick Victor) Zeller", "a carriage", "a vest", "The Adventures of Sherlock Holmes", "Ned Kelly", "a prostitution scandal", "the Blood elves", "(Julius) Caesar", "the Inca", "Alaska", "(Samuel) Kinison", "Wii", "the high jump", "Champagne", "corners", "Danica Patrick", "the pancreas", "the Midway Atoll", "stars", "(Henry) Cisneros", "the sacristy", "the Great Seal", "Rihanna", "\"24\"", "Tom Brady", "an ornament", "the final scene of the fourth season", "Billy Hill", "iron", "(Henry) Ford", "saint bartholomew", "1943", "Battle of Britain and the Battle of Malta", "Kaep", "not", "Kaka,", "vicious brutality which accompanied the murders of his father and brother.\"", "Lana Clarkson"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7313244047619047}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-9190", "mrqa_searchqa-validation-14196", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-1470", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-8701", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-4359", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-11261", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-13716", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-5383", "mrqa_newsqa-validation-1330"], "SR": 0.65625, "CSR": 0.5302631578947368, "EFR": 0.9545454545454546, "Overall": 0.7246960974880382}, {"timecode": 95, "before_eval_results": {"predictions": ["The Buckwheat Boyz", "1998", "between the Eastern Ghats and the Bay of Bengal", "global oceans, and about 3 times faster than the warming observed in the Pacific", "Gatiman express its ranges 160km / hour between Delhi to Agra In 100 min its cross 180km", "Dr. Rajendra Prasad", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "no more than 4.25 inches ( 108 mm )", "the Jews", "book and architecture", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "typically the player to the dealer's right", "his guilt in killing the bird", "February 6, 2005", "a global cultural icon of France and one of the most recognisable structures in the world", "in the episode `` Kobol's Last `` ''", "Philippe Petit", "Terrell Suggs", "anion", "four", "1952", "The User State Migration Tool ( USMT )", "2017", "restoring someone's faith in love and family relationships", "an illustration by Everest creative Maganlal Daiya back in the 1960s", "if the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "solemniser", "1961", "36 months", "an integral membrane protein that builds up a proton gradient across a biological membrane", "during the period of rest ( day )", "Brenda", "Randy VanWarmer", "Isle Vierge ( 48 \u00b0 38 \u2032 23 '' N 4 \u00b0 34 \u2032 13 '' W \ufeff", "Kevin Spacey", "Miller Lite", "the body - centered cubic ( BCC ) lattice", "1986", "their son Jack ( short for Jack - o - Lantern ) is born on Halloween 2023", "Spektor", "typically given in the evening, after 9pm ET ( UTC - 5 )", "February 2017 in Japan and in March 2018 in North America and Europe", "July 21, 1861", "his attractive Tatted neighbour ( Holden Nowell )", "Muhammad Yunus", "The Bangles", "Juliet", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Ethel Merman", "The New Croton Aqueduct", "Judith Cynthia Aline Keppel", "Kenny Everett", "Trinidad", "Ghost", "Hawaii", "William Bradford", "Stallworth", "Italian Serie A", "three", "suicides", "John Henry", "manure", "Two and a Half Men", "2004 Paris Motor Show"], "metric_results": {"EM": 0.5, "QA-F1": 0.6217079540149393}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1904761904761905, 0.2222222222222222, 1.0, 0.9428571428571428, 1.0, 0.0, 0.5, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.5454545454545454, 0.2857142857142857, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.4411764705882353, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.2666666666666667, 1.0, 0.5, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-10128", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-6035", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3217", "mrqa_newsqa-validation-2754", "mrqa_searchqa-validation-5787"], "SR": 0.5, "CSR": 0.5299479166666667, "EFR": 0.90625, "Overall": 0.7149739583333334}, {"timecode": 96, "before_eval_results": {"predictions": ["dhabi", "art", "to mark last weekend's mass shooting at a central Florida nightclub,", "Pisces", "The Law Society", "Russell Crowe", "two", "Saudi Arabia", "Clint Eastwood", "1921", "france", "David Bowie", "2. Star Wars", "German", "lake Superior Ice Volcanoes", "Porridge", "South Africa", "New Orleans", "the eye", "Pooh", "Ringo Starr", "Sir John Mortimer", "bushfires", "Idaho", "Danny DeVito", "Sweden", "four players", "Tony Washington, Willie Woods and Victor Thomas", "Rastafari", "Sydney", "800m", "Leo Tolstoy", "Lotus", "the Mayflower", "Yes", "Benghazi", "Brazil", "The Dick Van Dyke Show", "Gordon Jackson", "Scottish", "Irving Berlin", "West Sussex", "Laputa", "Colombia", "lurch", "a storm", "Tanzania", "Joan", "Robert Boyle", "Thailand", "Hugh Laurie", "Beijing for the 2020 Winter Olympics, Paris for the 2024 Summer Olympics, and Los Angeles for the 2028 Summer Olympics", "sometime between 124 and 800 CE", "the first to develop lethal injection as a method of execution", "1 January 1788", "guitar feedback", "Stephen Lee", "her husband", "Apple's only model with a sort of robot living inside.", "North Korea has positioned what is thought to be a long-range missile on its launch pad,", "The Vietnam War", "white granite", "the humerus", "a cup"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5437285539215686}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.4799999999999999, 0.0, 0.8, 1.0, 0.5, 0.6666666666666666, 0.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-1554", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-6139", "mrqa_triviaqa-validation-6359", "mrqa_triviaqa-validation-1431", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-2617", "mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-2788", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-6747", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-6562", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-1433", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-1941", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-1662", "mrqa_searchqa-validation-4903"], "SR": 0.453125, "CSR": 0.5291559278350515, "EFR": 1.0, "Overall": 0.7335655605670104}, {"timecode": 97, "before_eval_results": {"predictions": ["40 militants and six Pakistan soldiers dead,", "recall", "moore city", "Two United Arab Emirates based companies", "Madrid's Barajas International Airport", "The monarchy's end after 239 years of rule", "kerstin and two of her brothers,", "56,", "\"Freshman Year\" experience", "more than 200.", "Princess Diana", "1825", "maintain an \"aesthetic environment\" and ensure public safety,", "Caylee Anthony", "The elections", "Alexandre Caizergues, of France,", "Arroyo and her husband", "\"It was terrible, it was gut-wrenching just to hear them", "Stratfor,", "India", "permitted under Spanish Football Federation (RFEF) rules.", "Ronald Cummings", "four county GOP chairmen", "Toffelmakaren.", "won the 1994 World Cup,", "researchers", "in his late 30s and early 40s.", "opening of one of the country's most valued cultural institutions --the Louvre.", "3,000", "10 percent", "$250,000", "April 22.", "in the Mediterranean Sea.", "Caster Semenya", "Barack Obama", "flights affected", "to try to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Jeffrey Jamaleldine", "more than two years,", "it was unjustifiable", "up to $50,000", "Citizens", "41,280", "Adam Lambert", "President Clinton.", "allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "Iran's nuclear program.", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "133", "\"He tried to suppress the memories and to live as normal a life as possible;", "helicopters and unmanned aerial vehicles", "Richard Parker", "to harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "electron", "ACC", "groszy", "roman numbers", "Hawaii", "stolperstein", "Paper", "Las Vegas", "the hypothalamus", "the orangutan", "mary blyton"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6033893016705516}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.923076923076923, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666665, 0.0, 0.18181818181818182, 0.11111111111111112, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.16, 0.8, 0.12121212121212123, 1.0, 0.2666666666666667, 0.0, 1.0, 0.30303030303030304, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1451", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-4055", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-582", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7701", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-1532"], "SR": 0.484375, "CSR": 0.5286989795918368, "EFR": 1.0, "Overall": 0.7334741709183674}, {"timecode": 98, "before_eval_results": {"predictions": ["it does not", "Swedish Prime Minister Fredrik Reinfeldt", "it also travels through almost 30 tunnels, including the 6.2-mile Moffat Tunnel, which passes underneath the Continental Divide.", "Friday,", "Heshmatollah Attarzadeh", "Daniel Radcliffe", "Yusuf Saad Kamel", "the hunt for Nazi Gold and possibly the legendary Amber Room", "Susan Atkins,", "Pakistan's High Commission in India", "\"Wicked,\"", "Rolling Stone.", "Tsvangirai", "The oceans", "Asashoryu", "US Airways Flight 1549", "autonomy.", "South Africa's", "\"A good vegan cupcake has the power to transform everything for the better,\"", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "between 1917 and 1924", "Les Bleus", "Too many glass shards left by beer drinkers in the city center,", "unknown,", "40-year-old", "Karthik Rajaram,", "the tiny northern island of Sylt", "in the U.S.", "toffelmakaren.", "Kurt Cobain's", "Turkey,", "Moody and sinister,", "United States, NATO member states, Russia and India", "Ryder Russell,", "spend billions to improve America's education, infrastructure, energy and health care systems.", "Palestinian Islamic Army, which has links to al Qaeda,", "Haeftling,", "off the coast of Dubai with a celebrity-studded gala and a three-day party.", "change course", "Sri Lanka,", "eight-week", "drug cartels", "bench", "Anil Kapoor.", "\"The Closer.\"", "38 feet", "St. Louis.", "64,", "\"totaled,\"", "September 21.", "order", "1977", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "Nasdaq", "Van Rijn", "the parrot", "\"thirtysomething\"", "hulder", "Lucille Ball", "Ziploc", "South Africa", "Ivory", "Kraftwerk"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7586481227106227}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3434", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-452", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-2399"], "SR": 0.6875, "CSR": 0.5303030303030303, "EFR": 1.0, "Overall": 0.733794981060606}, {"timecode": 99, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1232", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13865", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-622", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8108", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1713", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3633", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3883", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.84375, "KG": 0.5328125, "before_eval_results": {"predictions": ["\" Number Ones\"", "\"She wasn't the best \"coach,\"", "$40 billion", "wheelchairs.", "10 municipal police officers", "Lifeway's 100-plus stores nationwide", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "2009", "two years,", "A group of college students of Pakistani background", "gasoline", "Her husband and attorney, James Whitehouse,", "North Korea,", "four Impressionist paintings worth about $163 million (180 million Swiss francs)", "One of Osama bin Laden's sons", "nude beaches.", "1-1", "Rep. Jason Chaffetz", "Martin \"Al\" Culhane,", "President Obama", "A member of the group dubbed the \"Jena 6\"", "Animal Planet", "Minerals Management Service Director Elizabeth Birnbaum", "Mandi Hamlin", "41,280", "Arlington National Cemetery's Section 60,", "Bill Haas", "Caster Semenya", "Michael Jackson", "Kerstin Fritzl,", "Djibouti,", "123 pounds of cocaine and 4.5 pounds of heroin,", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Brazil, Argentina, Mexico, Colombia and Venezuela.", "second-degree aggravated battery.", "two Emmys", "an American pop star's connection to the country", "The forward's lawyer", "Turkish President Abdullah Gul,", "children of street cleaners and firefighters.", "secure more funds from the region.", "five female pastors", "14", "South Carolina Republican Party Chairwoman Karen Floyd", "African National Congress Deputy President", "1913.", "second", "South Korean President Lee Myung-bak,", "Michael Arrington,", "asylum in Britain.", "Indian army", "New York City", "2009", "every colony except Massachusetts", "Phil Mickelson", "Quran", "Shanghai", "University of Columbia", "an advertisement figure", "Aldosterone", "freezing", "Power", "Maine", "November 17, 2017"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7172268098503563}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.9302325581395349, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.2222222222222222, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7692307692307693, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3895", "mrqa_naturalquestions-validation-644", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-397"], "SR": 0.59375, "CSR": 0.5309375000000001, "EFR": 1.0, "Overall": 0.730328125}]}