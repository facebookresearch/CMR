{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=1, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 7740, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Jelme and Bo'orchu", "gauge bosons", "consumer prices", "Albert C. Outler", "a computational problem", "1521", "River Tyne", "Boston", "San Jose Marriott", "illegal boycotts", "Mitochondria", "bilaterians", "Alexandre Yersin", "Methodists today", "Beyonc\u00e9", "the Rhine and its downstream extension", "7\u20134\u20132\u20133", "Horniman Museum", "400 AD to 1914", "early 1526", "The individual is the final judge of right and wrong", "five", "Battle of B\u1ea1ch \u0110\u1eb1ng", "Time Lady", "oxygen-16", "The Day of the Doctor", "Sierra Sky Park", "James Clerk Maxwell", "Bill Clinton", "in areas its forces occupied in Eastern Europe", "20,000", "Queen Elizabeth II", "The Daleks", "gas turbines", "Newton", "Miasma theory", "Ealy", "several medals", "remaining in black and white", "computability theory", "autoimmune", "American Sweetgum", "Pleistocene epoch", "Feynman diagrams", "orange", "oxygen compounds", "four", "Fort Caroline", "counties or powiats", "chemical bonds", "2015", "France's claim to the region was superior to that of the British", "double", "helps many proteins bind the polypeptide", "Islamism", "lines or a punishment essay", "mercuric oxide", "released Islamists from prison and welcomed home exiles", "Washington and Thomas Gage", "The individual", "Bruno Mars", "the public", "Thomas Edison and George Westinghouse", "a freshwater lake"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8040364583333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6848", "mrqa_squad-validation-9923", "mrqa_squad-validation-100", "mrqa_squad-validation-9434", "mrqa_squad-validation-6966", "mrqa_squad-validation-7725", "mrqa_squad-validation-8538", "mrqa_squad-validation-1703", "mrqa_squad-validation-3511", "mrqa_squad-validation-3597", "mrqa_squad-validation-9567", "mrqa_squad-validation-6973", "mrqa_squad-validation-2025"], "SR": 0.796875, "CSR": 0.796875, "EFR": 0.8461538461538461, "Overall": 0.8215144230769231}, {"timecode": 1, "before_eval_results": {"predictions": ["the General Sejm", "232", "New Holland", "the \"Rhine knee\"", "the U.S. South", "the Schmalkaldic League", "January 1985", "an Executive Committee", "King Sancho VI of Navarre", "the Seattle Seahawks", "36", "Chloroplasts", "Sydney", "the Panic of 1901", "Muslim medicine", "the Silk Road", "silicon dioxide", "statocyst", "Several thousand", "the Fourth Intercolonial War and the Great War for the Empire", "medieval", "30\u201360% of Europe's total population", "the laws of physics", "the Ten Commandments", "the San Fernando Valley", "Roger NFL", "Hugh L. Dryden", "metals", "1.5 gigatons", "Denver Broncos", "\u00a31 of capital", "the 2007 Christmas special episode, \"Voyage of the Damned\"", "megaprojects", "1024-bit primes", "the portrait of Fran\u00e7ois, Duc d'Alen\u00e7on by Fran\u00e7ois Clouet, Gaspard Dughet", "rice, coffee, sisal, pyrethrum, corn, and wheat", "the Electorate of Saxony", "ice-sheets", "the lion, leopard, buffalo, rhinoceros, and elephant", "A plea of no contest is sometimes regarded as a compromise between the two", "seven", "Demaryius Thomas", "Napoleon's", "the Santa Clara Marriott", "kinematic measurements", "shipping", "12th", "helical thylakoid model", "the A69", "14%", "Thomas Edison", "Toshiba", "detention", "antigen presentation", "hunter's garb", "The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League", "British Gas plc", "Demaryius Thomas", "alcohol", "Joe Scarborough", "win world titles in four weight classes", "Gareth Jones", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "the Rev. Jacquie Hood Martin"], "metric_results": {"EM": 0.734375, "QA-F1": 0.776943108974359}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.25000000000000006, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-1117", "mrqa_squad-validation-239", "mrqa_squad-validation-3664", "mrqa_squad-validation-85", "mrqa_squad-validation-4482", "mrqa_squad-validation-7758", "mrqa_squad-validation-8978", "mrqa_squad-validation-5490", "mrqa_squad-validation-8446", "mrqa_squad-validation-8278", "mrqa_squad-validation-6914", "mrqa_squad-validation-7155", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-2275"], "SR": 0.734375, "CSR": 0.765625, "retrieved_ids": ["mrqa_squad-train-37825", "mrqa_squad-train-39481", "mrqa_squad-train-470", "mrqa_squad-train-45076", "mrqa_squad-train-32004", "mrqa_squad-train-53321", "mrqa_squad-train-69281", "mrqa_squad-train-70886", "mrqa_squad-train-17860", "mrqa_squad-train-67298", "mrqa_squad-train-77061", "mrqa_squad-train-73203", "mrqa_squad-train-25450", "mrqa_squad-train-66070", "mrqa_squad-train-1001", "mrqa_squad-train-4453", "mrqa_squad-validation-6973", "mrqa_squad-validation-100", "mrqa_squad-validation-3597", "mrqa_squad-validation-9567", "mrqa_squad-validation-9923", "mrqa_squad-validation-2025", "mrqa_squad-validation-9434", "mrqa_squad-validation-6966", "mrqa_squad-validation-7725", "mrqa_squad-validation-1703", "mrqa_squad-validation-3511", "mrqa_squad-validation-6848", "mrqa_squad-validation-8538"], "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 2, "before_eval_results": {"predictions": ["Ugali with vegetables, sour milk, meat, fish or any other stew", "TFEU article 294", "over $20 billion", "was a major source of water pollution", "unity of God", "all war", "1000 and 1900", "Gamal Abdul Nasser", "R\u00fcdesheim am Rhein and Koblenz", "minor", "1162", "lost in the 5th Avenue laboratory fire of March 1895", "ABC Cable News", "22 May 2006", "Germany and Austria", "Golden Gate Bridge", "the Welsh", "acquiring nutrients", "Muslims in the semu class", "the Chinese", "temperature and light)", "12 May 1999", "1852", "the development of safety lamps", "stabilize the rest of the chloroplast genome", "Mongols' extensive West Asian and European contacts", "24%", "Milton Friedman Institute", "Donald Davies", "three", "student motivation and attitudes towards school", "1560", "1891", "Lutheran views", "electron", "fear of their lives", "Science", "John Pell, Lord of Pelham Manor", "Cam Newton", "Osama bin Laden", "international drug suppliers", "President", "expelled Jews", "arid and semi-arid", "Yosemite Freeway", "Annan and his UN-backed panel and African Union chairman Jakaya", "Warsaw Stock Exchange", "the Presiding Officer submits it to the Monarch for royal assent", "certification by a recognized body", "chain or screw stoking mechanism", "1288", "silver", "1947", "1860", "The club will participate in the Premier League, FA Cup, EFL Cup (as holders), UEFA Champions League and UEFA Super Cup.", "Detroit, Michigan", "the Emancipation Proclamation", "26,000", "Pakistan A", "Ricky Skaggs", "Saturday Night Live", "the last living pilot of the X-15 program", "Two new U.S. representatives are teaming up with CNN.com to report their \"Freshman Year\" experience", "250,000"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8538724296536797}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.9523809523809523, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.1818181818181818, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9343", "mrqa_squad-validation-9093", "mrqa_squad-validation-9388", "mrqa_squad-validation-5157", "mrqa_squad-validation-8399", "mrqa_squad-validation-4562", "mrqa_squad-validation-8383", "mrqa_squad-validation-9499", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-368"], "SR": 0.78125, "CSR": 0.7708333333333334, "retrieved_ids": ["mrqa_squad-train-26429", "mrqa_squad-train-39590", "mrqa_squad-train-14129", "mrqa_squad-train-66556", "mrqa_squad-train-44174", "mrqa_squad-train-12431", "mrqa_squad-train-23779", "mrqa_squad-train-38580", "mrqa_squad-train-33253", "mrqa_squad-train-73674", "mrqa_squad-train-56855", "mrqa_squad-train-47333", "mrqa_squad-train-46469", "mrqa_squad-train-84579", "mrqa_squad-train-50819", "mrqa_squad-train-17807", "mrqa_squad-validation-85", "mrqa_squad-validation-1703", "mrqa_squad-validation-9923", "mrqa_squad-validation-6914", "mrqa_squad-validation-2025", "mrqa_squad-validation-7758", "mrqa_squad-validation-1117", "mrqa_squad-validation-3664", "mrqa_hotpotqa-validation-4002", "mrqa_squad-validation-4482", "mrqa_squad-validation-8978", "mrqa_squad-validation-6973", "mrqa_squad-validation-9434", "mrqa_squad-validation-3511", "mrqa_squad-validation-5490", "mrqa_squad-validation-6966"], "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["10 February 1763", "good, clear laws, fairly and democratically", "shaping ideas about the free market", "SAP Center in San Jose", "younger", "university and military academy", "Foreign Protestants Naturalization Act", "inequality", "jigg TV", "three", "permafrost", "Silas B. Cobb", "traditional salute of a knight winning a bout", "Jane Kim", "the Monarch", "lipophilic alkaloid toxins", "one", "William Rainey Harper", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "June", "Hugues plus Eidgenosse", "the 2007 election aftermath", "Ralph Woodward", "Susan Foreman", "clinical pharmacists", "teleforce", "British failures in North America, combined with other failures in the European theater", "300", "66 million years ago", "1894", "commerce, schooling and government", "Krak\u00f3w", "France", "three", "power outage", "easier and more efficient", "Muslim and Chinese", "free trade", "15,100", "Cuba", "brecciated", "28,000", "21 to 11", "Cam Newton", "128,843", "Van Gend en Loos v Nederlandse Administratie der Belastingen", "four years", "Howard Keel", "half of the Pangaea supercontinent", "Mel Gibson", "George Washington", "John Uhler Lemmon III", "six", "ccoli", "Billy Preston", "As the first five series were broadcast during the winter, and many of these were split in half by the Christmas break.", "hedgehog", "George III", "The Time Machine", "the Oil Capital of Europe", "the natural world and mysticism", "more funds", "Brad Blauser", "$1.45 billion"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7729166666666667}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1831", "mrqa_squad-validation-5975", "mrqa_squad-validation-6223", "mrqa_squad-validation-3044", "mrqa_squad-validation-8326", "mrqa_squad-validation-4975", "mrqa_squad-validation-4065", "mrqa_squad-validation-973", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-1517", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-1148"], "SR": 0.734375, "CSR": 0.76171875, "retrieved_ids": ["mrqa_squad-train-2270", "mrqa_squad-train-56696", "mrqa_squad-train-42014", "mrqa_squad-train-35720", "mrqa_squad-train-52326", "mrqa_squad-train-71141", "mrqa_squad-train-76994", "mrqa_squad-train-62166", "mrqa_squad-train-81213", "mrqa_squad-train-24533", "mrqa_squad-train-76816", "mrqa_squad-train-9693", "mrqa_squad-train-69566", "mrqa_squad-train-50396", "mrqa_squad-train-7518", "mrqa_squad-train-56105", "mrqa_hotpotqa-validation-532", "mrqa_squad-validation-9093", "mrqa_squad-validation-3511", "mrqa_squad-validation-6848", "mrqa_squad-validation-5490", "mrqa_newsqa-validation-368", "mrqa_squad-validation-4482", "mrqa_squad-validation-8399", "mrqa_squad-validation-9567", "mrqa_squad-validation-5157", "mrqa_squad-validation-9343", "mrqa_squad-validation-8538", "mrqa_squad-validation-3597", "mrqa_squad-validation-8383", "mrqa_squad-validation-100", "mrqa_hotpotqa-validation-380"], "EFR": 1.0, "Overall": 0.880859375}, {"timecode": 4, "before_eval_results": {"predictions": ["A vote clerk", "$200,000", "carbohydrates", "redistributive taxation", "League of Nations", "two", "\"The Time of the Doctor\"", "Nairobi", "Missy", "free", "7:00 to 9:00 a.m.", "Professor Richard ( Dick) Geary", "lipid monolayer", "2009", "whether a state or threat of war existed", "the courts of member states", "Jin", "greater equality but not per capita income", "John Houghton", "carbohydrates", "both houses of Congress", "America's Funniest Home Videos", "42%", "19", "pharmacists know about the mode of action of a particular drug, and its metabolism and physiological effects on the human body in great detail", "layered basaltic lava flows", "October 2007", "Robert Maynard Hutchins", "Shoushi Li", "clerical marriage", "40%", "Kevin Harlan", "200 Troupes de la marine and 30 Indians", "Half", "Independence Day: Resurgence", "duty", "carbon dioxide", "Worldvision Enterprises", "hunter's garb", "Channel Islands", "\"Blue Harvest\" and \"420\"", "1225", "740 coordinates", "sheep", "AfricanHair", "\"Hey there Delilah, I know... God speed your love to me\"", "Octavian", "Cuba", "Warren E. Burger", "nouns that modify them", "metal", "Abraham Ihle", "a summer workwear rut", "andrew jackson", "a bird in the sky", "henry kelly", "andrew jackson", "Topix", "New York City", "Queen Elizabeth", "Casey Beane", "Islamabad", "Jefferson Memorial", "murder"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6630803571428572}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2491", "mrqa_squad-validation-3932", "mrqa_squad-validation-6284", "mrqa_squad-validation-8234", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-2118", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-14197", "mrqa_naturalquestions-validation-5040", "mrqa_newsqa-validation-839"], "SR": 0.640625, "CSR": 0.7375, "retrieved_ids": ["mrqa_squad-train-2928", "mrqa_squad-train-65677", "mrqa_squad-train-66114", "mrqa_squad-train-695", "mrqa_squad-train-16602", "mrqa_squad-train-63072", "mrqa_squad-train-66992", "mrqa_squad-train-74983", "mrqa_squad-train-80848", "mrqa_squad-train-79347", "mrqa_squad-train-56858", "mrqa_squad-train-39503", "mrqa_squad-train-73602", "mrqa_squad-train-58789", "mrqa_squad-train-80320", "mrqa_squad-train-3036", "mrqa_newsqa-validation-3170", "mrqa_squad-validation-9923", "mrqa_squad-validation-4482", "mrqa_hotpotqa-validation-532", "mrqa_squad-validation-8446", "mrqa_newsqa-validation-2275", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-380", "mrqa_squad-validation-6973", "mrqa_squad-validation-8978", "mrqa_squad-validation-2025", "mrqa_newsqa-validation-368", "mrqa_hotpotqa-validation-2798", "mrqa_squad-validation-8399", "mrqa_triviaqa-validation-1517"], "EFR": 1.0, "Overall": 0.86875}, {"timecode": 5, "before_eval_results": {"predictions": ["Justifying Grace or Accepting Grace", "neuronal dendrites", "an electrical generator", "Doctor Who", "coronary thrombosis", "his grandson", "San Francisco", "consumer prices", "2016", "as soon as they enter into force", "geographic scholars under colonizing empires", "\"hockey stick graph\"", "OpenTV", "intuition", "1720", "around 300", "2001", "The Chase", "cortisol and catecholamines", "Economist Intelligence Unit", "the Decalogue (the Ten Commandments) and the Lord's Prayer", "Paramount Pictures", "Thomas Coke", "The Neighbors", "waldzither", "United States", "\u20ac53,423", "the Helicosproidia", "build their own dedicated networks", "2001", "High Rhine", "Justin Tucker", "Colorado Springs", "1994", "Newton", "26", "University College London", "Jerricho Cotchery", "\"Bean farm\" in upstate New York", "the cube root of a negative number", "Claude Wheeler", "the temperature of a system which can be obtained in a perfect... processes can be determined by having that system brought", "McKenzie Head", "Truman", "pope", "\"Nittany\"", "Charles Lindbergh", "Richardter", "(2008)", "Ian Fleming", "The Caresse D'Eole Secret Duo", "The Thing", "\"Thai\"", "Ely", "Bolshevik-Mensheviks", "Charlotte Russe", "(and later, into the evil G Kessler)", "(Jue) Shikibu", "(200708)", "Ant & Dec", "(exactly 26 letters)", "Cherokee Nation", "$250,000", "UFC 50: The War of '04"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6106094426406926}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999]}}, "before_error_ids": ["mrqa_squad-validation-6267", "mrqa_squad-validation-9865", "mrqa_squad-validation-7827", "mrqa_squad-validation-2391", "mrqa_squad-validation-4874", "mrqa_squad-validation-5499", "mrqa_squad-validation-5214", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-8001", "mrqa_searchqa-validation-8713", "mrqa_triviaqa-validation-3042", "mrqa_hotpotqa-validation-1190"], "SR": 0.578125, "CSR": 0.7109375, "retrieved_ids": ["mrqa_squad-train-9654", "mrqa_squad-train-32592", "mrqa_squad-train-25665", "mrqa_squad-train-65037", "mrqa_squad-train-11603", "mrqa_squad-train-35578", "mrqa_squad-train-26156", "mrqa_squad-train-77258", "mrqa_squad-train-57617", "mrqa_squad-train-46733", "mrqa_squad-train-641", "mrqa_squad-train-51255", "mrqa_squad-train-14056", "mrqa_squad-train-83400", "mrqa_squad-train-28723", "mrqa_squad-train-5329", "mrqa_searchqa-validation-14493", "mrqa_squad-validation-5975", "mrqa_squad-validation-9196", "mrqa_squad-validation-4065", "mrqa_squad-validation-6914", "mrqa_newsqa-validation-368", "mrqa_squad-validation-85", "mrqa_squad-validation-3597", "mrqa_squad-validation-5157", "mrqa_squad-validation-9567", "mrqa_squad-validation-8446", "mrqa_squad-validation-6848", "mrqa_squad-validation-5490", "mrqa_searchqa-validation-9286", "mrqa_squad-validation-9499", "mrqa_searchqa-validation-14197"], "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["ten", "the force of gravity", "Von Miller", "downward pressure on wages", "Catholic", "nine", "11:28", "chest pains", "March 1896", "T cells", "economically", "private networks were often connected via gateways to the public network to reach locations not on the private network", "the college", "Yes\u00fcgei", "research", "toward the end of his life", "Bill Clinton", "Ollie Treiz", "actions-oriented", "San Andreas Fault", "30%\u201350% O2", "a double coronation", "ESPN", "p", "plantar fasciitis", "Peter Capaldi", "6000 Da", "Queen Victoria and Prince Albert", "cortisol and catecholamines", "Manakintown", "1985", "a stronger, tech-oriented economy", "stream capture", "the general number field sieve", "identity documents", "Bronx County District Attorneys Office", "a woman", "Nothing But Love", "a man's lifeless, naked body", "the Pakistani city of Lahore", "military commissions", "Cpl. Richard Findley", "8 p.m. local time Thursday", "the New Jersey Economic Development Authority", "diabetes and hypertension", "6-4", "a city of romance", "Silvan Shalom", "2009", "said, \"It has never been the policy of this president or this administration to torture.\"", "Himalayan", "Wednesday", "2,000 euros", "Siri", "88", "(The Frisky)", "0300", "NATO", "Representatives", "Bangladesh", "Argentinian", "a lion", "http://www.dhs.gov", "Bessarabia"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6528777356902355}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.962962962962963, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6008", "mrqa_squad-validation-3687", "mrqa_squad-validation-9213", "mrqa_squad-validation-6696", "mrqa_squad-validation-3193", "mrqa_squad-validation-2835", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3491", "mrqa_naturalquestions-validation-3569", "mrqa_triviaqa-validation-2240", "mrqa_hotpotqa-validation-985", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-9754", "mrqa_searchqa-validation-8198"], "SR": 0.5625, "CSR": 0.6897321428571428, "retrieved_ids": ["mrqa_squad-train-52952", "mrqa_squad-train-43041", "mrqa_squad-train-61189", "mrqa_squad-train-56711", "mrqa_squad-train-75668", "mrqa_squad-train-29670", "mrqa_squad-train-38065", "mrqa_squad-train-53054", "mrqa_squad-train-51035", "mrqa_squad-train-5287", "mrqa_squad-train-65431", "mrqa_squad-train-26914", "mrqa_squad-train-22054", "mrqa_squad-train-65688", "mrqa_squad-train-6215", "mrqa_squad-train-74208", "mrqa_searchqa-validation-6523", "mrqa_squad-validation-9388", "mrqa_searchqa-validation-14197", "mrqa_squad-validation-85", "mrqa_squad-validation-6973", "mrqa_squad-validation-9923", "mrqa_searchqa-validation-11030", "mrqa_squad-validation-8978", "mrqa_squad-validation-8278", "mrqa_squad-validation-3664", "mrqa_hotpotqa-validation-4018", "mrqa_searchqa-validation-5048", "mrqa_squad-validation-5157", "mrqa_newsqa-validation-368", "mrqa_squad-validation-5975", "mrqa_squad-validation-2491"], "EFR": 1.0, "Overall": 0.8448660714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["Blaydon Race", "The Central Region", "ten", "viral", "100\u2013150", "late 1886", "\u00a334m per year", "Alvaro Martin", "more efficient solutions", "Schedule 5", "BBC 1", "victory at Fort Niagara successfully cut off the French frontier forts further to the west and south.", "cantatas", "January 30", "seven", "\u20ac25,000 per year", "St. Bartholomew's Day massacre", "9th", "principle of equivalence", "The Entertainment Channel", "when they improve society as a whole", "1724", "the incentive for the democratic changes", "St. Johns River", "Life", "priest", "Jan Andrzej Menich", "Jane Kim", "~74,000 (BP = Before Present)", "biomass", "1562", "10 a.m.-1 p.m.", "Empire of the Sun", "22", "four", "Ross Perot", "Baghdad", "$1.45 billion", "her home", "more than 2,800", "Mumbai", "the contraband is then moved through an elaborate series of drop points and usually ferried into the walls of a prison by a guard or trustee", "\"The Dr. Phil Show\"", "July 4", "Evan Bayh", "Hu Jintao", "April 24", "Idriss Deby hopes the journalists and the flight crew will be freed", "scientific reasons", "September 6, 1918", "the first or second week in April", "Pakistan", "Pakistan", "jazz music", "appealed against the punishment", "the used-luxury market", "his former caddy", "Ali", "February", "Mickey's PhilharMagic", "Sugar Ray", "Oedipus Rex", "guitar feedback", "hyperaccumulators"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7624851302700567}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.846153846153846, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.8235294117647058, 1.0, 0.0, 0.25, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10269", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2809", "mrqa_triviaqa-validation-955", "mrqa_hotpotqa-validation-3972"], "SR": 0.671875, "CSR": 0.6875, "retrieved_ids": ["mrqa_squad-train-55617", "mrqa_squad-train-19034", "mrqa_squad-train-65607", "mrqa_squad-train-50933", "mrqa_squad-train-79796", "mrqa_squad-train-39408", "mrqa_squad-train-20197", "mrqa_squad-train-73058", "mrqa_squad-train-67897", "mrqa_squad-train-11875", "mrqa_squad-train-15259", "mrqa_squad-train-74193", "mrqa_squad-train-3856", "mrqa_squad-train-84689", "mrqa_squad-train-55755", "mrqa_squad-train-17687", "mrqa_searchqa-validation-10755", "mrqa_squad-validation-5214", "mrqa_searchqa-validation-8507", "mrqa_triviaqa-validation-5973", "mrqa_searchqa-validation-13996", "mrqa_hotpotqa-validation-532", "mrqa_searchqa-validation-13963", "mrqa_triviaqa-validation-2240", "mrqa_squad-validation-6966", "mrqa_squad-validation-8326", "mrqa_triviaqa-validation-3266", "mrqa_squad-validation-8538", "mrqa_naturalquestions-validation-3569", "mrqa_searchqa-validation-5752", "mrqa_squad-validation-6914", "mrqa_searchqa-validation-14493"], "EFR": 0.9523809523809523, "Overall": 0.8199404761904762}, {"timecode": 8, "before_eval_results": {"predictions": ["\"nolo contendere\"", "5,984", "Jacksonville", "over-fishing", "applied force", "DuMont Television Network", "Amtrak San Joaquins", "teaching", "the traditional salute of a knight winning a bout.", "a ribosome in the cytosol", "British", "1887", "February 7, 2016", "Cargill Meat Solutions and Foster Farms", "Henry Plitt", "1978", "Von Miller", "about 3.5 billion people.", "water", "The Electronic Frontier Foundation", "the Tyndale Bible", "specialised education and training", "Il milione", "Guo Shoujing", "1908", "1560", "reason", "only \"essentials\"", "30,000", "a Taliban member who had come for the talks about peace and reconciliation, and detonated the explosives as he entered the home.", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Keating Holland", "Al Gore.", "Rima Fakih", "his business dealings for possible securities violations", "summer", "the Dalai Lama's current \"middle way approach,\"", "the U.S. Holocaust Memorial Museum", "a body", "Brian Mabry", "completely changed the business of music", "more than 4,000", "consumer confidence", "autonomy", "1996", "1831", "Russia", "a ruthless cartel", "they go into stores.", "Arizona", "Muslim festival of Eid al-Adha.", "BET", "Daniel Cain,", "the use of torture and indefinite detention", "Mugabe's opponents", "The Casalesi Camorra clan", "\"It didn't matter if you were 60, 40 or 20 like I am.", "The bulk of Western Australian ore went to China", "the first locomotive to have reached that speed", "Margaret Thatcher", "Soft Matter", "July 4, 1898", "about 375 miles ( 600 km ) south of Newfoundland", "18th century"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7272113997113997}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1212121212121212, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.22222222222222224, 0.6666666666666666, 0.0, 1.0, 1.0, 0.16]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-8960", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-776", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-1352", "mrqa_naturalquestions-validation-8279", "mrqa_triviaqa-validation-2510", "mrqa_hotpotqa-validation-3165", "mrqa_searchqa-validation-12759", "mrqa_naturalquestions-validation-3505"], "SR": 0.671875, "CSR": 0.6857638888888888, "retrieved_ids": ["mrqa_squad-train-54833", "mrqa_squad-train-34420", "mrqa_squad-train-42650", "mrqa_squad-train-49607", "mrqa_squad-train-65962", "mrqa_squad-train-56034", "mrqa_squad-train-65133", "mrqa_squad-train-50992", "mrqa_squad-train-77480", "mrqa_squad-train-29108", "mrqa_squad-train-20868", "mrqa_squad-train-74943", "mrqa_squad-train-6818", "mrqa_squad-train-62610", "mrqa_squad-train-59612", "mrqa_squad-train-73128", "mrqa_squad-validation-8383", "mrqa_newsqa-validation-3838", "mrqa_searchqa-validation-14936", "mrqa_newsqa-validation-2790", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-8507", "mrqa_squad-validation-8446", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-2275", "mrqa_triviaqa-validation-1731", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2580", "mrqa_squad-validation-6973", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-3796"], "EFR": 0.9523809523809523, "Overall": 0.8190724206349206}, {"timecode": 9, "before_eval_results": {"predictions": ["Yassa", "An attorney", "The Quasiturbine", "s = \u22122, \u22124,...", "creates immunological memory", "three", "coal", "pseudorandom", "average teacher salaries", "the same message routing methodology as developed by Baran.", "Mongols beyond the Middle Kingdom saw them as too Chinese.", "Temecula and Murrieta", "antibodies", "Korean", "the revolution could only succeed in Russia as part of a world revolution.", "Williamite", "the Horn of Africa", "lipid monolayer", "Hymn for the Weekend", "Colonel Monckton", "Gymnosperms", "7 January 1943", "1.7 million", "ABC News Now", "Boomer Esiason and Dan Fouts", "Eintracht Frankfurt", "former U.S. secretary of state", "1,073 immigration detainees", "10", "nearly 28 years of rule.", "Islamabad", "her father's", "90", "Zimbabwe President Robert Mugabe", "Doral", "terminal brain cancer", "death squad killings", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Leaders of more than 30 Latin American and Caribbean nations", "the Beatles", "Citizens are picking members of the lower house of parliament,", "the 6.2-mile Moffat Tunnel,", "(l-r)", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "led the weekend box office,", "Some truly mind-blowing structures are being planned for the Middle East.", "DBG", "United Arab Emirates", "Barack Obama", "Polo because \"it was the sport of kings.", "the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating.", "made out of either heavy flannel or wool -- fabrics that would not be transparent when wet --", "Aniston, Demi Moore and Alicia Keys", "National Infrastructure Program", "the widows of John Lennon and George Harrison,", "Afghanistan", "William Strauss and Neil Howe", "Transvaginal ultrasonography", "Wikia", "Ben Hogan", "400", "\"Nebo Zovyot\"", "the Chesapeake Bay", "13"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6528341792527648}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.36363636363636365, 1.0, 0.125, 1.0, 0.6153846153846153, 0.0, 0.0, 0.631578947368421, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.9, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10128", "mrqa_squad-validation-9912", "mrqa_squad-validation-8880", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2128", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2016", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-2219"], "SR": 0.5625, "CSR": 0.6734375, "retrieved_ids": ["mrqa_squad-train-11039", "mrqa_squad-train-34605", "mrqa_squad-train-4436", "mrqa_squad-train-1038", "mrqa_squad-train-45873", "mrqa_squad-train-36085", "mrqa_squad-train-79238", "mrqa_squad-train-23729", "mrqa_squad-train-57612", "mrqa_squad-train-18757", "mrqa_squad-train-33857", "mrqa_squad-train-70007", "mrqa_squad-train-38055", "mrqa_squad-train-72776", "mrqa_squad-train-38771", "mrqa_squad-train-61735", "mrqa_squad-validation-9343", "mrqa_squad-validation-1117", "mrqa_squad-validation-9196", "mrqa_newsqa-validation-2983", "mrqa_squad-validation-9923", "mrqa_squad-validation-6008", "mrqa_searchqa-validation-11883", "mrqa_naturalquestions-validation-5040", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-2405", "mrqa_newsqa-validation-3012", "mrqa_searchqa-validation-14493", "mrqa_squad-validation-2491", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-574", "mrqa_squad-validation-8538"], "EFR": 1.0, "Overall": 0.83671875}, {"timecode": 10, "UKR": 0.798828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3412", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-100", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1886", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-8001", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9754", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10128", "mrqa_squad-validation-10155", "mrqa_squad-validation-10162", "mrqa_squad-validation-10167", "mrqa_squad-validation-1018", "mrqa_squad-validation-10198", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10269", "mrqa_squad-validation-10272", "mrqa_squad-validation-1029", "mrqa_squad-validation-103", "mrqa_squad-validation-10310", "mrqa_squad-validation-10315", "mrqa_squad-validation-10326", "mrqa_squad-validation-10345", "mrqa_squad-validation-1036", "mrqa_squad-validation-10380", "mrqa_squad-validation-10413", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10476", "mrqa_squad-validation-1048", "mrqa_squad-validation-1053", "mrqa_squad-validation-1088", "mrqa_squad-validation-1097", "mrqa_squad-validation-1119", "mrqa_squad-validation-1131", "mrqa_squad-validation-1197", "mrqa_squad-validation-1222", "mrqa_squad-validation-1231", "mrqa_squad-validation-1255", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-139", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1521", "mrqa_squad-validation-1537", "mrqa_squad-validation-1546", "mrqa_squad-validation-1561", "mrqa_squad-validation-1592", "mrqa_squad-validation-1611", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1831", "mrqa_squad-validation-1834", "mrqa_squad-validation-1876", "mrqa_squad-validation-1940", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-20", "mrqa_squad-validation-2048", "mrqa_squad-validation-2048", "mrqa_squad-validation-2087", "mrqa_squad-validation-2116", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2188", "mrqa_squad-validation-2235", "mrqa_squad-validation-2250", "mrqa_squad-validation-2374", "mrqa_squad-validation-239", "mrqa_squad-validation-2391", "mrqa_squad-validation-2403", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_squad-validation-2447", "mrqa_squad-validation-2462", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2580", "mrqa_squad-validation-2640", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2723", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-2797", "mrqa_squad-validation-282", "mrqa_squad-validation-2835", "mrqa_squad-validation-2848", "mrqa_squad-validation-2870", "mrqa_squad-validation-2873", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-30", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3048", "mrqa_squad-validation-3084", "mrqa_squad-validation-3086", "mrqa_squad-validation-3141", "mrqa_squad-validation-316", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3299", "mrqa_squad-validation-3304", "mrqa_squad-validation-3309", "mrqa_squad-validation-3319", "mrqa_squad-validation-3358", "mrqa_squad-validation-3368", "mrqa_squad-validation-3390", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3511", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3849", "mrqa_squad-validation-3932", "mrqa_squad-validation-3948", "mrqa_squad-validation-4032", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4165", "mrqa_squad-validation-4176", "mrqa_squad-validation-4186", "mrqa_squad-validation-4248", "mrqa_squad-validation-4265", "mrqa_squad-validation-4274", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4413", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4482", "mrqa_squad-validation-4488", "mrqa_squad-validation-4493", "mrqa_squad-validation-4562", "mrqa_squad-validation-4611", "mrqa_squad-validation-4623", "mrqa_squad-validation-4627", "mrqa_squad-validation-465", "mrqa_squad-validation-4698", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-4971", "mrqa_squad-validation-4976", "mrqa_squad-validation-501", "mrqa_squad-validation-506", "mrqa_squad-validation-5079", "mrqa_squad-validation-5113", "mrqa_squad-validation-5133", "mrqa_squad-validation-5150", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5214", "mrqa_squad-validation-5230", "mrqa_squad-validation-5295", "mrqa_squad-validation-5343", "mrqa_squad-validation-5355", "mrqa_squad-validation-5457", "mrqa_squad-validation-5478", "mrqa_squad-validation-5490", "mrqa_squad-validation-5499", "mrqa_squad-validation-55", "mrqa_squad-validation-5544", "mrqa_squad-validation-5563", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5642", "mrqa_squad-validation-5664", "mrqa_squad-validation-567", "mrqa_squad-validation-5698", "mrqa_squad-validation-5708", "mrqa_squad-validation-5762", "mrqa_squad-validation-5820", "mrqa_squad-validation-5835", "mrqa_squad-validation-586", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5978", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6008", "mrqa_squad-validation-6011", "mrqa_squad-validation-6079", "mrqa_squad-validation-6109", "mrqa_squad-validation-6124", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-616", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6223", "mrqa_squad-validation-6247", "mrqa_squad-validation-6267", "mrqa_squad-validation-6273", "mrqa_squad-validation-6284", "mrqa_squad-validation-6350", "mrqa_squad-validation-6362", "mrqa_squad-validation-6382", "mrqa_squad-validation-6421", "mrqa_squad-validation-6452", "mrqa_squad-validation-6475", "mrqa_squad-validation-6509", "mrqa_squad-validation-6535", "mrqa_squad-validation-6561", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6643", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6869", "mrqa_squad-validation-6879", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-7021", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7062", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7250", "mrqa_squad-validation-7306", "mrqa_squad-validation-7474", "mrqa_squad-validation-7521", "mrqa_squad-validation-7540", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7591", "mrqa_squad-validation-7592", "mrqa_squad-validation-7598", "mrqa_squad-validation-7653", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7733", "mrqa_squad-validation-7738", "mrqa_squad-validation-7751", "mrqa_squad-validation-7758", "mrqa_squad-validation-7775", "mrqa_squad-validation-778", "mrqa_squad-validation-7827", "mrqa_squad-validation-7842", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7937", "mrqa_squad-validation-7941", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8023", "mrqa_squad-validation-8028", "mrqa_squad-validation-8066", "mrqa_squad-validation-813", "mrqa_squad-validation-8132", "mrqa_squad-validation-8174", "mrqa_squad-validation-8213", "mrqa_squad-validation-8221", "mrqa_squad-validation-8222", "mrqa_squad-validation-824", "mrqa_squad-validation-8298", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8436", "mrqa_squad-validation-8446", "mrqa_squad-validation-8458", "mrqa_squad-validation-8466", "mrqa_squad-validation-8475", "mrqa_squad-validation-85", "mrqa_squad-validation-8505", "mrqa_squad-validation-8507", "mrqa_squad-validation-8533", "mrqa_squad-validation-8538", "mrqa_squad-validation-855", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8606", "mrqa_squad-validation-8636", "mrqa_squad-validation-8656", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8790", "mrqa_squad-validation-8790", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8836", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8880", "mrqa_squad-validation-890", "mrqa_squad-validation-8941", "mrqa_squad-validation-8960", "mrqa_squad-validation-8962", "mrqa_squad-validation-8978", "mrqa_squad-validation-9008", "mrqa_squad-validation-9101", "mrqa_squad-validation-9144", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9297", "mrqa_squad-validation-9308", "mrqa_squad-validation-9343", "mrqa_squad-validation-9388", "mrqa_squad-validation-9431", "mrqa_squad-validation-9470", "mrqa_squad-validation-9499", "mrqa_squad-validation-9567", "mrqa_squad-validation-9638", "mrqa_squad-validation-9661", "mrqa_squad-validation-9692", "mrqa_squad-validation-973", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9865", "mrqa_squad-validation-9912", "mrqa_squad-validation-9923", "mrqa_squad-validation-9935", "mrqa_squad-validation-9975", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-955"], "OKR": 0.923828125, "KG": 0.45078125, "before_eval_results": {"predictions": ["France", "Turkey", "League of Augsburg", "Marburg Colloquy", "1951", "nearly three hundred years", "enter the priesthood", "32%", "Southwest Fresno", "receptions, gatherings or exhibition purposes", "marry one of his wife's ladies-in-waiting", "Prime numbers", "six membraned chloroplast", "Timucua people", "capturing prey", "Presiding Officer", "1521", "around 5 million,", "a supervisory church body", "member state could \"not rely, as against individuals, on its own failure to perform the obligations which the Directive entails.\"", "Ed Asner", "bitstrings", "\"Quiet Nights,\"", "Friday,", "\"momentous discovery\"", "We Found Love", "five", "\"It was a comment that shouldn't have been made and certainly one that he wished he didn't make.", "Karl Kr\u00f8yer", "Austin Wuennenberg", "133", "the House of Blues", "\"Mad Men\"", "the area where the single-engine Cessna 206 went down", "water continues flow through the river channel and not spread out over land.", "Barney Stinson", "Mutassim", "Klein and Arnold", "\"the incitement of sectarian hatred or involved in the acts of violence\"", "ambassadors", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.", "the picturesque Gamla Vaster neighborhood", "17", "said she also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.", "women and breast cancer", "\"Memorial Day weekend brings even more activity and more visitors.", "e-mails", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "Fullerton, California", "central London", "45 minutes, five days a week.", "past and his future", "Arab Emirates", "Lashkar-e-Tayyiba", "July", "a U.S. military helicopter", "San Antonio", "February 6, 2005", "Falkland Islands", "hydrochloric acid and sodium hydroxide", "\"Lions for Lambs\"", "Dana Andrews", "160 SAT points or 4 ACT points on your score,", "funk rock band"], "metric_results": {"EM": 0.609375, "QA-F1": 0.677404695998446}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.23076923076923078, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.08333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9047", "mrqa_squad-validation-2468", "mrqa_squad-validation-4272", "mrqa_squad-validation-1676", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-2195", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-976", "mrqa_newsqa-validation-1170", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-5158", "mrqa_hotpotqa-validation-1398", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-15716"], "SR": 0.609375, "CSR": 0.6676136363636364, "retrieved_ids": ["mrqa_squad-train-50604", "mrqa_squad-train-14548", "mrqa_squad-train-51253", "mrqa_squad-train-80292", "mrqa_squad-train-280", "mrqa_squad-train-29704", "mrqa_squad-train-16274", "mrqa_squad-train-13651", "mrqa_squad-train-67017", "mrqa_squad-train-43055", "mrqa_squad-train-6120", "mrqa_squad-train-65240", "mrqa_squad-train-39310", "mrqa_squad-train-61723", "mrqa_squad-train-29600", "mrqa_squad-train-41586", "mrqa_squad-validation-8383", "mrqa_searchqa-validation-2118", "mrqa_squad-validation-6267", "mrqa_newsqa-validation-1352", "mrqa_triviaqa-validation-955", "mrqa_squad-validation-9093", "mrqa_newsqa-validation-3765", "mrqa_naturalquestions-validation-3569", "mrqa_newsqa-validation-1577", "mrqa_searchqa-validation-10525", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-3770", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-2135", "mrqa_newsqa-validation-3518", "mrqa_squad-validation-9343"], "EFR": 1.0, "Overall": 0.7682102272727273}, {"timecode": 11, "before_eval_results": {"predictions": ["manned lunar landings", "The Better Jacksonville Plan", "the perceived difficulty of its tune", "zero", "Hymn for the Weekend", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "\"Blue Harvest\" and \"420\"", "$105 billion", "on the road back to Samarkand", "27", "an occupancy permit", "1774", "circuit switching", "TEU articles 4 and 5", "ular plastoglobulus", "Pakistan", "San Jose State", "the mouth and pharynx", "allowed local area networks to be established ad hoc without the requirement for a centralized modem or server", "erosion", "Jada,", "African National Congress Deputy President", "12-hour-plus shifts", "Kit of Elsinore", "2,000 people", "bullet-riddle body", "Kim Clijsters", "not feel Misty Cummings has told them everything she knows.", "Sub-Saharan Africa", "short- and medium-range missile tests,", "the man was dead", "Dr. Jennifer Arnold and husband Bill Klein,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "not the one to be dealt with by us.", "his parents", "International Polo Club", "an antihistamine and an epinephrine auto-injector", "UNICEF", "Casey Anthony,", "a skilled hacker", "Leo Frank,", "in the west African nation", "Cash for Clunkers", "Turkey", "eco videos", "the shipping industry", "41,280 pounds", "Natalie Cole", "not", "North Carolina", "at least 25 dead", "Rolling Stone", "partially submerged in a stream", "\"Stagecoach\"", "Chinese nationals", "The Man", "to ensure party discipline in a legislature", "The wood", "La Toya Jackson", "Lincoln Memorial University", "Ant Timpson, Ted Geoghegan and Tim League.", "'s", "ashes", "over a 20 - year period"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7069369649447774}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.25, 0.4, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.30769230769230765, 0.0, 0.6666666666666666, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_squad-validation-8786", "mrqa_squad-validation-4789", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3035", "mrqa_naturalquestions-validation-8434", "mrqa_triviaqa-validation-3246", "mrqa_searchqa-validation-16774", "mrqa_naturalquestions-validation-7253"], "SR": 0.546875, "CSR": 0.6575520833333333, "retrieved_ids": ["mrqa_squad-train-29517", "mrqa_squad-train-25972", "mrqa_squad-train-80183", "mrqa_squad-train-28356", "mrqa_squad-train-36765", "mrqa_squad-train-75483", "mrqa_squad-train-66899", "mrqa_squad-train-43547", "mrqa_squad-train-80333", "mrqa_squad-train-7848", "mrqa_squad-train-56238", "mrqa_squad-train-73605", "mrqa_squad-train-68409", "mrqa_squad-train-83713", "mrqa_squad-train-80441", "mrqa_squad-train-52717", "mrqa_newsqa-validation-1012", "mrqa_triviaqa-validation-1731", "mrqa_naturalquestions-validation-8279", "mrqa_searchqa-validation-15779", "mrqa_newsqa-validation-1311", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-8713", "mrqa_squad-validation-1703", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-3012", "mrqa_squad-validation-85", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3100", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-3972", "mrqa_squad-validation-7827"], "EFR": 1.0, "Overall": 0.7661979166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["green spaces", "to make the hosts responsible for reliable delivery of data, rather than the network itself,", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "15 June 1899", "Ismail El Gizouli", "by department", "Rhine-Ruhr region", "trans-Atlantic wireless telecommunications facility known as Wardenclyffe", "bachelor's degree", "two", "Death wish Coffee", "The French Protestant Church of London", "Philip Webb and William Morris", "Francisco de Orellana", "missing self", "Bruno Mars", "Sybilla of Normandy", "420,000", "Continental drift", "primarily occurs in the liver and kidneys", "InterContinental Hotels Group", "Walter Brennan", "Warren Hastings", "Poems : Series 1", "April 1917", "the New Testament", "Aristotle", "4", "a No. 16 seed", "2015", "2018", "help batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship", "Zoe Zebra", "16 June", "Alaska", "Thomas Jefferson", "India", "HTTP / 1.1", "Labour Party", "Roger Dean Stadium", "May 2002", "light skin and blue, grey or green eyes", "a ranking used in combat sports", "portal tomb", "the team", "the Isthmus of Corinth", "159", "Missouri River", "Brazil", "17", "Kim Basinger", "General Armitage Hux", "on the chest, back, shoulders, torso and / or legs", "Internal epithelia", "religious Hindu musical theatre styles", "Corey Pavin", "duck", "Super Junior", "Nanyue", "18", "Iran", "power play", "What Happened to Anna K.", "\"howling\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6262898733211233}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.24000000000000002, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1040", "mrqa_squad-validation-4786", "mrqa_squad-validation-1384", "mrqa_squad-validation-5422", "mrqa_naturalquestions-validation-692", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-6137", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-976", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-4596", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-6195", "mrqa_searchqa-validation-3679", "mrqa_searchqa-validation-9108"], "SR": 0.546875, "CSR": 0.6490384615384616, "retrieved_ids": ["mrqa_squad-train-28766", "mrqa_squad-train-49366", "mrqa_squad-train-20774", "mrqa_squad-train-37539", "mrqa_squad-train-61684", "mrqa_squad-train-42814", "mrqa_squad-train-51303", "mrqa_squad-train-1889", "mrqa_squad-train-60714", "mrqa_squad-train-45664", "mrqa_squad-train-48136", "mrqa_squad-train-36755", "mrqa_squad-train-56961", "mrqa_squad-train-18671", "mrqa_squad-train-18233", "mrqa_squad-train-32733", "mrqa_triviaqa-validation-3266", "mrqa_squad-validation-2419", "mrqa_searchqa-validation-14197", "mrqa_newsqa-validation-3678", "mrqa_squad-validation-1676", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-2268", "mrqa_squad-validation-6973", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3457", "mrqa_searchqa-validation-15779", "mrqa_squad-validation-8399", "mrqa_newsqa-validation-3100", "mrqa_squad-validation-2491"], "EFR": 0.9655172413793104, "Overall": 0.7575986405835544}, {"timecode": 13, "before_eval_results": {"predictions": ["Ominde Commission", "2,000", "2005", "the university's off-campus rental policies", "seven", "Thomas Edison and George Westinghouse", "southern Suriname", "sell prescription drugs without requiring a prescription", "automobiles", "was the first NASA scientist astronaut to fly in space", "Cam Newton", "KGPE", "difference in potential energy", "prime elements and prime ideals", "Julia Butterfly Hill", "five", "Surficial", "Vienna", "40", "President Galtieri", "Spice Girls", "hose", "Sandi Toksvig", "Salvador Allende", "Paris", "Arkansas", "\"The Blind Side,\"", "a negative effect on your quality of life", "Dennis Potter", "Burma", "Peregrines", "peppers", "a wild vagrant", "MauritaniaMauritania", "piano", "Angus Hudson", "James Carville", "Charlie Sheen", "perry pear", "mammal", "Concepcion", "Republic of Upper Volta", "Laurie Lee", "Karl Marx and Friedrich Engels", "John Mortimer", "Beaujolais", "Humphrey Bogart", "Guns N' Roses", "Kansas", "Amy", "Carl Sagan and his wife and co-writer, Ann Druyan", "Alex Turner", "a St. Tropez drag-show nightclub", "commitment", "his brother", "2001 -- 2002 season", "\"Back to December\"", "Michael Lewis Greenwell", "an independent homeland", "a pool of blood beneath his head.", "(Attab)", "an obstetrician", "an Academy Award in the category Best Sound", "International Boxing Federation"], "metric_results": {"EM": 0.625, "QA-F1": 0.7246287719633308}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-3899", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-3507", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-530", "mrqa_searchqa-validation-3375", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-47"], "SR": 0.625, "CSR": 0.6473214285714286, "retrieved_ids": ["mrqa_squad-train-77446", "mrqa_squad-train-34855", "mrqa_squad-train-46423", "mrqa_squad-train-5344", "mrqa_squad-train-66304", "mrqa_squad-train-43822", "mrqa_squad-train-74452", "mrqa_squad-train-67609", "mrqa_squad-train-86257", "mrqa_squad-train-77751", "mrqa_squad-train-7917", "mrqa_squad-train-37455", "mrqa_squad-train-42394", "mrqa_squad-train-59888", "mrqa_squad-train-22768", "mrqa_squad-train-70586", "mrqa_newsqa-validation-3012", "mrqa_searchqa-validation-6195", "mrqa_squad-validation-9213", "mrqa_newsqa-validation-2580", "mrqa_naturalquestions-validation-8279", "mrqa_newsqa-validation-839", "mrqa_squad-validation-9865", "mrqa_newsqa-validation-1170", "mrqa_squad-validation-973", "mrqa_squad-validation-4786", "mrqa_squad-validation-7725", "mrqa_squad-validation-10128", "mrqa_squad-validation-4326", "mrqa_hotpotqa-validation-985", "mrqa_newsqa-validation-3979", "mrqa_hotpotqa-validation-4002"], "EFR": 0.9583333333333334, "Overall": 0.7558184523809524}, {"timecode": 14, "before_eval_results": {"predictions": ["Iberia", "contemporary accounts were exaggerations", "lectures", "Victoria", "erosion", "Cricket", "2", "Gabriel Zwilling", "The Day of the Doctor", "confirmation", "city of Deabolis", "Wijk bij Duurstede", "June 6, 1951", "24 September 2007", "18", "1648 - 51 war", "Teri Garr", "Malayalam", "lamb", "the red - bed country of its watershed", "tetranucleotide hypothesis", "Steve Russell", "Africa", "The first Twenty20 match held at Lord's", "Dalveer Bhandari", "boy", "Humphrey Bogart", "Michael Jackson and Lionel Richie", "6 March 1983", "11 : 15 p.m.", "Glenn Close", "Andreas Vesalius", "1959", "in a forest", "In the year 2026", "1901", "Gorakhpur", "the U.S. Electoral College", "1834", "Indian Standard Time", "New York University", "1971", "Sophia Akuffo", "Death Eaters", "2", "April 29, 2009", "regulatory", "restarting play after a minor infringement", "the Infamy Speech of US President Franklin D. Roosevelt", "Wembley Stadium", "1992", "Jonathan Cheban", "blue", "# 4 School of Public Health in the country", "henry", "Arabah", "May 27, 2016", "Frigate", "the HPV (human papillomavirus) vaccine", "Lonnie", "fish", "Felicity", "Lake Placid, New York", "Russell T Davies"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6218263507326007}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1188", "mrqa_squad-validation-1076", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9306", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3977", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-14433", "mrqa_hotpotqa-validation-2357"], "SR": 0.53125, "CSR": 0.6395833333333334, "retrieved_ids": ["mrqa_squad-train-56548", "mrqa_squad-train-39852", "mrqa_squad-train-59709", "mrqa_squad-train-84271", "mrqa_squad-train-85696", "mrqa_squad-train-58970", "mrqa_squad-train-40401", "mrqa_squad-train-69143", "mrqa_squad-train-35766", "mrqa_squad-train-19444", "mrqa_squad-train-79674", "mrqa_squad-train-5937", "mrqa_squad-train-75682", "mrqa_squad-train-74858", "mrqa_squad-train-61788", "mrqa_squad-train-24679", "mrqa_squad-validation-6696", "mrqa_newsqa-validation-776", "mrqa_triviaqa-validation-4957", "mrqa_squad-validation-3664", "mrqa_hotpotqa-validation-532", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-5048", "mrqa_naturalquestions-validation-3505", "mrqa_newsqa-validation-809", "mrqa_squad-validation-6973", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3491", "mrqa_triviaqa-validation-3246", "mrqa_searchqa-validation-14493"], "EFR": 0.9333333333333333, "Overall": 0.7492708333333333}, {"timecode": 15, "before_eval_results": {"predictions": ["The Writers Guild of America", "every five years", "NFIL3", "826", "Treaty of Logstown", "mid-Cambrian", "Cabot Science Library, Lamont Library, and Widener Library", "Palestine", "PNU and ODM camps", "The waxy cuticle of many leaves", "2,200", "KMBC-TV and KQTV", "whether he stood by their contents", "cloven", "Hudson River", "mutualism", "Helen Keller", "Farmer's Almanac", "blue whale", "Sally Field", "1908-1993", "Captain Nemo", "The Wizard of Oz", "jedoublen/jeopardy", "Agamemnon", "jedoublen/jeopardy", "holiday weekend staple", "rice", "Boeing Company", "Saskatchewan", "Steve Kuberski", "900 rebels", "between four and eight rounds,", "Willa Cather", "furniture", "keith urban", "French", "hanter", "explosive devices (IED)", "drowsiness", "change to a boat", "parrots, gorillas, and tarantulas", "Rick Springfield", "jabal", "isopropyl alcohol", "valley", "Custer", "an amputated arm", "kabbalah", "palomino", "piedmont glaciers", "French Lgion d' Honneur (Legion of Honour)", "Parsifal", "Rent: Reinventing the Musical Genre Through the Limitations of", "Camping World Stadium in Orlando, Florida", "John Cooper Clarke", "Woodrow Wilson", "Nicola Adams", "1943", "Pearl Jam", "\"vulnerable to disruption,\"", "there is not a mechanism at the federal level to ensure that drivers comply.", "two", "16\u201321"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5547475961538462}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4802", "mrqa_squad-validation-6435", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-9570", "mrqa_searchqa-validation-8796", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-10319", "mrqa_searchqa-validation-4536", "mrqa_searchqa-validation-1446", "mrqa_searchqa-validation-1715", "mrqa_searchqa-validation-16715", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-2200", "mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-576", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2358", "mrqa_hotpotqa-validation-5438"], "SR": 0.484375, "CSR": 0.6298828125, "retrieved_ids": ["mrqa_squad-train-71887", "mrqa_squad-train-33495", "mrqa_squad-train-80839", "mrqa_squad-train-62485", "mrqa_squad-train-9537", "mrqa_squad-train-50805", "mrqa_squad-train-64685", "mrqa_squad-train-52062", "mrqa_squad-train-48584", "mrqa_squad-train-43830", "mrqa_squad-train-7886", "mrqa_squad-train-73360", "mrqa_squad-train-40511", "mrqa_squad-train-25897", "mrqa_squad-train-9919", "mrqa_squad-train-4951", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-13367", "mrqa_hotpotqa-validation-380", "mrqa_squad-validation-85", "mrqa_newsqa-validation-2983", "mrqa_hotpotqa-validation-3972", "mrqa_newsqa-validation-1170", "mrqa_triviaqa-validation-2016", "mrqa_newsqa-validation-733", "mrqa_squad-validation-7827", "mrqa_newsqa-validation-832", "mrqa_squad-validation-1188", "mrqa_squad-validation-10269", "mrqa_newsqa-validation-2809", "mrqa_searchqa-validation-11030", "mrqa_hotpotqa-validation-4596"], "EFR": 1.0, "Overall": 0.7606640625}, {"timecode": 16, "before_eval_results": {"predictions": ["within the Church of England", "Lenin", "qualified majority vote", "36", "Brough Park in Byker", "2012", "Stress", "the results of a measurement are now sometimes \"quantized\"", "Jonathan Stewart", "George Westinghouse", "human", "the limited forces it had in New France", "Aquitaine", "the Lord of the Rings", "Florida", "the Tongass National Forest", "the Netherlands", "Marriott International", "drink wine", "white face", "(J. Bullock)", "the Sons of Liberty", "movie house", "National Security Agency", "(America) Ferrera", "a vagrant opinion", "the Key deer", "the All-New Blue Ribbon Cookbook", "flowers", "Arizona", "Amy Fisher", "A Portrait of the Artist as a Young Man", "a tumbler", "guttural", "polio", "Meg Tilly", "The Mausoleum", "King George III", "Annie Braddock", "the Sacraments", "(re)", "dark places", "(Jerry) Maguire", "Sister Wendy", "Ferris B Mueller's", "the catcher", "\"Bewitched, Bothered and Bewildered,\"", "the Situation Room", "Samuel Goldwyn", "Annika Sorenstam", "orange", "Thurman Munson", "Washington, D.C.,", "flip-flops", "The India and Pakistan Border", "1783", "jordan cheese", "Ray Robinson", "McComb, Mississippi", "Dorothy Zbornak", "Tutsi ethnic minority", "last week.", "18", "\"Twilight\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5907118055555556}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4216", "mrqa_squad-validation-5456", "mrqa_squad-validation-10388", "mrqa_squad-validation-10158", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-1961", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-10263", "mrqa_searchqa-validation-11531", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16832", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2618", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9957", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-2627", "mrqa_hotpotqa-validation-959", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-608"], "SR": 0.484375, "CSR": 0.6213235294117647, "retrieved_ids": ["mrqa_squad-train-420", "mrqa_squad-train-72840", "mrqa_squad-train-48671", "mrqa_squad-train-43537", "mrqa_squad-train-43682", "mrqa_squad-train-39991", "mrqa_squad-train-70999", "mrqa_squad-train-67997", "mrqa_squad-train-80158", "mrqa_squad-train-69780", "mrqa_squad-train-85486", "mrqa_squad-train-54625", "mrqa_squad-train-73032", "mrqa_squad-train-46564", "mrqa_squad-train-14738", "mrqa_squad-train-5096", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-15779", "mrqa_newsqa-validation-868", "mrqa_hotpotqa-validation-4596", "mrqa_squad-validation-6973", "mrqa_naturalquestions-validation-922", "mrqa_squad-validation-2835", "mrqa_searchqa-validation-2475", "mrqa_newsqa-validation-2195", "mrqa_squad-validation-9923", "mrqa_squad-validation-2391", "mrqa_searchqa-validation-9108", "mrqa_newsqa-validation-538", "mrqa_naturalquestions-validation-1863", "mrqa_searchqa-validation-16715", "mrqa_newsqa-validation-1837"], "EFR": 1.0, "Overall": 0.7589522058823529}, {"timecode": 17, "before_eval_results": {"predictions": ["49\u201315", "113", "protein structure prediction", "Deformational events", "August 2004", "Department of State Affairs", "Prague", "they stepped across the \"line\" and were immediately arrested", "governments", "M\u00f6ngke Khan", "by using net wealth (adding up assets and subtracting debts),", "Tiger Woods", "snow", "Cyril J. O'Brien", "Romeo and Juliet", "Jane Addams", "Rand McNally", "Robert M.", "the pound sterling", "Auguste Rodin", "Argentina", "Sherlock Holmes", "the Taj Mittal", "the trampoline", "a axe", "a cereal", "Constantine", "Daniel Inouye", "krypton", "a monastery", "Kung Fu", "the Star-Crossed Stars of Showgirls, Crossroads, and Glitter", "a cocktail", "the Jungle of Cities", "Milwaukee", "silver", "Bangkok", "the Soviet Union", "a Scotch Cocktail", "a young Belfast-born actor and director", "a doses", "Frank Sinatra", "Christopher Columbus", "the King of the Hill", "Private Benjamin", "Stephen King", "George Gordon Byron", "Japan", "Joan of Arc", "Jaguar", "the Oompa-Loompas", "a dog", "R. Stanton Avery", "at U.S. Bank Stadium in Minneapolis, Minnesota", "The Golden Gate Bridge", "Jim hacker", "Charlie Cotton ( Christopher Hancock)", "May 10, 1976", "VNO, ICAO: EYVI", "Cipro, Levaquin, Avelox, Noroxin and Floxin", "murder", "Parlophone Records", "five", "Nazi concentration camps"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6424727182539682}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6702", "mrqa_squad-validation-7554", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-2546", "mrqa_searchqa-validation-4402", "mrqa_naturalquestions-validation-5674", "mrqa_triviaqa-validation-6228", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-976", "mrqa_newsqa-validation-57", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-5499"], "SR": 0.53125, "CSR": 0.6163194444444444, "retrieved_ids": ["mrqa_squad-train-53102", "mrqa_squad-train-10227", "mrqa_squad-train-34251", "mrqa_squad-train-50172", "mrqa_squad-train-69109", "mrqa_squad-train-23428", "mrqa_squad-train-53221", "mrqa_squad-train-19366", "mrqa_squad-train-52943", "mrqa_squad-train-25049", "mrqa_squad-train-53977", "mrqa_squad-train-4599", "mrqa_squad-train-61690", "mrqa_squad-train-48397", "mrqa_squad-train-3577", "mrqa_squad-train-33939", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-11030", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-6014", "mrqa_newsqa-validation-3862", "mrqa_naturalquestions-validation-5818", "mrqa_triviaqa-validation-5873", "mrqa_naturalquestions-validation-2067", "mrqa_searchqa-validation-3966", "mrqa_squad-validation-8446", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-8279", "mrqa_newsqa-validation-1757", "mrqa_squad-validation-3044", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-912"], "EFR": 1.0, "Overall": 0.7579513888888889}, {"timecode": 18, "before_eval_results": {"predictions": ["hunter's garb", "from the official declaration of war in 1756 to the signing of the peace treaty in 1763", "two forces", "a decision problem", "10 to 15 million people", "17", "Establishing \"natural borders\"", "Houston Street lab", "vary by geographic area and subject taught", "\"Turks\" (Muslims) and Catholics", "provides the public with financial information about a nonprofit organization", "the Northeast Monsoon or Retreating Monsoon", "April 3, 1973", "Fa Ze Rug", "July 14, 1969", "Krypton", "Mandarin", "in the bone marrow", "Ukraine", "Coldplay", "Yugoslavia", "head coach", "May 19, 2017", "T - Bone Walker", "April 2, 2018", "Valene Kane", "Doug Diemoz", "Iran", "southern Anatolia", "classical architecture", "the pyloric valve", "1546", "100,000 writes", "the chryselephantine statue of Athena Parthenos", "16 seasons", "Long Island", "A lacteal", "Teri Hatcher as Mel Jones", "1987", "Timothy B. Schmit", "Germany", "Jikji", "Panning", "the RAF", "Pepsi", "Dave Kelly", "Isabela Moner", "Ethel Merman", "this legislation required many hospitals, nursing homes, home health agencies, hospice providers, health maintenance organizations ( HMOs ), and other health care institutions to provide information about advance health care directives to adult patients", "New Orleans", "The Outback", "Anakin Skywalker", "Earle Hyman", "Namibia", "sour", "all-time", "Rawlings", "23", "Los Angeles", "North by Northwest", "Naples", "Etna", "Edward VI", "North Rhine-Westphalia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6453738119179295}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.962962962962963, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8823529411764706, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10121", "mrqa_squad-validation-10395", "mrqa_squad-validation-1600", "mrqa_squad-validation-2054", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5885", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-466", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-9438"], "SR": 0.546875, "CSR": 0.6126644736842105, "retrieved_ids": ["mrqa_squad-train-72237", "mrqa_squad-train-73079", "mrqa_squad-train-72333", "mrqa_squad-train-32418", "mrqa_squad-train-29148", "mrqa_squad-train-49510", "mrqa_squad-train-31258", "mrqa_squad-train-62727", "mrqa_squad-train-47536", "mrqa_squad-train-55142", "mrqa_squad-train-77564", "mrqa_squad-train-11019", "mrqa_squad-train-39707", "mrqa_squad-train-27224", "mrqa_squad-train-18304", "mrqa_squad-train-29373", "mrqa_newsqa-validation-2", "mrqa_hotpotqa-validation-1586", "mrqa_triviaqa-validation-2510", "mrqa_searchqa-validation-11531", "mrqa_newsqa-validation-13", "mrqa_searchqa-validation-11633", "mrqa_newsqa-validation-3862", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-2240", "mrqa_searchqa-validation-6195", "mrqa_hotpotqa-validation-3728", "mrqa_searchqa-validation-576", "mrqa_newsqa-validation-2983", "mrqa_hotpotqa-validation-380", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1577"], "EFR": 0.9655172413793104, "Overall": 0.7503238430127042}, {"timecode": 19, "before_eval_results": {"predictions": ["Thomas Vasey and Richard Whatcoat", "continuing their protest", "sex offenders register", "Republic of Kenya", "violence", "aristocracy", "1905", "Saul Alinsky", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "division", "February 1834", "Walter", "President Yahya Khan", "minced potato", "BC Jean and Toby Gad", "Nacio Herb Brown ( music ) and Arthur Freed", "Anatomy", "in Pyeongchang County, Gangwon Province, South Korea", "Steffy Forrester", "Oregon Ringwald", "2017", "a child with Treacher Collins syndrome trying to fit in", "Andaman and Nicobar Islands", "the transcellular", "member states on a voluntary basis", "in the United Kingdom", "Rocinante", "1978", "May 1, 2018", "Hans Christian Andersen", "De pictura", "Pakhangba", "1", "alveolar process", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F ) at Vostok Station", "October 14, 2017", "16", "July 21, 1861", "a candidate state must be a free market democracy", "45", "in the bible", "tolled ( quota ) highways", "Niketa Calame", "Robert E. Lee", "Clarence L. Tinker", "defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the church at Philippi", "federal republic", "Thomas Edison", "mitosis", "hydrogen and oxygen", "pit road speed", "in Pyeongchang County, Gangwon Province, South Korea", "rosary", "Gianni Versace", "David Simon", "1999", "Madrid, Italy.", "in Seoul", "an Airborne", "pours tea", "three", "the Dalai Lama", "Christopher Savoie"], "metric_results": {"EM": 0.5, "QA-F1": 0.6539862453078398}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.13333333333333333, 0.8333333333333333, 0.8, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.8421052631578948, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 0.4615384615384615, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6720", "mrqa_squad-validation-9640", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9760", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-5069", "mrqa_triviaqa-validation-2196", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-84", "mrqa_searchqa-validation-10274", "mrqa_searchqa-validation-2516"], "SR": 0.5, "CSR": 0.60703125, "retrieved_ids": ["mrqa_squad-train-18822", "mrqa_squad-train-9216", "mrqa_squad-train-54256", "mrqa_squad-train-42715", "mrqa_squad-train-74615", "mrqa_squad-train-76279", "mrqa_squad-train-75266", "mrqa_squad-train-71976", "mrqa_squad-train-76654", "mrqa_squad-train-37912", "mrqa_squad-train-37421", "mrqa_squad-train-48083", "mrqa_squad-train-53653", "mrqa_squad-train-75327", "mrqa_squad-train-4584", "mrqa_squad-train-42662", "mrqa_searchqa-validation-9286", "mrqa_hotpotqa-validation-976", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1920", "mrqa_searchqa-validation-5048", "mrqa_newsqa-validation-3527", "mrqa_naturalquestions-validation-8607", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-2221", "mrqa_newsqa-validation-3979", "mrqa_triviaqa-validation-3034", "mrqa_squad-validation-4786", "mrqa_searchqa-validation-6195", "mrqa_newsqa-validation-1385", "mrqa_naturalquestions-validation-833", "mrqa_searchqa-validation-14046"], "EFR": 0.96875, "Overall": 0.7498437499999999}, {"timecode": 20, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1705", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4329", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-466", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-12", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3376", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9809", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-11360", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11598", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16636", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1924", "mrqa_searchqa-validation-1928", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2887", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-3899", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4750", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6233", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-9570", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10158", "mrqa_squad-validation-10162", "mrqa_squad-validation-10198", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10471", "mrqa_squad-validation-1076", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1188", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1330", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1424", "mrqa_squad-validation-1506", "mrqa_squad-validation-1540", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1611", "mrqa_squad-validation-1703", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1834", "mrqa_squad-validation-1908", "mrqa_squad-validation-1976", "mrqa_squad-validation-2015", "mrqa_squad-validation-2025", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2111", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2250", "mrqa_squad-validation-2395", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2532", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-3001", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3193", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-331", "mrqa_squad-validation-3368", "mrqa_squad-validation-3449", "mrqa_squad-validation-3493", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3626", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3948", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4159", "mrqa_squad-validation-4176", "mrqa_squad-validation-4248", "mrqa_squad-validation-4248", "mrqa_squad-validation-4272", "mrqa_squad-validation-4274", "mrqa_squad-validation-4301", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4686", "mrqa_squad-validation-4698", "mrqa_squad-validation-4765", "mrqa_squad-validation-4789", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-501", "mrqa_squad-validation-5133", "mrqa_squad-validation-5157", "mrqa_squad-validation-5214", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-55", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5664", "mrqa_squad-validation-5715", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5897", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6251", "mrqa_squad-validation-6253", "mrqa_squad-validation-6264", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6435", "mrqa_squad-validation-6452", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7191", "mrqa_squad-validation-7226", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7592", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7751", "mrqa_squad-validation-7775", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7889", "mrqa_squad-validation-7932", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8010", "mrqa_squad-validation-8019", "mrqa_squad-validation-8199", "mrqa_squad-validation-8213", "mrqa_squad-validation-826", "mrqa_squad-validation-8278", "mrqa_squad-validation-8298", "mrqa_squad-validation-830", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8383", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-861", "mrqa_squad-validation-8612", "mrqa_squad-validation-8636", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8786", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9308", "mrqa_squad-validation-9315", "mrqa_squad-validation-9322", "mrqa_squad-validation-9388", "mrqa_squad-validation-9405", "mrqa_squad-validation-9431", "mrqa_squad-validation-9495", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9640", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9865", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1646", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5140", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6531", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-88"], "OKR": 0.900390625, "KG": 0.478125, "before_eval_results": {"predictions": ["the dukes", "exposed to scrutiny", "Ed Lee", "40%", "post-World War I", "8000", "can produce both eggs and sperm at the same time", "\u00d6gedei Khan", "June 24, 1935", "Kohlberg K Travis Roberts", "John McClane", "I write What I Like", "1910s", "between 11 or 13 and 18", "Song Il-gon", "Let's Make Sure We Kiss Goodbye", "Martin \"Marty\" McCann", "astronaut, naval aviator, test pilot, and businessman", "Paul John Kushner Jr.", "December 17, 1974", "\"The Royal Family\"", "Sir Seretse Khama", "Mike Holmgren", "the Galaxy S6", "a British archaeologist, military officer, diplomat, and writer", "Don DeLillo", "the Raiders", "a farmers' co-op", "South America", "five", "Heart", "Bank of China", "Brendan Meadows", "romantic attraction, sexual attraction, or sexual behavior toward both males and females", "Winnie the Pooh", "Excalibur Hotel and Casino", "Rigoletto", "Knoxville, Tennessee", "Americana Manhasset", "Omega SA", "1978", "Pim Fortuyn", "eastern shore of the Firth of Clyde, Scotland", "Todd McFarlane", "M. Night Shyamalan", "Magdalen College", "Eric Allan Kramer", "1967", "Province of New York", "capitol building", "Ghana", "Michael Seater", "1933", "Mind your Ps and Qs", "Charles Darwin", "Osborne Road", "Queen Katherine Parr", "his past and his future", "London and Buenos Aires", "Engelbert Humperdinck", "the Colorado", "John Denver", "The Princess Bride", "Burkina Faso"], "metric_results": {"EM": 0.546875, "QA-F1": 0.642125496031746}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.2222222222222222, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-891", "mrqa_naturalquestions-validation-4109", "mrqa_triviaqa-validation-7452", "mrqa_newsqa-validation-3889", "mrqa_searchqa-validation-6205"], "SR": 0.546875, "CSR": 0.6041666666666667, "retrieved_ids": ["mrqa_squad-train-78099", "mrqa_squad-train-14596", "mrqa_squad-train-61506", "mrqa_squad-train-23205", "mrqa_squad-train-55696", "mrqa_squad-train-10730", "mrqa_squad-train-26660", "mrqa_squad-train-84544", "mrqa_squad-train-74846", "mrqa_squad-train-68027", "mrqa_squad-train-48512", "mrqa_squad-train-13286", "mrqa_squad-train-62778", "mrqa_squad-train-85346", "mrqa_squad-train-6377", "mrqa_squad-train-64457", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-14197", "mrqa_naturalquestions-validation-9944", "mrqa_squad-validation-3511", "mrqa_searchqa-validation-15560", "mrqa_squad-validation-10395", "mrqa_searchqa-validation-4426", "mrqa_naturalquestions-validation-10364", "mrqa_searchqa-validation-10993", "mrqa_squad-validation-1076", "mrqa_searchqa-validation-748", "mrqa_naturalquestions-validation-3297", "mrqa_newsqa-validation-3425", "mrqa_squad-validation-9388", "mrqa_squad-validation-6720", "mrqa_triviaqa-validation-2240"], "EFR": 0.9655172413793104, "Overall": 0.7447180316091954}, {"timecode": 21, "before_eval_results": {"predictions": ["sacked him", "the disbelieving (Kafir) colonial powers", "pharmacy practice science and applied information science", "thought it may have been a combination of anthrax and other pandemics", "1912", "rubisco", "Huguenot rebellions", "Psych", "Ladin", "August 11, 1946", "Protestant Christian", "Queens, New York", "Erreway", "Rochester Hills, Michigan", "Madeleine L' Engle", "FAI Junior Cup", "1966 US tour", "February 18, 1965", "Sydney", "Cuban descent", "1898", "Sarah Kerrigan", "Mickey's PhilharMagic", "Eielson Air Force Base", "Taylor Swift", "Italy", "Tel Aviv University", "Las Vegas", "John of Gaunt", "William Clark Gable", "Stage Stores", "C. J. Cherryh", "Spanish", "channel 33", "Guns N' Roses", "Daniil Shafran", "Sharman Joshi", "1912", "four", "13 October 1958", "more than 70", "3 May 1958", "the Women's World Curling Classic", "Vancouver", "Marlborough", "Fountains of Wayne", "Ealdorman", "sulfur mustard H or HD blister gas", "The Saturdays", "the Chick tract of the same name", "southwestern", "The Campbell Soup Company", "The Gang", "2010", "the vascular cambium", "A", "wine", "Rod Blagojevich", "Iraq", "branches of the Yoga Tree", "a motorcycle", "first home series defeat on Australia in almost 16 years", "$150 billion over 10 years on developing alternative energy", "taste a hamburger and pizza, and drink coffee from a cup, the \"things we take for granted every day,\""], "metric_results": {"EM": 0.453125, "QA-F1": 0.5690015282497534}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.28571428571428575, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.8571428571428571, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.3636363636363636, 0.20689655172413793]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_squad-validation-9918", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-1287", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1435", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2301", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1784", "mrqa_searchqa-validation-7453", "mrqa_searchqa-validation-2193", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-1093"], "SR": 0.453125, "CSR": 0.5973011363636364, "retrieved_ids": ["mrqa_squad-train-54854", "mrqa_squad-train-21712", "mrqa_squad-train-79444", "mrqa_squad-train-8109", "mrqa_squad-train-51112", "mrqa_squad-train-74745", "mrqa_squad-train-21857", "mrqa_squad-train-53061", "mrqa_squad-train-34168", "mrqa_squad-train-48238", "mrqa_squad-train-56402", "mrqa_squad-train-16935", "mrqa_squad-train-59749", "mrqa_squad-train-47327", "mrqa_squad-train-51331", "mrqa_squad-train-54377", "mrqa_naturalquestions-validation-8607", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-2358", "mrqa_squad-validation-5975", "mrqa_newsqa-validation-832", "mrqa_searchqa-validation-11855", "mrqa_naturalquestions-validation-976", "mrqa_searchqa-validation-733", "mrqa_naturalquestions-validation-8903", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2268", "mrqa_searchqa-validation-16012", "mrqa_hotpotqa-validation-3728", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-6050"], "EFR": 1.0, "Overall": 0.7502414772727273}, {"timecode": 22, "before_eval_results": {"predictions": ["one of the daughters of former King of Thebes, Oedipus", "environmental determinism", "1110 AM", "106 and 107", "infected corpses", "Cobb Lecture Hall", "Cortina d'Ampezzo", "Eric Edward Whitacre", "1983", "EQT Plaza in Pittsburgh, Pennsylvania", "Ruth Westheimer", "Giotto", "Nickelodeon", "Brad Wilk", "Robert Matthew Hurley", "11", "PEN American Center in New York City", "March 19, 2017", "Disney California Adventure", "Anah\u00ed Giovanna Puente Portilla de Velasco", "Nicholas John \"Nic\" Cester", "Anne Elizabeth Alice Louise", "Harry Robbins \"Bob\" Haldeman", "264,152", "deities, beings, and heroes", "Robin David Segal", "more than 40 million", "Billund, Denmark", "in Africa", "William Cavendish", "20 March to 1 May 2003", "23 July 1989", "Kinnairdy Castle", "Ken Rutherford", "\"The Catcher in the Rye\"", "Indianapolis Motor Speedway", "Marjorie Jacqueline \"Marge\" Simpson", "May 4, 1924", "Transporter 3", "Richard Allen Street", "mystery-romantic comedy", "Steve Carell", "Green Chair", "The Frog Prince", "23 December 1893", "Germany", "November 10, 2017", "CTV Television Network", "Australian", "Rafael Palmeiro", "Eric Allan Kramer", "Orchard County", "Orographic lift", "Reproductive system", "Coroebus of Elis", "\u201cMy Favorite Martian,\u201d", "Disraeli", "Terry Miller", "Sunday", "5 1/2-year-old", "one day,", "jazz", "the Supreme Court", "Francis Bellamy"], "metric_results": {"EM": 0.5, "QA-F1": 0.6299355158730159}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false], "QA-F1": [0.42857142857142855, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.2857142857142857, 1.0, 0.3333333333333333, 0.0, 0.0, 0.888888888888889, 0.0, 0.6666666666666666, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_squad-validation-4132", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5636", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-4204", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3629", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-4189", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-5396"], "SR": 0.5, "CSR": 0.5930706521739131, "retrieved_ids": ["mrqa_squad-train-833", "mrqa_squad-train-57298", "mrqa_squad-train-35045", "mrqa_squad-train-12973", "mrqa_squad-train-8155", "mrqa_squad-train-65744", "mrqa_squad-train-29592", "mrqa_squad-train-64835", "mrqa_squad-train-21266", "mrqa_squad-train-60642", "mrqa_squad-train-50218", "mrqa_squad-train-51838", "mrqa_squad-train-19301", "mrqa_squad-train-44242", "mrqa_squad-train-68934", "mrqa_squad-train-7738", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-8903", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-1837", "mrqa_searchqa-validation-11790", "mrqa_newsqa-validation-2431", "mrqa_squad-validation-6973", "mrqa_naturalquestions-validation-692", "mrqa_squad-validation-3899", "mrqa_triviaqa-validation-1517", "mrqa_searchqa-validation-3679", "mrqa_hotpotqa-validation-891", "mrqa_newsqa-validation-1093", "mrqa_naturalquestions-validation-6555", "mrqa_triviaqa-validation-2118"], "EFR": 1.0, "Overall": 0.7493953804347826}, {"timecode": 23, "before_eval_results": {"predictions": ["Article 17(3)", "brother", "Department of Justice", "tidal currents", "was sacked by DeMarcus Ware", "The Eleventh Doctor", "Malware", "Claims adjuster", "between 2004 and 2007", "Norma's brother", "Arunachal Pradesh", "2010", "Richard of Shrewsbury", "the Roman Empire", "1995", "Rufus and Chaka Khan", "18", "to form a higher alkane", "The Lightning thief", "Ali", "final scene of the fourth season", "UMBC", "the customer's account", "the New Croton Reservoir in Westchester and Putnam counties", "Elena Anaya", "January 2018", "prison", "biscuit - sized", "Woodrow Wilson", "negotiates treaties with foreign nations", "Waylon Jennings", "the Italian / Venetian John Cabot", "the New York Yankees", "protect the genome from lethal chemical and physical agents", "Turducken", "a balance sheet as an asset", "San Francisco Bay", "Mickey Mantle", "the ACU", "1956", "around 2011", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "April 2011", "David Gahan", "the town of Acolman, just north of Mexico City", "South Asia", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "a large roasted turkey", "the fourth C key from left on a standard 88 - key piano keyboard", "New England Patriots", "Tristan Rogers", "novella", "Mike Brady, a widowed architect with sons Greg, Peter, and Bobby,", "Ron Kovic", "Mexico", "Apsley George Benet Cherry-Garrard", "the Ecumenical Award", "Brig Gen Augustine Warner Robins", "Florida", "Department of Homeland Security Secretary Janet Napolitano", "Elisabeth,", "Colorado", "gusto", "Ulysses"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6902159758371283}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9411764705882353, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9411764705882353, 0.0, 0.5, 0.0, 1.0, 0.6976744186046512, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8230", "mrqa_squad-validation-818", "mrqa_naturalquestions-validation-8908", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-3242", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-4545", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2908"], "SR": 0.609375, "CSR": 0.59375, "retrieved_ids": ["mrqa_squad-train-41962", "mrqa_squad-train-52096", "mrqa_squad-train-22607", "mrqa_squad-train-77225", "mrqa_squad-train-23027", "mrqa_squad-train-50085", "mrqa_squad-train-55822", "mrqa_squad-train-1917", "mrqa_squad-train-75898", "mrqa_squad-train-16970", "mrqa_squad-train-59579", "mrqa_squad-train-9434", "mrqa_squad-train-32658", "mrqa_squad-train-4551", "mrqa_squad-train-72329", "mrqa_squad-train-86480", "mrqa_searchqa-validation-4402", "mrqa_searchqa-validation-3592", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-8695", "mrqa_searchqa-validation-12484", "mrqa_triviaqa-validation-955", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-4606", "mrqa_newsqa-validation-3765", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-9809", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-839", "mrqa_hotpotqa-validation-532", "mrqa_searchqa-validation-6205"], "EFR": 0.96, "Overall": 0.74153125}, {"timecode": 24, "before_eval_results": {"predictions": ["the first two series", "Doctor in Bible", "c1750", "60%", "a deterministic Turing machine", "Henry III", "Henkel", "David Bowie", "George Frideric Handel", "Pol Pot, butcher of Cambodia", "the Pyrenees mountains", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Motel 6", "a restaurant in New York\u2019s Greenwich Village", "Virginia", "Esau", "a crystal ball", "Houyhnhnm", "the gallbladder", "Thabo Mbeki", "1921", "a dog", "Robert Schumann", "the Benedictine Order", "oliver", "translations", "King County Executive", "Scotland", "the Penguin", "The Great Victoria Desert", "Gertrud Margarete", "Saturn's Hula-Hoops", "the Brisbane River", "The Aidensfield Arms", "armada", "Liechtenstein", "the Rolling Stones", "oliver", "Robert Shapiro", "Germany", "Prokofiev", "horses", "\"Stutter Rap (No Sleep til Bedtime)\"", "Kansas", "Australia", "Every Good Boy Deserves Favour", "Wooden Heart", "smell", "uncle-Bosheth", "drove for a local judge until he recently died.", "The Isles of the Blessed", "the Anthropocene", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Darren McGavin", "Sir Henry Cole", "Scottish", "Sarah Hurst", "1916", "Argentine", "Jason Chaffetz", "if Gadhafi suffered the wound in crossfire or at close-range,", "an analog watch", "oliver", "Henry Kissinger"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5975446428571429}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1819", "mrqa_triviaqa-validation-4523", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-1702", "mrqa_naturalquestions-validation-3189", "mrqa_hotpotqa-validation-5274", "mrqa_newsqa-validation-2385", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-1228"], "SR": 0.546875, "CSR": 0.5918749999999999, "retrieved_ids": ["mrqa_squad-train-57970", "mrqa_squad-train-60600", "mrqa_squad-train-10266", "mrqa_squad-train-21235", "mrqa_squad-train-32011", "mrqa_squad-train-7498", "mrqa_squad-train-81998", "mrqa_squad-train-18705", "mrqa_squad-train-31728", "mrqa_squad-train-28913", "mrqa_squad-train-70967", "mrqa_squad-train-13372", "mrqa_squad-train-71007", "mrqa_squad-train-24572", "mrqa_squad-train-28033", "mrqa_squad-train-876", "mrqa_hotpotqa-validation-4101", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-6050", "mrqa_newsqa-validation-3720", "mrqa_hotpotqa-validation-4596", "mrqa_searchqa-validation-2994", "mrqa_naturalquestions-validation-9400", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3126", "mrqa_hotpotqa-validation-5569", "mrqa_newsqa-validation-1486", "mrqa_triviaqa-validation-2246", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-912", "mrqa_searchqa-validation-3031", "mrqa_hotpotqa-validation-3455"], "EFR": 1.0, "Overall": 0.74915625}, {"timecode": 25, "before_eval_results": {"predictions": ["Royal Engineers", "prime for any natural number n.", "QuickBooks", "Westinghouse Electric", "The Bachelor", "The Statue of Freedom, also known as Armed Freedom or simply Freedom, is a bronze statue designed by Thomas Crawford ( 1814 -- 1857 ) that, since 1863, has crowned the dome of the U.S. Capitol building", "2018", "2017 Georgia Bulldogs football team against the Western Division Co-Champion, the 2017 Auburn Tigers football team", "Honor\u00e9 Mirabeau", "England and Wales", "Eleanor Roosevelt", "radioisotope thermoelectric generator", "1908", "honey bees", "Samantha Jo `` Mandy '' Moore ( born March 28, 1976 in St. Louis, Missouri and raised in Breckenridge, Colorado )", "Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist", "Mangal Pandey", "Woodrow Strode", "Woodrow Wilson", "four", "merging of tissues in the vicinity of the nose", "season two", "January 17, 1899", "Heliocentric model of Copernicus, Galileo and Kepler", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins, dealing with Christian dispensationalist End Times", "British Ultra code - breaking intelligence", "602", "inverted", "5.7 million", "Vincent Price", "Steve Russell, in collaboration with Martin Graetz and Wayne Wiitanen, and programmed by Russell with assistance from others including Bob Saunders and Steve Piner", "Janis Joplin", "1871", "September 9, 2012", "Ahmad ( Real ) selected Doll", "Haiti", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "October 2017", "2026", "2018", "The Outback", "The joint sitting of the Parliament is called by the President and is presided over by the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha", "January 2004", "Australia", "eleven", "Master Christopher Jones", "Around 1200", "12.65 m ( 41.50 ft )", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "2017", "U.S. Steel Central Furnaces in Cleveland, Ohio", "2011", "Italy", "music (to be performed) in a fiery manner", "Culture Club", "These Are Special Times", "Gujarat", "Sofia the First", "software magnate", "Hurricane Gustav", "21", "Kurt Vonnegut", "The Anabaptists", "uranium"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5657581481018981}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.25, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.38095238095238093, 0.0, 0.4, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.33333333333333337, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.07999999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.8, 0.0, 0.09523809523809523, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_squad-validation-8964", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2781", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7128", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-3842", "mrqa_newsqa-validation-1234", "mrqa_searchqa-validation-9088"], "SR": 0.4375, "CSR": 0.5859375, "retrieved_ids": ["mrqa_squad-train-56181", "mrqa_squad-train-253", "mrqa_squad-train-60339", "mrqa_squad-train-25556", "mrqa_squad-train-37094", "mrqa_squad-train-23393", "mrqa_squad-train-37760", "mrqa_squad-train-60972", "mrqa_squad-train-39401", "mrqa_squad-train-3701", "mrqa_squad-train-72222", "mrqa_squad-train-78152", "mrqa_squad-train-32287", "mrqa_squad-train-67657", "mrqa_squad-train-5482", "mrqa_squad-train-73780", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4330", "mrqa_searchqa-validation-3269", "mrqa_newsqa-validation-3796", "mrqa_hotpotqa-validation-2271", "mrqa_naturalquestions-validation-1068", "mrqa_triviaqa-validation-5973", "mrqa_naturalquestions-validation-839", "mrqa_hotpotqa-validation-3489", "mrqa_squad-validation-9640", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-14936", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-2385", "mrqa_hotpotqa-validation-2867", "mrqa_naturalquestions-validation-5818"], "EFR": 1.0, "Overall": 0.74796875}, {"timecode": 26, "before_eval_results": {"predictions": ["Soviet", "Thomas Commerford Martin", "seven", "labor inputs (workers)", "1996", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Indraprastha", "John Quincy Adams", "third season", "Christopher Columbus", "to fit in", "the initiator must go through an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "The United States Secretary of State", "presiding over by a Chief Justice and is composed of fifteen ( 15 ) Justices, including the Chief Justice", "in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape ''", "Mandy", "King Harold Godwinson", "during meiosis", "the Miracles", "Spanish / Basque origin", "in early evenings to call ( in spring and summer ) and hunt for food", "to universalize the topic of the song into something everyone could relate to and ascribe personal meaning to in their own way", "January 2, 1971", "Hirschman", "in the books of Exodus and Deuteronomy", "Lucknow in September 1906", "classical neurology", "The User State Migration Tool", "Michelle", "May 1, 2018", "291", "the Naturalization Act of 1790", "flawed democracy", "in Pashto and Persian as \u0647\u0646\u062f\u0648\u06a9\u0634 \u202c", "last Ice Age", "Ren\u00e9 Descartes", "in people and animals that collects and stores urine from the kidneys before disposal by urination", "Union", "Justin Fletcher", "in the original Star Wars film in 1977", "Germany", "London, United Kingdom", "Jeff East", "President Yahya Khan", "\u03b2\u03b1\u03c3\u03ba\u03b1\u03bd\u03af\u03b1", "6th century AD", "Arnold Schoenberg", "~ 3.5 million years old from Idaho, USA", "111", "when a population temporarily exceeds the long term carrying capacity of its environment", "Mike Leeson and Peter Vale and produced by Josh Deutsch", "the defendant owed a duty to the deceased to take care", "Joan Crawford", "Andorra", "Ned Sherrin", "Band-e Amir National Park", "Cartoon Network", "Chrysler", "Kabul in the eastern Afghan province of Logar,", "more than 100 tribal members", "comfort those in mourning,", "Roanoke Colony", "dancer", "apples"], "metric_results": {"EM": 0.5, "QA-F1": 0.6465377025338159}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666665, 0.1739130434782609, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.08, 0.13953488372093023, 1.0, 1.0, 0.2857142857142857, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.3333333333333333, 1.0, 0.7272727272727273, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.7499999999999999, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3771", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-2887", "mrqa_naturalquestions-validation-9962", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-4399", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3495", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-7787"], "SR": 0.5, "CSR": 0.5827546296296297, "retrieved_ids": ["mrqa_squad-train-18038", "mrqa_squad-train-44884", "mrqa_squad-train-28969", "mrqa_squad-train-6068", "mrqa_squad-train-57147", "mrqa_squad-train-24976", "mrqa_squad-train-40696", "mrqa_squad-train-29828", "mrqa_squad-train-29780", "mrqa_squad-train-34692", "mrqa_squad-train-58197", "mrqa_squad-train-79676", "mrqa_squad-train-41839", "mrqa_squad-train-62294", "mrqa_squad-train-49421", "mrqa_squad-train-13968", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-8208", "mrqa_hotpotqa-validation-3423", "mrqa_naturalquestions-validation-2100", "mrqa_searchqa-validation-2118", "mrqa_naturalquestions-validation-6500", "mrqa_newsqa-validation-2730", "mrqa_naturalquestions-validation-9809", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-8001", "mrqa_naturalquestions-validation-4667", "mrqa_squad-validation-4975", "mrqa_naturalquestions-validation-8585", "mrqa_newsqa-validation-3979", "mrqa_triviaqa-validation-2121"], "EFR": 1.0, "Overall": 0.7473321759259259}, {"timecode": 27, "before_eval_results": {"predictions": ["eight", "after sustaining an injury which would be fatal to most other species", "eating both fish larvae and small crustaceans that would otherwise feed the adult fish", "direct repeats", "Naples", "black, red or white", "2009", "cowardly lion", "Diego Maradona", "an \"unnamed international terror group\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "\"I don't watch TV,\"", "golf", "Consumer Reports", "Floxin", "Copts", "February 12", "Honduran President Jose Manuel Zelaya", "in the last few months,", "a group of college students of Pakistani background", "I, the chief executive officer, the one on the very top,", "two awards.", "African-Americans", "environmental and political events.", "said, \"explosive device\" or \"bomb\"", "three empty vodka bottles,", "Euna Lee,", "Angela Merkel", "the Ku Klux Klan", "dental work done, including removal of his diamond-studded teeth.", "Manuel Mejia Munera", "police to question people if there's reason to suspect they're in the United States illegally.", "UNICEF", "club managers,", "used", "(I) Jason Bendett", "\"I'm certainly not nearly as good of a speaker as he is.\"", "suicides", "$15 billion in 2008", "Karen Floyd", "Leo Frank,", "a skilled hacker could disrupt the system and cause a blackout.", "psychotropic drugs", "$2 billion", "Both men were hospitalized and expected to survive,", "Krishna Rajaram,", "flooding was so fast that the thing flipped over,\"", "Scudetto", "Dubai", "up three of the last four months.", "Sunday", "12-hour-plus", "three times", "Canada", "Iowa", "Homo sapiens", "potassium", "Something In The Air", "16 March 1987", "Peter Kay's Car Share", "Cyclic Defrost", "pfeffernuesse", "a jump school", "14"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6126778083028084}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.46153846153846156, 0.42857142857142855, 0.08, 1.0, 1.0, 0.0, 0.6666666666666666, 0.19999999999999998, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4648", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2325", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2456", "mrqa_naturalquestions-validation-6991", "mrqa_triviaqa-validation-5378", "mrqa_searchqa-validation-2197"], "SR": 0.546875, "CSR": 0.5814732142857143, "retrieved_ids": ["mrqa_squad-train-13578", "mrqa_squad-train-54409", "mrqa_squad-train-14877", "mrqa_squad-train-64430", "mrqa_squad-train-49197", "mrqa_squad-train-24905", "mrqa_squad-train-7831", "mrqa_squad-train-44787", "mrqa_squad-train-21421", "mrqa_squad-train-3552", "mrqa_squad-train-35055", "mrqa_squad-train-14396", "mrqa_squad-train-30608", "mrqa_squad-train-54493", "mrqa_squad-train-17966", "mrqa_squad-train-51806", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-1989", "mrqa_hotpotqa-validation-3714", "mrqa_searchqa-validation-2516", "mrqa_hotpotqa-validation-5467", "mrqa_newsqa-validation-981", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-8695", "mrqa_searchqa-validation-1562", "mrqa_newsqa-validation-3760", "mrqa_searchqa-validation-2732", "mrqa_triviaqa-validation-2406", "mrqa_squad-validation-8538", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-6205", "mrqa_naturalquestions-validation-7627"], "EFR": 0.9655172413793104, "Overall": 0.740179341133005}, {"timecode": 28, "before_eval_results": {"predictions": ["deserts", "political parties", "flew the first Earth orbital test mission Apollo 7", "Amos McCracken", "Cherokee Nation", "Flamingo Las Vegas", "Gran Sasso d'Italia", "Niger\u2013Congo", "2013", "Luis Resto", "the Salzburg Festival", "Jay Park", "Blackpool Football Club", "New Orleans Saints", "2012", "\"Charmed\"", "Dundalk", "Ashley Jensen", "Syracuse", "Dame Eileen June Atkins, DBE", "Mollie Elizabeth King", "Esteban Ocon", "the flags of dependent territories", "Ouse and Foss", "Emilia-Romagna", "Casablanca", "The Go-Go's", "1943", "Sleepy Hollow", "Ronnie Schell", "Wandsworth, London", "Christopher Lloyd Smalling", "Chevron", "World Music Awards", "La Liga", "Australian", "Floyd Nathaniel \"Nate\" Hills", "Fort Hood, Texas", "Michael Phelps", "Lauren Alaina", "1961", "Droga5", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Prudential Center in Newark, New Jersey", "\"The Clash of Triton\"", "Mach number", "1945 to 1951", "Mexico", "Chevy", "\"out and back\"", "Disco", "Theodor W. Adorno", "re-education", "Sir Rowland Hill", "compound sentence", "President Barack Obama", "John Logie Baird", "1066", "Joan Rivers", "her heart condition or the medical procedure.", "auction off one of the earliest versions of the Magna Carta later this year,", "the Wall of the Moon", "Vista", "a hat"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6283234126984127}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2791", "mrqa_squad-validation-4060", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-1751", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-1926", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1667", "mrqa_naturalquestions-validation-8329", "mrqa_triviaqa-validation-4927", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-9671", "mrqa_searchqa-validation-14497", "mrqa_searchqa-validation-14791"], "SR": 0.53125, "CSR": 0.5797413793103448, "retrieved_ids": ["mrqa_squad-train-30528", "mrqa_squad-train-62030", "mrqa_squad-train-25861", "mrqa_squad-train-46367", "mrqa_squad-train-43604", "mrqa_squad-train-78386", "mrqa_squad-train-73971", "mrqa_squad-train-79427", "mrqa_squad-train-48172", "mrqa_squad-train-23374", "mrqa_squad-train-58309", "mrqa_squad-train-29203", "mrqa_squad-train-30227", "mrqa_squad-train-71667", "mrqa_squad-train-45393", "mrqa_squad-train-51426", "mrqa_triviaqa-validation-203", "mrqa_squad-validation-3597", "mrqa_newsqa-validation-2844", "mrqa_searchqa-validation-7787", "mrqa_naturalquestions-validation-5611", "mrqa_squad-validation-4562", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-5674", "mrqa_newsqa-validation-2580", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-4678", "mrqa_newsqa-validation-591", "mrqa_searchqa-validation-2405", "mrqa_naturalquestions-validation-4930", "mrqa_searchqa-validation-2200"], "EFR": 0.9666666666666667, "Overall": 0.7400628591954023}, {"timecode": 29, "before_eval_results": {"predictions": ["some teachers and parents", "July 1969", "glaucophyte", "God and the just cause", "Mercury Records", "Evgeni Platov", "Nye County", "Karolina Dean,", "Firestorm", "ten", "the White Knights of the Ku Klux Klan", "the Chechen Republic", "Six Flags Great Adventure", "Kansas City", "1994", "English", "the Food and Agriculture Organization", "Albertsons", "Jeff Meldrum", "Crossed", "Romance language", "Philip K. Dick", "exercise power directly or elect representatives from among themselves to form a governing body, such as a parliament", "English", "Cartoon Network Too", "David Starkey", "the Cherokee River", "pop music and popular culture", "Field Marshal Lord Gort", "Hopeless Records", "Razor Ramon", "Godspell", "8 August 1907", "Boeing 757", "The 7.63\u00d725mm Mauser (.30 Mauser Automatic)", "Bangkok", "innie the Pooh", "his exploration and settlement", "August 28, 1774", "Kabul Valley", "British", "Potomac River", "the Netherlands", "Love the Way You Lie", "Rio Gavin Ferdinand", "Boston", "Las Vegas", "pilot and aviation enthusiasts", "Rockbridge County", "St. Louis, Missouri", "Tsung-Dao Lee", "Bay Ridge, Brooklyn", "Human anatomy", "Tyler, Ali, and Lydia", "Pebble Beach", "AFC Wimbledon", "The Duke of Plaza-Toro", "1", "Meira Kumar", "the FDA", "bartering -- trading goods and services without exchanging money", "spoiled", "a malted", "Iceland"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6489157393029675}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-5306", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-3362", "mrqa_triviaqa-validation-6131", "mrqa_triviaqa-validation-4462", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-719", "mrqa_searchqa-validation-11933"], "SR": 0.59375, "CSR": 0.5802083333333333, "retrieved_ids": ["mrqa_squad-train-16207", "mrqa_squad-train-48068", "mrqa_squad-train-86031", "mrqa_squad-train-70246", "mrqa_squad-train-54859", "mrqa_squad-train-6826", "mrqa_squad-train-80840", "mrqa_squad-train-62924", "mrqa_squad-train-11613", "mrqa_squad-train-14143", "mrqa_squad-train-5176", "mrqa_squad-train-65257", "mrqa_squad-train-13842", "mrqa_squad-train-46656", "mrqa_squad-train-6446", "mrqa_squad-train-10623", "mrqa_naturalquestions-validation-6660", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-4288", "mrqa_hotpotqa-validation-3165", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-3828", "mrqa_squad-validation-9087", "mrqa_searchqa-validation-7976", "mrqa_hotpotqa-validation-959", "mrqa_searchqa-validation-9315", "mrqa_triviaqa-validation-1595", "mrqa_searchqa-validation-5424", "mrqa_naturalquestions-validation-8908", "mrqa_squad-validation-3044", "mrqa_searchqa-validation-2200", "mrqa_newsqa-validation-1757"], "EFR": 1.0, "Overall": 0.7468229166666667}, {"timecode": 30, "UKR": 0.775390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1079", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1524", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2823", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3382", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4058", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4334", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4953", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-5313", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-932", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-999", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9867", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-958", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-1228", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1828", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9613", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1379", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1546", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1600", "mrqa_squad-validation-1751", "mrqa_squad-validation-1819", "mrqa_squad-validation-1908", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-2025", "mrqa_squad-validation-2106", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-2848", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-3001", "mrqa_squad-validation-3103", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3449", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-4065", "mrqa_squad-validation-4132", "mrqa_squad-validation-4159", "mrqa_squad-validation-4216", "mrqa_squad-validation-4248", "mrqa_squad-validation-4274", "mrqa_squad-validation-4472", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4698", "mrqa_squad-validation-4736", "mrqa_squad-validation-4765", "mrqa_squad-validation-4772", "mrqa_squad-validation-4789", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5270", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5908", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6382", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7043", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7217", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7564", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7775", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7937", "mrqa_squad-validation-8010", "mrqa_squad-validation-8023", "mrqa_squad-validation-826", "mrqa_squad-validation-8298", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8466", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-8612", "mrqa_squad-validation-8665", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9308", "mrqa_squad-validation-9499", "mrqa_squad-validation-9594", "mrqa_squad-validation-9638", "mrqa_squad-validation-9918", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4504", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6173", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7452", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-88"], "OKR": 0.8984375, "KG": 0.47578125, "before_eval_results": {"predictions": ["State Department", "immediately", "a second Gleichschaltung", "Westgate Las Vegas Resort & Casino", "Recording Industry Association of America", "between 7,500 and 40,000", "mountaineer", "Belgian", "Eve Hewson", "\"Slaughterhouse-Five\"", "BBC Formula One", "William Jefferson Clinton (born William Jefferson Blythe III; August 19, 1946) is an American politician who served as the 42nd President of the United States from 1993 to 2001", "Oldham County", "sandstone", "Channel 4", "25 December 2009", "Father Austin Purcell", "punk rock", "Robert &quot", "Lord Byron", "Laura Dern", "Carrefour", "American burlesque", "Venancio Flores", "Forever Living Products", "the FBI", "The Saturdays", "Indianapolis", "French", "1968", "Edinburgh", "Charles Bronson", "Oklahoma Sooners", "Orson Welles", "Sharyn McCrumb", "National Archives", "1.23 million", "Ford Motor Company", "J. K. Rowling", "Sullivan University College of Pharmacy", "Blue Dragon", "England", "January 28, 2016", "Martin Scorsese", "1979", "Charlie Kaufman", "Weare", "RAF Tangmere, West Sussex", "Brotherly Leader", "Suicide Kings", "North Kesteven", "stolperstein", "Earl ( John Doe )", "Montgomery", "Bart Howard", "South Park", "sonar", "Andre Agassi", "10 below", "Asashoryu", "heavy turbulence", "a bass range", "Engelbert", "The Secret"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7073520768025079}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20689655172413793, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-3846", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-4470", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3442", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-7912", "mrqa_newsqa-validation-1077", "mrqa_searchqa-validation-12237", "mrqa_searchqa-validation-8942"], "SR": 0.640625, "CSR": 0.5821572580645161, "retrieved_ids": ["mrqa_squad-train-37855", "mrqa_squad-train-53782", "mrqa_squad-train-29353", "mrqa_squad-train-23650", "mrqa_squad-train-6567", "mrqa_squad-train-31607", "mrqa_squad-train-80261", "mrqa_squad-train-64092", "mrqa_squad-train-12587", "mrqa_squad-train-35409", "mrqa_squad-train-2413", "mrqa_squad-train-63612", "mrqa_squad-train-14651", "mrqa_squad-train-69242", "mrqa_squad-train-9699", "mrqa_squad-train-65747", "mrqa_newsqa-validation-743", "mrqa_hotpotqa-validation-4508", "mrqa_triviaqa-validation-1497", "mrqa_naturalquestions-validation-6469", "mrqa_hotpotqa-validation-2847", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-5051", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-3760", "mrqa_searchqa-validation-2193", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-562", "mrqa_searchqa-validation-5048", "mrqa_hotpotqa-validation-2978", "mrqa_triviaqa-validation-2240"], "EFR": 0.9565217391304348, "Overall": 0.7376576744389902}, {"timecode": 31, "before_eval_results": {"predictions": ["NCAA Division I", "1985", "mistress of the Robes", "2006", "25 million", "age 15", "Brian Yorkey", "The Bye Bye Man", "Hermione Youlanda Ruby Clinton-Baddeley", "GameStop Corp.", "Fort Frederick", "Thorgan Ganael Francis Hazard", "Robert Marvin \"Bobby\" Hull, OC", "Love Actually", "Larnelle Steward Harris", "Queensland", "Southbank", "Commanding General", "1976", "Sean Yseult", "1998", "Benjamin Andrew \" Ben\" Stokes", "thirty-seventh", "Francis Keogh Gleason", "Royal Navy", "The Land of Enchantment", "$10\u201320 million", "Cumberland Plain", "Formula E", "residential", "Province of Canterbury", "the Anhaltisches Theater", "Alemannic", "1932", "128", "Telugu", "1937", "Windermere", "Curtis James Martin Jr.", "Marco Fu Ka-chun, MH, JP", "horror", "Isabella (Belle) Baumfree", "Katherine Murray Millett", "the fifth season", "Premier League", "Aqua", "St. Louis, Missouri", "Dan Castellaneta", "Bury St Edmunds, Suffolk, England", "Philip K. Dick", "Labour Party", "Greater Manchester, England", "Toby Keith", "Lady Olenna Tyrell", "August 2, 1990", "Kenya", "mALCOLM", "Sir William Hamilton", "Las Vegas", "\"Larry King Live.\"", "an \"unnamed international terror group\"", "the Church of Christ, Scientist", "Ronald Reagan", "East Germany"], "metric_results": {"EM": 0.515625, "QA-F1": 0.699464858058608}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 1.0, 0.15384615384615385, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5714285714285715, 0.4, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.7499999999999999, 1.0, 0.6666666666666666, 0.8571428571428571, 0.5, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2793", "mrqa_hotpotqa-validation-4223", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-3892", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5482", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-7447", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-2828", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-4128", "mrqa_searchqa-validation-1275"], "SR": 0.515625, "CSR": 0.580078125, "retrieved_ids": ["mrqa_squad-train-33071", "mrqa_squad-train-56520", "mrqa_squad-train-1773", "mrqa_squad-train-11292", "mrqa_squad-train-9019", "mrqa_squad-train-50386", "mrqa_squad-train-51259", "mrqa_squad-train-78807", "mrqa_squad-train-78058", "mrqa_squad-train-48056", "mrqa_squad-train-76150", "mrqa_squad-train-12074", "mrqa_squad-train-60393", "mrqa_squad-train-14355", "mrqa_squad-train-82562", "mrqa_squad-train-32028", "mrqa_naturalquestions-validation-9078", "mrqa_searchqa-validation-2405", "mrqa_naturalquestions-validation-5600", "mrqa_hotpotqa-validation-2268", "mrqa_newsqa-validation-3457", "mrqa_hotpotqa-validation-4503", "mrqa_searchqa-validation-6091", "mrqa_hotpotqa-validation-1586", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-1989", "mrqa_naturalquestions-validation-8908", "mrqa_triviaqa-validation-4189", "mrqa_squad-validation-10128", "mrqa_hotpotqa-validation-3977", "mrqa_naturalquestions-validation-10039", "mrqa_hotpotqa-validation-548"], "EFR": 1.0, "Overall": 0.7459374999999999}, {"timecode": 32, "before_eval_results": {"predictions": ["Licensed Local Pastor", "a power station generator", "13", "Hebrew", "Blenheim Palace", "five", "Edith Louisa Cavell", "Cotentin", "De Lorean DMC-12", "Cold War", "Action Comics", "Queen Elizabeth II", "The Merchant of Venice", "Northwestern University", "curling", "Mark Steyn", "Colorado", "Google", "Aviva plc", "oil", "Project Gutenberg", "surf", "Dr John Sentamu", "Kiel Canal", "Sir Herbert Kitchener", "Cevennes", "eggs", "Luigi Pirandello", "Sheffield United", "Elvis Costello", "Midtown", "Eddy Shah", "Hugh Laurie", "a cappella", "Netherlands", "Take That", "Red squirrel", "the Indus River", "Adam Smith", "Model T", "Spice Girls", "Brian Blessed", "Michael Caine", "king of Ickenham", "pig", "Bank of England", "William Gilbert", "Pet Sounds", "the monarch", "Bangladesh", "St Clements", "Pollux", "Nalini Negi as Ishaani Ishaan Sinha", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "Ole Einar Bj\u00f8rndalen", "Kansas Cityizards", "University of Mississippi", "The Rebirth", "General Gratien Kabiligi,", "Pixar's \"Toy Story.\"", "Akio Toyoda", "Laertes", "Gonzo", "chicken Kiev"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6427083333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1472", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-2721", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-6037", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-907", "mrqa_triviaqa-validation-299", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-6853", "mrqa_hotpotqa-validation-217", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-250"], "SR": 0.59375, "CSR": 0.5804924242424243, "retrieved_ids": ["mrqa_squad-train-26095", "mrqa_squad-train-77303", "mrqa_squad-train-9139", "mrqa_squad-train-21012", "mrqa_squad-train-78859", "mrqa_squad-train-5172", "mrqa_squad-train-69312", "mrqa_squad-train-32705", "mrqa_squad-train-13659", "mrqa_squad-train-70465", "mrqa_squad-train-38767", "mrqa_squad-train-52771", "mrqa_squad-train-18198", "mrqa_squad-train-33677", "mrqa_squad-train-10797", "mrqa_squad-train-62263", "mrqa_searchqa-validation-1757", "mrqa_hotpotqa-validation-3892", "mrqa_hotpotqa-validation-1887", "mrqa_naturalquestions-validation-8744", "mrqa_hotpotqa-validation-5074", "mrqa_naturalquestions-validation-3686", "mrqa_searchqa-validation-9438", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3816", "mrqa_searchqa-validation-748", "mrqa_hotpotqa-validation-3459", "mrqa_naturalquestions-validation-1904", "mrqa_searchqa-validation-1562", "mrqa_squad-validation-1076", "mrqa_searchqa-validation-4995", "mrqa_naturalquestions-validation-8585"], "EFR": 1.0, "Overall": 0.7460203598484848}, {"timecode": 33, "before_eval_results": {"predictions": ["both houses of Congress", "Duval County", "the Atlantic Ocean", "Virgin", "is the opposition of a body or substance to the flow of electrical current through it", "tibet", "the County of Gloucestershire", "1709", "Tutankhamun", "Morgan Spurlock", "the iris", "Massachusetts", "Andre Agassi", "JMW Turner", "Dutch", "Bruce", "pearls", "yellow", "Tbilisi", "Mrs Merton", "Austria/Habsburg Monarchy-Challenge Cup", "Wyoming", "Catherine Cookson", "Hugh Quarshie", "Flanagan", "\"My Sweet Lord\"", "Alan Sugar", "crying", "Henri Paul", "the Red Sea", "Helen Gurley Brown", "the Wash", "wool", "Benfica", "Mark Carney", "Eva Cassidy", "c\u00f4te d'Or", "Utah", "Toy Story", "David Lloyd George", "Italy", "'Lord Nelson'", "George Osborne", "August 10, 1960", "Apollo", "Gentlemen Prefer Blondes", "Waddington's House of Games", "Harry Shearer", "Vincent Van Gogh", "Tanzania", "hydrogen isotopes", "Demi Moore", "28 July 1914 to 11 November 1918", "Spike", "1995", "Ellie Kemper", "7 June 1954", "Oryzomyini", "Ryder Russell", "NATO", "The Washington Post", "Plymouth", "yorkshire", "Pearl Jam"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7526041666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-3127", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7330", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1375", "mrqa_triviaqa-validation-5950", "mrqa_hotpotqa-validation-5041", "mrqa_searchqa-validation-15915"], "SR": 0.71875, "CSR": 0.5845588235294117, "retrieved_ids": ["mrqa_squad-train-81137", "mrqa_squad-train-36453", "mrqa_squad-train-57453", "mrqa_squad-train-60630", "mrqa_squad-train-582", "mrqa_squad-train-81760", "mrqa_squad-train-41529", "mrqa_squad-train-16638", "mrqa_squad-train-85473", "mrqa_squad-train-25800", "mrqa_squad-train-46729", "mrqa_squad-train-49465", "mrqa_squad-train-33473", "mrqa_squad-train-54957", "mrqa_squad-train-31354", "mrqa_squad-train-15799", "mrqa_searchqa-validation-12146", "mrqa_triviaqa-validation-3246", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-2702", "mrqa_squad-validation-9213", "mrqa_squad-validation-8978", "mrqa_naturalquestions-validation-8582", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2039", "mrqa_naturalquestions-validation-6214", "mrqa_newsqa-validation-4122", "mrqa_squad-validation-6914", "mrqa_hotpotqa-validation-959", "mrqa_squad-validation-6966"], "EFR": 1.0, "Overall": 0.7468336397058823}, {"timecode": 34, "before_eval_results": {"predictions": ["San Diego\u2013Tijuana", "petroleum", "Pepsi", "New Orleans", "Dan Rather", "if", "GIGO", "a pawn", "landlords", "silk", "bamboo", "Arthur C. Clarke", "a rice measuring cup", "a smith", "if", "William smith", "smith", "Led Zeppelin", "Alderney", "Charles Lindbergh", "River Phoenix", "out of Oz", "smith", "the Krntnertor Theatre", "Jason", "The Curse of the Play", "the Marine Band", "AbeBooks.com Community Forum", "florence nightingale", "Profiles in Courage", "bogota", "Atonement", "Bea Arthur", "smith", "Naples", "Sacha Baron Cohen", "coal", "Lon Foucault", "Hanna Glawari", "humerus", "Harriet Tubman", "a horse", "Louisa May Alcott", "a watch", "Hugh Williams", "Margaret Atwood", "if Queso Anejo cheese is good for quesadillas, is Enchilado", "Khartoum", "Joaquin Phoenix", "Andrew Wyeth", "Moby Dick", "The Hot Chick", "iOS", "Mars Hill", "Aaron Harrison", "Gwyneth Paltrow", "gollum", "aromatherapy", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld", "140 million", "1923", "Saturday", "9-1", "South Carolina Republican Party Chairwoman Karen Floyd"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5278230676328503}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8695652173913044, 0.8, 0.0, 0.0, 0.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_squad-validation-2831", "mrqa_searchqa-validation-1570", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-4738", "mrqa_searchqa-validation-7325", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-4524", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-12487", "mrqa_searchqa-validation-15135", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-7837", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-12354", "mrqa_searchqa-validation-15325", "mrqa_searchqa-validation-8940", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-2976", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-7832", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4753", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-30", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2996", "mrqa_newsqa-validation-4058"], "SR": 0.46875, "CSR": 0.58125, "retrieved_ids": ["mrqa_squad-train-40740", "mrqa_squad-train-1026", "mrqa_squad-train-86152", "mrqa_squad-train-24916", "mrqa_squad-train-13694", "mrqa_squad-train-60931", "mrqa_squad-train-69400", "mrqa_squad-train-11038", "mrqa_squad-train-32060", "mrqa_squad-train-10623", "mrqa_squad-train-72118", "mrqa_squad-train-13337", "mrqa_squad-train-31631", "mrqa_squad-train-44251", "mrqa_squad-train-21802", "mrqa_squad-train-41421", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-6469", "mrqa_newsqa-validation-2607", "mrqa_squad-validation-6640", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-4523", "mrqa_squad-validation-10158", "mrqa_newsqa-validation-776", "mrqa_triviaqa-validation-3551", "mrqa_searchqa-validation-2538", "mrqa_newsqa-validation-3608", "mrqa_naturalquestions-validation-2146", "mrqa_newsqa-validation-2964", "mrqa_searchqa-validation-13963", "mrqa_newsqa-validation-2730", "mrqa_naturalquestions-validation-5538"], "EFR": 1.0, "Overall": 0.7461718749999999}, {"timecode": 35, "before_eval_results": {"predictions": ["an arrow", "the Chicago Bears", "Floridians", "green and yellow", "Central-Eastern Europe", "Regional Rural Bank", "M2M", "American 3D computer-animated comedy film", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies", "Division of Cook", "July 16, 1971", "13 May 2018", "Kentucky River", "Barbara Niven", "Ferengi Quark", "Firth of Forth", "Santiago del Estero Province", "a super-regional shopping mall", "the Passion", "Abbey Road", "Mel Blanc", "the Czech Kingdom", "2000", "Lamar Wyatt", "Alfred Preis", "Terry Malloy", "her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine", "The interview", "various registries", "the first season", "John Bingham, 7th Earl of Lucan", "January 15, 1975", "Chiwetel Ejiofor", "Appleby-in-Westmorland", "27 November 1956", "Charles de Gaulle Airport", "The St Andrews Agreement", "the Seasiders", "Victorian College of the Arts and Melbourne Conservatorium of Music", "south", "Nick Cassavetes", "Cate Blanchett", "Linda Ronstadt", "January 28, 2016", "Hopi", "John Meston", "Benvolio", "\u00c6thelwald Moll", "Shqiptar\u00ebt e Malit t\u00eb Zi", "University of North Staffordshire", "Battle of the Rosebud", "Jaffrey, New Hampshire", "1998", "the east African coast", "Sara Gilbert", "Elkie Brooks", "acrostic", "blue", "Asashoryu", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah", "1941", "(b) Salinger", "sapphire", "Luxor"], "metric_results": {"EM": 0.625, "QA-F1": 0.6997395833333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-3867", "mrqa_naturalquestions-validation-6452", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-1457", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-1442"], "SR": 0.625, "CSR": 0.5824652777777778, "retrieved_ids": ["mrqa_squad-train-33023", "mrqa_squad-train-14360", "mrqa_squad-train-58241", "mrqa_squad-train-6100", "mrqa_squad-train-24005", "mrqa_squad-train-48711", "mrqa_squad-train-79238", "mrqa_squad-train-38032", "mrqa_squad-train-80551", "mrqa_squad-train-34347", "mrqa_squad-train-14337", "mrqa_squad-train-21939", "mrqa_squad-train-56153", "mrqa_squad-train-57388", "mrqa_squad-train-20219", "mrqa_squad-train-58468", "mrqa_newsqa-validation-3101", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-2887", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-1102", "mrqa_searchqa-validation-8713", "mrqa_hotpotqa-validation-5438", "mrqa_searchqa-validation-7787", "mrqa_triviaqa-validation-3551", "mrqa_searchqa-validation-16869", "mrqa_hotpotqa-validation-2945", "mrqa_searchqa-validation-1562", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-97", "mrqa_triviaqa-validation-2221", "mrqa_naturalquestions-validation-5596"], "EFR": 1.0, "Overall": 0.7464149305555555}, {"timecode": 36, "before_eval_results": {"predictions": ["in an H+ or hydrogen ion gradient to generate ATP energy", "Caesars Palace Grand Prix", "A compact car", "Benjam\u00edn Arellano F\u00e9lix", "Fred Chase Koch", "Enigma", "Flavivirus", "Julia Compton Moore", "Lord's Resistance Army", "Workers' Party", "Yasiin Bey", "1763\u20131791", "Nationalism", "the Swiss national team", "A remake of George A. Romero's 1978 film of the same name,", "Oldham County, Kentucky", "The Captain Matchbox Whoopee Band", "Jeff Schaffer", "wild boar, and red, fallow and roe deer", "Ghana", "Nikolai Alexandrovich Morozov", "Rabies", "Switzerland", "Tennessee", "Godiva", "October 21, 2016", "August 1973", "Lawrence of Arabia", "The Ansonia Hotel", "1937", "Government of Ireland", "Leona Lewis", "John Robert Cocker", "$7.3 billion", "Angus Brayshaw", "This technique works on the relationship between the mind and the body and at developing an actor\u2019s conscious awareness.", "Black Abbots", "Sarah Kerrigan, the Queen of Blades", "German", "Katy Perry", "Bharat Ratna", "Wilderness Road", "75 mi southeast", "\"Nebo Zovyot\"", "\"Orchard County\"", "Kree", "110", "the shortest player ever to play in the National Basketball Association", "co-authorship", "Denmark", "Patricia Arquette", "James Corden", "Audrey II", "1998", "red", "Shakyamuni", "Jim Braddock", "\"He hears what I'm saying, but there's just no coming through,\"", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Brazilian supreme court judge", "New York Yankees", "the Washington Redskins", "( Samuel Taylor) Coleridge", "A - Abel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6069112872421696}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.4, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.11764705882352941, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.4, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8903", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-4571", "mrqa_hotpotqa-validation-5490", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1009", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1872", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-4569", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4184", "mrqa_searchqa-validation-11491", "mrqa_triviaqa-validation-768"], "SR": 0.515625, "CSR": 0.5806587837837838, "retrieved_ids": ["mrqa_squad-train-4205", "mrqa_squad-train-1697", "mrqa_squad-train-84781", "mrqa_squad-train-38084", "mrqa_squad-train-11400", "mrqa_squad-train-79035", "mrqa_squad-train-65352", "mrqa_squad-train-4895", "mrqa_squad-train-48932", "mrqa_squad-train-77815", "mrqa_squad-train-31895", "mrqa_squad-train-81028", "mrqa_squad-train-55804", "mrqa_squad-train-18556", "mrqa_squad-train-44831", "mrqa_squad-train-1200", "mrqa_searchqa-validation-16832", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-2327", "mrqa_naturalquestions-validation-8329", "mrqa_triviaqa-validation-1794", "mrqa_hotpotqa-validation-4301", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-3609", "mrqa_triviaqa-validation-5873", "mrqa_squad-validation-3597", "mrqa_naturalquestions-validation-3745", "mrqa_newsqa-validation-3527", "mrqa_hotpotqa-validation-5494", "mrqa_squad-validation-8399"], "EFR": 1.0, "Overall": 0.7460536317567568}, {"timecode": 37, "before_eval_results": {"predictions": ["the plain moraine plateau", "londres", "cauliflower", "Nizhny Novgorod", "James Bond", "a unit of money", "keeper of the Longstone (Fame Islands) lighthouse", "\"Carlos the Jackal\"", "Australia", "Annelies Marie Frank", "Belgium", "Sufjan Stevens", "Gibraltar", "Benny Hill", "Roddy Doyle", "Kevin Spacey", "Alexandria", "Chad", "1215", "the neck", "David Hockney", "Rudyard Kipling", "lactic acidosis", "Fleet Street", "The Netherlands", "fractal geometry", "Cosmos: A spacetime Odyssey", "duck-billed platypus", "Aquaman", "(Jean-Paul) Sartre", "Snowdrop", "carbs", "\"The Mad Axeman\"", "Scotch whisky", "Switzerland", "a sheep or lamb", "a piano", "Babe Ruth", "(James Thurber)", "a Morph", "Heston Blumenthal", "Tony Curtis", "U2", "Tiny", "lite", "Charlie Harper", "Morgan Choir", "Canada", "Buster Edwards", "Chief Inspector of Prisons", "Henley Royal Regatta", "Paul Lynde", "the city of Indianapolis", "Cairo, Illinois", "Lazio region", "Al D'Amato", "\" SKUM\"", "BET", "Eintracht Frankfurt", "October 2007", "turkeys", "Elena Ceausescu", "the Siberian Husky", "artiodactyl mammal"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5885416666666666}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-953", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-665", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6211", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-7575", "mrqa_triviaqa-validation-4500", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7109", "mrqa_naturalquestions-validation-8239", "mrqa_newsqa-validation-731", "mrqa_searchqa-validation-10306", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-10797", "mrqa_hotpotqa-validation-2366"], "SR": 0.515625, "CSR": 0.5789473684210527, "retrieved_ids": ["mrqa_squad-train-12959", "mrqa_squad-train-42651", "mrqa_squad-train-63281", "mrqa_squad-train-49654", "mrqa_squad-train-9362", "mrqa_squad-train-73281", "mrqa_squad-train-25204", "mrqa_squad-train-37541", "mrqa_squad-train-5575", "mrqa_squad-train-46552", "mrqa_squad-train-21002", "mrqa_squad-train-12420", "mrqa_squad-train-20450", "mrqa_squad-train-67253", "mrqa_squad-train-63063", "mrqa_squad-train-2399", "mrqa_triviaqa-validation-2638", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-4883", "mrqa_naturalquestions-validation-3483", "mrqa_hotpotqa-validation-2117", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-6366", "mrqa_naturalquestions-validation-6917", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-7054", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-987", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-3771", "mrqa_hotpotqa-validation-2088"], "EFR": 1.0, "Overall": 0.7457113486842105}, {"timecode": 38, "before_eval_results": {"predictions": ["avionics, telecommunications, and computers", "Jewish", "Peter, Paul and Mary", "Mount Rainier, Washington", "Zack Snyder", "Mondays", "The Cosmopolitan", "Anna Clyne", "Meghan Markle", "terrorist activity", "Commissioner", "August 1973", "Burnley", "Teenitans Go!", "Evey's mother", "Love and Theft", "1978", "SKUM", "Edmonton, Alberta", "Seattle", "\"The School Boys\"", "Orchard Central", "The Kennedy Center", "commanders of the Great Army", "Environmental Protection Agency", "Humberside", "Diamond Rio", "Prospero", "Northampton, England", "Mike Greenwell", "2017", "Scandinavian Airlines", "polka", "Brian Patrick Friel", "1860", "2006", "Ghanaian national team", "Coronation Street", "\"The Dragon\"", "Cold Spring Historic District", "white fox, polar fox, or snow fox", "Sophie Monk", "The Supremes", "Southern Progress Corporation", "Melbourne Storm", "twenty", "9 November 1967", "Retina display", "technical director", "Cincinnati metropolitan area", "Captain B.J. Hunnicutt", "19 June 2018", "naos", "31 - member", "kenjutsu", "Jeffrey Archer", "Arethusa", "little blue booties", "Majid Movahedi,", "the leader of a drug cartel", "February 2, 2016", "ninjutsu", "witch", "Mozart"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7258184523809523}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3208", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-1300", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-5545", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-16163", "mrqa_searchqa-validation-3163"], "SR": 0.65625, "CSR": 0.5809294871794872, "retrieved_ids": ["mrqa_squad-train-44136", "mrqa_squad-train-85869", "mrqa_squad-train-34573", "mrqa_squad-train-17824", "mrqa_squad-train-63534", "mrqa_squad-train-65205", "mrqa_squad-train-46848", "mrqa_squad-train-21934", "mrqa_squad-train-10919", "mrqa_squad-train-24972", "mrqa_squad-train-86200", "mrqa_squad-train-67662", "mrqa_squad-train-45533", "mrqa_squad-train-9649", "mrqa_squad-train-10957", "mrqa_squad-train-54758", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-4470", "mrqa_naturalquestions-validation-4891", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-1386", "mrqa_naturalquestions-validation-3242", "mrqa_newsqa-validation-223", "mrqa_searchqa-validation-12237", "mrqa_hotpotqa-validation-59", "mrqa_squad-validation-2391", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-3189", "mrqa_squad-validation-3664", "mrqa_searchqa-validation-11846", "mrqa_newsqa-validation-3457"], "EFR": 1.0, "Overall": 0.7461077724358974}, {"timecode": 39, "before_eval_results": {"predictions": ["funding education, sanitation, and traffic control within the city limits", "Aly Raisman", "40,400", "Seoul, South Korea", "American", "the famous Albert Bridge, London", "Learjet", "Distinguished Service Cross", "Revolver", "Sam Raimi", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "Appleby-in-Westmorland", "Wolfgang Amadeus Mozart", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Mercer University", "Dame Eileen June Atkins", "March 31, 1816", "Little Dixie", "1979", "1905", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Loughborough University", "Los Angeles", "2.1 million", "Granada", "Alfred Joel Horford Reynoso", "\"The Young Ones\"", "Mauritian", "Anthony Stephen Burke", "Alcorn State", "Wilderness Road", "Alfred Edward Housman", "The Killer", "London", "nearly 8 km", "25 October 1921", "\"War & Peace\"", "2016 U.S. Senate election", "Tayeb Salih", "Mickey Gilley's Club", "Jedi", "Edward Albert Heimberger", "Whoopi Goldberg", "Gian Carlo Menotti", "Indian", "Target Corporation", "Archbishop of Canterbury", "sub-Saharan Africa", "66 km south of Newcastle & 93 km north of Sydney", "gamecock", "Aaliyah Dana Haughton", "Mad - Eye Moody", "A vanishing point", "16,801", "Lady Gaga", "October", "Georgetown", "The Palm Jumeirah", "consumer confidence", "Peter Garrett", "Scarlett Johansson", "Fried Green Tomatoes", "Bon Jovi", "Marcie Blane"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6178127428127428}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666665, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.30769230769230765, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7246", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-4770", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1074", "mrqa_newsqa-validation-101", "mrqa_searchqa-validation-2773"], "SR": 0.53125, "CSR": 0.5796875, "retrieved_ids": ["mrqa_squad-train-79043", "mrqa_squad-train-49019", "mrqa_squad-train-75966", "mrqa_squad-train-78416", "mrqa_squad-train-6348", "mrqa_squad-train-41505", "mrqa_squad-train-31877", "mrqa_squad-train-76664", "mrqa_squad-train-44304", "mrqa_squad-train-15257", "mrqa_squad-train-83109", "mrqa_squad-train-17988", "mrqa_squad-train-77057", "mrqa_squad-train-13273", "mrqa_squad-train-65055", "mrqa_squad-train-23356", "mrqa_searchqa-validation-12487", "mrqa_searchqa-validation-12354", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1757", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-4343", "mrqa_newsqa-validation-4122", "mrqa_searchqa-validation-3375", "mrqa_triviaqa-validation-2240", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-6500", "mrqa_triviaqa-validation-2406", "mrqa_hotpotqa-validation-569", "mrqa_naturalquestions-validation-5040"], "EFR": 1.0, "Overall": 0.745859375}, {"timecode": 40, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1611", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2331", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3188", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5538", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-5705", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4998", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9480", "mrqa_squad-validation-10044", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-10326", "mrqa_squad-validation-10425", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1231", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1472", "mrqa_squad-validation-1608", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2006", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2819", "mrqa_squad-validation-297", "mrqa_squad-validation-3001", "mrqa_squad-validation-3262", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3812", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4078", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4543", "mrqa_squad-validation-4611", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5079", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5355", "mrqa_squad-validation-5563", "mrqa_squad-validation-5597", "mrqa_squad-validation-5616", "mrqa_squad-validation-5881", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6223", "mrqa_squad-validation-6251", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-7952", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8199", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9768", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6607", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.888671875, "KG": 0.503125, "before_eval_results": {"predictions": ["gilt bronze", "\"The oceans are kind of the last frontier for use and development,\"", "the Taliban", "said they are \"still trying to absorb the impact of this week's stunning events.\"", "Pakistan's North West Frontier Province,", "$250,000", "The Valley swim Club", "the actor who created one of British television's most surreal thrillers", "in a stream in Shark River Park in Monmouth County", "air support", "Governor Sanford", "between 1917 and 1924", "Frank Ricci", "Janet Napolitano", "Mahmoud Ahmadinejad", "\"utterly baseless.\"", "Jacob", "Police", "Microsoft", "Lousiana", "prostate cancer,", "German Bundesliga", "Tsvangirai", "the FBI.", "\"GoldenEye\"", "inconclusive", "a dog", "the Kurdish militant group", "Elizabeth Taylor", "Ralph Cifaretto", "not", "the People's Republic of Arizona", "17", "a rally", "an empty water bottle", "Val d'Isere, France", "a head injury", "News of the World tabloid", "Pakistan's", "200", "Manny Pacquiao", "1964", "Pixar's", "Mandi Hamlin", "environmental", "80", "United Nations World Food Program vessels", "The meter reader who led authorities last week to remains believed to be those of Caylee Anthony called police", "BC Place Stadium.", "Gary Player,", "200", "Adam", "Erika Mitchell Leonard", "1969", "V\u00e1clav Havel", "left", "the Industrial Revolution", "London", "703", "20 October 1980", "glaciers", "Richard M Daley", "Cerberus", "1967"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5822104286378583}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.8695652173913044, 0.0, 1.0, 0.5, 0.18181818181818182, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.05714285714285715, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-2237", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1159", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-2427", "mrqa_searchqa-validation-2894"], "SR": 0.515625, "CSR": 0.578125, "retrieved_ids": ["mrqa_squad-train-54427", "mrqa_squad-train-20665", "mrqa_squad-train-32319", "mrqa_squad-train-32669", "mrqa_squad-train-50987", "mrqa_squad-train-28393", "mrqa_squad-train-54125", "mrqa_squad-train-39415", "mrqa_squad-train-52822", "mrqa_squad-train-28390", "mrqa_squad-train-67065", "mrqa_squad-train-71869", "mrqa_squad-train-65615", "mrqa_squad-train-13106", "mrqa_squad-train-54156", "mrqa_squad-train-20540", "mrqa_naturalquestions-validation-6917", "mrqa_newsqa-validation-3872", "mrqa_hotpotqa-validation-5569", "mrqa_searchqa-validation-7832", "mrqa_naturalquestions-validation-1479", "mrqa_hotpotqa-validation-3435", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-10680", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-23", "mrqa_hotpotqa-validation-1217", "mrqa_squad-validation-3193", "mrqa_hotpotqa-validation-2177", "mrqa_triviaqa-validation-3896", "mrqa_searchqa-validation-7453", "mrqa_hotpotqa-validation-5467"], "EFR": 1.0, "Overall": 0.750234375}, {"timecode": 41, "before_eval_results": {"predictions": ["Business Connect", "to change the music on the CD player", "40-year-old", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "tortured (Mohammed al-) Qahtani,\"", "five", "Kevin Kuranyi", "Kenyan and Somali governments", "Aung San Suu Kyi", "legitimacy of that race.", "\"gotten the balance right\"", "inspiring people", "Mashhad", "Swat Valley.", "North Korea", "pesos", "Steven Green", "Muslim Eid-ul-Adha", "over 1000 square meters", "sailor", "they'd get to bring a new puppy with them to the White House in January.", "27-year-old's", "Friday,", "Los Ticos", "Seasons of My Heart", "helping on the sandbags", "$17,000", "opium", "not doing more since taking office.", "10 years", "$8.8 million.", "Noida, located in the outskirts of the capital New Delhi.", "Lisa Brown", "at least two and a half hours.", "his death cast a shadow over festivities ahead of South Africa's highly-anticipated appearance in the rugby World Cup final with England this weekend.", "Zimbabwean President Robert Mugabe", "31 meters (102 feet)", "London's Heathrow airport", "165-room", "Transport Workers Union leaders", "state senators who will decide whether to remove him from office", "84-year-old", "Nieb\u00fcll", "Roland S. Martin", "Sen. Barack Obama", "a mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Anil Kapoor", "could become a very serious risk.", "Marie-Therese Walter.", "South African police", "Hurricane Gustav", "Felix Baumgartner ( German : ( \u02c8fe\u02d0l\u026aks \u02c8ba\u028a\u032fm\u02cc\u0261a\u0250\u032ftn\u0250 )", "July 2012", "1260 cubic centimeters ( cm ) for men", "George Bernard Shaw", "Dutch", "Kievan Rus", "Toronto", "1909", "Robert L. Stone", "Nike", "Barbara Bush", "QWERTY", "corpulent"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6033630993189817}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 1.0, 0.4444444444444445, 0.6666666666666666, 0.5333333333333333, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-3430", "mrqa_newsqa-validation-2271", "mrqa_newsqa-validation-2370", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-7458", "mrqa_triviaqa-validation-2480", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-2926", "mrqa_searchqa-validation-7340"], "SR": 0.515625, "CSR": 0.5766369047619048, "retrieved_ids": ["mrqa_squad-train-30998", "mrqa_squad-train-43849", "mrqa_squad-train-17748", "mrqa_squad-train-17679", "mrqa_squad-train-32334", "mrqa_squad-train-70577", "mrqa_squad-train-74", "mrqa_squad-train-59248", "mrqa_squad-train-13314", "mrqa_squad-train-70718", "mrqa_squad-train-74067", "mrqa_squad-train-63083", "mrqa_squad-train-33643", "mrqa_squad-train-813", "mrqa_squad-train-56026", "mrqa_squad-train-17786", "mrqa_searchqa-validation-2057", "mrqa_naturalquestions-validation-7627", "mrqa_hotpotqa-validation-2357", "mrqa_triviaqa-validation-5431", "mrqa_newsqa-validation-185", "mrqa_triviaqa-validation-5834", "mrqa_naturalquestions-validation-9390", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-23", "mrqa_naturalquestions-validation-6843", "mrqa_newsqa-validation-57", "mrqa_hotpotqa-validation-4204", "mrqa_searchqa-validation-11846", "mrqa_squad-validation-1703", "mrqa_squad-validation-8960", "mrqa_squad-validation-1076"], "EFR": 1.0, "Overall": 0.749936755952381}, {"timecode": 42, "before_eval_results": {"predictions": ["Wittenberg", "alcohol", "Bligh", "Parkinson's", "Changing Places", "Lindanisa", "Moscow", "g-10", "Portugal", "first among equals", "Friedrich Nietzsche", "the moon", "Moldova", "Zak Starkey", "Craggy Island", "the Suez Canal", "otters", "shagbark", "Port Talbot", "Rapa Nui", "Diary of a Tuber", "Charlie Cairoli", "Salvador Allende", "Mike Tyson", "Edward Elgar", "conductor", "Boyle", "a medium", "a winter fur hat", "Tony Blair", "Adolf Hitler", "Jamaica", "June Brae", "hypertension", "1066", "The Three Graces", "Jesse James", "The Purple Heart Medal", "Cerebro- Spinal Fluid", "Jessica Simpson", "bacteria", "the MacKenzie River", "Damian Green", "NASCAR", "Canada", "Pennsylvania", "Nic\u00e9phore Ni\u00e9pce", "Argentina", "Kwame Nkrumah", "The Color Purple", "terrorism", "zinc - carbon cells", "two goods", "Mariah Carey", "best in film and American television", "unidentified flying objects", "the Chicago Bears", "a grizzly bear", "Turkey", "Pew Research Center", "inflammation", "Ferrari", "Smoky Mountains National Park", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6163847117794486}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-2165", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-797", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-9911", "mrqa_naturalquestions-validation-4915"], "SR": 0.546875, "CSR": 0.5759447674418605, "retrieved_ids": ["mrqa_squad-train-18871", "mrqa_squad-train-14181", "mrqa_squad-train-34906", "mrqa_squad-train-74362", "mrqa_squad-train-59021", "mrqa_squad-train-72436", "mrqa_squad-train-81902", "mrqa_squad-train-37545", "mrqa_squad-train-61621", "mrqa_squad-train-62365", "mrqa_squad-train-48299", "mrqa_squad-train-39686", "mrqa_squad-train-16261", "mrqa_squad-train-20521", "mrqa_squad-train-53714", "mrqa_squad-train-82349", "mrqa_newsqa-validation-1281", "mrqa_searchqa-validation-1368", "mrqa_newsqa-validation-2230", "mrqa_searchqa-validation-5048", "mrqa_hotpotqa-validation-96", "mrqa_squad-validation-8903", "mrqa_newsqa-validation-223", "mrqa_triviaqa-validation-3114", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-1926", "mrqa_triviaqa-validation-1074", "mrqa_hotpotqa-validation-5286", "mrqa_naturalquestions-validation-5564", "mrqa_hotpotqa-validation-5636", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-8329"], "EFR": 1.0, "Overall": 0.7497983284883721}, {"timecode": 43, "before_eval_results": {"predictions": ["John M. Grunsfeld", "dancing with the Stars", "psychotropic drugs", "opium", "severe famine", "Illlinois.", "Democrat", "robert f. Ferguson", "16", "Vrishti Bhowmik.", "forgery and flying without a valid license,", "President Bush", "15-year-old", "seven", "upper respiratory infection,\"", "543", "Kevin Kuranyi", "Roy Miller", "Susan Atkins,", "Ameneh Bahrami", "Singapore Airlines", "$1,500", "Al Nisr Al Saudi", "We Found Love", "his parents", "Ralph Lauren", "Joe Patane", "humanitarian mission.", "North Korea", "beetles", "Old Trafford", "Bronx County District Attorneys Office", "Arabic, French and English", "Britain.", "Arsene Wenger", "The Justice Department", "Michael Jackson", "all day starting at 10 a.m.,", "Ma Khin Khin Leh", "South African police", "an American who entered the country illegally from China", "Palestinian Islamic Army,", "equality", "was killed", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "consumer confidence", "Phil Spector", "District of Columbia National Guard", "Australia and New Zealand", "Steven Gerrard", "opened considerably higher Tuesday", "with an armature of piped masonry often carved in decorative patterns", "Hermann Ebbinghaus", "southern whites", "a dog", "myxoma virus", "Wisconsin", "Battle of Britain and the Battle of Malta", "Viacom Media Networks", "five", "cadeado", "the Star-Spangled Banner", "Bing Crosby", "c"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6071885442773601}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.8571428571428571, 0.9473684210526316, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-540", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2065", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9516", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-2853", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-7435"], "SR": 0.53125, "CSR": 0.5749289772727273, "retrieved_ids": ["mrqa_squad-train-81951", "mrqa_squad-train-10625", "mrqa_squad-train-54863", "mrqa_squad-train-51398", "mrqa_squad-train-21563", "mrqa_squad-train-75378", "mrqa_squad-train-16306", "mrqa_squad-train-54726", "mrqa_squad-train-70119", "mrqa_squad-train-43004", "mrqa_squad-train-52534", "mrqa_squad-train-15371", "mrqa_squad-train-54558", "mrqa_squad-train-58459", "mrqa_squad-train-21898", "mrqa_squad-train-44350", "mrqa_squad-validation-10158", "mrqa_squad-validation-1384", "mrqa_triviaqa-validation-7128", "mrqa_searchqa-validation-1715", "mrqa_hotpotqa-validation-3921", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-5358", "mrqa_hotpotqa-validation-4223", "mrqa_searchqa-validation-9911", "mrqa_triviaqa-validation-4263", "mrqa_hotpotqa-validation-1358", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-3163", "mrqa_newsqa-validation-990", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-1437"], "EFR": 0.9666666666666667, "Overall": 0.7429285037878788}, {"timecode": 44, "before_eval_results": {"predictions": ["Supreme Court of the United Kingdom", "Seal", "nasal cavity", "coffin-maker", "144 inches", "the Wye", "Zorro", "USS Thresher", "tattoo", "Chongqing", "Mitsubishi", "eagle", "Morgan", "Jackie Jackson", "different levels of human psychological and physical needs", "Prague", "Yellowstone", "Hornets", "Nevada", "muezzin", "sheep", "Rihanna", "Tintin", "Alexandrina", "22", "(Hector) Berlioz", "Azerbaijan", "Ireland", "Common Ash", "Madness", "the Dalton Gang", "Australia", "Phil Woolas", "bats", "American", "Penelope Keith", "Alexei Kosygin", "(John) Brodsky", "Vinegar Joe", "Van Allen Probes", "a sentence", "caccamo", "Steel Beads", "Nicaragua", "Jules Verne", "of Wellington", "Manchester", "Manet", "Burger King", "Thebes", "Rover", "Xiu Li Dai and Yongge Dai", "the President", "David Ben - Gurion", "Chow Tai Fook Enterprises", "an organ", "Point of Entry", "1973's", "a news blackout was imposed on the foreign media.", "Morgan Tsvangirai.", "Hungarian", "China", "China", "Isabella (Belle) Baumfree"], "metric_results": {"EM": 0.5, "QA-F1": 0.5494791666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-1631", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-5745", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-4643", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7048", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4657", "mrqa_naturalquestions-validation-2208", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-15957"], "SR": 0.5, "CSR": 0.5732638888888889, "retrieved_ids": ["mrqa_squad-train-77553", "mrqa_squad-train-67592", "mrqa_squad-train-29006", "mrqa_squad-train-40677", "mrqa_squad-train-41408", "mrqa_squad-train-38961", "mrqa_squad-train-52169", "mrqa_squad-train-55930", "mrqa_squad-train-39654", "mrqa_squad-train-15882", "mrqa_squad-train-46015", "mrqa_squad-train-49006", "mrqa_squad-train-34371", "mrqa_squad-train-4933", "mrqa_squad-train-42681", "mrqa_squad-train-79439", "mrqa_newsqa-validation-858", "mrqa_hotpotqa-validation-4223", "mrqa_searchqa-validation-11633", "mrqa_naturalquestions-validation-734", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-2073", "mrqa_naturalquestions-validation-10625", "mrqa_hotpotqa-validation-303", "mrqa_searchqa-validation-3269", "mrqa_triviaqa-validation-4523", "mrqa_hotpotqa-validation-5559", "mrqa_triviaqa-validation-7460", "mrqa_newsqa-validation-2983", "mrqa_searchqa-validation-6091", "mrqa_hotpotqa-validation-5482", "mrqa_triviaqa-validation-4191"], "EFR": 1.0, "Overall": 0.7492621527777779}, {"timecode": 45, "before_eval_results": {"predictions": ["trespassing at a nuclear-missile installation", "new york", "2\u00ef\u00bf\u00bd", "the spine and skull", "Ethiopia", "jean spurts", "united states", "Nuuk", "Philippines", "pool", "China", "Richie McCaw", "cotton", "new york", "king george ii", "green beans", "Leeds", "fence", "Elizabeth II", "a dog", "a llamas", "London Underground Piccadilly Line", "antonyms", "barbara delle Bande Nere", "Nepal", "scurvy", "tailors", "Indonesia", "purple coneflower", "the Variations", "keane", "gauteng", "crimson", "Pakistan", "Uranus", "come find yourself", "barbara newton", "uncle martian", "Niki Lauda", "petronas Towers", "the Daily Mirror", "Eric Morley", "Radio waves", "york", "notts County at Goodison Park", "football", "the Reform Club", "William Shakespeare", "Snarks", "trimdon", "germany", "approximately 26,000 years", "US - grown fruit", "Orange Juice", "seventh", "Plato", "Joan Feynman", "Florida", "Martin \"Al\" Culhane,", "400 years", "freelance", "the mouth", "barbara newton", "arrested, arraigned and jailed,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5028799019607844}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-6714", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-2995", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-3890", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-4038", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-5221", "mrqa_naturalquestions-validation-10402", "mrqa_hotpotqa-validation-1169", "mrqa_newsqa-validation-4100", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-11241", "mrqa_newsqa-validation-3806"], "SR": 0.46875, "CSR": 0.5709918478260869, "retrieved_ids": ["mrqa_squad-train-8697", "mrqa_squad-train-15365", "mrqa_squad-train-40329", "mrqa_squad-train-31785", "mrqa_squad-train-52403", "mrqa_squad-train-47241", "mrqa_squad-train-72490", "mrqa_squad-train-14720", "mrqa_squad-train-29077", "mrqa_squad-train-74060", "mrqa_squad-train-864", "mrqa_squad-train-22053", "mrqa_squad-train-82066", "mrqa_squad-train-30868", "mrqa_squad-train-58055", "mrqa_squad-train-12355", "mrqa_triviaqa-validation-1981", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2792", "mrqa_hotpotqa-validation-2076", "mrqa_triviaqa-validation-4573", "mrqa_squad-validation-9388", "mrqa_triviaqa-validation-277", "mrqa_searchqa-validation-8940", "mrqa_triviaqa-validation-4657", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-873", "mrqa_triviaqa-validation-7336", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-1667", "mrqa_newsqa-validation-981", "mrqa_squad-validation-5490"], "EFR": 0.9705882352941176, "Overall": 0.7429253916240409}, {"timecode": 46, "before_eval_results": {"predictions": ["Konwiktorska Street, a ten-minute walk north from the Old Town", "a crust of mashed potato", "Lalo Schifrin", "16 November 2001", "Donovan Regehr", "7 correct numbers", "Billy Hill", "Paul Lynde", "halogenated paraffin hydrocarbons", "the body - centered cubic ( BCC ) lattice", "May 2002", "1996", "virtual reality simulator", "beneath the liver", "1885", "prior to European contact", "January 2012", "Most days are sunny throughout the year", "caused by chlorine and bromine from manmade organohalogens", "Gamora", "1973", "differs in ingredients", "the homicidal thoughts of a troubled youth", "The Hunger Games : Mockingjay -- Part 2 ( 2015 )", "derived from the Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear ) and \u1f00\u03b4\u03b5\u03bb\u03c6\u03cc\u03c2 adelph\u00f3s", "October 22, 2017", "Amitabh Bachchan", "1998", "nine", "Jeff Bezos", "in response to a perceived harmful event, attack, or threat to survival", "the main type of cell found in lymph", "electron donors", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "produced with constant technology and resources per unit of time", "The Queen of Hearts", "2004", "Samuel Chase", "Cetshwayo", "the early 1960s", "Asuka", "Erica Rivera", "set and filmed in New York City", "September 2001", "in teaching elocution", "Chris Rea", "mashed potato", "the Supreme Court holds both original and exclusive jurisdiction", "three", "Sheppard and Louisa Johnson were the opening acts on selected European dates", "Mongol - led Yuan dynasty", "candy", "1948", "Ruth Rendell", "Hidden America with Jonah Ray", "the United Kingdom", "people working in film and the performing arts", "Jean Van de Velde", "between 1917 and 1924", "that former U.S. soldier Steven Green exhibited clear symptoms of acute stress disorder in Iraq and that a military psychiatric nurse-practitioner failed to diagnose the troubled infantryman and pull him out of combat.", "My Name Is Earl", "The Virgin Spring", "cookies", "uncle"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6123273191699081}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true], "QA-F1": [0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.35294117647058826, 1.0, 0.4, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.5, 1.0, 1.0, 0.10526315789473685, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.7692307692307692, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6, 0.0, 0.4615384615384615, 1.0, 1.0, 0.07692307692307691, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.17647058823529413, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-735", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-6321", "mrqa_triviaqa-validation-7778", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-2156", "mrqa_searchqa-validation-7061"], "SR": 0.515625, "CSR": 0.569813829787234, "retrieved_ids": ["mrqa_squad-train-37383", "mrqa_squad-train-52099", "mrqa_squad-train-74127", "mrqa_squad-train-45814", "mrqa_squad-train-20096", "mrqa_squad-train-19440", "mrqa_squad-train-67401", "mrqa_squad-train-82078", "mrqa_squad-train-48250", "mrqa_squad-train-11468", "mrqa_squad-train-54177", "mrqa_squad-train-51532", "mrqa_squad-train-1543", "mrqa_squad-train-15229", "mrqa_squad-train-10497", "mrqa_squad-train-54929", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-6195", "mrqa_naturalquestions-validation-7253", "mrqa_newsqa-validation-1311", "mrqa_hotpotqa-validation-3869", "mrqa_searchqa-validation-1446", "mrqa_squad-validation-6223", "mrqa_searchqa-validation-2976", "mrqa_hotpotqa-validation-2905", "mrqa_squad-validation-6973", "mrqa_triviaqa-validation-1583", "mrqa_newsqa-validation-2792", "mrqa_triviaqa-validation-2638", "mrqa_squad-validation-8399", "mrqa_squad-validation-6435", "mrqa_hotpotqa-validation-5490"], "EFR": 0.967741935483871, "Overall": 0.742120528054221}, {"timecode": 47, "before_eval_results": {"predictions": ["Acadia National Park", "Earl Long", "Luxembourg", "paiyar", "the Space Shuttle Challenger", "the Rolling Stones", "Thomas G Nazareth", "Lapis lazuli", "the Pentagon", "a valley fold", "snails", "bamboo", "the Vietnam War", "Kingston", "Minnie Pearl", "a sailor", "Barnum & Bailey Circus", "Dizzy Gillespie", "oh no", "Ernie Els", "Macedonia", "gestation", "soot", "There Will Be Blood", "Herb Alpert", "John Adams", "the Rolling Stones", "Bernard Montgomery", "Zeus", "Fast Food Nation: The Dark Side of the All-American Meal", "the Tasmanian devil", "Asoka", "a terrarium", "the cyclotron", "John Alden", "Brothels", "Plaza Catalunya", "Yellow Ribbon", "the Indianapolis 500", "Laguna Beach", "Alto", "porter", "minot", "Strait of Gibraltar", "le bton rouge", "Kamehameha", "menudo", "Alan Alda", "cold air", "parkland", "sirloin", "in 1958", "Walter Pauk", "the lungs", "the American Civil War", "jean passe", "the Soviet Union", "Rousillon Rupes", "Obafemi Martins", "\"Twice in a Lifetime\"", "two-state solution", "Republican Party", "minimal delays remained", "The Tempest"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6595643939393939}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-847", "mrqa_searchqa-validation-11698", "mrqa_searchqa-validation-9392", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-13199", "mrqa_searchqa-validation-4024", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-4731", "mrqa_searchqa-validation-16787", "mrqa_searchqa-validation-4769", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-3764", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-9919", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-3693", "mrqa_naturalquestions-validation-3893", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-109", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-3063", "mrqa_newsqa-validation-904"], "SR": 0.5625, "CSR": 0.5696614583333333, "retrieved_ids": ["mrqa_squad-train-22335", "mrqa_squad-train-76640", "mrqa_squad-train-76976", "mrqa_squad-train-42066", "mrqa_squad-train-61334", "mrqa_squad-train-70777", "mrqa_squad-train-33416", "mrqa_squad-train-6312", "mrqa_squad-train-68781", "mrqa_squad-train-38491", "mrqa_squad-train-82969", "mrqa_squad-train-17950", "mrqa_squad-train-38930", "mrqa_squad-train-22653", "mrqa_squad-train-31190", "mrqa_squad-train-18135", "mrqa_newsqa-validation-4122", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-10009", "mrqa_squad-validation-9923", "mrqa_triviaqa-validation-514", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-7614", "mrqa_searchqa-validation-9754", "mrqa_hotpotqa-validation-2271", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-4460", "mrqa_hotpotqa-validation-2366", "mrqa_searchqa-validation-7891", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-948", "mrqa_searchqa-validation-2929"], "EFR": 1.0, "Overall": 0.7485416666666665}, {"timecode": 48, "before_eval_results": {"predictions": ["photoelectric", "31", "Kravitz", "The Satavahanas", "a specific individual to operate one or more types of motorized vehicles", "Somatic motor neurons", "bohrium", "London", "the Chernobyl Nuclear Power Plant", "Dalveer Bhandari", "Bush", "at the center of the Northern Hemisphere", "Exodus", "David Tennant", "a political protest and mercantile protest by the Sons of Liberty in Boston, Massachusetts", "the original timeline is eventually restored", "on Chesapeake Bay, south of Annapolis in Maryland", "A footling breech", "at the 1964 Republican National Convention in San Francisco, California", "four", "Lee Oakes", "1990", "off - road vehicles", "Bob Dylan", "A wide range of research methods", "Johannes Gutenberg", "A firm, flexible bell - shaped device worn inside the vagina", "a single prefix", "William the Conqueror", "Ernest Rutherford", "Nicole Gale Anderson", "William Chatterton Dix", "April 12, 2017", "the Gupta Empire", "Freedom Day", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "Gustav Bauer", "the internal reproductive anatomy", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "Within an English - language book", "Antarctica", "Ren\u00e9 Verdon", "revenge and karma", "the efferent nerves that directly innervate muscles", "1986", "1546", "a crime", "1942", "Bachendri Pal", "John Adams", "early 2014", "Ballets Russes", "Jessica Smith", "J. M. W. Turner", "USS \"Massachusetts\"", "six", "First Family of Competitive Eating", "$2 billion", "the player", "on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "the St. Valentine's Day Massacre", "Andy Warhol", "Gabriel", "opposition parties"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5624663978494623}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true], "QA-F1": [0.4, 0.0, 0.0, 0.0, 0.6, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.9333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.06451612903225806, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-8393", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-6410", "mrqa_hotpotqa-validation-994", "mrqa_hotpotqa-validation-1210", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1290"], "SR": 0.484375, "CSR": 0.5679209183673469, "retrieved_ids": ["mrqa_squad-train-71366", "mrqa_squad-train-58744", "mrqa_squad-train-6903", "mrqa_squad-train-61620", "mrqa_squad-train-12408", "mrqa_squad-train-13333", "mrqa_squad-train-16674", "mrqa_squad-train-4295", "mrqa_squad-train-84303", "mrqa_squad-train-64909", "mrqa_squad-train-47159", "mrqa_squad-train-42919", "mrqa_squad-train-35822", "mrqa_squad-train-24449", "mrqa_squad-train-36459", "mrqa_squad-train-48304", "mrqa_triviaqa-validation-6355", "mrqa_naturalquestions-validation-9342", "mrqa_triviaqa-validation-1374", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8596", "mrqa_newsqa-validation-3159", "mrqa_squad-validation-4326", "mrqa_triviaqa-validation-3266", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-10039", "mrqa_newsqa-validation-3235", "mrqa_hotpotqa-validation-1065", "mrqa_searchqa-validation-6283", "mrqa_hotpotqa-validation-532"], "EFR": 1.0, "Overall": 0.7481935586734695}, {"timecode": 49, "before_eval_results": {"predictions": ["1912", "Standard Oil", "noh.", "These Boots Are Made for Walkin", "sauropods", "greece", "Nancy Lopez", "the ozone hole", "Who's the Boss?", "Donnie Wahlberg", "Tasmania", "the Baltimore Orioles", "Abu Dhabi", "cunard", "Zionism", "Prague", "dressage", "(Isaac) Newton", "Toby Keith", "the accordion", "a black swan", "Edith Piaf", "the Stratosphere Tower", "parkinsonism", "Strings", "Burger", "Guinevere", "Department of Energy", "tangerine", "the German occupation of Belgium", "Dead Ringers", "( Johann) Strauss", "Solidarity", "(James) Carter", "Nick & Norah's", "Charles Lindbergh", "a Lotus", "Haunted Mansion", "a blog", "an eagle", "a war between political factions", "heat", "Teen Titans Go!", "an owl", "Istanbul", "Mary Poppins", "St. Louis", "the Amish", "the Air Force", "Levi's", "Badminton", "Canada", "Sunday", "autopistas", "Phar Lap", "a dove", "Lewis Carroll", "1983", "Excalibur Hotel and Casino", "\"\u010cesk\u00e9 kr\u00e1lovstv\u00ed\"", "Arsenal", "Abdullah Gul,", "humans", "Action Comics"], "metric_results": {"EM": 0.640625, "QA-F1": 0.721875}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3755", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-14374", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-767", "mrqa_searchqa-validation-13380", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-1337", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-6473", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-8129", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-15313", "mrqa_naturalquestions-validation-8350", "mrqa_hotpotqa-validation-4904"], "SR": 0.640625, "CSR": 0.569375, "retrieved_ids": ["mrqa_squad-train-78970", "mrqa_squad-train-40924", "mrqa_squad-train-61237", "mrqa_squad-train-53205", "mrqa_squad-train-24866", "mrqa_squad-train-6233", "mrqa_squad-train-62865", "mrqa_squad-train-16967", "mrqa_squad-train-4361", "mrqa_squad-train-56891", "mrqa_squad-train-71617", "mrqa_squad-train-32955", "mrqa_squad-train-53804", "mrqa_squad-train-6550", "mrqa_squad-train-6053", "mrqa_squad-train-10622", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-1679", "mrqa_hotpotqa-validation-1751", "mrqa_newsqa-validation-1299", "mrqa_triviaqa-validation-1065", "mrqa_hotpotqa-validation-3876", "mrqa_squad-validation-6973", "mrqa_searchqa-validation-11491", "mrqa_squad-validation-2468", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-2702", "mrqa_triviaqa-validation-2427", "mrqa_naturalquestions-validation-4096", "mrqa_triviaqa-validation-2638", "mrqa_searchqa-validation-5553"], "EFR": 1.0, "Overall": 0.748484375}, {"timecode": 50, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1338", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1604", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7716", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12265", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3025", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-3764", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8267", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9911", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3635", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.900390625, "KG": 0.53046875, "before_eval_results": {"predictions": ["Aerosmith", "Willa Cather", "Senate", "the Who", "a science fiction novel", "Bismarck", "analog", "a travel partner", "Luisa Tetrazzini", "Renoir", "the polio vaccine", "a deer", "the Rothschilds", "the bar exam", "Fyodor Dostoevsky", "a Smuckers", "a Spanish conquistador", "China", "grease", "Hollandaise", "Esau", "Dry ice", "Martin Luther King III", "a spontaneous generation", "a catalyst", "Paris", "an American infantryman of WWI", "Uganda", "senators", "Sappho", "the Battle of Thermopylae", "the Maccabean", "Jones", "Hamlet", "a flounder", "the Ganga", "New Brunswick", "Copacabana", "I Write the Songs", "We Own the Night", "Shimon Peres", "By the Sea", "Triceratops", "Lemon Meringue", "BB King", "Thomas Mann", "Krackel", "a dog eat dog world", "Dmitri Mendeleev", "Azkaban", "the Faerie Realm", "thia Weil", "an Arabic masculine given name and occasional surname with the meaning `` beloved ''", "the heads of federal executive departments who form the Cabinet of the United States", "Microsoft", "General John J. Pershing", "Charlie Brooks", "Taylor Swift", "September 23, 1935", "An invoice, bill or tab", "Martin \"Al\" Culhane,", "will be able to gamble in a casino, buy a drink in a pub or see the horror film \"hostel: Part II,\"", "Dangjin", "2015"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6426474567099567}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.25000000000000006, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.4, 1.0, 0.08333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15858", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-130", "mrqa_searchqa-validation-16767", "mrqa_searchqa-validation-803", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-14038", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-14305", "mrqa_searchqa-validation-729", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-15320", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-16283", "mrqa_searchqa-validation-2780", "mrqa_searchqa-validation-159", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7741", "mrqa_naturalquestions-validation-998", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-6798", "mrqa_hotpotqa-validation-5801", "mrqa_newsqa-validation-762"], "SR": 0.546875, "CSR": 0.5689338235294117, "retrieved_ids": ["mrqa_squad-train-54261", "mrqa_squad-train-45447", "mrqa_squad-train-12754", "mrqa_squad-train-15027", "mrqa_squad-train-84457", "mrqa_squad-train-47574", "mrqa_squad-train-85052", "mrqa_squad-train-16063", "mrqa_squad-train-77206", "mrqa_squad-train-79276", "mrqa_squad-train-81177", "mrqa_squad-train-11919", "mrqa_squad-train-64917", "mrqa_squad-train-58271", "mrqa_squad-train-70366", "mrqa_squad-train-11335", "mrqa_triviaqa-validation-4896", "mrqa_hotpotqa-validation-2913", "mrqa_naturalquestions-validation-4728", "mrqa_triviaqa-validation-7054", "mrqa_squad-validation-6848", "mrqa_naturalquestions-validation-5611", "mrqa_hotpotqa-validation-5438", "mrqa_hotpotqa-validation-1238", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-5041", "mrqa_newsqa-validation-2435", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-5632", "mrqa_newsqa-validation-3741", "mrqa_triviaqa-validation-855", "mrqa_triviaqa-validation-7523"], "EFR": 0.9655172413793104, "Overall": 0.7500933379817445}, {"timecode": 51, "before_eval_results": {"predictions": ["cupcake", "Wilkie Collins", "chief of staff", "Lamarr", "polydeuces", "the Civil War", "the Nobel Prize", "Roussimoff", "Twilight", "periodic table", "heaven", "Tommy Tutone", "Miss Havisham", "Thailand", "Taft", "opal", "Taft", "dense", "air pressure", "Echidna", "anaerobic", "porcelain", "Synchronicity", "Tim Duncan", "bees", "dark energy", "Reptiles", "Conakry", "William Herschel", "POMPEIA", "Barbara Walters", "Jubal Anderson", "Perimeter", "a volcanic rock", "watermelon", "Cole Porter", "a sockpuppet", "Taft", "drapery", "Cosmopolitan", "Madagascar", "C.G. Jung", "Carnarvon", "Ontario", "Olympia", "Copernicus", "Lily Allen", "Candlestick Park", "the Bubonic Plague", "Google", "Defense", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "27 January -- 16 April 1898", "Brevet Colonel Robert E. Lee", "Brian Clough", "Gilda", "the Buddha", "Walter R\u00f6hrl", "1986", "Brittany Snow", "Christopher Savoie", "authorizing killings and kidnappings by paramilitary death squads.", "\"Watchmen\"", "Cork"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6006696428571427}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9428571428571428, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-3888", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-1496", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-440", "mrqa_searchqa-validation-5000", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-10284", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-9206", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-11468", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-4971", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-1277", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-5643"], "SR": 0.53125, "CSR": 0.5682091346153846, "retrieved_ids": ["mrqa_squad-train-60095", "mrqa_squad-train-64709", "mrqa_squad-train-3042", "mrqa_squad-train-19192", "mrqa_squad-train-23867", "mrqa_squad-train-42300", "mrqa_squad-train-22474", "mrqa_squad-train-58094", "mrqa_squad-train-10493", "mrqa_squad-train-57772", "mrqa_squad-train-43714", "mrqa_squad-train-35249", "mrqa_squad-train-67419", "mrqa_squad-train-23592", "mrqa_squad-train-1126", "mrqa_squad-train-71822", "mrqa_searchqa-validation-2976", "mrqa_hotpotqa-validation-1009", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4212", "mrqa_squad-validation-3597", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-2475", "mrqa_naturalquestions-validation-1904", "mrqa_searchqa-validation-9570", "mrqa_searchqa-validation-7832", "mrqa_newsqa-validation-57", "mrqa_naturalquestions-validation-1834", "mrqa_hotpotqa-validation-823", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-6786"], "EFR": 1.0, "Overall": 0.756844951923077}, {"timecode": 52, "before_eval_results": {"predictions": ["Deere", "(Ella) VICTORIA", "an electron", "the Missouri River", "brandy", "George Babbitt", "GIGO", "(Gioachino) Rossini", "Ophelia", "Rome", "the Isle of Wight", "Colorado Springs", "hay", "Possession", "( Sir Walter) Scott", "a contact lens", "km", "a surface-to-air missile", "Vibe", "Pulp Fiction", "yelpan", "Frederick Forsyth", "August 1947", "Princess Leia", "Vietnam", "Vince Lombardi", "the global village", "Dubliners", "Sudan", "Kwanza", "Warren Buffett", "Charlie\\'s Angels", "Lincoln", "imagism", "Whimper", "an obsolete word", "oscar wilde", "Taiwan", "Mickey Spillane", "Buzz Lightyear", "Jack Bauer", "a scissors", "kidney stones", "Necessity", "Skull", "The Biggest Blowout", "Atlanta", "Texas", "glucose", "holidays", "tiger's eye Starbuck", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Sir Donald Bradman", "the Caucasus region", "Toplis", "Darby and Joan", "antelope", "Jung Yun-ho", "7.63\u00d725mm Mauser", "united Ireland", "robbery and foreclosure.", "\"a work in progress -- trying to make the love of Jesus known while learning to know Him better ourselves.\"", "a series of wildfires from late summer through autumn in Bastrop County.", "Estadio Victoria"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6546130952380953}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-3292", "mrqa_searchqa-validation-14452", "mrqa_searchqa-validation-10292", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-14015", "mrqa_searchqa-validation-9935", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-12531", "mrqa_searchqa-validation-13465", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5962", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-9744", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-8817", "mrqa_searchqa-validation-14998", "mrqa_naturalquestions-validation-1260", "mrqa_triviaqa-validation-6223", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3364"], "SR": 0.578125, "CSR": 0.5683962264150944, "retrieved_ids": ["mrqa_squad-train-73125", "mrqa_squad-train-31769", "mrqa_squad-train-17358", "mrqa_squad-train-26098", "mrqa_squad-train-1402", "mrqa_squad-train-82237", "mrqa_squad-train-64231", "mrqa_squad-train-42973", "mrqa_squad-train-66163", "mrqa_squad-train-78361", "mrqa_squad-train-13980", "mrqa_squad-train-86124", "mrqa_squad-train-28303", "mrqa_squad-train-81462", "mrqa_squad-train-77356", "mrqa_squad-train-25624", "mrqa_squad-validation-10121", "mrqa_squad-validation-8964", "mrqa_naturalquestions-validation-8596", "mrqa_triviaqa-validation-6714", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-3890", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-96", "mrqa_newsqa-validation-990", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-10402", "mrqa_hotpotqa-validation-2739", "mrqa_squad-validation-1076", "mrqa_newsqa-validation-1218", "mrqa_triviaqa-validation-6697", "mrqa_naturalquestions-validation-3498"], "EFR": 1.0, "Overall": 0.7568823702830189}, {"timecode": 53, "before_eval_results": {"predictions": ["to encourage rebellion against the British authorities", "Debbie Gibson", "three levels", "February 28 or March 1", "Ireland", "December 2, 1942", "an expression of unknown origin", "the heart", "March 26, 1973", "Necator americanus and Ancy Lostoma duodenale", "April 7, 2016", "St Pancras International", "number of games where the player played, in whole or in part", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "Frank Langella", "The Maidstone Studios in Maidstone, Kent", "Human fertilization", "with the approach of Aslan, her magical winter thaws, and Edmund is rescued after his treason", "16 seasons", "Bill Russell", "the Washington Redskins", "Donald Trump", "vascular cambium is the main growth layer in the stems and roots of many plants, specifically in dicots such as buttercups and oak trees, and gymnosperms such as pine trees", "with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "1895", "contestant", "between the Mediterranean Sea to the north and the Red Sea to to the south", "Border Collie", "the Washington metropolitan area", "Julie Adams", "Gatiman express its ranges 160km / hour between Delhi to Agra In 100 min its cross 180km", "John Young", "Kevin Spacey", "novella", "the nucleus with densely coiled chromatin fibres, surrounded anteriorly by an acrosome, which contains enzymes used for penetrating the female egg", "Gene MacLellan", "a flash music video featuring an animated dancing banana was created", "2010", "on location", "Frankie Muniz", "before the first letter of an interrogative sentence or clause to indicate that a question follows", "Jason Lee", "Jenna Boyd", "not about drugs", "2017", "1978", "a loanword of the Visigothic word guma `` man ''", "Rust", "birch", "First Lieutenant Israel Greene", "brothers Henry, Jojo and Ringo Garza", "Annie Walker", "wool", "Roman history", "Buddha's delight", "Matt Kemp", "pro-Confederate partisan rangers", "not feel Misty Cummings has told them everything she knows.", "30,000", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Pablo Picasso", "(Scott) Peterson", "(Charles) Bulfinch", "the thumb"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6630705282499356}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.06451612903225806, 0.9523809523809523, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.08333333333333333, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-4809", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-4240", "mrqa_triviaqa-validation-4244", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-458", "mrqa_newsqa-validation-3873", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-5691"], "SR": 0.546875, "CSR": 0.5679976851851851, "retrieved_ids": ["mrqa_squad-train-79997", "mrqa_squad-train-35863", "mrqa_squad-train-56536", "mrqa_squad-train-78667", "mrqa_squad-train-63480", "mrqa_squad-train-77631", "mrqa_squad-train-6729", "mrqa_squad-train-29181", "mrqa_squad-train-25389", "mrqa_squad-train-50657", "mrqa_squad-train-76144", "mrqa_squad-train-60237", "mrqa_squad-train-71467", "mrqa_squad-train-59834", "mrqa_squad-train-27011", "mrqa_squad-train-69337", "mrqa_squad-validation-6720", "mrqa_naturalquestions-validation-4865", "mrqa_searchqa-validation-1961", "mrqa_newsqa-validation-3518", "mrqa_naturalquestions-validation-6917", "mrqa_hotpotqa-validation-3777", "mrqa_squad-validation-10158", "mrqa_searchqa-validation-10274", "mrqa_naturalquestions-validation-6214", "mrqa_triviaqa-validation-1516", "mrqa_hotpotqa-validation-3489", "mrqa_newsqa-validation-3850", "mrqa_searchqa-validation-9957", "mrqa_searchqa-validation-4927", "mrqa_squad-validation-2491", "mrqa_triviaqa-validation-2221"], "EFR": 0.9310344827586207, "Overall": 0.7430095585887612}, {"timecode": 54, "before_eval_results": {"predictions": ["Norway", "Ecuador", "Home Improvement", "iron", "salmon", "Berlin", "Iago", "Fidel", "Patrick Floyd Garrett", "Montana", "Harvard University", "Sitting Bull", "an arboretum", "Marie Curie", "Abnormal Psychology", "love", "Lincoln", "the new Italian flag", "Samuel Butler", "Kitty Kelley", "Abraham Lincoln", "teddy bears", "Crouching Tiger", "baseball", "upsilon", "banknotes", "a snake", "Jupiter", "conformation", "the Ziegfeld Girl", "David Cassidy", "a mountain", "the Louvre", "Greek", "royal", "Oxygen", "a house of prayer", "the Wessex", "the snowmobile", "Aaron Copland", "Blue", "voltage", "Royal Ballet", "Tesla", "Lil Jon", "the diamond", "the plum", "Lizzie Borden", "hockey", "Pop-Tarts", "Mad Cow", "Henry Purcell", "Mankombu Sambasivan Swaminathan", "eight", "Ethiopia", "a cactus", "Argentina", "Real Madrid and the Spain national team", "1983", "Richard Street", "Brazil", "mpire of the Sun", "whether he should be charged with a crime", "four"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6604166666666667}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-8650", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-8125", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-8825", "mrqa_searchqa-validation-16878", "mrqa_searchqa-validation-10404", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-13693", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9692", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5952", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-13757", "mrqa_hotpotqa-validation-4436", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-3273", "mrqa_triviaqa-validation-261"], "SR": 0.5625, "CSR": 0.5678977272727272, "retrieved_ids": ["mrqa_squad-train-81968", "mrqa_squad-train-60284", "mrqa_squad-train-36539", "mrqa_squad-train-69172", "mrqa_squad-train-11626", "mrqa_squad-train-22715", "mrqa_squad-train-53959", "mrqa_squad-train-13643", "mrqa_squad-train-71010", "mrqa_squad-train-33469", "mrqa_squad-train-43929", "mrqa_squad-train-35508", "mrqa_squad-train-72469", "mrqa_squad-train-39347", "mrqa_squad-train-44443", "mrqa_squad-train-83123", "mrqa_naturalquestions-validation-9715", "mrqa_hotpotqa-validation-797", "mrqa_squad-validation-6435", "mrqa_triviaqa-validation-7145", "mrqa_hotpotqa-validation-571", "mrqa_naturalquestions-validation-9576", "mrqa_newsqa-validation-839", "mrqa_naturalquestions-validation-10571", "mrqa_newsqa-validation-1598", "mrqa_naturalquestions-validation-6786", "mrqa_newsqa-validation-3741", "mrqa_hotpotqa-validation-757", "mrqa_searchqa-validation-15560", "mrqa_naturalquestions-validation-4096", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-1564"], "EFR": 1.0, "Overall": 0.7567826704545455}, {"timecode": 55, "before_eval_results": {"predictions": ["Santa Fe", "Beanie Babies", "kick drum", "chess", "cola", "Berlin", "Comedy Central", "the best", "the Mighty Five", "Romeo and Juliet", "Cerberus", "death", "the Nile", "silver", "Plutarch", "( Sarah) Hughes", "the submarine-launched ballistic missile", "St. Augustine", "Trinity", "Fiji", "the burnoose", "Thomas A. Edison", "the Mekong", "the 36th", "Valentina Tereshkova", "Canada", "acrophobia", "Missouri", "ribonucleic acid", "Rubeus Hagrid", "Manitoba", "Death of a Salesman", "chocolate", "inshallah", "Saudi Arabia", "Pamela Anderson", "Jenny Craig", "Idaho", "candy shells", "King Henry VIII", "the Empire State Building", "Laugh", "Tennessee", "the Constitution", "Toronto", "University of Exeter", "(Berenice) Queen of Egypt", "Lawrence of Arabia", "Andy Warhol", "baseball games", "Tara Reid", "Big Ten Conference Champions Michigan State Spartans", "moist temperate climates", "John B. Watson", "ringtone", "General Paulus", "Cyprus", "2010", "Tufts", "Crips", "U.S. Court of Appeals for the District of Columbia.", "he fears a desperate country with a potential power vacuum that could lash out.", "Monday.", "Megan Lynn Touma,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6084821428571429}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-13435", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15699", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-9361", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9266", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-15996", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-10897", "mrqa_searchqa-validation-6437", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-2256", "mrqa_triviaqa-validation-5885", "mrqa_hotpotqa-validation-4049", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2517"], "SR": 0.53125, "CSR": 0.5672433035714286, "retrieved_ids": ["mrqa_squad-train-8833", "mrqa_squad-train-60486", "mrqa_squad-train-74186", "mrqa_squad-train-36982", "mrqa_squad-train-37134", "mrqa_squad-train-48652", "mrqa_squad-train-64914", "mrqa_squad-train-14165", "mrqa_squad-train-69792", "mrqa_squad-train-66626", "mrqa_squad-train-53502", "mrqa_squad-train-27945", "mrqa_squad-train-21373", "mrqa_squad-train-12801", "mrqa_squad-train-16553", "mrqa_squad-train-72064", "mrqa_triviaqa-validation-3114", "mrqa_newsqa-validation-1218", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3084", "mrqa_newsqa-validation-1397", "mrqa_squad-validation-6973", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-15519", "mrqa_newsqa-validation-467", "mrqa_triviaqa-validation-6227", "mrqa_newsqa-validation-1299", "mrqa_hotpotqa-validation-1100", "mrqa_searchqa-validation-13480", "mrqa_naturalquestions-validation-6555", "mrqa_newsqa-validation-604", "mrqa_searchqa-validation-5952"], "EFR": 1.0, "Overall": 0.7566517857142857}, {"timecode": 56, "before_eval_results": {"predictions": ["former English county of Humberside", "Federal Bureau of Prisons", "\"Dumb and Dumber\"", "trans-Pacific flight", "John Hunt", "Walt Disney Productions", "Reinhard Heydrich", "British", "Sheen Michaels Entertainment", "Roc Me Out", "1770", "Sunflower County", "2005", "A Bug's Life", "U.S. military", "a few minutes", "the Qin dynasty", "Kentucky River", "fourth", "\"The Bob Edwards Show\" on Sirius XM Radio and \"Bob Edwards Weekend\"", "The S7 series", "White Knights of the Ku Klux Klan", "Key West", "Charlie Puth", "Fort Oranje", "Golden Globe Award", "Soviet Union", "National Society of Daughters of the American Revolution", "Martin Scorsese", "John Monash", "Protestant Christian", "Cardinals RiverBats", "comic book series Molly Danger", "Agra", "50 million", "Henry II", "Scotty Grainger", "music to students", "An agricultural cooperative", "Kairi", "Texas", "Democratic Unionist Party (DUP)", "five", "Candice Swanepoel", "period dependent", "Chris \"Izzy\" Cole", "multiple awards", "McComb, Mississippi", "King of Cool", "1995", "\"Losing My Religion\"", "The Royalettes", "six degrees", "1982", "jordan-Healey", "'Q'", "Chapel curve", "\"Britain's Got Talent.\"", "Turkey", "Seoul", "Charleston", "emerald", "Don Juan", "Pandora"], "metric_results": {"EM": 0.5625, "QA-F1": 0.64140625}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.25, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-5131", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-314", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1229", "mrqa_newsqa-validation-154", "mrqa_searchqa-validation-8091"], "SR": 0.5625, "CSR": 0.5671600877192983, "retrieved_ids": ["mrqa_squad-train-3583", "mrqa_squad-train-83588", "mrqa_squad-train-83478", "mrqa_squad-train-8721", "mrqa_squad-train-11103", "mrqa_squad-train-16233", "mrqa_squad-train-8682", "mrqa_squad-train-36091", "mrqa_squad-train-61333", "mrqa_squad-train-1278", "mrqa_squad-train-46347", "mrqa_squad-train-26198", "mrqa_squad-train-15979", "mrqa_squad-train-51976", "mrqa_squad-train-72051", "mrqa_squad-train-44424", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-3421", "mrqa_squad-validation-5975", "mrqa_naturalquestions-validation-10249", "mrqa_hotpotqa-validation-3777", "mrqa_triviaqa-validation-1731", "mrqa_searchqa-validation-2538", "mrqa_naturalquestions-validation-3893", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-3292", "mrqa_hotpotqa-validation-5804", "mrqa_searchqa-validation-10897", "mrqa_searchqa-validation-1771", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-9390", "mrqa_triviaqa-validation-4097"], "EFR": 1.0, "Overall": 0.7566351425438598}, {"timecode": 57, "before_eval_results": {"predictions": ["Italian architect and art theorist Leon Battista Alberti", "counter clockwise", "sixth season", "December 2, 1942", "Ray Charles", "mid November", "The period of being a junior doctor starts when they qualify as a medical practitioner following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Daniel Suarez", "its population", "The Day of the Dead", "Matthew Roberts", "The Death of Archie storyline", "Adam Mitchell", "9.7 m", "Guwahati", "Foofa, Brobee, and Toodee", "modern state system", "efferent nerves", "William Wyler", "Dragon Ball GT", "Nitty Gritty Dirt Band", "Donald Fauntleroy Duck", "2013", "Friedman Billings Ramsey", "2018", "skeletal muscle", "Joe Spano", "Spanish moss", "Georges Auguste Escoffier", "Nodar Kumaritashvili", "October 29, 2015", "Donna Mills", "the New England Patriots", "Ashoka", "Britain of Florida", "Government House at New Delhi", "an unknown recipient", "Andy Warhol", "`` Elected Emperor of the Romans", "Dalveer Bhandari", "Middle Eastern alchemy", "Vienna", "Nalini Negi", "The Rashidun Caliphs", "Lituya Bay in Alaska", "Mike Higham", "2002", "George Strait", "appear to converge", "The United States is a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "in order to prove disparate impact you first must establish", "Emma Chambers", "Hans Lippershey", "Thermopylae", "August 6, 1845", "an album", "Spain", "Dr. Jennifer Arnold and husband Bill Klein,", "A family friend of a U.S. soldier", "60 euros", "Blue", "lump", "a knish", "CBS"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6532062977630076}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.28571428571428575, 0.08163265306122448, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.04878048780487805, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-31", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-419", "mrqa_searchqa-validation-416"], "SR": 0.59375, "CSR": 0.5676185344827587, "retrieved_ids": ["mrqa_squad-train-82695", "mrqa_squad-train-76373", "mrqa_squad-train-63374", "mrqa_squad-train-29776", "mrqa_squad-train-73385", "mrqa_squad-train-35570", "mrqa_squad-train-22230", "mrqa_squad-train-15908", "mrqa_squad-train-2960", "mrqa_squad-train-74905", "mrqa_squad-train-84073", "mrqa_squad-train-26699", "mrqa_squad-train-73729", "mrqa_squad-train-30972", "mrqa_squad-train-74924", "mrqa_squad-train-43549", "mrqa_hotpotqa-validation-2955", "mrqa_triviaqa-validation-907", "mrqa_searchqa-validation-13480", "mrqa_squad-validation-6640", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-246", "mrqa_newsqa-validation-3838", "mrqa_triviaqa-validation-2627", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-10292", "mrqa_naturalquestions-validation-4007", "mrqa_newsqa-validation-562", "mrqa_hotpotqa-validation-532"], "EFR": 0.9230769230769231, "Overall": 0.7413422165119364}, {"timecode": 58, "before_eval_results": {"predictions": ["the five - year time jump", "111", "Herbert Hoover", "Uralic", "22", "IBM", "John Adams", "2018", "Jesus Christ", "14 November 2001", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "24", "Coriolis force", "Hugh S. Johnson", "Paul Lynde", "Erica Rivera", "Malina Weissman", "the Sui", "Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s", "1970", "DeWayne Warren", "the nucleus", "1996", "Egypt", "statistical advantage for the casino that is built into the game", "Tom Brady", "pilgrimages to Jerusalem", "1996", "Coconut Cove", "Curtis Armstrong", "Dolby Theatre in Hollywood, Los Angeles, California", "Category 4", "Rust", "Karen Gillan", "$19.8 trillion", "1,228 km / h ( 763 mph )", "Tommy Shaw", "warplanes", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "Central Germany", "Atlanta", "Ricky Nelson", "James Chadwick", "Welch, West Virginia", "Tristan Rogers", "1994", "Houston Astros", "Americans who served in the armed forces and as civilians", "it connects with the Arafura Sea through the Torres Strait", "the middle of the 15th century", "Gladys Knight & the Pips", "Lago de Nicaragua", "priests", "rue", "Delphi Lawrence", "#5", "Edward James Olmos", "vitamin injections", "Scotland", "World leaders", "a artificial heart", "James Stewart", "Frank Sinatra", "Salisbury"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7575631313131314}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.07999999999999999, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8181818181818181, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4134", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2503", "mrqa_searchqa-validation-9866", "mrqa_hotpotqa-validation-3324"], "SR": 0.65625, "CSR": 0.5691207627118644, "retrieved_ids": ["mrqa_squad-train-85674", "mrqa_squad-train-52179", "mrqa_squad-train-3934", "mrqa_squad-train-74400", "mrqa_squad-train-38729", "mrqa_squad-train-5151", "mrqa_squad-train-55363", "mrqa_squad-train-82339", "mrqa_squad-train-32494", "mrqa_squad-train-2587", "mrqa_squad-train-8503", "mrqa_squad-train-43227", "mrqa_squad-train-42982", "mrqa_squad-train-64639", "mrqa_squad-train-83414", "mrqa_squad-train-12247", "mrqa_searchqa-validation-15246", "mrqa_squad-validation-100", "mrqa_naturalquestions-validation-1455", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-4770", "mrqa_newsqa-validation-591", "mrqa_hotpotqa-validation-3391", "mrqa_naturalquestions-validation-8851", "mrqa_triviaqa-validation-2056", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-10402", "mrqa_hotpotqa-validation-4269", "mrqa_newsqa-validation-3391", "mrqa_squad-validation-2791", "mrqa_naturalquestions-validation-9760"], "EFR": 0.9545454545454546, "Overall": 0.7479363684514638}, {"timecode": 59, "before_eval_results": {"predictions": ["journalists, a seven-member Spanish flight crew and one Belgian", "Six", "\"including taking any and all appropriate personnel actions including termination, discipline and referral of any wrongdoing for criminal prosecution.\"", "Frank Ricci,", "the banned substance cortisone.", "Wednesday.", "Linda Hogan", "be silent.", "Crandon, Wisconsin,", "Turkey", "John Demjanjuk,", "says Somalia's piracy problem was fueled by environmental and political events.", "eight", "Haiti.", "Missouri", "\"peregruzka,\"", "Haiti.", "9 percent", "many as 250,000", "Maj. Nidal Malik Hasan,", "A former Alabama judge", "to stop the Afghan opium trade", "Nick Adenhart", "order", "his father", "\"disagreements\" with the Port Authority of New York and New Jersey,", "\"Operation Pipeline Express.\"", "Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Susan Boyle", "10-person", "April.", "the three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist police characterized as \"spectacular.\"", "promotes fuel economy and safety while boosts the economy", "gasoline", "to do jobs that Arizonans wouldn't do.", "a \"prostitute\"", "digging", "Tottenham", "Iran could be secretly working on a nuclear weapon", "Joe Pantoliano,", "3-2", "\"The deceased appeared to have been there for some time.\"", "\"People find the contestants so relatable,\"", "56,", "\"Larry King Live.\"", "15-month", "intravenously in operating rooms", "Summer", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify.", "heavy turbulence", "Zac Efron", "drivers who were Daytona Pole Award winners, former Clash race winners, former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs", "the spectroscopic notation for the associated atomic orbitals", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "sound and light", "6", "John Buchan", "NCAA Division I Football Bowl Subdivision", "Kristoffer Kristofferson", "Ben Savage", "the Wardrobe", "Bering Sea", "Job", "Ayhuasca"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5739239926739926}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.06666666666666667, 0.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.9523809523809523, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.14285714285714288, 0.0, 1.0, 1.0, 0.08333333333333333, 0.875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6153846153846153, 1.0, 1.0, 0.5499999999999999, 1.0, 0.8205128205128205, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-920", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-318", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4387", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3871", "mrqa_searchqa-validation-16736", "mrqa_searchqa-validation-13957"], "SR": 0.4375, "CSR": 0.5669270833333333, "retrieved_ids": ["mrqa_squad-train-32422", "mrqa_squad-train-72146", "mrqa_squad-train-22522", "mrqa_squad-train-66310", "mrqa_squad-train-33", "mrqa_squad-train-36306", "mrqa_squad-train-2587", "mrqa_squad-train-13388", "mrqa_squad-train-5651", "mrqa_squad-train-60534", "mrqa_squad-train-23617", "mrqa_squad-train-39275", "mrqa_squad-train-23841", "mrqa_squad-train-45735", "mrqa_squad-train-79130", "mrqa_squad-train-11292", "mrqa_squad-validation-10388", "mrqa_searchqa-validation-13380", "mrqa_searchqa-validation-13963", "mrqa_hotpotqa-validation-4883", "mrqa_triviaqa-validation-1401", "mrqa_searchqa-validation-11713", "mrqa_naturalquestions-validation-7242", "mrqa_hotpotqa-validation-4223", "mrqa_hotpotqa-validation-2726", "mrqa_newsqa-validation-112", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-3828", "mrqa_squad-validation-4786", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-1306", "mrqa_hotpotqa-validation-431"], "EFR": 1.0, "Overall": 0.7565885416666667}, {"timecode": 60, "UKR": 0.7578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2665", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10461", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11890", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-11925", "mrqa_searchqa-validation-12105", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12441", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14450", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16382", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-2634", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9852", "mrqa_searchqa-validation-9911", "mrqa_searchqa-validation-9935", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1876", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1145", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5886", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937"], "OKR": 0.88671875, "KG": 0.484375, "before_eval_results": {"predictions": ["$7.8 million", "prostate cancer,", "Donald Duck", "Whitney Houston", "new DNA evidence", "South Africa", "consumer confidence", "General Motors", "Prague", "35,000.", "Osama", "The EU naval force", "Kerstin Fritzl,", "threatening messages", "Nigeria", "misdemeanor", "New Haven, Connecticut, firefighter", "$273 million", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "reports came in the form of tweets that alternated between raw descriptions and expressions of hope", "air support.", "20", "$250,000 for Rivers' charity: God's Love We Deliver.", "March 22,", "18", "Blacks and Hispanics", "Australian officials", "Alberto Espinoza Barron,", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "Rev. Alberto Cutie", "meter reader", "the man facing up, with his arms out to the side.", "Garth Brooks", "Monday", "Friday,", "10,000", "Ryan Adams.", "$1.5 million.", "three out of four", "relatives of the five suspects,", "a cancerous tumor.", "581", "his health and about a comeback.", "up", "\"utterly baseless.\"", "businesses,", "Caylee,", "1-1", "her boyfriend, Dodi Fayed, and their driver, Henri Paul.", "Lonnie", "the first", "the words spoken to Adam and Eve after their sin", "Tokyo", "access to US courts", "Doncaster Rovers", "Shelly-Ann Fraser", "1973", "2,099", "PPG Paints Arena", "early 2017", "lactic acid", "The Greatest Show on Earth", "republic", "Germany"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7573074494949494}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-110", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-332", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-2958", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7987", "mrqa_triviaqa-validation-3833", "mrqa_hotpotqa-validation-4619"], "SR": 0.703125, "CSR": 0.5691598360655737, "retrieved_ids": ["mrqa_squad-train-55013", "mrqa_squad-train-85407", "mrqa_squad-train-21118", "mrqa_squad-train-38308", "mrqa_squad-train-45359", "mrqa_squad-train-39796", "mrqa_squad-train-48673", "mrqa_squad-train-9139", "mrqa_squad-train-83054", "mrqa_squad-train-10038", "mrqa_squad-train-4402", "mrqa_squad-train-39914", "mrqa_squad-train-69799", "mrqa_squad-train-3325", "mrqa_squad-train-72240", "mrqa_squad-train-8899", "mrqa_squad-validation-6720", "mrqa_newsqa-validation-2690", "mrqa_naturalquestions-validation-5564", "mrqa_triviaqa-validation-3161", "mrqa_searchqa-validation-12146", "mrqa_newsqa-validation-2133", "mrqa_searchqa-validation-4995", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-6848", "mrqa_searchqa-validation-12759", "mrqa_hotpotqa-validation-3741", "mrqa_triviaqa-validation-6411", "mrqa_naturalquestions-validation-5818", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-420", "mrqa_searchqa-validation-8091"], "EFR": 1.0, "Overall": 0.7396132172131147}, {"timecode": 61, "before_eval_results": {"predictions": ["Windows Easy Transfer", "John Cooper Clarke", "Charlotte of Mecklenburg - Strelitz", "O'Meara", "Judi Dench", "accomplish the objectives of the organization", "Omar Khayyam", "P.V. Sindhu", "1750", "Saturday", "1982", "Siddharth Arora / Vibhav Roy", "1948", "mitosis", "Butter Island off North Haven, Maine in the Penobscot Bay", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Pat McCormick", "10,605", "U.S. Electoral College", "American country music artists Reba McEntire and Linda Davis", "Kid Creole & The Coconuts", "China and in provinces in the south", "New York City", "July 21, 1861", "Nashville, Tennessee", "104 colonists and Discovery", "cella", "at 11 : 40 p.m. ship's time", "April 12, 2017", "October 2012", "Tom Burlinson, Red Symons and Dannii Minogue", "John Joseph Patrick Ryan", "49 cents", "counter clockwise", "Kit Harington", "Human anatomy", "above the light source and under the sample in an upright microscope", "divergent tectonic", "Organisms", "neutral", "Speaker of the House of Representatives", "1877", "18", "active absorption of water from the soil", "sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "winter festivals", "government regulations ( including the jurisdiction's corporations law ) and the organization's own constitution and bylaws", "the 1820s", "The Royalettes", "Katherine Kiernan Maria `` Kate '' Mulgrew", "Fats Waller", "Felix", "\"Land of the Rising Sun\"", "James Hogg", "Lawrence", "\"Realty Bites\"", "national aviation branch", "Athens and Thessaloniki and Athens,", "\"extremely weak\"", "\"The Screening Room\"", "Chicken Little", "Saturn", "Russia", "Isolde"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7048179970436068}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6428571428571429, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2, 0.8780487804878049, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1915", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-2426", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3093", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3398", "mrqa_triviaqa-validation-6243"], "SR": 0.609375, "CSR": 0.5698084677419355, "retrieved_ids": ["mrqa_squad-train-70190", "mrqa_squad-train-59466", "mrqa_squad-train-43978", "mrqa_squad-train-60628", "mrqa_squad-train-22751", "mrqa_squad-train-83490", "mrqa_squad-train-29062", "mrqa_squad-train-6080", "mrqa_squad-train-33393", "mrqa_squad-train-20319", "mrqa_squad-train-29400", "mrqa_squad-train-84971", "mrqa_squad-train-41625", "mrqa_squad-train-7000", "mrqa_squad-train-33811", "mrqa_squad-train-55522", "mrqa_squad-validation-818", "mrqa_triviaqa-validation-2307", "mrqa_searchqa-validation-5671", "mrqa_hotpotqa-validation-3388", "mrqa_searchqa-validation-11520", "mrqa_triviaqa-validation-1516", "mrqa_searchqa-validation-767", "mrqa_searchqa-validation-6474", "mrqa_newsqa-validation-2237", "mrqa_hotpotqa-validation-3391", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-203", "mrqa_naturalquestions-validation-7852", "mrqa_squad-validation-3687", "mrqa_triviaqa-validation-157", "mrqa_hotpotqa-validation-983"], "EFR": 0.92, "Overall": 0.7237429435483871}, {"timecode": 62, "before_eval_results": {"predictions": ["1983", "Bacon", "from 1922 to 1991", "73", "Gibraltar", "1 January 1904", "Thebes", "Brooke Wexler", "March 29, 2018", "in the 1980s", "Montgomery", "Evermoist", "in the mid - to late 1920s", "differential erosion", "Kanawha River", "Graham McTavish", "Thomas Alva Edison", "the first", "the rise of literacy, technological advances in printing", "Richard Masur", "Frankie Valli", "one", "JackScanlon", "Saturday", "sperm and ova", "sometime in 2018", "2015", "Sarah Josepha Hale", "Nickelback", "Ledger", "known locally as the International Border ( IB )", "in a 1945 NCAA game between Columbia and Fordham", "2017", "permanently absorbed the superhuman powers and the psyche of Carol Danvers", "at 10 Stigwood Avenue", "Pink Floyd", "1983", "in the late 1970s", "1986", "1939", "Himadri Station", "on a beach in Malibu, California", "tree", "February 2017 in Japan and in March 2018 in North America and Europe", "FaZe Rug", "at the fictional elite conservative Vermont boarding school Welton Academy", "1973", "9 February 2018", "The FCI accepted the long - haired type in 2010", "94 by 50 feet", "Tom\u00e1s de Torquemada", "1943", "Kent", "Spanish", "Dusty Dvoracek", "South America", "Los Angeles", "has a dream to meet her hero.", "the foyer of the BBC building in Glasgow, Scotland", "\"a violent and brutal extremist group with a number of individuals affiliated with al Qaeda.", "Frederic Chopin", "spring", "pesos", "Rev. Alberto Cutie"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6950379583651642}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.8181818181818181, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.2666666666666667, 1.0, 0.9411764705882353, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10583", "mrqa_triviaqa-validation-4726", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-219", "mrqa_newsqa-validation-1330"], "SR": 0.578125, "CSR": 0.5699404761904762, "retrieved_ids": ["mrqa_squad-train-64957", "mrqa_squad-train-77241", "mrqa_squad-train-8208", "mrqa_squad-train-29169", "mrqa_squad-train-67645", "mrqa_squad-train-10836", "mrqa_squad-train-47870", "mrqa_squad-train-22996", "mrqa_squad-train-31543", "mrqa_squad-train-70127", "mrqa_squad-train-52091", "mrqa_squad-train-57597", "mrqa_squad-train-29596", "mrqa_squad-train-66189", "mrqa_squad-train-81623", "mrqa_squad-train-39614", "mrqa_naturalquestions-validation-6452", "mrqa_searchqa-validation-15325", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-11634", "mrqa_squad-validation-10269", "mrqa_searchqa-validation-4426", "mrqa_hotpotqa-validation-5565", "mrqa_naturalquestions-validation-5538", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-367", "mrqa_naturalquestions-validation-5526", "mrqa_hotpotqa-validation-2366", "mrqa_naturalquestions-validation-1423", "mrqa_hotpotqa-validation-1285", "mrqa_triviaqa-validation-2340"], "EFR": 1.0, "Overall": 0.7397693452380952}, {"timecode": 63, "before_eval_results": {"predictions": ["Malay", "Billy Martin", "Nova Scotian", "a chainmaille", "(Henrik) Ibsen", "Hiroshima", "Morocco", "Opera Flashcards", "Anne Rice", "(Henry) Ford", "Pop art", "embalming", "Portland", "Mariah Carey", "Dionysus", "an eel", "symbiosis", "Planets", "Donald Trump", "Hold On", "Sherlock Holmes", "Prince Edward Island", "Queens", "Bab el Mandeb Strait", "Red Heat", "Atlas Mountains", "Kafkaesque", "Heather Mills", "polar", "Monticello", "Mont Blanc On", "Rene Lacoste", "preemption", "the Nobel", "summer", "osca", "Pandit Jawaharlal Nehru", "around 5:00 p.m., the end", "Michelangelo", "a cat", "congruent", "Spain", "toad", "San Francisco", "A Brief", "a crossword", "Macy\\'s", "Geoffrey", "Good Bad Jokes", "\"Walker, Texas Ranger\"", "Benazir Bhutto", "CBS", "Gibraltar", "Orographic lift", "Virginia Plain", "strawberry", "Venice", "Chad", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "Stephen King", "Pat Quinn", "murder", "\"Walk -- Don't Run.\"", "TransAd Adelaide"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6242559523809523}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-6802", "mrqa_searchqa-validation-5508", "mrqa_searchqa-validation-14989", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-13629", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-16392", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-15031", "mrqa_hotpotqa-validation-5688", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2308", "mrqa_hotpotqa-validation-883"], "SR": 0.5625, "CSR": 0.56982421875, "retrieved_ids": ["mrqa_squad-train-43304", "mrqa_squad-train-42771", "mrqa_squad-train-6278", "mrqa_squad-train-41375", "mrqa_squad-train-39954", "mrqa_squad-train-18422", "mrqa_squad-train-50895", "mrqa_squad-train-59245", "mrqa_squad-train-72925", "mrqa_squad-train-15286", "mrqa_squad-train-55427", "mrqa_squad-train-10061", "mrqa_squad-train-456", "mrqa_squad-train-19712", "mrqa_squad-train-33717", "mrqa_squad-train-7420", "mrqa_triviaqa-validation-1631", "mrqa_searchqa-validation-733", "mrqa_triviaqa-validation-1429", "mrqa_newsqa-validation-1350", "mrqa_naturalquestions-validation-10571", "mrqa_searchqa-validation-2049", "mrqa_naturalquestions-validation-9275", "mrqa_squad-validation-5456", "mrqa_naturalquestions-validation-4667", "mrqa_searchqa-validation-11091", "mrqa_hotpotqa-validation-569", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-3189", "mrqa_newsqa-validation-2668", "mrqa_naturalquestions-validation-10680", "mrqa_hotpotqa-validation-3692"], "EFR": 1.0, "Overall": 0.73974609375}, {"timecode": 64, "before_eval_results": {"predictions": ["a person of Spanish descent living in the U.S.", "Bonnie & Clyde", "Forrest Gump", "a relationship with a man who proves to", "The Crossing Guard", "Martin Van Buren", "I Have No Mouth, and I Must Scream", "Friday Night Lights", "contractions", "the Skull and Crossbones", "India", "Florida State", "Ukraine", "the Paul Revere", "a Tibetan antelope", "a police station", "a bolt", "Australia", "Napalm", "Roald Dahl", "Mount Kenya", "John Lennon", "the Stamp Act", "Yale Basmati", "CO2", "The Battle of Thermopylae", "Argentina", "Mulberry Street", "Romeo & Juliet", "Tucson", "Helen Hayes", "Wesley Clark", "cobalt", "Sing Sing", "salmon", "Catch A Falling Star", "Herman Melville", "Abercrombie & Fitch", "Beatrix Potter", "the Romaunt", "the Kagu", "the Gadsden Treaty", "the umbilical cord", "trees", "Sweden", "the House of Lords", "the Red Cross", "terrorists", "KC & The Sunshine Band", "the Somme", "Graceland", "David Tennant", "on the southeastern coast of the Commonwealth of Virginia in the United States", "pulmonary heart disease ( cor pulmonale )", "9", "squash", "website", "Gospel Starlighter", "500-room", "Wal-Mart Canada Corp.", "Tuesday in Los Angeles.", "Diversity,", "directly involved in an Internet broadband deal with a Chinese firm.", "gang rape"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6938368055555555}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11200", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-6468", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-16325", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-16100", "mrqa_searchqa-validation-8741", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-11743", "mrqa_searchqa-validation-2681", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-12394", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-14256", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-15724", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-3727", "mrqa_naturalquestions-validation-1680", "mrqa_hotpotqa-validation-2565", "mrqa_newsqa-validation-1275"], "SR": 0.578125, "CSR": 0.569951923076923, "retrieved_ids": ["mrqa_squad-train-10285", "mrqa_squad-train-16523", "mrqa_squad-train-50450", "mrqa_squad-train-4496", "mrqa_squad-train-15467", "mrqa_squad-train-21844", "mrqa_squad-train-12705", "mrqa_squad-train-41703", "mrqa_squad-train-4164", "mrqa_squad-train-10481", "mrqa_squad-train-43726", "mrqa_squad-train-28499", "mrqa_squad-train-59119", "mrqa_squad-train-47305", "mrqa_squad-train-16652", "mrqa_squad-train-3164", "mrqa_triviaqa-validation-1413", "mrqa_newsqa-validation-2122", "mrqa_triviaqa-validation-6366", "mrqa_squad-validation-9087", "mrqa_naturalquestions-validation-3427", "mrqa_hotpotqa-validation-3423", "mrqa_naturalquestions-validation-839", "mrqa_newsqa-validation-1290", "mrqa_triviaqa-validation-277", "mrqa_searchqa-validation-13367", "mrqa_newsqa-validation-2937", "mrqa_hotpotqa-validation-2997", "mrqa_squad-validation-2419", "mrqa_hotpotqa-validation-2477", "mrqa_newsqa-validation-1856", "mrqa_hotpotqa-validation-3095"], "EFR": 1.0, "Overall": 0.7397716346153846}, {"timecode": 65, "before_eval_results": {"predictions": ["Brown Mountain Lights", "3,384,569", "Vishal Bhardwaj", "around 169 CE", "Ed O'Neill", "Milwaukee Bucks", "138,535", "Dennis Hull, as well as painter Manley MacDonald.", "Max Martin, Savan Kotecha and Ilya Salmanzadeh", "alternate", "Love Letter", "Brazil", "1968", "1961", "Stacey Kent", "Shenandoah National Park", "Regionalliga Nord", "Galo", "Samantha Spiro", "William Shakespeare", "1.6 million", "Joulupukki", "Ghana", "special economic zone", "Portal A Interactive", "Graduados", "Sada Carolyn Thompson", "World Health Organization", "Chow Tai Fook Enterprises", "Michelle Anne Sinclair", "2011", "2012", "Haleiwa, Hawaii", "Lalit", "Kal Ho Naa Ho", "Kolkata", "Ronald Wilson Reagan", "Musicology", "his left hand", "1835", "1926 Paris", "Erreway", "Forbes", "January 28, 2016", "69.7 million", "500-room", "Sochi", "2027 Fairmount Avenue", "southern portion of Carroll County, the eastern portion of Grafton County", "Black Panther Party", "globetrotters", "1980", "into the intermembrane space", "Butter Island off North Haven, Maine in the Penobscot Bay", "eye", "Leo Tolstoy", "gizzard", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "2,000", "Asashoryu", "Sir Loin", "Lili Taylor", "Sue Miller", "Dan Parris, 25, and Rob Lehr,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6308362369337979}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4878048780487806, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-3880", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4776", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-180", "mrqa_triviaqa-validation-3498", "mrqa_newsqa-validation-1287", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-1265", "mrqa_newsqa-validation-2296"], "SR": 0.515625, "CSR": 0.5691287878787878, "retrieved_ids": ["mrqa_squad-train-36419", "mrqa_squad-train-28689", "mrqa_squad-train-16489", "mrqa_squad-train-43523", "mrqa_squad-train-53919", "mrqa_squad-train-25043", "mrqa_squad-train-15533", "mrqa_squad-train-19840", "mrqa_squad-train-50464", "mrqa_squad-train-39686", "mrqa_squad-train-14250", "mrqa_squad-train-21277", "mrqa_squad-train-71067", "mrqa_squad-train-85937", "mrqa_squad-train-5488", "mrqa_squad-train-10180", "mrqa_naturalquestions-validation-9752", "mrqa_triviaqa-validation-3916", "mrqa_newsqa-validation-2627", "mrqa_squad-validation-6267", "mrqa_searchqa-validation-8091", "mrqa_triviaqa-validation-178", "mrqa_naturalquestions-validation-10147", "mrqa_newsqa-validation-715", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-8471", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-2100", "mrqa_newsqa-validation-6", "mrqa_searchqa-validation-11883", "mrqa_triviaqa-validation-1595", "mrqa_naturalquestions-validation-863"], "EFR": 0.967741935483871, "Overall": 0.7331553946725318}, {"timecode": 66, "before_eval_results": {"predictions": ["musician", "Captain Hans Geering", "the 50JJB Sports Fitness Clubs and the attached retail stores", "2015", "Jacksonville", "Seventeen", "Bhushan Patel", "Pamela Chopra", "Mark O'Connor", "Kinnairdy Castle", "South African", "Barbara Feldon", "The 2008\u201309 UEFA Champions League", "National Hockey League", "his advocacy of young earth creationism and intelligent design", "Parlophone Records", "a Soldier in Truck", "eight", "Cuban", "arts manager", "\"The Royal Family\"", "girls aged 11 to 18", "Laura Elizabeth \"Laurie\" Metcalf", "water", "National Basketball Development League", "Operation Overlord", "invoice", "Wiltshire", "1851", "during the first month of World War I", "12-year", "World War II", "Graham Payn", "Martin Truex Jr.", "twice", "Malayalam cinema", "47,818", "Every Rose Has Its Thorn", "13", "New England and New York", "1953", "German", "the state capital", "future AC/DC founders Angus Young and Malcolm Young", "Boston Celtics", "1912", "Dutch", "Bill Curry", "youngest publicly documented people to be identified as transgender", "2004", "311", "Jason Lee", "Missi Hale", "slavery", "gulf Stream", "The History Boys", "tabby", "Mexico", "2,500", "'overcharged.'\"", "Pommes Anna", "Gary", "Henry Hudson", "retinal"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6369818983100233}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.375, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4061", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-2842", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5294", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-2168", "mrqa_triviaqa-validation-7417", "mrqa_searchqa-validation-7226", "mrqa_naturalquestions-validation-3368"], "SR": 0.546875, "CSR": 0.5687966417910448, "retrieved_ids": ["mrqa_squad-train-64762", "mrqa_squad-train-42717", "mrqa_squad-train-66335", "mrqa_squad-train-17747", "mrqa_squad-train-77322", "mrqa_squad-train-21613", "mrqa_squad-train-22301", "mrqa_squad-train-25233", "mrqa_squad-train-59012", "mrqa_squad-train-75164", "mrqa_squad-train-16761", "mrqa_squad-train-37231", "mrqa_squad-train-31725", "mrqa_squad-train-28023", "mrqa_squad-train-80012", "mrqa_squad-train-32202", "mrqa_hotpotqa-validation-2328", "mrqa_triviaqa-validation-2480", "mrqa_newsqa-validation-2195", "mrqa_newsqa-validation-4039", "mrqa_searchqa-validation-13996", "mrqa_triviaqa-validation-5873", "mrqa_searchqa-validation-13693", "mrqa_newsqa-validation-2324", "mrqa_squad-validation-3193", "mrqa_newsqa-validation-3609", "mrqa_searchqa-validation-11846", "mrqa_hotpotqa-validation-1475", "mrqa_searchqa-validation-11936", "mrqa_newsqa-validation-2668", "mrqa_squad-validation-8278", "mrqa_searchqa-validation-12759"], "EFR": 1.0, "Overall": 0.739540578358209}, {"timecode": 67, "before_eval_results": {"predictions": ["classical realism", "Dr. Alberto Taquini", "democracy and personal freedom", "Rudolf Kehrer", "2015", "FK Austria Wien", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Edward Albert Heimberger", "Squam Lake", "lambics", "John Starks", "the Harpe brothers", "Adam Rex", "Agent Carter", "Everton", "\"Holinshed's Chronicles\"", "twelfth", "Coalwood", "31 July 1975", "Dark Heresy", "Ted Bundy", "1943", "Doctor of Philosophy", "URO VAMTAC", "Malta", "East Knoyle", "Philadelphia", "Maria Brink", "Jyothika", "\"Sippin' on Some Syrup,\"", "24 January 76 \u2013 10 July 138", "Leonard Cohen", "General Theological Seminary", "\"BraveStarr\"", "25 million records sold", "Paul Avery", "Sunflower County", "848 km", "Ellesmere Port", "Homer Hickam, Jr.", "South America", "Montreal", "Eugene", "Chief of the Operations Staff of the Armed Forces High Command", "CBS News", "Philadelphia", "Parlophone", "October", "the best known globetrotters", "Henry Lau", "John Richard Schlesinger", "Pasek & Paul", "Diary of a Wimpy Kid", "Ed Sheeran", "Sarah Palin", "Wayne Allwine", "heartbeat", "onto the college campus.\"", "14 bodies had been found", "one American diplomat to a \"prostitute\"", "a mouse button", "The Muppet", "Morocco", "missing"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6543087121212121}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.9090909090909091, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-705", "mrqa_hotpotqa-validation-259", "mrqa_hotpotqa-validation-5082", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-5385", "mrqa_hotpotqa-validation-3648", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-147", "mrqa_newsqa-validation-3337", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-13251", "mrqa_searchqa-validation-5330", "mrqa_newsqa-validation-1398"], "SR": 0.546875, "CSR": 0.5684742647058824, "retrieved_ids": ["mrqa_squad-train-11410", "mrqa_squad-train-73804", "mrqa_squad-train-4937", "mrqa_squad-train-19573", "mrqa_squad-train-16163", "mrqa_squad-train-47066", "mrqa_squad-train-15997", "mrqa_squad-train-11781", "mrqa_squad-train-48774", "mrqa_squad-train-18357", "mrqa_squad-train-24488", "mrqa_squad-train-12660", "mrqa_squad-train-9484", "mrqa_squad-train-30287", "mrqa_squad-train-22022", "mrqa_squad-train-78833", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-1093", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-1161", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-2405", "mrqa_hotpotqa-validation-3871", "mrqa_triviaqa-validation-244", "mrqa_hotpotqa-validation-4673", "mrqa_newsqa-validation-2156", "mrqa_naturalquestions-validation-2256", "mrqa_searchqa-validation-3888", "mrqa_searchqa-validation-4536", "mrqa_naturalquestions-validation-1904"], "EFR": 0.9655172413793104, "Overall": 0.7325795512170385}, {"timecode": 68, "before_eval_results": {"predictions": ["Stephen T. Kay", "Trey Parker and Matt Stone", "Tampa Bay Storm", "Wayman Tisdale", "New Boston Air Force Station", "the Lewis and Clark Expedition", "stringed", "Fleetwood Mac", "County Louth", "Chelmsford", "2009", "Comedy Central", "five", "Lazio region", "Mick Jackson", "The Livingston family", "saloon", "Fort Albany", "\"Kitty Hawk\"", "the Qin dynasty", "our greatest comedienne - Australia's Lucille Ball", "France.", "Francophone", "Diana Canova", "cricket fighting", "the design, development, manufacture and sale of vehicles bearing the Jaguar and Land Rover (including Range Rover) marques", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Noel Gallagher", "1966", "Love", "band director", "Ford Field in Detroit, Michigan", "drafted", "Mary O'Connell", "Bolton, England", "February 9, 1994", "Las Vegas", "Kongo", "Missouri River", "World War II", "Ector County", "Norse mythology", "Mercedes-Benz Superdome", "1979 to 2013", "wrestler", "December 12, 1967", "rural", "Lombardy", "August 14, 1848", "Punjabi/Pashtun", "43rd", "Samaria", "Valens", "1948", "Mallard", "Sherlock Holmes", "Tokyo", "intention to set up headquarters in Dublin.", "on the Ohio River near Warsaw, Kentucky,", "the Revolutionary Armed Forces of Colombia,", "(W. Somerset) Maugham", "a pine", "type around 70 words per minute", "Thomas Jefferson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.665264071637427}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.3157894736842105, 0.5, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.25, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.5, 0.0, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-78", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-1316", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7939", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-1037", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-16102", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-1104"], "SR": 0.515625, "CSR": 0.5677083333333333, "retrieved_ids": ["mrqa_squad-train-31439", "mrqa_squad-train-10620", "mrqa_squad-train-63040", "mrqa_squad-train-9258", "mrqa_squad-train-36366", "mrqa_squad-train-43087", "mrqa_squad-train-61987", "mrqa_squad-train-79290", "mrqa_squad-train-37815", "mrqa_squad-train-37009", "mrqa_squad-train-82902", "mrqa_squad-train-53436", "mrqa_squad-train-56405", "mrqa_squad-train-23138", "mrqa_squad-train-44026", "mrqa_squad-train-71067", "mrqa_newsqa-validation-2386", "mrqa_triviaqa-validation-1375", "mrqa_searchqa-validation-13465", "mrqa_newsqa-validation-367", "mrqa_searchqa-validation-9692", "mrqa_naturalquestions-validation-220", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-1991", "mrqa_triviaqa-validation-1298", "mrqa_squad-validation-7155", "mrqa_hotpotqa-validation-4092", "mrqa_squad-validation-3771", "mrqa_squad-validation-6435", "mrqa_newsqa-validation-112", "mrqa_naturalquestions-validation-2239", "mrqa_newsqa-validation-4058"], "EFR": 1.0, "Overall": 0.7393229166666666}, {"timecode": 69, "before_eval_results": {"predictions": ["November 1999", "the senior-most judge of the supreme court", "surname of Norman origin", "March 16, 2018", "Irsay", "Germany", "relieves the driving motor from the load of holding the elevator cab", "federal", "Bongos", "December 2, 2013", "7th century", "the original title", "2018", "Walter Pauk", "Richard Dashut", "1955", "maquila", "1959", "Schwarzenegger", "Salman Khan", "2016", "2003", "Isekai wa Sum\u0101tofon to Tomo ni", "Representatives", "a recognized group of people who jointly oversee the activities of an organization", "New Zealand", "stuffing", "currently a free agent", "Joe Pizzulo", "Times Square in New York City west to Lincoln Park in San Francisco", "1 BC", "Captaincy General of Guatemala", "the mid-1970s", "Florida and into the town of Coconut Cove", "the state sector", "tropical desert climate", "1988", "in the 1970s", "anembryonic gestation", "February 27, 2015", "John Hancock", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "lakes or reservoirs at high altitudes", "March 5, 2014", "Munilla Construction Management ( MCM )", "Carol Ann Susi", "hydrolysis reaction", "crown cutting", "Chris Rea", "2017", "16 August 1975", "ovule", "Samuel", "ships", "Dutch", "MGM Grand fire", "India Today", "Jet Republic", "Thousands", "Kabul", "Prison Break", "the Lone Ranger", "Ethiopia", "President Obama and Britain's Prince Charles"], "metric_results": {"EM": 0.5, "QA-F1": 0.5813961988304094}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.631578947368421, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 0.42857142857142855, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-9328", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-7398", "mrqa_triviaqa-validation-6372", "mrqa_triviaqa-validation-2261", "mrqa_hotpotqa-validation-886", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-12778", "mrqa_newsqa-validation-2497"], "SR": 0.5, "CSR": 0.5667410714285714, "retrieved_ids": ["mrqa_squad-train-76479", "mrqa_squad-train-2274", "mrqa_squad-train-9868", "mrqa_squad-train-10168", "mrqa_squad-train-84229", "mrqa_squad-train-32869", "mrqa_squad-train-4111", "mrqa_squad-train-84057", "mrqa_squad-train-52516", "mrqa_squad-train-53531", "mrqa_squad-train-52104", "mrqa_squad-train-12370", "mrqa_squad-train-40393", "mrqa_squad-train-44970", "mrqa_squad-train-70423", "mrqa_squad-train-57499", "mrqa_naturalquestions-validation-9516", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-3145", "mrqa_naturalquestions-validation-8845", "mrqa_searchqa-validation-11698", "mrqa_triviaqa-validation-1498", "mrqa_newsqa-validation-2958", "mrqa_hotpotqa-validation-5587", "mrqa_triviaqa-validation-5873", "mrqa_newsqa-validation-2517", "mrqa_naturalquestions-validation-2100", "mrqa_squad-validation-4789", "mrqa_triviaqa-validation-7128", "mrqa_hotpotqa-validation-5286", "mrqa_naturalquestions-validation-2124", "mrqa_searchqa-validation-3773"], "EFR": 0.875, "Overall": 0.7141294642857143}, {"timecode": 70, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4068", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-757", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8993", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10731", "mrqa_searchqa-validation-10872", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16049", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1330", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1949", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3687", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4301", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-562", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-6362", "mrqa_squad-validation-66", "mrqa_squad-validation-6962", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7693", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-855", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_squad-validation-9923", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4657", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-7726"], "OKR": 0.8515625, "KG": 0.48046875, "before_eval_results": {"predictions": ["jujitsu", "Peter Stuyvesant", "Cornell", "a cactus", "NASCAR", "(Vivaldi) Vivaldi", "seven", "Grace Slick", "London", "Sweden", "Phil Lynott", "purple", "Zachary Taylor", "Kempton Park", "( Leonardo) da Vinci", "isabella", "Belfast", "coconuts", "watford", "Kent", "vascular", "Tientsin", "jordan", "a cheese wheel", "a strings", "(George) Gray", "William", "Mackinac Bridge", "a wave", "Sunny afternoon", "tea", "Joan Crawford", "red", "Alexandria", "1969", "the queen", "boxing", "(Lord Beaconsfield)", "64-Earls", "Babylon", "Nottingham", "(1760\u20131820)", "25", "(S. Jimmy Beck)", "Antoine Lavoisier", "australia", "Agenor", "X-Men Origins: Wolverine", "(George) Washington", "David Mitchell", "(King) George I", "December 15, 2017", "Ben Rosenbaum", "Fox Ranch in Malibu Creek State Park, northwest of Los Angeles, essentially the backlot of 20th Century Fox", "Coal Miner's daughter", "National Association for the Advancement of Colored People (NA NAACP)", "gemeinn\u00fctzige", "Second seed Fernando Gonzalez of Chile also went through as he beat Ivan Ljubicic of Croatia 6-4 6-3.", "Alaska or Hawaii.", "\"falling space debris,\"", "jury", "ova", "( Brooke) Shields", "Nepal"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5347701149425287}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7586206896551724, 1.0, 0.19999999999999998, 0.0, 0.19999999999999998, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-924", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-968", "mrqa_triviaqa-validation-188", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6408", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-7380", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-6959", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-6308", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-468", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-3347", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-14926"], "SR": 0.46875, "CSR": 0.5653609154929577, "retrieved_ids": ["mrqa_squad-train-8827", "mrqa_squad-train-66042", "mrqa_squad-train-63532", "mrqa_squad-train-1338", "mrqa_squad-train-59424", "mrqa_squad-train-71038", "mrqa_squad-train-41473", "mrqa_squad-train-52587", "mrqa_squad-train-67047", "mrqa_squad-train-58813", "mrqa_squad-train-62571", "mrqa_squad-train-57417", "mrqa_squad-train-22475", "mrqa_squad-train-5035", "mrqa_squad-train-26553", "mrqa_squad-train-29205", "mrqa_triviaqa-validation-4304", "mrqa_newsqa-validation-38", "mrqa_searchqa-validation-4524", "mrqa_squad-validation-3771", "mrqa_searchqa-validation-12533", "mrqa_newsqa-validation-2552", "mrqa_searchqa-validation-10094", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-9383", "mrqa_searchqa-validation-2200", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4046", "mrqa_newsqa-validation-57", "mrqa_triviaqa-validation-6037", "mrqa_naturalquestions-validation-654"], "EFR": 0.9411764705882353, "Overall": 0.7200574772162386}, {"timecode": 71, "before_eval_results": {"predictions": ["four", "Wyoming", "alcohol", "Oprah", "Phil Spector", "Margaret Beckett", "King Arthur", "Easter Island", "Fringillidae", "Greyfriars", "Humphrey Bogart", "campanula", "jean Ritchie", "Harry Palmer", "Augustus", "True History of the Kelly Gang", "Yorkshire", "China", "dame Catherine Cookson", "jordan", "trumpet player", "go!", "king Midas", "sheep", "the French Open", "a cactus", "a child", "peaches", "Greece", "bone", "bruise", "barber", "yellow-brown", "Uzbekistan", "federal powers", "Daedalus", "Tommy Roe", "carglioni", "fluid drachms", "Kopassus", "Uranus", "Deimos", "exploits", "mental disorder", "pascal", "hippocampus", "ash", "1985", "Sisyphus", "Kathryn C. Taylor", "Honolulu", "usually in May", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "parthenogenesis", "Derry City F.C.", "URO VAMTAC", "Christian", "Stop the War Coalition", "U.S. program to assassinate terrorists in Iraq.", "\"procedure on her heart,\"", "a lexicographer", "a Great Blue Heron", "Billy the Kid", "the underprivileged."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5685606060606061}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-5335", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-7241", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-574", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6028", "mrqa_naturalquestions-validation-5831", "mrqa_hotpotqa-validation-1630", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2547", "mrqa_searchqa-validation-16275", "mrqa_newsqa-validation-3686"], "SR": 0.484375, "CSR": 0.5642361111111112, "retrieved_ids": ["mrqa_squad-train-12236", "mrqa_squad-train-391", "mrqa_squad-train-77238", "mrqa_squad-train-63099", "mrqa_squad-train-15472", "mrqa_squad-train-41972", "mrqa_squad-train-14269", "mrqa_squad-train-61980", "mrqa_squad-train-7235", "mrqa_squad-train-72072", "mrqa_squad-train-9734", "mrqa_squad-train-7804", "mrqa_squad-train-58504", "mrqa_squad-train-18943", "mrqa_squad-train-58350", "mrqa_squad-train-33860", "mrqa_naturalquestions-validation-6660", "mrqa_newsqa-validation-3063", "mrqa_hotpotqa-validation-5467", "mrqa_searchqa-validation-13042", "mrqa_hotpotqa-validation-3489", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-3425", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-1217", "mrqa_naturalquestions-validation-3189", "mrqa_newsqa-validation-319", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-440", "mrqa_searchqa-validation-133", "mrqa_newsqa-validation-2156"], "EFR": 0.9393939393939394, "Overall": 0.7194760101010101}, {"timecode": 72, "before_eval_results": {"predictions": ["explore traded in for the comforts of home and domestic Bliss.", "Colombia", "U.S. and NATO forces.", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "they'd get to bring a new puppy with them to the White House in January.", "three", "Kurdish militant group in Turkey", "The social and political vitality of the nation may depend on closing these racial gaps.", "Barbara Streisand", "U.S. President-elect Barack Obama", "via phone calls or by text messaging", "travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Friday", "off the coast", "Pixar's", "Friday", "state senators who will decide whether to remove him from office", "The Rosie Show", "President Bush", "South African captain Graeme Smith", "Brazil", "said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds", "Clifford Harris,", "Jeffrey Jamaleldine", "sniff out cell phones.", "Nearly eight in 10 say things are going badly in the country,", "five", "Karen Floyd", "around 8 p.m. local time Thursday", "15-year-old's", "150", "hit with a stun gun by police after an encounter; he crashed his vehicle in another incident and threatened to kill himself.", "Ennis, County Clare", "Haitians", "start a dialogue of peace based on the conversations she had with Americans along the way.", "Daniel Radcliffe", "1 million", "55-year-old", "Southeast,", "Venus Williams", "Two Russian bombers", "Lashkar-e-Jhangvi, was planning to conduct attacks in Karachi,", "\"still in denial\" about his conduct.", "Caylee Anthony", "March 22,", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "Rebecca Guerrero,", "prisoners", "Choi", "Apple employees", "September 1973", "Norman Greenbaum", "17 December 1968", "kiki", "l Leeds Rhinos", "carmen", "Matthew Ward Winer", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Sun Woong", "darts", "Joe Louis", "16th", "Roman Empire"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6570201731344347}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true], "QA-F1": [0.08695652173913043, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.47058823529411764, 0.0, 1.0, 0.0, 0.0689655172413793, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.28571428571428575, 0.0, 0.9600000000000001, 0.19354838709677416, 1.0, 1.0, 0.888888888888889, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.27027027027027023, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-1995", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-4092", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-690", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-81", "mrqa_naturalquestions-validation-1000", "mrqa_triviaqa-validation-4232", "mrqa_hotpotqa-validation-3757", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-10852"], "SR": 0.546875, "CSR": 0.5639982876712328, "retrieved_ids": ["mrqa_squad-train-23546", "mrqa_squad-train-63313", "mrqa_squad-train-16025", "mrqa_squad-train-20288", "mrqa_squad-train-63053", "mrqa_squad-train-23891", "mrqa_squad-train-31366", "mrqa_squad-train-69138", "mrqa_squad-train-46257", "mrqa_squad-train-39297", "mrqa_squad-train-63848", "mrqa_squad-train-60165", "mrqa_squad-train-72319", "mrqa_squad-train-76516", "mrqa_squad-train-39057", "mrqa_squad-train-26448", "mrqa_hotpotqa-validation-729", "mrqa_newsqa-validation-1784", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-2100", "mrqa_hotpotqa-validation-3435", "mrqa_triviaqa-validation-1374", "mrqa_searchqa-validation-416", "mrqa_searchqa-validation-1337", "mrqa_triviaqa-validation-4093", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-7624", "mrqa_squad-validation-3687", "mrqa_naturalquestions-validation-2569", "mrqa_searchqa-validation-8471", "mrqa_hotpotqa-validation-3833", "mrqa_newsqa-validation-2128"], "EFR": 1.0, "Overall": 0.7315496575342466}, {"timecode": 73, "before_eval_results": {"predictions": ["12", "Sodra nongovernmental organization,", "the United States", "not feel Misty Cummings has told them everything she knows.", "Mogadishu", "suicide car bombing", "more than 100", "apparently died after shooting himself three times in the head", "Scarlett Keeling", "took on water", "one bomber.", "Omar", "Dr. Cade", "Christian farmer", "keystroke", "preventing our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia.", "relatives of the five suspects,", "165-room", "U.S. Holocaust Memorial Museum,", "the simple puzzle video game,", "curfew", "40-year-old", "to stand down.", "Kingman Regional Medical Center,", "different women coping with breast cancer in five vignettes.", "two years", "269,000", "Harare", "UK", "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "\"against people who independent of their race, religion, ethnicity, social condition etc.", "Hundreds of militants, believed to be foreign fighters,", "they'd get to bring a new puppy with them to the White House in January.", "as he tried to throw a petrol bomb at the officers,", "Austin, Texas,", "forgery and flying without a valid license,", "Kurt Cobain's", "ALS6", "he and the other attackers were from Pakistan", "a rabbit hole,", "Larry Ellison,", "anesthetic and sedative.", "10", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "39,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "Old Trafford", "12.3 million", "Stratfor", "Seasons of My Heart", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "Justice A.K Mathur", "somatic cell nuclear transfer", "1956", "The Parson Russell Terrier", "Brazil", "Nicolas Cage", "Donald Richard \"Don\" DeLillo", "10 Years", "\"Queen City\"", "Carl Sagan", "Israel", "Rio", "50\u201340\u201390"], "metric_results": {"EM": 0.625, "QA-F1": 0.7085218824560979}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5882352941176471, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.2857142857142857, 0.5, 1.0, 0.10526315789473685, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-534", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-3615", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2397", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-345", "mrqa_searchqa-validation-8780", "mrqa_hotpotqa-validation-1757"], "SR": 0.625, "CSR": 0.5648226351351351, "retrieved_ids": ["mrqa_squad-train-39370", "mrqa_squad-train-86350", "mrqa_squad-train-73653", "mrqa_squad-train-53393", "mrqa_squad-train-13475", "mrqa_squad-train-48272", "mrqa_squad-train-82346", "mrqa_squad-train-78276", "mrqa_squad-train-52147", "mrqa_squad-train-64248", "mrqa_squad-train-73379", "mrqa_squad-train-82586", "mrqa_squad-train-7566", "mrqa_squad-train-10656", "mrqa_squad-train-3025", "mrqa_squad-train-48157", "mrqa_hotpotqa-validation-3127", "mrqa_triviaqa-validation-6037", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-6917", "mrqa_squad-validation-3511", "mrqa_hotpotqa-validation-887", "mrqa_searchqa-validation-16099", "mrqa_newsqa-validation-4079", "mrqa_triviaqa-validation-4196", "mrqa_hotpotqa-validation-139", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-11035", "mrqa_hotpotqa-validation-4926", "mrqa_naturalquestions-validation-3505", "mrqa_hotpotqa-validation-4101"], "EFR": 0.9583333333333334, "Overall": 0.7233811936936937}, {"timecode": 74, "before_eval_results": {"predictions": ["brazil", "north yorkshire", "Lou Gehrig", "Goat Island", "Loretta Lynn", "bats", "mail or junk newsgroup listings", "Andrew Lloyd Webber", "east of Eden", "Edward de Vere", "Mark Hamill", "Aslan", "kabaddi", "The Merchant of Venice", "kvetch", "cricketer", "Harold Wilson", "Bleak House", "h Halifax", "George Miller", "puffer fish", "Sheffield United", "Capricorn", "johannes k Kennedy", "the bluebird", "Toy Story", "Dean Wareham", "black", "Kiel Canal", "cork", "Avro", "Sarah Vaughan", "Abu Dhabi", "33", "Emily Davison", "Marc Brunel", "Ceredigion", "oasis", "Peter Sellers", "the Indus Valley", "georgia Belgica", "an even break", "David Bowie", "Lorne Greene", "1709", "fusilli", "Thai", "Viola", "\u00e1stron", "Ramadan", "sewing machines", "Prince George's County", "11 January 1923", "the Roman Empire", "1994", "Two Pi\u00f1a Coladas", "Tamara Ecclestone Rutland", "Kaka,", "his former caddy,", "three", "a Corporal", "William Henry Harrison", "Shelley", "1982"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7308779761904761}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-5470", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-531", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-3882", "mrqa_naturalquestions-validation-5958", "mrqa_hotpotqa-validation-4672", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-9552"], "SR": 0.671875, "CSR": 0.5662499999999999, "retrieved_ids": ["mrqa_squad-train-75679", "mrqa_squad-train-85390", "mrqa_squad-train-16404", "mrqa_squad-train-33037", "mrqa_squad-train-48988", "mrqa_squad-train-44158", "mrqa_squad-train-66638", "mrqa_squad-train-68432", "mrqa_squad-train-10571", "mrqa_squad-train-3592", "mrqa_squad-train-67606", "mrqa_squad-train-3908", "mrqa_squad-train-13100", "mrqa_squad-train-72396", "mrqa_squad-train-29928", "mrqa_squad-train-59799", "mrqa_newsqa-validation-2230", "mrqa_triviaqa-validation-3034", "mrqa_newsqa-validation-3899", "mrqa_triviaqa-validation-626", "mrqa_searchqa-validation-576", "mrqa_naturalquestions-validation-3004", "mrqa_squad-validation-4065", "mrqa_newsqa-validation-2682", "mrqa_hotpotqa-validation-2994", "mrqa_newsqa-validation-3961", "mrqa_naturalquestions-validation-1260", "mrqa_triviaqa-validation-2039", "mrqa_hotpotqa-validation-5438", "mrqa_naturalquestions-validation-9741", "mrqa_newsqa-validation-3325", "mrqa_naturalquestions-validation-4096"], "EFR": 0.9523809523809523, "Overall": 0.7224761904761905}, {"timecode": 75, "before_eval_results": {"predictions": ["Kgalema Motlanthe,", "reached an agreement late Thursday", "Dead Weather's \"Horehound\" at No. 6,", "two", "Oprah Winfrey's school in South Africa", "twice.", "Haleigh Cummings,", "the eradication of the Zetas cartel", "Hamas,", "a bank", "Rima Fakih", "using injectable vitamin supplements", "davon torg", "Arthur E. Morgan III,", "job training", "has to move out of her rental house because it is facing foreclosure", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "22", "1969", "diabetes and hypertension,", "Tillakaratne Dilshan scored his sixth Test century", "Tuesday", "100,000", "collapse", "$1.4 million", "President Obama", "Caylee Anthony", "BBC's central London offices", "the journalists and the flight crew will be freed,", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "returning combat veterans", "September 21.", "by Thursday.", "246", "40", "stole", "fine", "visitors aren't allowed onto the property to view the elephants, and only a handful of media members are able to visit each year, in an effort to make the animals' lives as natural as possible.", "At least 15 people", "polka Dot Bikini", "Arabic, French and English", "$40 and a bread.", "many as 250,000", "state senators", "Iran", "Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "the 1950s", "Orbiting Carbon Observatory,", "more than 2.5 million", "5,600", "Yemen,", "a Roman Catholic and fan of The Godfather Part II ( 1974 ), whose character Fredo had popularized the phrase )", "amphetamines", "set in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "pantomime The Miraculous Mandarin", "fridge", "blackcurrant", "Premier League club Everton", "Magic Band", "Fat Man", "messenger RNA", "the beaver", "Jan Hus", "R2-D2"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7161942755109734}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.888888888888889, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.8571428571428571, 0.5454545454545454, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.44776119402985076, 1.0, 0.06060606060606061, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-3454", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-864", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2207", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-5388", "mrqa_searchqa-validation-13173"], "SR": 0.578125, "CSR": 0.56640625, "retrieved_ids": ["mrqa_squad-train-74270", "mrqa_squad-train-49794", "mrqa_squad-train-10479", "mrqa_squad-train-59951", "mrqa_squad-train-40265", "mrqa_squad-train-80678", "mrqa_squad-train-57351", "mrqa_squad-train-53571", "mrqa_squad-train-43606", "mrqa_squad-train-71327", "mrqa_squad-train-84435", "mrqa_squad-train-14570", "mrqa_squad-train-67208", "mrqa_squad-train-85080", "mrqa_squad-train-48074", "mrqa_squad-train-65043", "mrqa_hotpotqa-validation-4301", "mrqa_searchqa-validation-13957", "mrqa_hotpotqa-validation-831", "mrqa_searchqa-validation-4769", "mrqa_squad-validation-1040", "mrqa_squad-validation-4272", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3876", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-6028", "mrqa_hotpotqa-validation-1989", "mrqa_squad-validation-2793", "mrqa_triviaqa-validation-1975", "mrqa_searchqa-validation-15031", "mrqa_newsqa-validation-1350"], "EFR": 0.8888888888888888, "Overall": 0.7098090277777778}, {"timecode": 76, "before_eval_results": {"predictions": ["Hawaii", "\"The Real Housewives of Atlanta\"", "commission, led by former U.S. Attorney Patrick Collins,", "The number of deaths linked to cantaloupes contaminated with the Listeria monocytogenes bacteria has risen to 28,", "he was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.", "Sheikh Sharif Sheikh Ahmed", "two", "golf", "ended his playing career at his original club of Argentinos Juniors in 2007 and has been coaching at Independiente.", "Nigeria", "Ameneh Bahrami", "the chief executive officer,", "Daytime Emmy Lifetime Achievement Award", "test-launched a rocket capable of carrying a satellite", "\"exceptional circumstances surround these memos and require their release.\"", "35,000.", "Roger Federer", "March 22,", "Venezuela", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing massacre.", "sanctions against Zimbabwe,", "\"Operation Crank Call,\"", "the body of the aircraft", "Rima Fakih", "more than 100", "Obama and McCain camps", "Ameneh Bahrami", "about 5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "President Bill Clinton", "Haleigh Cummings,", "Transportation Security Administration", "allegedly faking a doctor's note", "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "J. Crew,", "was killed", "nook", "Islamabad", "three", "Nigeria", "Her husband and attorney, James Whitehouse,", "fill a million sandbags and place 700,000 around our city,\"", "women, have fewer kids and more often have college educations.\"", "15-year-old", "normal", "David Bowie,", "15,000", "looked depressed", "Franklin", "such joint exercises between nations are not unusual.", "Authorities in Fayetteville, North Carolina,", "Saturday", "Lake Powell", "Lew Brown", "Rockwell", "royal duchess of marlborough", "Neighbours", "Pesach", "Walcha", "Dunlop", "Hern\u00e1n Crespo", "Iceland", "( Giuseppe) Verdi", "Serengeti", "germany"], "metric_results": {"EM": 0.625, "QA-F1": 0.7097094924950528}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.06896551724137932, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.1904761904761905, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-2396", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2519", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-648", "mrqa_hotpotqa-validation-5413", "mrqa_searchqa-validation-1821", "mrqa_triviaqa-validation-925"], "SR": 0.625, "CSR": 0.5671672077922079, "retrieved_ids": ["mrqa_squad-train-13828", "mrqa_squad-train-1543", "mrqa_squad-train-61034", "mrqa_squad-train-32381", "mrqa_squad-train-5829", "mrqa_squad-train-24945", "mrqa_squad-train-5652", "mrqa_squad-train-23801", "mrqa_squad-train-71802", "mrqa_squad-train-6159", "mrqa_squad-train-81110", "mrqa_squad-train-73142", "mrqa_squad-train-28577", "mrqa_squad-train-84772", "mrqa_squad-train-2085", "mrqa_squad-train-44778", "mrqa_triviaqa-validation-2955", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-4558", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-5804", "mrqa_searchqa-validation-13218", "mrqa_naturalquestions-validation-9306", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-319", "mrqa_newsqa-validation-2937", "mrqa_naturalquestions-validation-919", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-1282", "mrqa_naturalquestions-validation-8393", "mrqa_newsqa-validation-3564", "mrqa_hotpotqa-validation-1435"], "EFR": 0.9166666666666666, "Overall": 0.7155167748917749}, {"timecode": 77, "before_eval_results": {"predictions": ["Sunday", "The mother (Charlotte Gainsbourg) is consumed with grief and guilt.", "Patrick McGoohan,", "flooding and debris", "\"very important that Brazil and the United States work closely in this field,\"", "Woosuk Ken Choi,", "head injury.", "1994,", "Mawise Gumba", "gun charges,", "25 dead", "At least 15", "at least nine people dead.", "shows the world that you love the environment and hate using fuel,\"", "\"falling space debris,\"", "Transportation Security Administration", "$10 billion", "\"The Orchid thief\"", "101", "his business dealings", "Jaime Andrade", "A receptionist with a gunshot wound in her stomach played dead under her desk and called 911 on Friday after a shooting massacre in a Binghamton, New York, immigration center.", "skyscrapers", "the United States", "financial gain,", "Arnold Drummond", "trading goods and services without exchanging money", "Euna Lee,", "high tide -- expected to reach about 4 meters (13 feet) high -- is forecast for Sunday morning.", "July", "16", "the Register -- Iowa's largest newspaper -- backed Romney in his bid for the Republican presidential nomination", "Nearly all", "reached an agreement late Thursday to form a government of national reconciliation.", "Zelaya and Roberto Micheletti,", "inferior,", "At the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "prison inmates.", "Cannes Film Festival,", "Mark Obama Ndesandjo", "Ronald Reagan UCLA Medical Center,", "two", "the Afghan opium trade", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "executive director of the Americas Division of Human Rights Watch,", "\"Empire of the Sun,\"", "Basel", "\"@\"", "Jeffrey Jamaleldine", "Heshmat Tehran Attarzadeh", "At least 14", "Daryl Sabara", "in the eighth episode of Arrow's second season", "Gibraltar", "Barry White", "george i Adams", "Anita Brookner", "9", "British", "consulting", "flamboyant", "Orson Welles", "barbed wire", "loyal to the ideas of the British Conservative party"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6490885224703917}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.21052631578947367, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.5, 1.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_newsqa-validation-3928", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-795", "mrqa_naturalquestions-validation-9330", "mrqa_triviaqa-validation-635", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-12008"], "SR": 0.5625, "CSR": 0.5671073717948718, "retrieved_ids": ["mrqa_squad-train-1539", "mrqa_squad-train-68873", "mrqa_squad-train-77131", "mrqa_squad-train-84250", "mrqa_squad-train-81758", "mrqa_squad-train-36490", "mrqa_squad-train-76652", "mrqa_squad-train-72145", "mrqa_squad-train-2388", "mrqa_squad-train-25401", "mrqa_squad-train-57228", "mrqa_squad-train-3317", "mrqa_squad-train-4700", "mrqa_squad-train-80185", "mrqa_squad-train-11348", "mrqa_squad-train-8367", "mrqa_searchqa-validation-2773", "mrqa_triviaqa-validation-574", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-2538", "mrqa_hotpotqa-validation-1142", "mrqa_newsqa-validation-3337", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-1611", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-821", "mrqa_hotpotqa-validation-4517", "mrqa_naturalquestions-validation-8446", "mrqa_triviaqa-validation-7704", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-193", "mrqa_triviaqa-validation-3890"], "EFR": 0.9285714285714286, "Overall": 0.71788576007326}, {"timecode": 78, "before_eval_results": {"predictions": ["dress shop", "callable bonds", "British Columbia, Canada", "45 %", "September 14, 2008", "to prevent further offense by convincing the offender that their conduct was wrong", "Brad Johnson", "the closing of the atrioventricular valves and semilunar valves", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "a compiler", "Dalveer Bhandari", "bone marrow", "to collect menstrual flow", "the Kansas City Chiefs", "the second season episode ``Pretty Much Dead Already ''", "Filipino", "the 10th century", "Wakanda", "the Roman Empire", "Joel", "a young girl ( an illustration by Everest creative Maganlal Daiya back in the 1960s", "Gertrude Niesen", "Australia", "1983", "Kathleen Erin Walsh", "Set six months after Kratos killed his wife and child, he has been imprisoned", "The photoelectric ( optical ) smoke detector", "the head", "jazz", "sedimentary rock", "$2 million", "Michael Buffer", "Eric Clapton", "115", "c. 3000 BC", "Andrew Garfield", "each team", "1898", "a routing table", "the Western world", "Twickenham", "Glenn Close", "Kenneth Cook", "Tim Russert", "1 - 2 spinal nerve segments above the point of entry", "1957", "displacement", "the Second Continental Congress meeting at the Pennsylvania State House ( Independence Hall ) in Philadelphia", "the winter solstice", "a mixture of phencyclidine and cocaine", "an integral membrane protein that builds up a proton gradient across a biological membrane", "Lewis Carroll", "lithium", "The Apprentice", "A123 Systems", "41st President of the United States", "Andrew J. West", "34", "Caylee Anthony,", "Michael Brewer,", "a signal", "Tennessee Williams", "Nathaniel Hawthorne", "Friday,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6834573412698413}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9333333333333333, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.2857142857142857, 1.0, 1.0, 0.9, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9, 0.0, 1.0, 0.4, 0.4, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.8, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-5536", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-1433", "mrqa_newsqa-validation-3438", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-16420"], "SR": 0.515625, "CSR": 0.5664556962025317, "retrieved_ids": ["mrqa_squad-train-55119", "mrqa_squad-train-20374", "mrqa_squad-train-12047", "mrqa_squad-train-46383", "mrqa_squad-train-72407", "mrqa_squad-train-2698", "mrqa_squad-train-60851", "mrqa_squad-train-86376", "mrqa_squad-train-17312", "mrqa_squad-train-39485", "mrqa_squad-train-83194", "mrqa_squad-train-71053", "mrqa_squad-train-70496", "mrqa_squad-train-83043", "mrqa_squad-train-13646", "mrqa_squad-train-5708", "mrqa_naturalquestions-validation-3329", "mrqa_hotpotqa-validation-2130", "mrqa_newsqa-validation-249", "mrqa_naturalquestions-validation-863", "mrqa_newsqa-validation-2288", "mrqa_triviaqa-validation-4500", "mrqa_searchqa-validation-9866", "mrqa_newsqa-validation-1994", "mrqa_squad-validation-9865", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-560", "mrqa_naturalquestions-validation-7880", "mrqa_triviaqa-validation-6798", "mrqa_hotpotqa-validation-3127", "mrqa_searchqa-validation-16283"], "EFR": 0.967741935483871, "Overall": 0.7255895263372805}, {"timecode": 79, "before_eval_results": {"predictions": ["Bury, Greater Manchester, England", "Great Sioux War", "Morocco", "Potomac River", "Hermione Baddeley", "Harmony Korine", "March", "Pennsylvania", "R&B", "Red and Assiniboine Rivers", "King George IV and the Duke of Wellington", "June 24, 1935", "The Indianapolis Times", "An odd-eyed cat", "2001", "Perth's number one rating radio station, MIX 94.5", "1999", "Tempo", "Presbyterian Church", "2002", "English", "Metro Memphis", "Anheuser-Busch InBev", "University of Missouri-Kansas City", "Francis the Talking Mule", "Les Clark", "Giuseppe Verdi", "County Louth", "Gal Gadot", "Kurt Vonnegut Jr.", "top division", "film", "Laurel, Mississippi", "Vince Guaraldi", "2007", "Grave Digger", "Mulberry", "Isabella (Belle) Baumfree", "Jay Park", "The final of 2011 AFC Asian Cup", "Mel Blanc", "Centers for Medicare & Medicaid Services", "Pakistan", "\"Godspell\"", "Steven Selling", "Scunthorpe", "Australian", "Argentinian", "Jack St. Clair Kilby", "1970s and 1980s", "Personal History", "Timothy B. Schmit", "Stephen Curry", "the direction from which the wind is blowing", "France", "Welsh", "plants", "Alfredo Astiz,", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip,", "Kim", "Catherine of Aragon", "Little Boy Blue", "Cheyenne", "March 27, 2017"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6829545454545455}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-2277", "mrqa_hotpotqa-validation-3795", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4248", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-5515", "mrqa_naturalquestions-validation-601", "mrqa_triviaqa-validation-6644", "mrqa_triviaqa-validation-4205", "mrqa_newsqa-validation-433", "mrqa_searchqa-validation-5939", "mrqa_naturalquestions-validation-5649"], "SR": 0.578125, "CSR": 0.5666015625, "retrieved_ids": ["mrqa_squad-train-51963", "mrqa_squad-train-62071", "mrqa_squad-train-28707", "mrqa_squad-train-48715", "mrqa_squad-train-58592", "mrqa_squad-train-77811", "mrqa_squad-train-16745", "mrqa_squad-train-44279", "mrqa_squad-train-20945", "mrqa_squad-train-36588", "mrqa_squad-train-55042", "mrqa_squad-train-74907", "mrqa_squad-train-72642", "mrqa_squad-train-25296", "mrqa_squad-train-70392", "mrqa_squad-train-33635", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-3826", "mrqa_triviaqa-validation-514", "mrqa_naturalquestions-validation-9078", "mrqa_hotpotqa-validation-431", "mrqa_triviaqa-validation-2369", "mrqa_newsqa-validation-148", "mrqa_hotpotqa-validation-461", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-4399", "mrqa_hotpotqa-validation-3780", "mrqa_searchqa-validation-13218", "mrqa_naturalquestions-validation-7935", "mrqa_triviaqa-validation-6464", "mrqa_naturalquestions-validation-1155", "mrqa_squad-validation-6284"], "EFR": 1.0, "Overall": 0.7320703125}, {"timecode": 80, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1288", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.888671875, "KG": 0.51484375, "before_eval_results": {"predictions": ["John Nash", "Two Greedy Italians", "ross henry", "constant", "Mel Brooks", "Devil May Care", "Edward Woodward", "Scotland", "Fiat", "three-stringed", "bury St Edmunds", "Andre Agassi", "jean", "gharatavarsha", "Piet\u00e0", "Mark Darcy", "tran\u017flate", "Reggie Kray", "Milan", "Stephen Hendry", "Bash Street", "eddie howerd", "robin hood", "Me and My Girl", "johannam", "Augustus", "Shepherd Neame", "Titanic", "simon falk", "tax collector", "robert Maxwell", "Mikhail Gorbachev", "Pocahontas", "Noah Beery, Jr.", "Argentina", "the peripheral nerves", "myxomatosis", "avocadoes", "World War I", "Captain America", "hARIBO", "chicken livers", "New Zealand", "his long-time lover, Eva Braun", "tibet", "Devon Loch", "Macau", "Bruce Willis", "Kwame Nkrumah", "The Fifth Amendment", "macrame", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "pneumonoultramicroscopicsilicovolcanoconiosis", "Hercules", "Philip Livingston", "2010", "\"The Process\"", "Ameneh Bahrami", "Nearly eight in 10", "a tenement in the Mumbai suburb of Chembur,", "camels", "Shrek", "Seoul", "Asia"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7094494047619047}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7305", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-2223", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-7544", "mrqa_triviaqa-validation-7116", "mrqa_searchqa-validation-11588", "mrqa_searchqa-validation-457"], "SR": 0.671875, "CSR": 0.5679012345679013, "retrieved_ids": ["mrqa_squad-train-4461", "mrqa_squad-train-6448", "mrqa_squad-train-21290", "mrqa_squad-train-81455", "mrqa_squad-train-48230", "mrqa_squad-train-74326", "mrqa_squad-train-52012", "mrqa_squad-train-5499", "mrqa_squad-train-8758", "mrqa_squad-train-45624", "mrqa_squad-train-15302", "mrqa_squad-train-32761", "mrqa_squad-train-47772", "mrqa_squad-train-32560", "mrqa_squad-train-60349", "mrqa_squad-train-47697", "mrqa_squad-validation-9093", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-2627", "mrqa_hotpotqa-validation-2160", "mrqa_newsqa-validation-3126", "mrqa_squad-validation-10395", "mrqa_newsqa-validation-1367", "mrqa_naturalquestions-validation-1068", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3991", "mrqa_searchqa-validation-7891", "mrqa_squad-validation-1831", "mrqa_newsqa-validation-1398", "mrqa_triviaqa-validation-4896", "mrqa_newsqa-validation-2118", "mrqa_naturalquestions-validation-1786"], "EFR": 0.9523809523809523, "Overall": 0.7382751873897707}, {"timecode": 81, "before_eval_results": {"predictions": ["London", "his writings about the outdoors, especially mountain-climbing", "50th anniversary of the founding of the National Basketball Association (NBA)", "Roger Staubach", "World Health Organization", "Pulitzer Prize", "Argentine", "Pittsburgh Steelers", "Kim Jong-hyun", "Las Vegas", "Scandinavian design", "romantic comedy", "Anthony Davis", "Stephen Lee", "December 19, 1967", "South African-born", "Betty", "1822", "1926 Paris", "Bulgarian", "Quahog, Rhode Island", "Landing Barge, Kitchen", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Roslyn Castle", "Battle of Dresden", "2015", "Violet", "Free Range Films", "Mondays", "Chrysler K platform", "Edinburgh", "Laurel, Mississippi", "Matt Flynn", "Camber Sands", "Jaguar Land Rover Limited", "Hanoi", "Adelaide", "Shepardson Microsystems, Inc.", "The Fault in Our Stars", "sixteen", "Crips", "Deftones", "Doctor of Philosophy", "Donald Richard \"Don\" DeLillo", "Claude Mak\u00e9l\u00e9l\u00e9", "#364", "Magic Band", "Salzburg Festival", "German princely Battenberg family", "German political and military leader", "The Nassau Herald", "Fox Ranch in Malibu Creek State Park, northwest of Los Angeles", "You are a puzzle", "pineapple", "euthanasia", "peter", "Poland", "Steven Green", "Steve Williams", "Umar Farouk AbdulMutallab", "Dr. No", "lat pulldown", "Big Brown", "Victor Manuel Mejia Munera was a drug lord with ties to paramilitary groups,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7561994453578533}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.2857142857142857, 0.35294117647058826, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-712", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-376", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2925", "mrqa_naturalquestions-validation-468", "mrqa_triviaqa-validation-5053", "mrqa_newsqa-validation-1205", "mrqa_searchqa-validation-11199", "mrqa_newsqa-validation-877"], "SR": 0.640625, "CSR": 0.5687881097560976, "retrieved_ids": ["mrqa_squad-train-33127", "mrqa_squad-train-79617", "mrqa_squad-train-33259", "mrqa_squad-train-84349", "mrqa_squad-train-12191", "mrqa_squad-train-62056", "mrqa_squad-train-11334", "mrqa_squad-train-60731", "mrqa_squad-train-52964", "mrqa_squad-train-43763", "mrqa_squad-train-15628", "mrqa_squad-train-75612", "mrqa_squad-train-79166", "mrqa_squad-train-29315", "mrqa_squad-train-44175", "mrqa_squad-train-7121", "mrqa_triviaqa-validation-2427", "mrqa_naturalquestions-validation-8117", "mrqa_hotpotqa-validation-655", "mrqa_searchqa-validation-10993", "mrqa_triviaqa-validation-1270", "mrqa_naturalquestions-validation-5452", "mrqa_newsqa-validation-3170", "mrqa_hotpotqa-validation-186", "mrqa_naturalquestions-validation-4240", "mrqa_searchqa-validation-5691", "mrqa_newsqa-validation-1210", "mrqa_triviaqa-validation-1229", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-1102", "mrqa_triviaqa-validation-244", "mrqa_squad-validation-4216"], "EFR": 0.9565217391304348, "Overall": 0.7392807197773065}, {"timecode": 82, "before_eval_results": {"predictions": ["Kenny Young", "40 million", "\"American Idol\"", "brother-in-law", "Terence Winter", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Daimler-Benz", "Edward M. Kennedy", "private", "Lee Seok-hoon", "Two Pi\u00f1a Coladas", "early traditions", "Most observers viewed the election as blatantly unfair", "Eucritta melanolimnetes", "Umberto II", "1866", "1860", "Attorney General and as Lord Chancellor of England", "Sexred", "British", "Westfield Tea Tree Plaza", "924", "1951", "Darci Kistler", "1966", "Potomac River", "Europe", "Harry Potter series", "Portal", "England", "Black Panthers", "An extended play", "\"Sausage Party\"", "July 8, 2014", "The Primettes", "Twelfth Night, or What You Will", "Wolf Creek", "Sky News", "palleus Scotorum", "Chelsea Does", "bobsledder", "Double Agent", "Oregon Ducks", "Aksel Sandemose", "Sim Theme Park", "the Earth", "Eric Whitacre", "University of Kentucky College of Pharmacy", "24 January 76 \u2013 10 July 138", "Marvel Comics", "Zambesi river", "along the Californian coast at The Inn at Newport Ranch", "1961", "September 14, 2008", "Harry S. Truman", "The Kentucky Derby", "Gianni Versace", "London", "his former Boca Juniors teammate and national coach Diego Maradona,", "his club", "Tough Enough to wear Pink", "Beaker", "Claddagh", "Piano Concerto No. 5"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7700987554112554}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.0, 0.6399999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 0.4, 0.33333333333333337, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-5066", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-4293", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-4974", "mrqa_naturalquestions-validation-2250", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1461", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-10992"], "SR": 0.6875, "CSR": 0.5702183734939759, "retrieved_ids": ["mrqa_squad-train-9027", "mrqa_squad-train-25426", "mrqa_squad-train-51807", "mrqa_squad-train-30730", "mrqa_squad-train-42656", "mrqa_squad-train-59954", "mrqa_squad-train-51007", "mrqa_squad-train-64506", "mrqa_squad-train-42599", "mrqa_squad-train-9704", "mrqa_squad-train-18191", "mrqa_squad-train-40105", "mrqa_squad-train-56903", "mrqa_squad-train-35380", "mrqa_squad-train-17302", "mrqa_squad-train-70283", "mrqa_triviaqa-validation-7640", "mrqa_searchqa-validation-12487", "mrqa_hotpotqa-validation-1272", "mrqa_searchqa-validation-8208", "mrqa_hotpotqa-validation-3904", "mrqa_squad-validation-8964", "mrqa_triviaqa-validation-3246", "mrqa_naturalquestions-validation-9516", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-7535", "mrqa_naturalquestions-validation-5599", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4619", "mrqa_newsqa-validation-1386", "mrqa_triviaqa-validation-1702"], "EFR": 1.0, "Overall": 0.7482624246987951}, {"timecode": 83, "before_eval_results": {"predictions": ["Thunder Road", "The original surname may have denoted someone from the former Kingdom of Strathclyde who spoke Cumbric, a close relative of the Welsh language, or possibly an incomer from Wales, or the Welsh Marches", "between the Eastern Ghats and the Bay of Bengal", "Lincoln Park in San Francisco", "New York University", "the oral mucosa ( a mucous membrane ) lining the mouth and also on the tongue and palates and mouth floor", "Eydie Gorm\u00e9", "Werner Ruchti", "Stephen Lang", "used obscure languages as a means of secret communication during wartime", "the English", "Himadri Station", "1959", "Sun Tzu", "their son Jack ( short for Jack - o - Lantern )", "multiple alternative realities rather than a novel", "Pope Gregory I the Great", "first staged at the Edinburgh Festival Fringe in 1966", "1955", "1970", "Warren Hastings", "The mixing of sea water and fresh water", "water ice", "June 1992", "John Vincent Calipari", "1975", "Charles Darwin and Alfred Russel Wallace", "Left Behind", "1997", "Judiththia Aline Keppel", "The first Old World equid fossil was found in the gypsum quarries in Montmartre, Paris, in the 1820s", "May 18, 2018", "Russia", "France, the United Kingdom, and the United States", "whistleblowers", "Fusajiro Yamauchi", "Etienne de Mestre", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "pre-Christian festivals that were celebrated around the winter solstice", "Speaker of the House of Representatives", "the NFL", "Italy", "Montreal Bruins", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral", "Captain Jones", "a fictional character", "Florida and into the town of Coconut Cove", "S - shaped", "February 16, 2010", "at least 18 or 21 years old", "Lori Rom", "Jennifer Eccles", "Sinclair Lewis", "Akon", "Lily Hampton", "Havenhurst", "Gregg Harper", "23-year-old", "more than 200.", "Transportation Security Administration", "Vaio Z Canvas", "Newman", "(The Color Purple)", "East Knoyle"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7256766272121875}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.06896551724137931, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5714285714285715, 0.125, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-1447", "mrqa_searchqa-validation-8173"], "SR": 0.65625, "CSR": 0.5712425595238095, "retrieved_ids": ["mrqa_squad-train-70645", "mrqa_squad-train-64608", "mrqa_squad-train-77794", "mrqa_squad-train-40006", "mrqa_squad-train-6382", "mrqa_squad-train-46058", "mrqa_squad-train-32289", "mrqa_squad-train-67528", "mrqa_squad-train-15994", "mrqa_squad-train-46134", "mrqa_squad-train-70559", "mrqa_squad-train-27300", "mrqa_squad-train-58989", "mrqa_squad-train-78718", "mrqa_squad-train-69918", "mrqa_squad-train-83210", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4487", "mrqa_newsqa-validation-912", "mrqa_naturalquestions-validation-1704", "mrqa_triviaqa-validation-7641", "mrqa_hotpotqa-validation-3388", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-786", "mrqa_naturalquestions-validation-9328", "mrqa_naturalquestions-validation-832", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-319", "mrqa_searchqa-validation-10284", "mrqa_searchqa-validation-3149", "mrqa_naturalquestions-validation-3893"], "EFR": 1.0, "Overall": 0.7484672619047619}, {"timecode": 84, "before_eval_results": {"predictions": ["to connect the CNS to the limbs and organs", "the heart", "Grand Inquisition", "John Findley Wallace", "the Western Bloc ( the United States, its NATO allies and others )", "Gemma Baker", "s - block", "late - night", "Ewan McGregor", "April 10, 2018", "the New York Yankees", "a visible cross", "the left", "Ireland", "Saphira", "Pre-evaluated, strategic planning, operative planning, implementation, and post-evaluation", "Saint Peter", "active osmotic water absorption", "1983", "restricted naturalization to `` free white persons '' of `` good moral character ''", "a heart rate that exceeds the normal resting rate", "Ptolemy", "1986", "Battle of Antietam", "Emma Watson", "1939", "Upon braking to a full stop", "Thomas Mundy Peterson", "Filipino Americans", "Glynis Johns", "the thirteen American colonies regarded themselves as a new nation, the United States of America, and were no longer part of the British Empire", "eusebeia", "Napoleon Bonaparte", "Once Upon a Time in India", "A blighted ovum or anembryonic gestation", "September 19 - 22, 2017", "in the New Testament", "Kirsten Simone Vangsness", "asphyxia", "1931", "Camp Green Lake", "limited period of time", "1999", "10,605", "the East Coast of the United States", "the Reverse - Flash", "March 1930", "John Donne", "Category 4", "season four", "6 January 793", "Rugby School", "gold", "Doctor John Dolittle", "Premier League", "Christian Kern", "50 best cities to live in", "Interior Department's inspector general,", "Miguel Cotto", "scraped together his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and food.", "Rocky", "the Cumberland Gap", "19.05 ppt", "George Fox"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6193923408767159}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, true], "QA-F1": [0.5185185185185185, 0.4, 0.5, 0.0, 0.3636363636363636, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.625, 0.8333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-1910", "mrqa_naturalquestions-validation-1688", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3826", "mrqa_hotpotqa-validation-3900", "mrqa_newsqa-validation-2653", "mrqa_searchqa-validation-6687"], "SR": 0.515625, "CSR": 0.5705882352941176, "retrieved_ids": ["mrqa_squad-train-48354", "mrqa_squad-train-26704", "mrqa_squad-train-32203", "mrqa_squad-train-26547", "mrqa_squad-train-12306", "mrqa_squad-train-77933", "mrqa_squad-train-418", "mrqa_squad-train-85242", "mrqa_squad-train-28035", "mrqa_squad-train-33843", "mrqa_squad-train-16247", "mrqa_squad-train-74318", "mrqa_squad-train-79064", "mrqa_squad-train-58581", "mrqa_squad-train-15090", "mrqa_squad-train-53313", "mrqa_triviaqa-validation-6500", "mrqa_squad-validation-1600", "mrqa_newsqa-validation-2122", "mrqa_hotpotqa-validation-3731", "mrqa_triviaqa-validation-4191", "mrqa_naturalquestions-validation-9328", "mrqa_newsqa-validation-2817", "mrqa_triviaqa-validation-3537", "mrqa_hotpotqa-validation-3714", "mrqa_naturalquestions-validation-6771", "mrqa_hotpotqa-validation-2861", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-924", "mrqa_searchqa-validation-2049", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8845"], "EFR": 0.9032258064516129, "Overall": 0.728981558349146}, {"timecode": 85, "before_eval_results": {"predictions": ["John Marshall", "Pirates of the Caribbean: Dead Man\\'s Chest", "Samuel de Champlain", "Louis XIV", "Lady Jane Grey", "the Barbary Coast", "Iceland", "Excalibur", "Richard Cory", "Volkswagen Passat", "baldness", "Athens", "rum", "tea rose", "Aida", "give love a bad name", "Chukchi Sea", "Marie Antoinette", "rotunda", "the magnolia", "haryana", "a bicentennial", "( Julius) Caesar", "auction", "the peace sign", "Michael Dell", "pizza", "Lusitania", "1972", "a hurricane", "Amish", "the Rocky Mountains", "carbon", "Materials", "Boston", "Wu-Tang Clan", "king", "(Jose de San) Martin", "a whale", "Salt Lake City", "Luxembourg", "Texas", "drag & drop", "The Knight of Ni", "a dove", "Las Vegas", "Laura", "The New Yorker", "People of the Book", "a snout beetle", "Tufts University", "The results of the Avery -- MacLeod -- McCarty experiment", "the cast", "the lower limb", "Amy Tan", "carbon", "The Truman Show", "Robert Redford", "Great Lakes and Midwestern", "three", "five", "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "March 3, 2008,", "Elizabeth Birnbaum"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7703125}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-12392", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-6995", "mrqa_searchqa-validation-1981", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-4152", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-11933", "mrqa_searchqa-validation-3852", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-2130", "mrqa_newsqa-validation-4210"], "SR": 0.6875, "CSR": 0.5719476744186047, "retrieved_ids": ["mrqa_squad-train-80502", "mrqa_squad-train-65255", "mrqa_squad-train-22770", "mrqa_squad-train-395", "mrqa_squad-train-57317", "mrqa_squad-train-23129", "mrqa_squad-train-11379", "mrqa_squad-train-83756", "mrqa_squad-train-67993", "mrqa_squad-train-37072", "mrqa_squad-train-84357", "mrqa_squad-train-3806", "mrqa_squad-train-59414", "mrqa_squad-train-53156", "mrqa_squad-train-36620", "mrqa_squad-train-35312", "mrqa_newsqa-validation-1397", "mrqa_naturalquestions-validation-3602", "mrqa_searchqa-validation-5071", "mrqa_naturalquestions-validation-8227", "mrqa_searchqa-validation-5048", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-13435", "mrqa_newsqa-validation-3827", "mrqa_triviaqa-validation-1470", "mrqa_naturalquestions-validation-998", "mrqa_squad-validation-818", "mrqa_squad-validation-4216", "mrqa_searchqa-validation-12817", "mrqa_naturalquestions-validation-3162", "mrqa_searchqa-validation-2890", "mrqa_newsqa-validation-2326"], "EFR": 1.0, "Overall": 0.748608284883721}, {"timecode": 86, "before_eval_results": {"predictions": ["granite", "Bull", "Horse Feathers", "Bleak House", "Chaillot", "driving miss daisy", "Coloring", "Asteroids", "a peace", "Yves Saint Laurent", "New Zealand", "England", "Lend-Lease Act", "Spanglish", "Monica Lewinsky", "Friday Night Lights", "Google", "Medusa", "the vest", "Prince", "a gull", "Hammurabi", "Nixon", "rain", "Erwin Rommel", "jump", "Ned", "the 747", "Terry Bradshaw", "Chris Evert", "Azerbaijan", "Mamma Mia!", "Fallingwater", "Alanis Morissette", "dashes", "a barrel", "Etna", "a law clerk", "Faneuil Hall", "Louisiana", "George Orwell", "black tea", "toro", "Stalin", "Metallica", "change horses", "Jeopardy", "Lafayette", "Nick Carraway", "Captain Kangaroo", "Kosher", "1996", "Super Bowl XIX", "Gabrielle - Suzanne Barbot de Villeneuve", "paste", "nitrogen", "South Africa", "Cartoon Network Too", "Wilton Mall at Saratoga", "Bruce Grobbelaar", "Tehran.", "The Palm Jumeirah", "35,000.", "Mashhad"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7265625}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-14053", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-13500", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-7312", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-2504", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-16876", "mrqa_searchqa-validation-4740", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-4307", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-1471", "mrqa_hotpotqa-validation-337", "mrqa_newsqa-validation-3141"], "SR": 0.65625, "CSR": 0.5729166666666667, "retrieved_ids": ["mrqa_squad-train-25112", "mrqa_squad-train-26959", "mrqa_squad-train-85075", "mrqa_squad-train-84969", "mrqa_squad-train-2886", "mrqa_squad-train-33600", "mrqa_squad-train-49311", "mrqa_squad-train-10551", "mrqa_squad-train-49343", "mrqa_squad-train-2667", "mrqa_squad-train-10742", "mrqa_squad-train-14777", "mrqa_squad-train-15635", "mrqa_squad-train-28124", "mrqa_squad-train-851", "mrqa_squad-train-81834", "mrqa_naturalquestions-validation-2151", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-4553", "mrqa_searchqa-validation-7325", "mrqa_newsqa-validation-1613", "mrqa_hotpotqa-validation-5306", "mrqa_squad-validation-2831", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-8617", "mrqa_searchqa-validation-14038", "mrqa_naturalquestions-validation-8534", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5603", "mrqa_naturalquestions-validation-863", "mrqa_hotpotqa-validation-4672", "mrqa_searchqa-validation-14237"], "EFR": 0.9545454545454546, "Overall": 0.7397111742424242}, {"timecode": 87, "before_eval_results": {"predictions": ["Inverness", "a chiffon", "Corpus Christi", "Grover Cleveland", "an eye", "the Federalist Papers", "Martin Luther King", "transitive & intransitive", "California", "the Central Pacific", "ACTIVE", "Tom Cruise", "Sicilian pizza", "the giant panda", "Risk", "rice", "Kansas State", "scrabble", "1945", "Kentucky", "a stork", "the Lord of the Rings", "a rat", "anime", "Daisy Miller", "Icelandic", "Prince of Thieves", "the Moon", "the Nissan Armada", "the Stars and Stripes Forever", "One Hundred Years of Solitude", "lethal", "Henry Cavendish", "vanilla", "a terminal", "Italy", "Night of the Iguana", "Rhode Island", "Baseball", "Anne Rice", "the root", "the Book of the Wars of Jehovah", "1066", "Sir John Soane", "the Bull", "the hip", "a hearse", "City Slickers", "Ned Kelly", "Sans Souci", "the Doge", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "the southeastern United States", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Italy", "Pakistan International Airlines", "sculpture", "1998", "\"Peshwa\" ( Prime Minister)", "\"Histoires ou contes du temps pass\u00e9\"", "the Beatles", "Adidas", "President Bush", "Used Acura"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7526537698412699}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-15068", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-4561", "mrqa_searchqa-validation-5164", "mrqa_searchqa-validation-11025", "mrqa_searchqa-validation-13444", "mrqa_searchqa-validation-11517", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-2206", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-8657", "mrqa_searchqa-validation-727", "mrqa_searchqa-validation-4315", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12454", "mrqa_searchqa-validation-16917", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3663", "mrqa_hotpotqa-validation-4588", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-2967"], "SR": 0.65625, "CSR": 0.5738636363636364, "retrieved_ids": ["mrqa_squad-train-31542", "mrqa_squad-train-52297", "mrqa_squad-train-8928", "mrqa_squad-train-56670", "mrqa_squad-train-27590", "mrqa_squad-train-9841", "mrqa_squad-train-68581", "mrqa_squad-train-43796", "mrqa_squad-train-20614", "mrqa_squad-train-6443", "mrqa_squad-train-26167", "mrqa_squad-train-85438", "mrqa_squad-train-18320", "mrqa_squad-train-68536", "mrqa_squad-train-25737", "mrqa_squad-train-80019", "mrqa_triviaqa-validation-7452", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-5553", "mrqa_naturalquestions-validation-7844", "mrqa_hotpotqa-validation-1925", "mrqa_naturalquestions-validation-8958", "mrqa_triviaqa-validation-4059", "mrqa_searchqa-validation-16420", "mrqa_hotpotqa-validation-4418", "mrqa_searchqa-validation-416", "mrqa_naturalquestions-validation-3698", "mrqa_searchqa-validation-10094", "mrqa_naturalquestions-validation-10357", "mrqa_hotpotqa-validation-2526", "mrqa_searchqa-validation-11846", "mrqa_hotpotqa-validation-1065"], "EFR": 1.0, "Overall": 0.7489914772727273}, {"timecode": 88, "before_eval_results": {"predictions": ["Sputnik", "A Good Day to Die Hard", "malaysia hard rock", "Gorbachev", "Jerez de la Frontera", "Buncefield Depot", "royal court", "cable", "Westminster Abbey", "Cast", "A poker hand", "Hawaii", "World War II", "aromatherapy", "malaysia kelly", "Downton Abbey", "Bobby Darin", "France", "Montmorency", "Kent", "Cliff Thorburn", "chamonix", "three", "cymbals", "violin", "Ireland", "Venus", "beetles", "paralysis", "eight", "Japanese silvergrass", "swindon Town", "Bruce Fisher", "Happy Birthday to You", "Everton", "a bowl window", "mar", "Karamazov", "Michael Angelo Titmarsh", "Mud", "Dumbo", "Jimmy Knapp", "Check Me", "31536000", "7,926 miles", "John Galliano", "Bloodaxe", "russ seddon", "Chiricahua", "Enda Kenny", "Aug. 24", "in the very late 1980s", "Taylor Michel Momsen", "Malayalam", "2008", "Western Canada", "Dutch", "the peace with Israel", "Jiverly Wong,", "28", "Shirley Jackson", "Jumbo", "jellies", "Mike Mills"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6151041666666667}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-2791", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-4019", "mrqa_hotpotqa-validation-3566", "mrqa_searchqa-validation-9544", "mrqa_searchqa-validation-3464"], "SR": 0.578125, "CSR": 0.5739115168539326, "retrieved_ids": ["mrqa_squad-train-75958", "mrqa_squad-train-25292", "mrqa_squad-train-83908", "mrqa_squad-train-39922", "mrqa_squad-train-86086", "mrqa_squad-train-67633", "mrqa_squad-train-64126", "mrqa_squad-train-85243", "mrqa_squad-train-4881", "mrqa_squad-train-67477", "mrqa_squad-train-25474", "mrqa_squad-train-79651", "mrqa_squad-train-20844", "mrqa_squad-train-80974", "mrqa_squad-train-53950", "mrqa_squad-train-1047", "mrqa_newsqa-validation-904", "mrqa_hotpotqa-validation-823", "mrqa_searchqa-validation-4470", "mrqa_triviaqa-validation-4500", "mrqa_newsqa-validation-3889", "mrqa_searchqa-validation-4635", "mrqa_squad-validation-9434", "mrqa_hotpotqa-validation-426", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3457", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-14998", "mrqa_searchqa-validation-4524", "mrqa_hotpotqa-validation-47", "mrqa_triviaqa-validation-4573", "mrqa_searchqa-validation-661"], "EFR": 0.9629629629629629, "Overall": 0.7415936459633791}, {"timecode": 89, "before_eval_results": {"predictions": ["18", "A third beluga whale belonging to the world's largest aquarium has died", "Los Ticos", "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Aniston, Demi Moore and Alicia Keys", "Haiti,", "Rwanda", "\"Dance Your Ass Off.\"", "new materials", "Azzam", "7,000", "$41.1 million", "35,000 kilometers", "helping to plan the September 11, 2001, terror attacks,", "to provide security as needed.", "Ghana", "some dental work done,", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "fill a million sandbags and place 700,000 around our city,\"", "helping on the sandbag lines", "Immigration Minister Eric Besson", "since 1983.", "133", "promotes fuel economy and safety while boosting the economy.", "growing crowded,", "Jewish", "12.3 million", "The Ski Train", "two years", "Robert Park", "Piedad Cordoba,", "eight.\"", "Itawamba County School District", "Frank Ricci,", "1959,", "$249", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "1983", "At least 13", "The Valley Swim Club", "\"still trying to absorb the impact of this week's stunning events.\"", "near West Palm Beach, Florida,", "Darrel Mohler", "Sharon Bialek", "not", "John Dillinger,", "fifth", "New Year's Day", "the Stooges", "two", "Opryland", "Garfield Sobers", "R.E.M.", "at the Cow Palace, before they moved to their present home, the SAP Center at San Jose in 1993", "John Donne", "127 Hours", "Kiel Canal", "\"Slaughterhouse-Five\"", "1885", "Germanic", "hearsay", "a buffoon", "17th", "Esther"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6863590083162452}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-268", "mrqa_naturalquestions-validation-878", "mrqa_hotpotqa-validation-4986", "mrqa_searchqa-validation-5316"], "SR": 0.609375, "CSR": 0.5743055555555556, "retrieved_ids": ["mrqa_squad-train-27393", "mrqa_squad-train-61131", "mrqa_squad-train-7004", "mrqa_squad-train-10497", "mrqa_squad-train-47708", "mrqa_squad-train-77840", "mrqa_squad-train-72798", "mrqa_squad-train-12015", "mrqa_squad-train-55951", "mrqa_squad-train-23211", "mrqa_squad-train-81856", "mrqa_squad-train-34527", "mrqa_squad-train-80301", "mrqa_squad-train-76513", "mrqa_squad-train-49103", "mrqa_squad-train-27751", "mrqa_searchqa-validation-7328", "mrqa_naturalquestions-validation-8617", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-3454", "mrqa_triviaqa-validation-3266", "mrqa_searchqa-validation-3852", "mrqa_triviaqa-validation-3896", "mrqa_newsqa-validation-869", "mrqa_naturalquestions-validation-10565", "mrqa_newsqa-validation-1248", "mrqa_squad-validation-7827", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-5611", "mrqa_searchqa-validation-12226", "mrqa_hotpotqa-validation-2277", "mrqa_searchqa-validation-6437"], "EFR": 0.96, "Overall": 0.7410798611111111}, {"timecode": 90, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.89453125, "KG": 0.54296875, "before_eval_results": {"predictions": ["More than 15,000", "Jezebel.com's", "Gov. Mark Sanford", "Russian air force,", "Chevron", "one", "serving its fast burgers in the Carrousel du Louvre,", "Melbourne.", "severe", "opium", "Tuesday.", "acute stress disorder", "the fact that the teens were charged as adults.", "Wednesday,", "fastest time in circling the globe in a powerboat", "order", "Saturn", "London, Ontario,", "\"The Da Vinci Code,\"", "Nearly eight in 10", "the 66th annual Golden Globe Awards", "no motive has been determined for the killing, which took place in the poor neighborhood of Rione Sanita, where Camorra -- the name for organized crime in Naples", "Opry Mills,", "striker", "Gov. Jan Brewer.", "the oceans", "Columbia", "freedom of speech,", "anti- strikers", "Leo Frank,", "Ralph Lauren,", "the government.", "Islamabad", "\"A Child's Garden of Verses,\"", "September,", "of Olympia", "\"Avatar,\"", "Frank Ricci,", "U.S. Navy", "the Russian air force,", "why do genocides and mass atrocities happen.", "she wonders if part of the appeal of plus-sized", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "Climatecare,", "Jaipur", "Venezuela", "likening one American diplomat to a \"prostitute\"", "meeting with the president to discuss her son.", "Toffelmakaren.", "said she also believed police were trying to cover up the truth behind her daughter's murder,", "buckling under pressure from the ruling party.", "16 seasons", "12.9 - kilometre ( 8 mi )", "1939", "Afghanistan", "Agnolo", "60", "Nicolas Winding Refn", "Father Dougal McGuire", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "Alfalfa", "Jefferson", "The Shining", "Adolphe Adam"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7347098214285714}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.07142857142857142, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-2723", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-3262", "mrqa_searchqa-validation-15070"], "SR": 0.71875, "CSR": 0.5758928571428572, "retrieved_ids": ["mrqa_squad-train-74202", "mrqa_squad-train-34876", "mrqa_squad-train-5316", "mrqa_squad-train-84326", "mrqa_squad-train-52025", "mrqa_squad-train-81997", "mrqa_squad-train-34857", "mrqa_squad-train-71534", "mrqa_squad-train-30853", "mrqa_squad-train-55464", "mrqa_squad-train-73149", "mrqa_squad-train-16253", "mrqa_squad-train-46470", "mrqa_squad-train-63170", "mrqa_squad-train-83902", "mrqa_squad-train-10725", "mrqa_newsqa-validation-2796", "mrqa_searchqa-validation-7679", "mrqa_naturalquestions-validation-1838", "mrqa_newsqa-validation-101", "mrqa_hotpotqa-validation-4307", "mrqa_searchqa-validation-13583", "mrqa_naturalquestions-validation-9383", "mrqa_hotpotqa-validation-2262", "mrqa_naturalquestions-validation-10617", "mrqa_newsqa-validation-4092", "mrqa_naturalquestions-validation-941", "mrqa_newsqa-validation-2690", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-2127", "mrqa_newsqa-validation-3076"], "EFR": 0.9444444444444444, "Overall": 0.7509424603174603}, {"timecode": 91, "before_eval_results": {"predictions": ["British Airways", "Linus", "chestnut", "almond", "Mark Twain", "Oslo", "Peter Ustinov", "Hawaii", "glockenspiel", "george Orwell", "Goldtrail", "The Archers", "Ben Franklin", "Jack Nicholson", "photography", "single malt", "Taiwan", "Willem de Zwijger", "Oliver Stone", "Neil Armstrong and Edwin \" Buzz\" Aldrin", "state of Oregon", "your Excellency", "Nikola Tesla", "Thomas De Quincey", "Susie Dent", "Pancho Villa", "The Crusades", "Ivan Owen", "1919", "copper", "Pickwick", "Bluebell Girls", "Chillicothe", "swiss", "Ann Darrow", "blue", "Flying Pickets", "St Moritz", "swiss", "Vietnam", "1985", "Bogota", "united states", "James Murdoch", "Crystal Palace", "Belfast", "Elton John - Your", "Thermopylae", "Elton John", "Seattle", "Marshalsea", "card security code ( CSC ; also called card verification number ( CVD ), card verification value ( CVV )", "Jacques Cousteau", "at Tandi, in Lahaul", "94", "August 6, 1845 - October 6, 1931", "Ted", "24", "Tuesday", "\"Drugs not only poison people, but they poison economies and governments,", "Ham", "cardiac arrest", "the Hudson", "Subway"], "metric_results": {"EM": 0.65625, "QA-F1": 0.715515392985151}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.2580645161290323, 0.4444444444444445, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4026", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-6962", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-6887", "mrqa_hotpotqa-validation-5628", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-2175", "mrqa_searchqa-validation-13767", "mrqa_searchqa-validation-8329"], "SR": 0.65625, "CSR": 0.5767663043478262, "retrieved_ids": ["mrqa_squad-train-80771", "mrqa_squad-train-56154", "mrqa_squad-train-6181", "mrqa_squad-train-86472", "mrqa_squad-train-12241", "mrqa_squad-train-4045", "mrqa_squad-train-80376", "mrqa_squad-train-74158", "mrqa_squad-train-6033", "mrqa_squad-train-42462", "mrqa_squad-train-29293", "mrqa_squad-train-23105", "mrqa_squad-train-41641", "mrqa_squad-train-60233", "mrqa_squad-train-12610", "mrqa_squad-train-56255", "mrqa_searchqa-validation-14998", "mrqa_newsqa-validation-2983", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8582", "mrqa_squad-validation-9388", "mrqa_newsqa-validation-762", "mrqa_naturalquestions-validation-941", "mrqa_searchqa-validation-9361", "mrqa_newsqa-validation-2607", "mrqa_searchqa-validation-14374", "mrqa_newsqa-validation-4180", "mrqa_hotpotqa-validation-2042", "mrqa_triviaqa-validation-4059", "mrqa_hotpotqa-validation-5791", "mrqa_naturalquestions-validation-8845"], "EFR": 0.9545454545454546, "Overall": 0.7531373517786562}, {"timecode": 92, "before_eval_results": {"predictions": ["orangutans", "lowestoft", "Solomon", "new zealand", "morksyn", "hmsingway", "king john IV", "Godfather of Italian cooking", "NASA\\'s Hubble Space Telescope", "France", "james johnson", "a window", "hitler", "coffee house", "The little dog laugh'd to see such Craft,", "baseball cards", "johann doppler", "netherlands", "Neighbours", "kursk", "Jessica Simpson", "3-4-5-6", "blind beggar", "Mark Darcy", "one Thousand and One", "Denisovans", "alistair Darling", "Yuri Andropov", "surrey", "net Worth", "alabaptists", "Great Britain", "sanchez", "surrey", "downtown", "g Graeme Colquhoun, Scotland", "dice", "Saturn", "Sinclair Lewis", "the Fleet River", "alkh-Morpork", "james chadwick", "british", "1879", "a polecat", "table tennis", "buffalo", "alberich", "geomorphology", "marus", "Tina Turner", "com TLD", "abdicated", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Guangzhou, China", "October 29, 1985", "Jane Mayer", "a storm,", "a member of the band for more than 40 years", "American", "a Avanti", "Coors Field", "the Chrysler Building", "the USS \"Enterprises\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5162140376984127}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.125, 0.22222222222222218, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4018", "mrqa_triviaqa-validation-7714", "mrqa_triviaqa-validation-1959", "mrqa_triviaqa-validation-5157", "mrqa_triviaqa-validation-1504", "mrqa_triviaqa-validation-1795", "mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6329", "mrqa_triviaqa-validation-904", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-4741", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3737", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-2717", "mrqa_newsqa-validation-3990", "mrqa_searchqa-validation-6690", "mrqa_hotpotqa-validation-2871"], "SR": 0.484375, "CSR": 0.5757728494623655, "retrieved_ids": ["mrqa_squad-train-25699", "mrqa_squad-train-84942", "mrqa_squad-train-16907", "mrqa_squad-train-42406", "mrqa_squad-train-68279", "mrqa_squad-train-47178", "mrqa_squad-train-9530", "mrqa_squad-train-43567", "mrqa_squad-train-32007", "mrqa_squad-train-47069", "mrqa_squad-train-46186", "mrqa_squad-train-80273", "mrqa_squad-train-85663", "mrqa_squad-train-43062", "mrqa_squad-train-13925", "mrqa_squad-train-9087", "mrqa_newsqa-validation-2070", "mrqa_triviaqa-validation-6166", "mrqa_newsqa-validation-4124", "mrqa_naturalquestions-validation-4869", "mrqa_squad-validation-2419", "mrqa_searchqa-validation-3993", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-5295", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-4426", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-320", "mrqa_naturalquestions-validation-4960", "mrqa_triviaqa-validation-3870", "mrqa_searchqa-validation-8125", "mrqa_triviaqa-validation-6366"], "EFR": 0.9090909090909091, "Overall": 0.743847751710655}, {"timecode": 93, "before_eval_results": {"predictions": ["Robert Barnett,", "a public housing project,", "a hospital in Amstetten,", "Saudi Arabia", "Zimbabwean", "a one-shot victory", "progress,", "\"an accomplished pilot\"", "Susan Atkins,", "80,", "her daughter and granddaughter attend Oprah Winfrey's school in South Africa considers the talk-show host heaven-sent,", "Iraqi", "Passers-by", "4,000 credit cards and the company's \"private client\" list,", "the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "Former Mobile County Circuit Judge Herman Thomas", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "heavy flannel or wool", "Diego Milito's", "Kit of Elsinore", "The worst snowstorm to hit Britain", "potential revenues from oil and gas", "nearly $162 billion", "two", "finance", "Indonesian", "The Charlie Daniels Band,", "three out of four", "trading goods and services without exchanging money", "gasoline", "Thailand", "South Africa", "Harrison Ford", "Jacob,", "Cash for Clunkers", "100 meter", "Shenzhen in southern China.", "Cipro, Levaquin, Avelox, Noroxin and Floxin.", "Chinese and international laws", "the player", "the charter", "27,", "five", "Secretary of State", "The ACLU", "it was unjustifiable", "his father,", "\"The Sopranos,\"", "finance", "$60 billion", "managing his time.", "bachata music", "the U.S. state of Georgia", "currently a free agent", "jimmy smith", "bill bryson", "argument form", "1961", "Ariel Ram\u00edrez", "the \"Boston Herald\" Rumor Clinic", "the Yangtze River", "Ouija board", "the Andes", "season"], "metric_results": {"EM": 0.59375, "QA-F1": 0.713568722943723}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.8571428571428571, 0.5, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.09523809523809522, 1.0, 1.0, 0.2, 0.8666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.2857142857142857, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3233", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-4073", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4841", "mrqa_triviaqa-validation-3004", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-13769", "mrqa_triviaqa-validation-2535"], "SR": 0.59375, "CSR": 0.5759640957446808, "retrieved_ids": ["mrqa_squad-train-73439", "mrqa_squad-train-37947", "mrqa_squad-train-59440", "mrqa_squad-train-80505", "mrqa_squad-train-76301", "mrqa_squad-train-53307", "mrqa_squad-train-55871", "mrqa_squad-train-46168", "mrqa_squad-train-77960", "mrqa_squad-train-4838", "mrqa_squad-train-85782", "mrqa_squad-train-9491", "mrqa_squad-train-56863", "mrqa_squad-train-45413", "mrqa_squad-train-65087", "mrqa_squad-train-12547", "mrqa_naturalquestions-validation-9253", "mrqa_searchqa-validation-10259", "mrqa_squad-validation-2054", "mrqa_searchqa-validation-10306", "mrqa_searchqa-validation-9957", "mrqa_hotpotqa-validation-2646", "mrqa_searchqa-validation-7327", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-110", "mrqa_naturalquestions-validation-5791", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-1210", "mrqa_naturalquestions-validation-5538", "mrqa_hotpotqa-validation-4316", "mrqa_naturalquestions-validation-2893", "mrqa_searchqa-validation-14015"], "EFR": 0.9615384615384616, "Overall": 0.7543755114566284}, {"timecode": 94, "before_eval_results": {"predictions": ["Larry King", "Immigration Minister Eric Besson", "$500,000", "who is responsible for causing it and what should be done about it", "bankruptcies", "At least 38", "hooked up with Mildred, a younger woman of about 80, in March.", "Eleven", "41,", "McDonald's", "an organization that even opponents called brilliant.", "10.1,\"", "Democrats and Republicans", "Zac Efron", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "Sylt", "other women who couldn't or wouldn't.\"", "Congress", "Cash for Clunkers", "11th year in a row.", "nearly 2,000", "\"face of the peace initiative has been attacked.\"", "after Wood went missing off Catalina Island, near the California coast,", "grossed $55.7 million during its first frame,", "Tulsa, Oklahoma.", "almost 100", "Janet Napolitano", "56,", "There's no chance", "Barack Obama,", "and renewable energy at home everyday,\"", "83", "30,000", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "$56.2 million.", "of the Movement for Democratic Change,", "eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Orbiting Carbon Observatory,", "Caster Semenya", "Pakistan", "Kim", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Daryeel Bulasho Guud", "onto the college campus.", "a body", "Gaslight Theater.", "Caylee Anthony,", "in the neighboring country of Djibouti,", "18", "school,", "Hayden", "to collect menstrual flow", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Massachusetts", "greece", "Muhammad Ali", "jimmy allwine", "Croatan, Nantahala, and Uwharrie", "4,613", "Black Sabbath", "Raytheon", "The Lion King", "Radiohead", "Johnny Got His Gun"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6616390306122448}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.24489795918367346, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-955", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-1528", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-2341", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-644", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-147", "mrqa_hotpotqa-validation-3036"], "SR": 0.609375, "CSR": 0.5763157894736842, "retrieved_ids": ["mrqa_squad-train-43427", "mrqa_squad-train-70898", "mrqa_squad-train-33495", "mrqa_squad-train-72738", "mrqa_squad-train-37528", "mrqa_squad-train-56551", "mrqa_squad-train-51432", "mrqa_squad-train-45882", "mrqa_squad-train-32096", "mrqa_squad-train-55036", "mrqa_squad-train-13652", "mrqa_squad-train-57577", "mrqa_squad-train-53035", "mrqa_squad-train-53070", "mrqa_squad-train-12044", "mrqa_squad-train-42151", "mrqa_newsqa-validation-3899", "mrqa_searchqa-validation-8749", "mrqa_hotpotqa-validation-4047", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-3727", "mrqa_newsqa-validation-3765", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-1905", "mrqa_naturalquestions-validation-6771", "mrqa_hotpotqa-validation-4436", "mrqa_triviaqa-validation-6411", "mrqa_searchqa-validation-12744", "mrqa_newsqa-validation-3451", "mrqa_triviaqa-validation-4741", "mrqa_naturalquestions-validation-10604"], "EFR": 0.92, "Overall": 0.7461381578947368}, {"timecode": 95, "before_eval_results": {"predictions": ["Ford", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "around 3,000 - 5,000 program - erase cycles, but some flash drives have single - level cell ( SLC ) based memory that is good for around 100,000 writes", "a legal case in certain legal systems", "4WD", "October 6, 2017", "autopistas", "Bemis Heights", "Fusajiro Yamauchi", "1854", "Tim McGraw", "The Viscount Hardinge, then Governor General of India", "the Devastator", "Abraham Gottlob Werner", "A lacteal", "Fix You ''", "As late as the 1890s, building regulations in London did not require working - class housing to have indoor toilets ; into the early 20th century", "Britain - Hungary", "the House of Representatives", "architecture", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "the arms of Ireland", "a pop ballad", "2010", "2017", "Annette Strean", "Charles Path\u00e9", "endocytosis", "World War II", "dromedary", "2003", "Thomas Hobbes in his Leviathan, though with a somewhat different meaning ( similar to the meaning used by the British associationists )", "fall of 2015", "Rodney Crowell", "Spanish moss", "Guant\u00e1namo", "minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "961", "Percy Jackson series", "milling cutter", "The Cornett family", "Allison Janney", "around 2011", "Patrick Swayze", "McFerrin", "Ann Gillespie", "Lula", "2017", "March 15, 1945", "an Aldabra giant tortoise", "oliver goldsmith", "Brussels", "iron", "Seventeen", "Cartoon Network", "What You Will", "Kurt Cobain", "Madonna", "Brian Smith.", "Shakespeare", "Bob Hope", "three", "$40 and a loaf of bread."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6743716353091354}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.8181818181818181, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.45454545454545453, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-3309", "mrqa_triviaqa-validation-3684", "mrqa_newsqa-validation-1963", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-7262"], "SR": 0.5625, "CSR": 0.576171875, "retrieved_ids": ["mrqa_squad-train-67223", "mrqa_squad-train-38045", "mrqa_squad-train-59411", "mrqa_squad-train-3741", "mrqa_squad-train-41286", "mrqa_squad-train-81959", "mrqa_squad-train-71427", "mrqa_squad-train-45918", "mrqa_squad-train-73575", "mrqa_squad-train-29939", "mrqa_squad-train-47718", "mrqa_squad-train-70776", "mrqa_squad-train-55425", "mrqa_squad-train-61457", "mrqa_squad-train-27544", "mrqa_squad-train-28164", "mrqa_newsqa-validation-2237", "mrqa_naturalquestions-validation-4865", "mrqa_newsqa-validation-1483", "mrqa_naturalquestions-validation-3498", "mrqa_searchqa-validation-16767", "mrqa_newsqa-validation-1705", "mrqa_hotpotqa-validation-4097", "mrqa_newsqa-validation-3235", "mrqa_naturalquestions-validation-601", "mrqa_hotpotqa-validation-2717", "mrqa_triviaqa-validation-4151", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3527", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-5494", "mrqa_triviaqa-validation-2240"], "EFR": 0.9642857142857143, "Overall": 0.7549665178571429}, {"timecode": 96, "before_eval_results": {"predictions": ["Swamp Soccer", "stood 6 feet 6 inches,", "Another high tide", "April 22,", "\"Hawaii Five-O\"", "a number of calls, and those calls were intriguing, and we're chasing those down now,\"", "30", "U.S. senators", "The son of Gabon's former president", "Republican who favors smaller government,", "could be secretly working on a nuclear weapon", "the Philippine National Police.", "Islamabad", "the U.S. Holocaust Memorial Museum,", "bicycles", "hot and humid", "a hospital", "in Fayetteville, North Carolina,", "\"We're not going to forget you in Washington, D.C.\"", "Tom Hanks", "September,", "\"This is not something that anybody can reasonably anticipate,\"", "genocide", "any person who has been abused by any priest of the Diocese of Cloyne during my time as bishop or at any time,\"", "Barney Stinson,", "19-year-old", "Dr. Death in Germany", "collaborating with the Colombian government,", "the U.S. Holocaust Memorial Museum,", "sportswear", "fiber supplements, probiotics, antidepressants, behavioral-based therapies, psychotherapy, food modification, acupuncture, and laxatives.", "Sunday's", "Unseeded Frenchwoman Aravane Rezai", "researchers", "Five of us for the United States and two against us", "30-minute", "Grease", "in good spirits, especially comforted to be receiving care from talented doctors in a world-class hospital named in honor of her late husband,\"", "gunned down four Lakewood, Washington, police officers Sunday.", "the estate with its 18th-century sights, sounds, and scents.", "that students often know ahead of time when and where violence will flare up on campus.", "between 1917 and 1924", "some work rule issues.", "Kerstin Fritzl,", "his salary", "for his efforts to help male veterans struggling with homelessness and addiction.", "to clean up Washington State's decommissioned Hanford nuclear site,", "over 1,000 pounds", "3,000 kilometers (1,900 miles),", "Two UH-60 Blackhawk helicopters", "Marc Jacobs", "Lorazepam", "Coppolas and, technically, the Farrow / Previn / Allens", "To capitalize on her publicity", "ecclesiastical", "robert novello", "grasses", "\u00c6thelwald Moll", "The Bears", "Prince Nikolai Sergeyevich Trubetzkoy", "fraternities", "Charles Dana Gibson", "the Constitution", "jack-in-the-box"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6049522072086411}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.9600000000000001, 1.0, 0.4, 0.0, 0.18181818181818182, 1.0, 1.0, 0.5454545454545455, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.22222222222222224, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-2447", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-3543", "mrqa_hotpotqa-validation-5590", "mrqa_searchqa-validation-114", "mrqa_triviaqa-validation-7222"], "SR": 0.515625, "CSR": 0.5755476804123711, "retrieved_ids": ["mrqa_squad-train-82149", "mrqa_squad-train-59110", "mrqa_squad-train-68555", "mrqa_squad-train-37664", "mrqa_squad-train-16595", "mrqa_squad-train-52118", "mrqa_squad-train-8377", "mrqa_squad-train-66987", "mrqa_squad-train-15966", "mrqa_squad-train-20846", "mrqa_squad-train-11973", "mrqa_squad-train-23785", "mrqa_squad-train-77835", "mrqa_squad-train-23974", "mrqa_squad-train-51322", "mrqa_squad-train-57099", "mrqa_newsqa-validation-3678", "mrqa_naturalquestions-validation-2887", "mrqa_naturalquestions-validation-5444", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2519", "mrqa_naturalquestions-validation-3782", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3093", "mrqa_triviaqa-validation-7575", "mrqa_newsqa-validation-1860", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-9286", "mrqa_naturalquestions-validation-922", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-5545", "mrqa_searchqa-validation-2301"], "EFR": 0.8064516129032258, "Overall": 0.7232748586631195}, {"timecode": 97, "before_eval_results": {"predictions": ["Michael Schumacher", "Andy Murray", "Six", "girls", "David Bowie,", "1957,", "behind the counter.", "'We want to reset our relationship and so we will do it together.'\"", "heavy brush,", "1959.", "Phoenix, Arizona,", "\"project work\"", "Buddhism", "40", "\"and I am not confident to what degree our sincerity", "one of Africa's most stable nations.", "Molotov cocktails, rocks and glass.", "Alfredo Astiz,", "on the 11th anniversary of the September 11, 2001,", "the northeastern Iranian city of Mashhad", "wars in Iraq and Afghanistan", "$106.5 million", "Obama should have met with the Dalai Lama.", "Matthew Fisher,", "autonomy.", "head", "control and censorship", "Guinea, Myanmar, Sudan and Venezuela.", "Brazil's", "Uzbekistan.", "45 minutes, five days a week.", "urged NATO to take a more active role in countering the spread of the", "took on water", "bodies of four people believed to be illegal immigrants", "in July", "Visitors aren't allowed", "an auxiliary lock", "Diego Milito's", "a music video", "Robert Mugabe", "gang rape", "fatally shot her brother in the family's Braintree, Massachusetts,", "threatening messages", "Mexico", "more than 1.2 million", "Sunday.", "tennis", "Osama bin Laden's sons", "Chesley \"Sully\" Sullenberger", "Gary Player,", "Karen Floyd", "Asuka", "Authority", "Prince William, Duke of Cambridge", "Astor family", "Gryffindor", "Rowan Atkinson", "841", "Adelaide", "Genderqueer", "Windsor Castle", "Tad Hamilton", "Israel", "Bill Irwin"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7413961038961039}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1362", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-1775", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-2337", "mrqa_triviaqa-validation-5311"], "SR": 0.65625, "CSR": 0.5763711734693877, "retrieved_ids": ["mrqa_squad-train-46799", "mrqa_squad-train-5482", "mrqa_squad-train-32417", "mrqa_squad-train-38415", "mrqa_squad-train-1920", "mrqa_squad-train-6118", "mrqa_squad-train-43175", "mrqa_squad-train-66671", "mrqa_squad-train-20658", "mrqa_squad-train-21924", "mrqa_squad-train-23013", "mrqa_squad-train-73273", "mrqa_squad-train-45847", "mrqa_squad-train-76940", "mrqa_squad-train-80002", "mrqa_squad-train-16799", "mrqa_newsqa-validation-655", "mrqa_searchqa-validation-6286", "mrqa_hotpotqa-validation-3842", "mrqa_triviaqa-validation-2480", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-377", "mrqa_hotpotqa-validation-1872", "mrqa_naturalquestions-validation-998", "mrqa_searchqa-validation-16205", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-7920", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-993"], "EFR": 0.9545454545454546, "Overall": 0.7530583256029685}, {"timecode": 98, "before_eval_results": {"predictions": ["Cambodian officials", "monarchy's", "Public Citizen asked the FDA in August 2006 to put the \"black box\" warning on Cipro and other fluoroquinolones,", "AbdulMutallab", "partially submerged in a stream in Shark River Park in Monmouth County", "consumer confidence", "anesthetic", "Madonna", "Wigan Athletic", "Iran's parliament speaker", "18", "2050,", "U.S. State Department and British Foreign Office", "Janet Napolitano", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "make sure water continues flow through the river channel and not spread out over land.", "likely to top $60 million", "unable to pass significant restrictions on war funding", "Islamabad", "the 3rd District of Utah.", "75.", "Nineteen", "\"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "nearly $2 billion", "Gulf of Aden", "Al-Shabaab,", "February 12", "Kenneth Cole", "T.I.", "heavy turbulence", "killing of a 15-year-old boy", "to sniff out cell phones.", "California, Texas and Florida,", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "\"He focuses on Ozzy,", "Revolutionary Armed Forces of Colombia, better known as FARC,", "suicides", "Jaime Andrade", "Aung San Suu Kyi", "$81,88010.", "Patrick McGoohan,", "\"GoldenEye\"", "J.Crew,", "Dublin.", "1995", "Chester Arthur Stiles,", "Idriss Deby hopes the journalists and the flight crew", "a skilled hacker", "$7.8 million", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "remains committed to British sovereignty", "Lee Thompson Young", "September 2017", "Norman occupational surname ( meaning tailor ) in France", "Monopoly", "cogito ergo sum", "congenital hydrocephalus", "India Today", "Eliot Cutler", "7 January 1936", "Bran Mak Morn", "Istanbul", "the Washington Redskins", "Mrs. Miniver"], "metric_results": {"EM": 0.625, "QA-F1": 0.7323882470424947}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 0.25, 0.6923076923076924, 1.0, 0.9, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 0.9565217391304348, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285714, 1.0, 1.0, 1.0, 0.21052631578947364, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-3888", "mrqa_naturalquestions-validation-8858", "mrqa_triviaqa-validation-3864", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-6956"], "SR": 0.625, "CSR": 0.5768623737373737, "retrieved_ids": ["mrqa_squad-train-77944", "mrqa_squad-train-17080", "mrqa_squad-train-30903", "mrqa_squad-train-19897", "mrqa_squad-train-70707", "mrqa_squad-train-17211", "mrqa_squad-train-15198", "mrqa_squad-train-33253", "mrqa_squad-train-10214", "mrqa_squad-train-19626", "mrqa_squad-train-75015", "mrqa_squad-train-3554", "mrqa_squad-train-25131", "mrqa_squad-train-50050", "mrqa_squad-train-46042", "mrqa_squad-train-69194", "mrqa_naturalquestions-validation-2571", "mrqa_searchqa-validation-14989", "mrqa_searchqa-validation-12385", "mrqa_squad-validation-6973", "mrqa_searchqa-validation-10525", "mrqa_newsqa-validation-4179", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-1444", "mrqa_searchqa-validation-8129", "mrqa_newsqa-validation-3661", "mrqa_hotpotqa-validation-1313", "mrqa_naturalquestions-validation-6853", "mrqa_hotpotqa-validation-182", "mrqa_triviaqa-validation-4569", "mrqa_hotpotqa-validation-4511"], "EFR": 0.875, "Overall": 0.7372474747474748}, {"timecode": 99, "UKR": 0.796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2871", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1961", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-739", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-5339", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-904"], "OKR": 0.84375, "KG": 0.50625, "before_eval_results": {"predictions": ["bacteria", "1648 - 51", "England", "Darren McGavin", "certified question or proposition of law", "Lana Del Rey", "The pour point", "Madison", "May 29, 2018", "Dan Stevens", "AD 95 -- 110", "Bulgaria", "lithium", "1986", "2018", "U + 2234", "July 8, 1998", "September 2017", "Clarence Anglin", "Ariana Clarice Richards", "James Intveld", "Darlene Cates", "one", "mashed potato", "2018", "in the dress shop", "Eric Clapton", "New Jersey Devils", "July 2014", "Universal Pictures", "Cyanea capillata", "supported modern programming practices and enabled business applications to be developed with Flash", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "Lee County, Florida", "extremely slowly in the absence of a catalyst", "Theodosius I", "Christopher Allen Lloyd", "reared", "Mark Jackson", "Fall 1998", "TLC", "Inequality of opportunity was higher", "1908", "Abid Ali Neemuchwala", "Bhupendranath Dutt", "May 19, 2017", "the people of France", "two to three barrel vaults", "through Brazil, Bolivia, Paraguay and Argentina", "Gustav Bauer", "either in front or on top of the brainstem", "Leicestershire", "The curse is come upon me", "15", "Mike Biden", "Soviet Union", "Heinkel Flugzeugwerke", "she returned to Pakistan", "Zac Efron", "Aryan Airlines Flight 1625", "Ocean\\'s Twelve", "Trajan", "Anne Rice", "Wilkie Collins"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7518005432067931}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.5454545454545454, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.5, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.07692307692307691, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-10706", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-1395", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-5094", "mrqa_hotpotqa-validation-5509", "mrqa_newsqa-validation-846"], "SR": 0.640625, "CSR": 0.5775, "retrieved_ids": ["mrqa_squad-train-28198", "mrqa_squad-train-85773", "mrqa_squad-train-12364", "mrqa_squad-train-61378", "mrqa_squad-train-52132", "mrqa_squad-train-81924", "mrqa_squad-train-55726", "mrqa_squad-train-8945", "mrqa_squad-train-15629", "mrqa_squad-train-45261", "mrqa_squad-train-75667", "mrqa_squad-train-21577", "mrqa_squad-train-59040", "mrqa_squad-train-71737", "mrqa_squad-train-8679", "mrqa_squad-train-36082", "mrqa_hotpotqa-validation-3869", "mrqa_naturalquestions-validation-10719", "mrqa_newsqa-validation-2237", "mrqa_triviaqa-validation-1375", "mrqa_hotpotqa-validation-3692", "mrqa_searchqa-validation-3773", "mrqa_hotpotqa-validation-4720", "mrqa_newsqa-validation-1114", "mrqa_triviaqa-validation-4729", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1275", "mrqa_triviaqa-validation-4304", "mrqa_newsqa-validation-3229", "mrqa_searchqa-validation-15960", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-4634"], "EFR": 0.8260869565217391, "Overall": 0.7100923913043478}]}