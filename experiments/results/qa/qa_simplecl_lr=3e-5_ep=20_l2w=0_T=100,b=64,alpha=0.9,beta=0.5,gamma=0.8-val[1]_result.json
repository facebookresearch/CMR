{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=50, kg_eval_mode='metric', kr_eval_freq=50, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=100, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=20_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 8400, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["the Cobham\u2013Edmonds thesis", "15 February 1546", "special efforts", "17", "southwestern France", "CBS Sports", "different viewpoints and political parties", "Thomas Commerford Martin", "24 August \u2013 3 October 1572", "long, slender tentacles", "45 minutes", "Town Moor", "BBC HD", "Ealy", "August 15, 1971", "a squared integer", "declared Japan a \"nonfriendly\" country", "a cubic interpolation formula", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "1852", "an intuitive understanding", "the Small Catechism", "learning of the execution of Johann Esch and Heinrich Voes", "Super Bowl XLVII", "Ozone depletion and global warming", "widespread education", "chloroplasts", "Warraghiggey", "The Scotland Act 1998", "The Bachelor", "delivery of these messages by store and forward switching", "9000 BP", "criminal investigations", "2002", "sculptures, friezes and tombs", "Sonderungsverbot", "The Simpsons", "826", "English", "energize electrons", "Catholicism", "Robert R. Gilruth", "He prayed, consulted friends, and gave his response the next day", "young men who had not fought", "Manakin Town", "tidal delta", "A Charlie Brown Christmas", "formal", "Establishing \"natural borders\"", "(sworn brother or blood brother)", "Tyneside's shipbuilding heritage, and inventions which changed the world", "structural collapse, cost overruns, and/or litigation", "severely reduced rainfall and increased temperatures", "sponges", "Cam Newton", "science fiction", "Sonia Shankman Orthogenic School", "an aided or an unaided school", "steam turbine plant", "metamorphic processes", "faith", "article 49", "the meeting of the Church's General Assembly", "missing self"], "metric_results": {"EM": 0.765625, "QA-F1": 0.781423611111111}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_squad-validation-2974", "mrqa_squad-validation-1763", "mrqa_squad-validation-4621", "mrqa_squad-validation-2394", "mrqa_squad-validation-8719", "mrqa_squad-validation-8896", "mrqa_squad-validation-5773", "mrqa_squad-validation-5812", "mrqa_squad-validation-2113", "mrqa_squad-validation-5676", "mrqa_squad-validation-5226", "mrqa_squad-validation-337", "mrqa_squad-validation-1662", "mrqa_squad-validation-6947"], "SR": 0.765625, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 1, "before_eval_results": {"predictions": ["The Adventures of Ozzie and Harriet", "The Open Championship golf and The Wimbledon tennis tournaments", "32.9%", "365.2425 days of the year", "health care", "the 1970s", "Sunni Arabs from Iraq and Syria", "P,NP-complete, orNP-intermediate", "Thomas Murphy", "the highest terrace", "major national and international patient information projects and health system interoperability goals", "three", "net force", "12 January", "1976\u201377", "Cleveland, Phoenix, Detroit and Denver", "zoning and building code requirements", "river Deabolis", "1968", "King George III", "Baden-W\u00fcrttemberg", "lines or a punishment essay", "The Book of Discipline", "complicated definitions", "coordinating lead author", "TFEU article 294", "G. H. Hardy", "30-second", "Royal Ujazd\u00f3w Castle", "Church and the Methodist-Christian theological tradition", "the main hall", "the Teaching Council", "One could wish that Luther had died before ever [On the Jews and Their Lies] was written", "Russell T Davies", "Cape Town", "Gospi\u0107, Austrian Empire", "Classic FM's Hall of Fame", "optimisation of a drug treatment for an individual", "2014", "late 1970s", "30% less steam", "1983", "Happy Days", "1,230 kilometres", "23 November 1963", "Apollo 20", "six divisions", "scoil phr\u00edobh\u00e1ideach", "business", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Saul Bellow, political philosopher, literary critic and author of the New York Times bestseller \"The Closing of the American Mind\" Allan Bloom", "1991", "organisms", "41", "carbon", "the fertile highlands", "harder", "50% to 60%", "Norman Greenbaum", "appellate courts are also called appeals courts, courts of appeals, superior courts, or supreme courts", "The Prisoners ( Temporary Discharge for Ill Health ) Act, commonly referred to as the Cat and Mouse Act, was an Act of Parliament passed in Britain under Herbert Henry Asquith's Liberal government in 1913", "Mrs. Wolowitz", "Daenerys Targaryen", "Raabta"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8028687280399117}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5, 0.0, 0.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.06451612903225806, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6323", "mrqa_squad-validation-9752", "mrqa_squad-validation-1791", "mrqa_squad-validation-5952", "mrqa_squad-validation-6388", "mrqa_squad-validation-6059", "mrqa_squad-validation-8616", "mrqa_squad-validation-2611", "mrqa_squad-validation-1906", "mrqa_squad-validation-8035", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-7859", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-7792", "mrqa_hotpotqa-validation-1006"], "SR": 0.765625, "CSR": 0.765625, "EFR": 0.9333333333333333, "Overall": 0.8494791666666667}, {"timecode": 2, "before_eval_results": {"predictions": ["235", "P", "\"Smith and Jones\"", "1767", "53,000", "Fu\u00dfach", "leptin, pituitary growth hormone, and prolactin", "beat so that the propulsion stroke is away from the mouth, although they can also reverse direction", "7 West 66th Street", "patent archives", "Any member", "4-week period", "six", "Katharina", "Colorado Desert", "John Pell, Lord of Pelham Manor", "United States", "2014", "Alberto Calder\u00f3n", "Roger Goodell", "1950s", "1980s", "Cologne, Germany", "second use of the law", "free", "1973", "September 1969", "Mansfeld", "Warsaw Stock Exchange", "390 billion individual trees divided into 16,000 species", "a suite of network protocols", "eighteenth century", "journal Nature", "2009", "Franz Pieper", "geochemical evolution of rock units", "three times", "rhetoric", "Genoese traders", "the flail of God", "Saudi Arabia and Iran", "149,025", "1898", "Lunar Module Pilot", "citizenship", "Merritt Island", "accountants", "severed all relations with his family to hide the fact that he dropped out of school", "June 4, 2014", "kinetic friction force", "\u2153 to Tesla", "signal amplification", "Lituya Bay in Alaska", "120 m ( 390 ft )", "Game of Throne", "100 members", "photoelectric", "Welch, West Virginia", "Indian National Congress", "twelve Wimpy Kid books", "Hal David and Burt Bacharach", "six points", "Merrimac", "Spain"], "metric_results": {"EM": 0.71875, "QA-F1": 0.787724883623321}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.4, 1.0, 0.1875, 0.38095238095238093, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1759", "mrqa_squad-validation-4731", "mrqa_squad-validation-5972", "mrqa_squad-validation-2689", "mrqa_squad-validation-9173", "mrqa_squad-validation-5788", "mrqa_squad-validation-4415", "mrqa_squad-validation-4673", "mrqa_squad-validation-1841", "mrqa_squad-validation-1146", "mrqa_squad-validation-1220", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-6125", "mrqa_searchqa-validation-3996"], "SR": 0.71875, "CSR": 0.75, "EFR": 0.9444444444444444, "Overall": 0.8472222222222222}, {"timecode": 3, "before_eval_results": {"predictions": ["immunosuppressive", "William of Volpiano and John of Ravenna", "April 1523", "Excellent job opportunities", "rebellion is much more destructive; therefore, the defects justifying rebellion must be much more serious than those justifying disobedience", "the principle of inclusions and components", "they were accepted and allowed to worship freely", "12 December 2007", "six", "redistributive taxation", "rubisco", "recalled and replaced by Jeffery Amherst", "Egypt", "algae", "245,306", "the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "chromoplasts and amyloplasts", "spy network and Yam route systems", "Stairs", "genetically modified plants", "around 300,000", "three", "Von Miller", "Africa", "clinical services that pharmacists can provide for their patients", "Raghuram Rajan", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "Bruno Mars", "the Calvin cycle", "their Annual Conference", "Philo of Byzantium", "the mayor (the President of Warsaw), who may sign them into law", "cloud storage service", "Doritos", "Warsaw University of Technology building", "the Great Yuan", "Lenin", "the Solim\u00f5es Basin", "Charles Darwin", "23 November", "oppidum Ubiorum (\"town of the Ubii\")", "John Elway", "Downtown Riverside", "Capital Cities Communications", "lamprey and hagfish", "physicians and other healthcare professionals", "the Golden Gate Bridge", "Michael Schumacher", "10.5 %", "The Man", "President Gerald Ford", "Jane Fonda", "Janie Crawford", "it extends from the optic disc to the optic chiasma and continues as the optic tract to the lateral geniculate nucleus, pretectal nuclei, and superior colliculus", "Jerry Ekandjo", "961", "in awe of Novalee, and had seen her enter the store at closing time, smashes through the window to help deliver her child", "September 1973", "the land itself, while blessed, did not cause mortals to live forever", "the middle of the 15th century", "6 March 1983", "James G. Kiernan", "horror fiction", "26,000"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7909939132425545}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 0.13333333333333333, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6798", "mrqa_squad-validation-4108", "mrqa_squad-validation-8830", "mrqa_squad-validation-10293", "mrqa_squad-validation-4759", "mrqa_squad-validation-8763", "mrqa_squad-validation-6154", "mrqa_squad-validation-298", "mrqa_squad-validation-6614", "mrqa_squad-validation-962", "mrqa_squad-validation-9298", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-4433"], "SR": 0.6875, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 4, "before_eval_results": {"predictions": ["infrequent rain", "the king of France", "approximately 80 avulsions", "15", "Fort Le Boeuf", "wireless", "Beyonc\u00e9 and Bruno Mars", "Yuan", "same-gender marriages with resolutions", "red algae red", "after their second year", "1960s", "Yameester van Maastricht", "Napoleon", "Immunology", "geophysical surveys", "topographic gradients", "130 million cubic foot (3.7 million cubic meter)", "the 50 fund", "force, stating that British colonists would not be safe as long as the French were present", "ctenophores and cnidarians", "motivated students", "Michael Mullett", "15", "James Gamble & Reuben Townroe", "power, famine, and bitterness among the populace", "the Establishment Clause of the First Amendment or individual state Blaine Amendments", "\"Turks\" (Muslims) and Catholics", "six", "Big Ten Conference", "Thames River", "NDS", "shipping toxic waste", "anarchists", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "immunoglobulins and T cell receptors", "previously separated specialties", "their parent thylakoid", "motorway underpass without pedestrian access", "to protect their tribal lands from commercial interests", "religious beliefs", "force on a guilty plea", "YamI Notes - Gvsu  All of these older editions have the same pagination, and all are out of print", "Yamchand Gandhi", "Vlad the Impaler", "The Little Foxes", "the 1982 Sony SL-2000 portable", "Leonard Nimoy", "Yam + MP", "Tiger Woods", "1867 to 1877", "Marshall Dillon", "Yam/Who Do You", "Seminyak Beach", "Mary F. Kennedy", "LASER abbreviation", "Hans Christian Andersen", "Yamahuah Reigns", "a genus (scientific group) called Sphagnum", "Yam perch", "Andrew Taggart", "Yamchophobia- Fear of scratches or being scratched", "American", "Enrique Torres"], "metric_results": {"EM": 0.5, "QA-F1": 0.5813244047619048}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9357", "mrqa_squad-validation-10204", "mrqa_squad-validation-110", "mrqa_squad-validation-8840", "mrqa_squad-validation-4461", "mrqa_squad-validation-3703", "mrqa_squad-validation-10186", "mrqa_squad-validation-1960", "mrqa_squad-validation-8131", "mrqa_squad-validation-7088", "mrqa_squad-validation-2804", "mrqa_squad-validation-8767", "mrqa_squad-validation-5214", "mrqa_squad-validation-6721", "mrqa_searchqa-validation-12428", "mrqa_searchqa-validation-14338", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-12311", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-10360", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12931", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-6541", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-16377", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-11224", "mrqa_naturalquestions-validation-124", "mrqa_triviaqa-validation-6073", "mrqa_newsqa-validation-496"], "SR": 0.5, "CSR": 0.6875, "EFR": 0.96875, "Overall": 0.828125}, {"timecode": 5, "before_eval_results": {"predictions": ["bacteriophage T4", "6.7", "second-largest", "time and space", "the Meuse", "Western Union superintendent", "Super Bowl XLIV", "1891", "New Orleans", "fell from his horse while hunting", "the member state cannot enforce conflicting laws", "British bacteriologist J. F. D. Shrewsbury", "a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals", "inversely to member state size", "Europe", "he arrived too late", "colonies", "$37.6 billion", "Kenyan athletes", "1269", "the 17th century", "Time Warner Cable", "toward the Atlantic", "economic", "CrossCountry", "ITV", "SAP Center in San Jose", "lymphocytes-derived molecule", "the Edict of Fontainebleau", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California", "ten million people", "the Lippe", "Video On Demand content", "time and storage", "semester calendar beginning in early September and ending in mid-May", "the courts of member states and the Court of Justice of the European Union", "Thomas Edison", "1971", "quantum mechanics", "The Jewish Star", "the League of the Three Emperors", "the field of science", "143,007", "the National Intelligence Council (NIC)", "Waltham Abbey", "Secretariat", "coaxial", "Mary Harron", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Thomas Christopher Ince", "American Chopper", "drawing the name out of a hat", "German", "Fort Valley, Georgia", "American", "Easy", "Belvoir", "Congo River", "Abigail", "Murwillumbah, New South Wales, Australia", "Br'er Rabbit", "corruption", "24 hours", "Dover Beach"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8270517676767677}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.9523809523809523, 0.8, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1775", "mrqa_squad-validation-6218", "mrqa_squad-validation-4210", "mrqa_squad-validation-1187", "mrqa_squad-validation-457", "mrqa_squad-validation-6676", "mrqa_squad-validation-12", "mrqa_squad-validation-9753", "mrqa_squad-validation-1672", "mrqa_squad-validation-7214", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-4573", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2315", "mrqa_triviaqa-validation-1616", "mrqa_searchqa-validation-14229"], "SR": 0.71875, "CSR": 0.6927083333333333, "EFR": 0.9444444444444444, "Overall": 0.8185763888888888}, {"timecode": 6, "before_eval_results": {"predictions": ["the 1540s", "the Court of Justice of the European Union", "its circle logo", "three", "negative long-term impact", "fear of their lives", "80%", "1521", "Gibraltar and the \u00c5land islands", "starch buildup in the chloroplasts, possibly due to less sucrose being exported out of the chloroplast (or more accurately, the plant cell)", "exceeds any given number", "Hulagu Khan", "poet", "quality rental units", "Grover Cleveland", "to overthrow a government (or to change cultural traditions, social customs, religious beliefs, etc...revolution doesn't have to be political", "entertainment", "vote clerk", "high growth rates", "vicious and destructive", "Sony", "Stagecoach", "Silk Road", "San Diego", "Central Poland", "four public charter schools on the South Side of Chicago", "invest in new sources of creating wealth or to otherwise leverage the accumulation of wealth", "Spanish", "Structural geologists", "president and CEO of ABC", "indulgences for the living", "BSkyB", "terrorist organisation", "Cam Newton", "The U2 360\u00b0 Tour", "The 5 foot 9 inch tall twins", "James Victor Chesnutt", "Benjamin Burwell Johnston", "Sinclair Oil Corporation", "Taylor Swift", "Eric Edward Whitacre", "Joint Chiefs of Staff", "Linux Format", "Jasenovac concentration camp", "Rabat", "11 or 13 and 18", "Heather Elizabeth Langenkamp", "Henry Gwyn Jeffreys Moseley", "paracyclist", "Vilnius Airport (IATA: VNO, ICAO: EYVI)", "Bury St Edmunds, Suffolk, England", "Charmed", "Jas (Jasmine) Salford", "Liverpool and England international player", "the Philadelphia Eagles", "Rickie Lee Skaggs", "48,982", "the Ashanti Region", "25.2 % ( 79 out of 313 )", "Algeria", "a novel", "the Eastern part", "Polar Bear", "The Atlantic City Boardwalk"], "metric_results": {"EM": 0.515625, "QA-F1": 0.7070354278074866}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.2, 0.8, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9411764705882353, 0.9333333333333333, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4, 0.4, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.9090909090909091, 0.8, 0.6666666666666666, 1.0, 0.8, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3939", "mrqa_squad-validation-5774", "mrqa_squad-validation-5213", "mrqa_squad-validation-8914", "mrqa_squad-validation-6788", "mrqa_squad-validation-6029", "mrqa_squad-validation-913", "mrqa_squad-validation-7983", "mrqa_squad-validation-7543", "mrqa_squad-validation-5651", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1291", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5300", "mrqa_naturalquestions-validation-2159", "mrqa_newsqa-validation-3377", "mrqa_searchqa-validation-5279", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-13072"], "SR": 0.515625, "CSR": 0.6674107142857143, "EFR": 1.0, "Overall": 0.8337053571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["IgG", "Amazoneregenwoud", "co-NP", "BBC Radio Newcastle", "England, Wales, Scotland, Denmark, Sweden, Switzerland, the Dutch Republic", "the working fluid", "suite of network protocols created by Digital Equipment Corporation", "American Baptist Education Society", "Dutch", "the solution", "means to invest in new sources of creating wealth or to otherwise leverage the accumulation of wealth", "the center of mass", "attention-seeking and disruptive students", "more than $45,000", "Defensive ends", "MLB", "papacy", "through homologous recombination", "a modern canalized section", "in protest against the occupation of Prussia by Napoleon in 1806-07", "improved markedly", "nearly visible along the entire length of the lake", "computer programs", "General Conference of the United Methodist Church", "1996", "dreams", "The Judiciary", "a deterministic Turing machine", "Bart Starr", "allotrope", "Karluk Kara-Khanid ruler", "Perth", "Ian Rush", "Gerry Adams", "New Orleans Saints", "1974", "four", "Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Alfred Edward Housman", "the capital of the Socialist Republic of Vietnam", "Sevens", "Fennec fox or fennec (\"Vulpes zerda\")", "Bart Conner", "fantasy role-playing game", "Martin McCann", "Black Mountain College", "a historic house museum", "Bothtec", "Cody Miller", "140 to 219", "John Locke", "Christophe Lourdelet", "Pablo Escobar", "African descent", "Teotihuacan", "an orally transmitted version of the originally literary tale published by Charles Perrault in \"Histoires ou contes du temps pass\u00e9\"", "Disneyland", "1985", "Noddy", "an successor to President Omar Bongo", "Wheat Chex", "Ray Harroun", "Emily Blunt", "David Tennant"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6736156204906205}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.20000000000000004, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3019", "mrqa_squad-validation-7547", "mrqa_squad-validation-3091", "mrqa_squad-validation-9287", "mrqa_squad-validation-1819", "mrqa_hotpotqa-validation-1898", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-398", "mrqa_hotpotqa-validation-3413", "mrqa_hotpotqa-validation-919", "mrqa_hotpotqa-validation-4405", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-3885", "mrqa_naturalquestions-validation-4388", "mrqa_newsqa-validation-3925", "mrqa_searchqa-validation-15869", "mrqa_naturalquestions-validation-1618"], "SR": 0.5625, "CSR": 0.654296875, "EFR": 1.0, "Overall": 0.8271484375}, {"timecode": 8, "before_eval_results": {"predictions": ["Russian", "cellular respiration", "railroad", "Revolutionary civil disobedience", "the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point", "Lunar Excursion Module", "Zwickau prophets", "six years", "700", "the 5th Avenue laboratory fire of March 1895", "arms", "two independent mechanisms", "minor", "Fringe or splinter movements", "17", "lower temperatures", "architect or engineer", "1917", "Columbus Avenue and West 66th Street", "teachers through the web in order to earn supplemental income", "stratigraphic correlation", "commensal flora", "a + bi", "Dallas, Texas", "Central Asian Muslims", "from home viewers who made tape recordings of the show", "1330 Avenue of the Americas in Manhattan", "Alberta and British Columbia", "Pimp My Ride", "Don Johnson", "Section.80", "25 million", "8,515", "13 October 1958", "jet-powered tailless delta wing high-altitude strategic bomber", "Environmental Protection Agency", "between 1932 and 1934", "an English professional footballer", "Los Angeles", "England", "Armin Meiwes", "Jean- Marc Vall\u00e9e", "Miss Universe 2010", "Dusty Dvoracek", "boxer", "Boston University", "Fulham", "A55", "Hugh de Kevelioc", "\u00c6thelstan", "Madras Export Processing Zone", "44", "Division I", "Harriet Tubman", "Manchester United", "Dragon TV", "Greek-American", "A diastema ( plural diastemata )", "Alison Krauss", "Iran", "Bigfoot", "Papua New Guinea", "Renoir", "Manchester"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7933666300097751}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6451612903225806, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3391", "mrqa_squad-validation-3405", "mrqa_squad-validation-2238", "mrqa_squad-validation-9859", "mrqa_squad-validation-7643", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1633", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-305"], "SR": 0.71875, "CSR": 0.6614583333333333, "EFR": 1.0, "Overall": 0.8307291666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["$159 million", "centrifugal governor", "Orange County", "chloroplast peripheral reticulum", "1962", "the European Court of Justice held that a Commissioner giving her dentist a job, for which he was clearly unqualified, did in fact not break any law", "Rugby", "Germany", "politically and socially unstable", "Theatre Museum", "90\u00b0", "iTunes", "its unpaired electrons", "Louis XIV", "Museum of the Moving Image in London", "he sent missionaries, backed by a fund to financially reward converts to Catholicism", "pyrenoid and thylakoids", "Woodward Park", "force and violence and refusal to submit to arrest", "25 May 1521", "essentially holy people", "diplomacy or military force", "an increase in the land available for cultivation", "the value of the spin", "pivotal event", "one of the youngest publicly documented people to be identified as transgender", "Trent Alexander-Arnold", "David Michael Bautista Jr.", "Black Friday", "American actor", "Prince Amedeo", "Lambic", "Mazatl\u00e1n", "Assistant Director Neil J. Welch", "March 30, 2025", "England", "Kentucky, Virginia, and Tennessee", "Autopia", "Yasir Hussain", "USC Marshall School of Business", "Stephen Ireland", "Marko Tapani \" Marco\" Hietala", "Estadio de L\u00f3pez Cort\u00e1zar", "Kohlberg K Travis Roberts", "Fort Albany", "I'm Shipping Up to Boston", "2500 ft", "Central Park", "Robert John Day", "Afroasiatic", "James Tinling", "Italy", "the PGA Tour", "Kristoffer Rygg", "University of Kentucky College of Pharmacy", "William Shakespeare", "Bob Dylan", "Erika Mitchell Leonard", "Santiago", "couscous", "22 million", "morphine sulfate oral solution 20 mg/ml", "Harvard Law", "Clarias"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6899203707819694}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333326, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7173", "mrqa_squad-validation-4147", "mrqa_squad-validation-7674", "mrqa_squad-validation-3130", "mrqa_squad-validation-8651", "mrqa_squad-validation-4572", "mrqa_squad-validation-6797", "mrqa_squad-validation-9735", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-3553", "mrqa_hotpotqa-validation-1500", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-10208", "mrqa_newsqa-validation-1668", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-3622"], "SR": 0.609375, "CSR": 0.65625, "EFR": 1.0, "Overall": 0.828125}, {"timecode": 10, "before_eval_results": {"predictions": ["November 1979", "Timucuan Ecological and Historic Preserve", "suburban shopping areas", "early vertebrates", "Fears of being labelled a pedophile or hebephile", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "tight end Owen Daniels", "Sanders", "economic instability", "Gamal Abdul Nasser", "Diets lacking sufficient protein are associated with impaired cell-mediated immunity, complement activity, phagocyte function, IgA antibody concentrations, and cytokine production", "counterflow", "lithium-ion battery developer John B. Goodenough", "the installation of pagan images in the Temple in Jerusalem", "machine gun", "the Autons with the Nestene Consciousness and Daleks", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "Standard Model", "Tolui", "the Rhine-Ruhr region", "course of study", "Prevenient grace", "Kansas State", "Captain Cook's Landing Place", "Chris Pine", "Yoo Seung-ho", "the Battle of the Philippines", "NCAA Division I", "The Onion", "Mickey's PhilharMagic", "A Bug's Life", "1978", "May 2008", "Italy", "La Familia Michoacana", "Okinawa Uzumaki", "Tom Jones", "the RATE project", "Barbara Niven", "13\u20133", "John Faso", "5,042", "European culture", "the first integrated circuit", "Tianhe Stadium", "1952", "the fourth Thursday", "William Shakespeare", "Germany and other parts of Central Europe", "New Jersey", "Massachusetts", "Ector County", "Jim Davis", "Buck Owens", "World Health Organization", "Emmanuel Ofosu Yeboah", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Heather Stebbins", "the Royal Firework Music", "Sir Giles Gilbert Scott", "the rig that sank the oil rig", "the Comoros Islands", "Onomastic Sobriquets In The Food And Beverage Industry", "London"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6454004329004329}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7269", "mrqa_squad-validation-5010", "mrqa_squad-validation-797", "mrqa_squad-validation-6495", "mrqa_squad-validation-8072", "mrqa_squad-validation-6927", "mrqa_squad-validation-9815", "mrqa_squad-validation-7729", "mrqa_squad-validation-1166", "mrqa_squad-validation-6166", "mrqa_squad-validation-1877", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-2977", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-3200", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-2783", "mrqa_naturalquestions-validation-7415", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3339", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-10351"], "SR": 0.5625, "CSR": 0.6477272727272727, "EFR": 1.0, "Overall": 0.8238636363636364}, {"timecode": 11, "before_eval_results": {"predictions": ["UHF", "deflate the cocky", "Battle of Olustee", "French", "Only 100\u2013150", "Philo of Byzantium", "The climate is cooler", "in marine waters worldwide", "$60,000", "his mother's genetics and influence", "shock", "cytotoxic natural killer cells", "a new element to the standard Christian suspicion of Judaism", "the building is ready to occupy", "boom-and-bust cycles", "Edinburgh", "Richard Allen and Absalom Jones", "earn as much as a healthy young man", "Jamukha", "1969", "a whole industry", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations", "body bags", "near Warsaw, Kentucky", "Arthur E. Morgan III", "April 2010", "Paul McCartney", "the deal, which promises cabinet positions to the splinter group of the Movement for Democratic Change, does not involve MDC head Morgan Tsvangirai", "\"If you don't have a cause of death, isn't it possible that it might have been an accident?\"", "200", "a very small number of young people taking drugs. We are now more worried about the trend,\" Wong said. \"We don't want a runaway trend; that's why we are stepping up action.\"", "opposition party members", "North Carolina", "to step down as majority leader", "executive director of the Americas Division of Human Rights Watch", "Casa de Campo International Airport", "90", "The station", "a space for aspiring entrepreneurs to brainstorm with like-minded people", "in her home", "Employee Free Choice act", "Bush administration", "more than 200", "It is done with the parents' full consent", "their own", "Kaka", "Christopher Savoie", "Dan Parris, 25, and Rob Lehr, 26", "near Fort Bragg", "two", "$2 billion", "Jacob", "Molotov cocktails, rocks and glass", "as many as 250,000", "Andrew Morris", "Ark of the Covenant", "Jean F Kernel ( 1497 -- 1558 ), a French physician", "The dust was, that as she now stood excited, wild, and honest as the day", "Richmondshire Museum", "1994", "The films", "The Gallipoli Campaign", "Lake Michigan", "Nowhere Boy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6332472432081807}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.25, 0.7499999999999999, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.4, 1.0, 0.0, 0.0625, 0.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285714, 0.0, 1.0, 1.0, 0.4444444444444445, 0.5, 1.0, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 0.22222222222222224, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1919", "mrqa_squad-validation-3087", "mrqa_squad-validation-4611", "mrqa_squad-validation-4524", "mrqa_squad-validation-1313", "mrqa_squad-validation-1257", "mrqa_squad-validation-6588", "mrqa_squad-validation-2493", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-373", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-6176", "mrqa_hotpotqa-validation-3952", "mrqa_searchqa-validation-2548", "mrqa_searchqa-validation-8335"], "SR": 0.484375, "CSR": 0.6341145833333333, "EFR": 1.0, "Overall": 0.8170572916666666}, {"timecode": 12, "before_eval_results": {"predictions": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British.", "wealth", "every good work designed to attract God's favor is a sin.", "Napoleon", "new technology and machinery", "Arley D. Cathey", "private actors.", "Bell Northern Research", "body of treaties and legislation,", "1227", "lower lake", "three", "Elders", "587,000", "A further type of committee", "Mark Ronson", "the Catechism", "Stagg Field.", "Ian Botham", "Pyotr Tchaikovsky", "Vincent Motorcycle Company", "Minnie Marx", "Salvador Allende", "Marie Antoinette", "Hawaii", "Erik Thorvaldson", "Apollon", "Pal Joey", "Mary Jane Grant", "green", "Indonesia", "supreme religious leader of the Israelites", "Antonio", "European Economic Community", "Christine Keeler", "Jesus", "Nicholson", "four", "Netherlands", "Sugar Baby Love", "Rosa Parks Bus", "Sean", "John Denver", "Stage 1", "Travis", "The Show", "Robert Kennedy", "Q", "umbrella", "Jean-Paul Sartre", "barber", "Evonne Goolagong Cawley", "Murrah Federal Office Building", "Evita", "tobacco", "fortified complex", "bohrium", "Eleanor of Aquitaine", "Mickey Gilley", "that \"Sex and the City's\" Kim Cattrall doesn't get along with her co-star Kristin Davis,", "a delegation of American Muslim and Christian leaders", "The marriage to Henry VIII lasted less than a year,", "USC Columbia", "Juan Martin Del Potro."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6151785714285714}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.9333333333333333, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2262", "mrqa_squad-validation-5431", "mrqa_squad-validation-7974", "mrqa_squad-validation-9418", "mrqa_squad-validation-670", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-2240", "mrqa_triviaqa-validation-1390", "mrqa_triviaqa-validation-3027", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-2028", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-215", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-6974", "mrqa_naturalquestions-validation-4905", "mrqa_hotpotqa-validation-3819", "mrqa_newsqa-validation-3987", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-5929"], "SR": 0.53125, "CSR": 0.6262019230769231, "EFR": 0.9666666666666667, "Overall": 0.7964342948717948}, {"timecode": 13, "before_eval_results": {"predictions": ["Cram\u00e9r's conjecture", "Chilaun", "the Pittsburgh Steelers", "Sky Digital", "Allston Science Complex", "divergent boundaries", "Medieval Latin, 9th century", "many", "1775\u20131795", "Dorothy and Michael Hintze", "William Ellery Channing and Ralph Waldo Emerson", "to counteract the constant flooding and strong sedimentation in the western Rhine Delta", "the Wesleyan Holiness Consortium", "Maxwell", "in whole by charging their students tuition fees.", "Dublin, Cork, Youghal and Waterford", "Tangled", "julius", "moles", "leucippus", "fred", "Anne Boleyn", "Calvin", "Steve McQueen", "Portugal", "albert tatum", "three", "komando Pasukan Khusus", "the northwest of England", "carbon dioxide", "the state capital of Ohio", "Lucas McCain", "Antarctica", "mercury gilding", "achromatopsia", "stearns Eliot", "the River Forth", "woe", "the newspaper Daily Bugle", "Burmese", "Italy", "Canada", "typhoid fever", "fred Adams", "an action figure", "al Bundy", "2010", "einasto's law", "Venezuela", "Laurel and Hardy", "the ozone layer", "40", "phrenology", "San Francisco", "Fall 1998", "Marcus Atilius Regulus", "Christopher Weidman", "Drillers Stadium", "one", "Virgin America", "John Grisham", "National Secretaries Week", "Iran's parliament speaker", "fK Ventspils."], "metric_results": {"EM": 0.5, "QA-F1": 0.5895833333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8994", "mrqa_squad-validation-1002", "mrqa_squad-validation-9233", "mrqa_squad-validation-6983", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-6316", "mrqa_triviaqa-validation-5996", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-2222", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-2992", "mrqa_triviaqa-validation-4777", "mrqa_triviaqa-validation-824", "mrqa_triviaqa-validation-813", "mrqa_triviaqa-validation-4391", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6030", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7510", "mrqa_triviaqa-validation-2290", "mrqa_triviaqa-validation-2927", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-1733", "mrqa_naturalquestions-validation-5675", "mrqa_hotpotqa-validation-1390", "mrqa_searchqa-validation-2972", "mrqa_searchqa-validation-15784", "mrqa_newsqa-validation-2281"], "SR": 0.5, "CSR": 0.6171875, "EFR": 1.0, "Overall": 0.80859375}, {"timecode": 14, "before_eval_results": {"predictions": ["an adult plant's apical meristems", "Tugh Temur", "Persia", "Parliament Square, High Street and George IV Bridge in Edinburgh", "Revolutionary civil disobedience", "Beijing", "three years", "27 July 2008", "chemically bonded to each other", "Aristotle", "St. George's Church", "Missy", "Strathclyde Regional Council debating chamber in Glasgow, and to the University of Aberdeen", "public official", "the most cost efficient bidder", "gaius caesar augustus germanicus", "dexter bennett", "gaius caesar augustus germanicus", "Olympia", "Ukrainian Soviet Republic", "gaius caesar augustus germanicus", "andrew johnson", "gaius caesar augustus bennett", "amber", "andrew johnson", "a pardon", "gaius caesar augustus germanicus", "bishkek Tajikistan", "anamosa", "andrew johnson", "Ephesus", "Camelot", "film", "knife", "fiery light", "Cologne", "andrew johnson", "gaius caesar augustus germanicus", "Kosovo", "andrew johnson", "Prague", "tennis", "prufon", "dexter bennett", "andrew johnson", "gaius virginianus", "Japan", "gaius caesar augustus bennett", "kung fu grip", "and girls", "accordion", "prufrock and other observations", "gaius caesar augustus germanicus", "Augusta", "in a counter clockwise direction around the Sun", "March 31, 2013", "prufrock and other observations", "gaius caesar augustus germanicus", "December 24, 1973", "David Weissman", "bikinis", "the Dalai Lama's current \"middle way approach,\"", "gaius caesar augustus ginsberg", "Israel"], "metric_results": {"EM": 0.34375, "QA-F1": 0.39765625000000004}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.7000000000000001, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.25, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2105", "mrqa_squad-validation-3488", "mrqa_squad-validation-7818", "mrqa_squad-validation-9402", "mrqa_squad-validation-6801", "mrqa_searchqa-validation-2291", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-15477", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-16197", "mrqa_searchqa-validation-12064", "mrqa_searchqa-validation-10459", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6146", "mrqa_searchqa-validation-9588", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-4439", "mrqa_searchqa-validation-6335", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-14997", "mrqa_searchqa-validation-3873", "mrqa_searchqa-validation-405", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-12545", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-1976", "mrqa_searchqa-validation-5100", "mrqa_searchqa-validation-3586", "mrqa_searchqa-validation-2445", "mrqa_searchqa-validation-4459", "mrqa_searchqa-validation-10412", "mrqa_naturalquestions-validation-4647", "mrqa_triviaqa-validation-224", "mrqa_triviaqa-validation-6129", "mrqa_newsqa-validation-3502", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-3084"], "SR": 0.34375, "CSR": 0.5989583333333333, "EFR": 1.0, "Overall": 0.7994791666666666}, {"timecode": 15, "before_eval_results": {"predictions": ["younger", "gambling", "28,000", "Muhammad ibn Zakar\u012bya R\u0101zi", "river Deabolis", "April 20", "Gaulish name R\u0113nos", "1996", "wine", "German-Swiss", "Melbourne", "enter the priesthood", "Seattle Seahawks", "IBM", "crossword", "Paula Abdul", "Ohio", "Flanders", "MasterCard", "Robert Stempel", "Nashville", "the olfactory nerve", "Ivan the Terrible", "Nancy Astor", "(solar lentigo)", "Kurt Russell", "Toronto Maple", "Zsa Zsa Gabor", "Vladimir Nemirovich-Danchenko", "Utah", "Rum", "(Rabbit) Angstrom", "Johann Strauss II", "joey", "pro bono", "Bologna", "a candy store", "a brown beer", "Anthony Fokker", "Nacho Libre", "copper", "black magic or of dealings with the devil", "hemlock", "Jeffrey Wigand", "National Poetry Month", "(Lettuce)", "meager", "Casablanca", "squadrons", "Gustav Kirchhoff", "a geisha", "a mermaid", "Altruism", "Frederic Remington", "Juan Francisco Ochoa", "ThonMaker", "a tin star", "Noir", "The Legend of Sleepy Hollow", "Doc Hollywood", "Afghanistan", "two", "Belgium", "Rio de Janeiro"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6375000000000001}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-9270", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-10558", "mrqa_searchqa-validation-2440", "mrqa_searchqa-validation-14330", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-11884", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-10427", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-5375", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-15167", "mrqa_searchqa-validation-9332", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-12729", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-3653", "mrqa_naturalquestions-validation-309", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-3675", "mrqa_newsqa-validation-2036"], "SR": 0.546875, "CSR": 0.595703125, "EFR": 1.0, "Overall": 0.7978515625}, {"timecode": 16, "before_eval_results": {"predictions": ["Keraite tribe", "respiration", "1997", "late 1920s", "\u00a34.2bn", "27 July 2008", "unequal", "October 1973", "military troops", "Isiah Bowman", "assembly center", "Ominde Commission", "Rhus", "Evita", "Ho Chi Minh", "circum", "the Inuit", "Detroit", "the (Montreal) Blue Jays", "Walt Whitman", "(Ray Bradbury)", "hate crimes", "King Julien", "Nicolas Sarkozy", "the Rubicon", "(Montreal)", "17", "(Louisa) May Alcott", "Play-Doh", "Aphrodite", "Jesus", "The Prince and the Pauper", "Crystal Pepsi", "Hillary Clinton", "King Philip", "( Bellerophontes)", "Balaam", "the Wharton School", "The Caine Mutiny", "Rolling Stone", "F. W. Woolworth Company", "(John) Coltrane", "the peace sign", "oxygen", "the Sphinx", "Jan Hus", "the (Nashville Star)", "the Mavericks", "Onegin", "Macy's", "a spinning mule", "Santa Claus", "(Denzel) Washington", "a doctor", "courts", "a person has the usual two copies of chromosome 21, plus extra material from chromosome 21 attached to another chromosome", "Preston", "Australia", "The Jefferson Memorial", "between 11 or 13 and 18", "Michoacan Family", "( Brad) Blauser", "salary", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6347813644688645}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.23076923076923078]}}, "before_error_ids": ["mrqa_squad-validation-1796", "mrqa_squad-validation-3132", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-16726", "mrqa_searchqa-validation-8220", "mrqa_searchqa-validation-15303", "mrqa_searchqa-validation-1355", "mrqa_searchqa-validation-6202", "mrqa_searchqa-validation-11707", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-13648", "mrqa_searchqa-validation-568", "mrqa_searchqa-validation-15453", "mrqa_searchqa-validation-8757", "mrqa_searchqa-validation-15626", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-6265", "mrqa_naturalquestions-validation-794", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-1759"], "SR": 0.5625, "CSR": 0.59375, "EFR": 0.9642857142857143, "Overall": 0.7790178571428572}, {"timecode": 17, "before_eval_results": {"predictions": ["September 5, 1985", "mannerist architecture", "stratigraphers", "trade unions", "23.9%", "earn as much as a healthy young man", "Centrum", "Tesla", "him", "Party of National Unity", "22", "the Dauphin", "Phillip Marlowe", "piracy", "Cliff Lee", "The Crystal Method", "Puerto Rico", "The Mausoleum", "Million Dollar Baby", "Switzerland", "Lufthansa", "The Old Man", "French", "Joe Louis", "the lion", "the Three Musketeers", "the Bayeux Tapestry", "Porch", "China", "Sunni", "notes", "Stephen Hawking", "Cicero", "Memphis", "Mountain Dew", "Blanche DuBois", "Quilt Crazy", "FRAM", "the House of Representatives", "Blue", "Michael Moore", "Oman", "Silverado", "Ingenue", "Pennsylvania", "Don Juan", "Ian Fleming", "Ichabod Crane", "London", "Yellowstone", "Ronald Reagan", "Fiddler", "Ethiopian", "six 50 minute ( one - hour with advertisements ) episodes", "1992", "a salt", "Bromley", "the Ruul", "John R. Dilworth", "Caylee Anthony", "a list of cars", "a Taliban member", "a nuclear weapon", "ten golf movies ever made"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6588541666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1659", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-1272", "mrqa_searchqa-validation-11215", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-1920", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-1087", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-12176", "mrqa_searchqa-validation-14873", "mrqa_searchqa-validation-3176", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-8864", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-12814", "mrqa_naturalquestions-validation-3267", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-316", "mrqa_hotpotqa-validation-3449", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-286", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-4110"], "SR": 0.59375, "CSR": 0.59375, "EFR": 1.0, "Overall": 0.796875}, {"timecode": 18, "before_eval_results": {"predictions": ["Super Bowl XXXIII", "1993", "June 1979", "Tesla's friend", "tentacles", "Robert R. Gilruth", "circuit complexity", "same-gender marriages", "the 2006 Israel-Lebanon conflict", "the mid-18th century", "orange", "A Raisin in the Sun", "Italy", "White Russia", "one", "a trowel", "Big Bang", "The Sex Pistols", "endodontist", "Saturn", "White Cliffs of Dover", "Genoa", "John Galt", "Jersey Boys", "the door of the Castle Church in Wittenberg", "Utah", "Meg Ryan", "a rose", "Chow Main Street", "21", "the Civil", "Copella arnoldi", "Paul McCartney", "omega", "Raphael", "Bachman Turner Overdrive", "ParaNorman", "Caddy Shack", "Tokyo", "Panama", "Ellen", "Narnia", "Finnegans Wake", "Wordsworth", "Norway", "the Berenstain Bears", "a quake", "Judas", "the African elephant", "the Mazur", "Finland", "a clandestine love affair", "Our Country", "May 2010", "Texas", "Guanabara bay", "Thailand", "gender queer", "Minister for Social Protection", "Berga", "the estate", "Bill Irwin", "ase", "Michigan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6210069444444444}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.1111111111111111]}}, "before_error_ids": ["mrqa_squad-validation-1648", "mrqa_squad-validation-1696", "mrqa_searchqa-validation-5116", "mrqa_searchqa-validation-3420", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-15811", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-13718", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-7964", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-3043", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-15157", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-5208", "mrqa_searchqa-validation-3547", "mrqa_searchqa-validation-9991", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-15717", "mrqa_searchqa-validation-15305", "mrqa_searchqa-validation-10266", "mrqa_searchqa-validation-9572", "mrqa_naturalquestions-validation-554", "mrqa_triviaqa-validation-2612", "mrqa_hotpotqa-validation-2217", "mrqa_newsqa-validation-2421", "mrqa_naturalquestions-validation-2870"], "SR": 0.5, "CSR": 0.5888157894736843, "EFR": 1.0, "Overall": 0.7944078947368421}, {"timecode": 19, "before_eval_results": {"predictions": ["to avoid trivialization", "genetically modified crops", "Earth", "53,000", "one", "poet", "two", "20,000", "the kip", "skeletal muscle and the brain", "2014", "a single peptide bond or one amino acid with two peptide bonds", "the Wanderers", "the results show moved to Sunday evenings", "a zygote with n pairs of chromosomes", "volcanic activity", "Montgomery", "Rock Island, Illinois", "April 9, 2012", "Squamish, British Columbia, Canada", "Proposition 103", "mindfulness", "Charlene Holt", "Captain Leland Stottlemeyer", "1991", "electron shells", "The Cornett family", "Acid rain", "April 15, 2018", "more of one good can be produced only by producing less of the other", "he cheated on Miley", "2001", "democracy", "735", "1871", "Rick Rude", "an assistant at Toledo, Bowling Green, and Mount Union", "a form of business network", "a cylinder of glass or plastic", "Abraham Gottlob Werner", "Wakanda and the Savage Land", "prejudice in favour of or against one thing, person, or group compared with another", "Ancy Lostoma duodenale", "March 1", "CSR Racing 2", "the nature of Abraham Lincoln's war goals", "oxygen", "Cecil Lockhart", "Mara Jade", "British and French Canadian fur traders", "semi-autonomous", "Lou Rawls", "a man called Lysander", "Jupiter", "Mediterranean", "15", "John Robert Cocker", "Silvan Shalom", "a puzzle video game", "a palace", "the olfactory nerve", "a mottlecah", "a lion", "oxygen"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5656397855616606}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.2222222222222222, 1.0, 0.625, 0.0, 0.2, 0.8, 0.4, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.5, 0.19999999999999998, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-2928", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-4824", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-5804", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-5964", "mrqa_hotpotqa-validation-4926", "mrqa_newsqa-validation-2379", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-2227"], "SR": 0.46875, "CSR": 0.5828125, "EFR": 0.9411764705882353, "Overall": 0.7619944852941176}, {"timecode": 20, "before_eval_results": {"predictions": ["petroleum", "the Cloth of St Gereon", "Thomas Sowell", "70", "death of a heretic", "choosing their own ministers", "1886", "\"Blue Harvest\" and \"420\"", "Jacob Zuma", "gang rape", "Florida", "10", "Wednesday", "201-262-2800", "different women coping with breast cancer in five vignettes.", "over 1,000 pounds", "Egyptian State TV", "Mutassim", "Oklahoma", "Ralph Lauren", "\"The Jacksons: A Family Dynasty\"", "Amstetten", "computer problems left travelers across the United States waiting in airports", "Silvan Shalom", "Jonathan Breeze", "Steve Jobs", "12-hour", "prisoners", "June 2004", "consumer confidence", "5:20 p.m.", "North vs. South", "India", "1964", "Davidson", "Swat Valley", "Monday", "1979", "the United States", "The Tom Joyner Morning Show", "Akio Toyoda", "The National September 11 Memorial & Museum Foundation", "\"I have issued an order. I don't want these people interfered with in any way,\"", "Giovani dos Santos", "Michael Schumacher", "Hurricane Gustav", "gun", "Henrik Stenson", "children that a French charity attempted to take to France from Chad for adoption", "40", "Derek Mears", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military", "two years", "1966", "winter", "Whit Sunday", "Aberdeen", "\"Dumb and Dumber\"", "The 2003 LSU Tigers football team", "Earl Warren", "a converging lens", "autu", "season six", "The Force Fighters"], "metric_results": {"EM": 0.515625, "QA-F1": 0.595765128968254}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8750000000000001, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-911", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-1549", "mrqa_naturalquestions-validation-7266", "mrqa_hotpotqa-validation-1094", "mrqa_searchqa-validation-9508", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-3422"], "SR": 0.515625, "CSR": 0.5796130952380952, "EFR": 1.0, "Overall": 0.7898065476190477}, {"timecode": 21, "before_eval_results": {"predictions": ["Cologne, Germany", "occupational stress among teachers.", "El Centro metropolitan area and San Diego-Carlsbad-San Marcos metropolitan area", "chief electrician", "Newton", "static friction, generated between the object and the table surface", "the assassination of US President John F. Kennedy the previous day;", "\"an affront to Somalia's territorial sovereignty.\"", "Union Station in Denver, Colorado.", "Casalesi Camorra clan", "Awearness Fund", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "\"no more than an official of the most tyrannical dictatorial state in the world.\"", "\"Golden Girls\"", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Wednesday.", "Cash for Clunkers", "Bobby Jindal", "9:20 p.m. ET Wednesday.", "Kim Clijsters", "Mashhad, Iran.", "Amanda Knox's aunt", "great jazz music and a very cheerful crowd.", "$530 million in debt", "\"Doogie Howser, M.D.\"", "Luiz Inacio Lula da Silva", "his father's parenting skills.", "two contestants.", "Bill", "J.G. Ballard", "nurse who tried to treat Jackson's insomnia with natural remedies", "Michelle Obama", "saving and planning for retirement long before his career neared its end.", "1981", "\"17 Again,\"", "Nigeria", "$81,8709", "Republican", "EU naval force", "Chris Robinson", "son of Gabon's former president", "steam-driven, paddlewheeled overnight passenger boat.", "Hyundai Steel", "skeletal dysplasia,", "London Heathrow's Terminal 5.", "\"very diverse\"", "February 12", "more than 30", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "India", "Steve Williams", "military action because we're getting frustrated seems to me somewhat dangerous.", "White House Executive chef", "Russell Huxtable", "Willy Russell", "London", "\"Mortal Kombat\"", "Old English", "to give birth to children who sin.", "an iron fist for 25 years until he was overthrown and executed on Christmas Day in 1989.", "Argentinian", "Mercedes-Benz Superdome in New Orleans, Louisiana.", "Ems Telegram"], "metric_results": {"EM": 0.40625, "QA-F1": 0.49883680963644195}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.4, 0.13333333333333333, 0.0, 0.5714285714285715, 1.0, 0.0, 0.11764705882352941, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2717", "mrqa_squad-validation-10313", "mrqa_squad-validation-7746", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-3926", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-1462", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-729", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-110", "mrqa_hotpotqa-validation-251", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-8602", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-1056"], "SR": 0.40625, "CSR": 0.5717329545454546, "EFR": 1.0, "Overall": 0.7858664772727273}, {"timecode": 22, "before_eval_results": {"predictions": ["high energy single terminal vacuum tube", "WMO Executive Council and UNEP Governing Council", "Germans", "New York and Virginia", "two", "glowed even when turned off.", "a number of celebrities and ministers,", "water in a very dark and very cold place.", "sovereignty over them.", "April 6, 1994", "Prague", "backbreaking labor", "a federal judge in Mississippi on March 22,", "\"We have double work,\"", "$22 million", "severe flooding", "a music video on his land.", "walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "\"Watchmen\"", "The Real Housewives of Atlanta", "18", "88", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide", "change", "military trials", "Sachina Verma", "Larry King", "Steven Chu", "racially motivated.", "Michael Partain", "male veterans struggling with homelessness and addiction.", "longest domestic relay in Olympic history", "Zimbabwe.", "No. 1 slot", "nine", "ash and rubble", "Friday", "Kingdom City", "Rima Fakih", "Tuesday night", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Ben Roethlisberger", "one", "Lee Myung-Bak", "Alwin Landry's supply vessel Damon Bankston", "scientists", "war crimes and crimes against humanity.", "opium", "warning about tendon problems.", "84-year-old", "Robert Park", "Rima Fakih", "the Isthmus of Corinth", "Nalini Negi", "2017", "Runcorn", "collarbone", "paris", "UFC 50: The War of '04", "June 11, 1973", "The Del Mar Fairgrounds", "Toy Story", "Emiliano Zapata", "A Fairy Tale of Home"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5285937255146814}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 0.22222222222222224, 1.0, 1.0, 0.33333333333333337, 0.13333333333333333, 1.0, 0.5, 0.19999999999999998, 0.23529411764705882, 0.7272727272727273, 0.0, 1.0, 1.0, 0.5714285714285715, 0.15384615384615383, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.058823529411764705, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444444, 1.0, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1407", "mrqa_squad-validation-2356", "mrqa_squad-validation-3127", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-2760", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3681", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1418", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-3875", "mrqa_triviaqa-validation-7532", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4449", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-4464"], "SR": 0.40625, "CSR": 0.5645380434782609, "EFR": 1.0, "Overall": 0.7822690217391304}, {"timecode": 23, "before_eval_results": {"predictions": ["phycoerytherin", "was lost in the 5th Avenue laboratory fire of March 1895.", "economic inequality", "Davros", "Church and the Methodist-Christian theological tradition", "Behind the Sofa", "Tulsa, Oklahoma.", "56,", "in Yemen", "2005", "Karen Floyd", "Four Americans", "the missing person.", "Haiti", "Susan Boyle", "Saturday just hours before he was scheduled to perform at the BET Hip Hop Awards.", "Spain", "Jared Polis", "Janet and La Toya,", "Hyundai", "30", "Miriam Brown", "lightning strikes", "Evans", "Italian government", "the flooding was so fast that the thing flipped over,\"", "threatening messages", "stop Noriko Savoie from being able to travel to Japan for summer vacation.", "drafting a new constitution after three decades of Mubarak's rule.", "fake his own death", "Tim Masters,", "martial arts", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "then-Sen. Obama", "Congress", "curfew", "Anne Frank,", "June,", "the government in Islamabad \"has so far not received any information or evidence relating to the Mumbai incident from the government of India.", "Zuma", "haute, bandeau-style little numbers", "nine", "Iraq", "2000", "50", "15-year-old", "in body bags on the roadway near the bus,", "Diana, her boyfriend, Dodi Fayed, and their driver, Henri Paul.", "Desmond Tutu", "$17,000", "Toy Story", "$81,880", "provide school districts with federal funds", "repudiation, change of mind, repentance, and atonement", "Jason Lee", "sleep", "n\u014dmen", "Kent", "beer and soft drinks", "five aerial victories.", "the Cherokee River", "Snowball", "Apollo 13", "Florida"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5771860206070731}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.962962962962963, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15789473684210525, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8595", "mrqa_newsqa-validation-1493", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3039", "mrqa_newsqa-validation-3671", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2616", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1721", "mrqa_hotpotqa-validation-162", "mrqa_searchqa-validation-8458"], "SR": 0.515625, "CSR": 0.5625, "EFR": 1.0, "Overall": 0.78125}, {"timecode": 24, "before_eval_results": {"predictions": ["black-and-yellow", "Frederick II the Great", "Muslims in the semu class", "manually suppress the fire", "compound", "Nigeria", "Vonn", "Frenchwoman", "him to step down as majority leader.", "United Nations World Food Program", "gang rape", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "The Louvre", "his club", "to best your own fuel economy achievements,\"", "1979", "Heshmat Tehran Attarzadeh", "jazz", "an antihistamine and an epinephrine auto-injector for emergencies,", "Bangladesh,", "Michael Arrington,", "one", "Ahmed,", "Sonia, a single mother with HIV in Brazil, travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Britain's Got Talent", "military personnel", "placed behind the counter.", "11", "one Iraqi soldier,", "Michael Partain,", "her fianc\u00e9,", "racial intolerance.", "a vegan diet to some of the flavorful foods they can eat.", "Amado Carrillo Fuentes", "the self-styled revolutionary Symbionese Liberation Army", "$8.8 million", "to work together to stabilize Somalia and cooperate in security and military operations.", "would compromise the public broadcaster's appearance of impartiality.", "it -- you know -- black is beautiful,\"", "$104,168,000", "Picasso's muse and mistress, Marie-Therese Walter.", "to stop the Afghan opium trade", "nearly $162 billion in war funding", "off the coast of Dubai", "military veterans", "Springfield, Virginia,", "eight", "Mark Obama Ndesandjo", "\"Dance Your Ass Off.\"", "Russia", "\"Stagecoach\"", "adultery", "nucleus", "Vienna", "Sebastian Lund ( Rob Kerkovich )", "Jimmy Carter", "Tom Watson", "Sandi Toksvig", "The Spyker F1 Team", "3rd Earl of Limerick", "Lake Buena Vista, Florida", "Iceland", "wedlock", "platinum"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5547845511310585}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.4, 0.11764705882352941, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.05555555555555555, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.7499999999999999, 1.0, 0.14285714285714288, 0.0, 0.6666666666666666, 0.0, 1.0, 0.12500000000000003, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-1461", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1103", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-899", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-1744", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1282", "mrqa_hotpotqa-validation-1346", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-8678"], "SR": 0.4375, "CSR": 0.5575, "EFR": 1.0, "Overall": 0.77875}, {"timecode": 25, "before_eval_results": {"predictions": ["unity of God", "Treaty of Logstown", "Jordan Norwood", "RNA silencing", "concurring, smaller assessments of special problems", "Anthony Hopkins", "New Zealand", "Tamar", "rhododendron", "35", "specialist", "beetle", "phylum", "Wayne Allwine", "Westminster Abbey", "holography", "Pelias", "Daniel Boaventura", "Northumbria", "Harvard", "cricket", "Seymour Hersh,", "quant", "copper and zinc", "Tigris", "Cordelia", "told both news and rumours.", "seborrheic dermatitis", "four", "a native French grape,", "Joseph Smith,", "Huntington Beach, California", "gold", "shadow", "a number between 10 and 20.", "a palla.", "The Apartment", "Canada", "Clement Attlee", "Stockholm", "Peter Parker", "kibbutznik", "Giorgio Armani,", "corrida de toros,", "The Number One Song in Heaven", "Ginger Rogers", "Plymouth Rock", "Steptoe and Son, Till Death Us do Part, The Liver Birds, All Gas and Gaitors, Are You Being Served?", "citric", "Dr Karen James,", "\"Take Me Home, Country Roads\"", "Mrs. Boddy", "Marie Van Brittan Brown", "southern California", "1995", "Bourbon County", "Taylor Swift", "\"Home\"", "Peterson had his personal.40-caliber Glock when police found him.", "The Detroit, Michigan,", "Amy Bishop Anderson,", "calathus", "the Louvre", "an American private, not-for-profit, coeducational research university"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4961309523809524}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6547", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-147", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-7070", "mrqa_triviaqa-validation-7210", "mrqa_triviaqa-validation-3096", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-2160", "mrqa_triviaqa-validation-3082", "mrqa_triviaqa-validation-2301", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-5832", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-1059", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6038", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-3796", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-7521", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-2288", "mrqa_searchqa-validation-7980", "mrqa_searchqa-validation-2376"], "SR": 0.453125, "CSR": 0.5534855769230769, "EFR": 0.9714285714285714, "Overall": 0.7624570741758241}, {"timecode": 26, "before_eval_results": {"predictions": ["\"The Name of the Doctor\"", "third", "affordable housing", "Mao Zedong", "Verona", "pontiac", "elephants", "charcoal brazier", "Frank McCourt", "jules Verne", "julius Cassab", "moyra Fraser", "Schengen Area", "red", "city of chicago", "Famous Players-Lasky Corporation", "Gary Puckett", "Gerald Durrell", "jonathan", "County Cork", "jason", "can eat and drink anything,", "Halifax", "mccartney", "jason mccartney", "Frank Wilson", "Carlos the jackal", "Edwina Currie", "tulia Lipnitskaya", "Robert Maxwell", "1768", "\u201cFor Gallantry;\u201d", "face", "peninsula", "tuscaloosa", "ever Concentasing Circles", "tahrir", "plutonium", "c. 1595", "27", "jack Ruby", "tintoretto", "jaunty", "Saudi Arabia", "\"reckless\"", "Thailand", "Sydney", "doves", "Tunisia", "Prince Philip", "grosvenor cotes Wyatt", "Tokyo", "Edgar Lungu", "49 cents", "over 100 beats per minute", "672", "\"Linda McCartney's Life in Photography\",", "Franconia, New Hampshire", "\"BRB,\"", "jon Martin Del Potro", "27", "england", "Richard Cory", "Buddhism"], "metric_results": {"EM": 0.4375, "QA-F1": 0.51796875}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.75, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7774", "mrqa_squad-validation-8026", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-86", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-1354", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-7193", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-6259", "mrqa_triviaqa-validation-3354", "mrqa_naturalquestions-validation-10131", "mrqa_hotpotqa-validation-5372", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1150", "mrqa_searchqa-validation-12829"], "SR": 0.4375, "CSR": 0.5491898148148149, "EFR": 1.0, "Overall": 0.7745949074074074}, {"timecode": 27, "before_eval_results": {"predictions": ["two", "80", "more than 70", "forced Tesla out leaving him penniless.", "Benazir Bhutto", "nuclear program.", "an Awa", "louis armstrong", "FBI Special Agent Daniel Cain,", "acid", "Wally", "2008", "after Wood went missing off Catalina Island,", "Rima Fakih", "Afghanistan", "everglades", "a fine start to the third match of their series against India in Mumbai on Wednesday.", "1950s", "64", "Iran's parliament speaker", "27-year-old", "Alexandros Grigoropoulos,", "$4.5 million", "glamour and hedonism", "a stifled of innovation and development.", "ensenada", "Orbiting Carbon Observatory", "Switzerland", "Kenneth Cole", "Janet and La Toya", "Nine out of 10 children", "about 5:20 p.m.", "combat veterans", "improve health and beauty.", "U.S. Chamber of Commerce", "burned over 65 percent of his body after being set on fire,", "al-Shabaab", "was booked on an outstanding arrest warrant relating to a domestic violence case,", "sustain future exploration of the moon and beyond.", "his business dealings for possible securities violations", "Opry Mills", "Number Ones", "attempting illegal crossings", "he was diagnosed with skin cancer.", "al Qaeda", "president Barack Obama", "Obama should have met with the Dalai Lama.", "oceans", "barbara wilson", "doctors", "off the coast of Dubai", "Bill Haas", "Oona caplin", "1932", "between 1923 and 1925", "Gilda", "jeremy tate", "table tennis", "Tamil", "DreamWorks Animation", "Indianola", "Empire State Building", "Disraeli", "a red"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6583881578947368}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.7368421052631579, 1.0, 0.16666666666666669, 1.0, 0.6, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-850", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1698", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2022", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4072", "mrqa_triviaqa-validation-4193", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1816", "mrqa_searchqa-validation-15354"], "SR": 0.5625, "CSR": 0.5496651785714286, "EFR": 1.0, "Overall": 0.7748325892857143}, {"timecode": 28, "before_eval_results": {"predictions": ["a hybrid Bermuda 419 turf", "25-foot", "manipulates symbols", "Dangjin", "Monday night", "Florida", "journalists and the flight crew will be freed,", "40", "the Illuminati", "in a public housing project,", "toxic smoke from burn pits", "Lucky Dube,", "two Israeli soldiers,", "space shuttle Discovery", "Gavin de Becker", "a nuclear weapon", "in Japan", "Arizona", "in the Intertropical Convergence Zone", "simple puzzle video game,", "outside influences", "aid to Gaza,", "rolled over Tuesday near Campbellton, Texas, killing two people and injuring more than a dozen,", "suppress the memories and to live as normal a life as possible.", "Tuesday", "immediately appeal the ruling and seek a stay of the order with the U.S. Court of Appeals for the District of Columbia.", "the helicopter went down in Talbiya,", "his death cast a shadow over festivities ahead of South Africa's highly- anticipated appearance in the rugby World Cup final with England this weekend.", "Cash for Clunkers", "Oregon Fire Lines", "one of the most influential, powerful and admired public figures of our time,", "80 percent", "in London's 20,000-capacity O2 Arena.", "to try to make life a little easier for these families", "johnson", "$50", "Australian officials", "the iconic Hollywood headquarters of Capitol Records,", "Bill Klein,", "gun", "more than 38", "Argentina", "the underprivileged.", "Somalia's piracy problem was fueled by environmental and political events", "\"17 Again\"", "Kabul", "22", "Steven Gerrard", "12.3 million", "a U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U.M.-backed Iraqi forces and gunmen.", "Fakih", "at Old Trafford", "to `` help bring creative projects to life ''", "season two", "Mary Elizabeth Patterson", "trombone", "The Fifth Amendment", "Nepal", "Merck & Co.", "Fort Albany", "Knoxville, Tennessee", "Nehru", "stomata", "a hypomanic episode"], "metric_results": {"EM": 0.5, "QA-F1": 0.6066524621212122}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.3333333333333333, 0.42857142857142855, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1789", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2380", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-4004", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1265", "mrqa_naturalquestions-validation-10292", "mrqa_triviaqa-validation-919", "mrqa_triviaqa-validation-7116", "mrqa_triviaqa-validation-79", "mrqa_searchqa-validation-4465"], "SR": 0.5, "CSR": 0.5479525862068966, "EFR": 1.0, "Overall": 0.7739762931034483}, {"timecode": 29, "before_eval_results": {"predictions": ["Mike Carey", "100% oxygen", "Betty Meggers", "ancient cult activity", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "ovaries, fallopian tubes, uterus, vulva, vagina, testes, vas deferens, seminal vesicles, prostate and penis", "Tarn\u00f3w Offensive of the Central Powers against the Russian army", "near the end of their main sequence lifetime", "August 6 and 9, 1945", "Doug Diemoz", "Virginia", "Monk's Caf\u00e9", "in the central plains", "al - Mamlakah al - \u02bbArab\u012byah", "Southport, North Carolina", "Iran", "'s hard drive", "July 4, 1776", "\" pick yourself up and dust yourself off and keep going '", "John Garfield as Al Schmid", "used by captains of sailing ships to cross the world's oceans for centuries,", "October 12, 1979", "Lorazepam", "the 2013 non-fiction book of the same name by David Finkel", "salamislice", "Ethel `` Edy '' Proctor", "used in combat sports,", "Husrev Pasha", "Jodie Sweetin", "a nerve that runs near the ulna bone", "McFerrin, Robin Williams, and Bill Irwin", "Watson and Crick", "Gorakhpur", "Patris et Filii et Spiritus Sancti", "rightly Guided Caliphs", "Lake Powell", "topper or treetopper", "September 6, 2019", "two senators,", "substitute good", "Archie Marries Betty", "over 74", "1987", "cunnilingus", "October 2000", "New York City", "Prafulla Chandra Ghosh of the Indian National Congress", "October 1929", "first heart sound ( S )", "Hermann Ebbinghaus", "The Miracles", "used obscure languages as a means of secret communication during wartime", "\"Too Young/Sweet and innocent\"", "Carthage", "George W. Bush,", "\"Gesellschafter\"", "7.63\u00d725mm Mauser (.30 Mauser Automatic)", "seven", "Muslim", "two remaining crew members from the helicopter,", "Saturday's Hungarian Grand Prix.", "Rickey Henderson", "Lake Baikal", "on a quest to satisfy their desire for White Castle burgers."], "metric_results": {"EM": 0.34375, "QA-F1": 0.5243057575742467}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.5555555555555556, 0.0, 0.4, 0.1111111111111111, 0.9090909090909091, 0.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.7142857142857143, 0.5714285714285715, 0.13793103448275862, 0.4, 0.0, 0.0, 0.0, 1.0, 0.32, 1.0, 1.0, 0.25, 0.5, 0.5, 0.5, 1.0, 0.4, 1.0, 0.10526315789473684, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.5, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 0.18181818181818182]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-2194", "mrqa_naturalquestions-validation-7376", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-9447", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-5010", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-3025", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-1733", "mrqa_searchqa-validation-9259", "mrqa_searchqa-validation-5753"], "SR": 0.34375, "CSR": 0.5411458333333333, "EFR": 0.9523809523809523, "Overall": 0.7467633928571429}, {"timecode": 30, "before_eval_results": {"predictions": ["a setup phase in each involved node before any packet is transferred to establish the parameters of communication", "cydippid Pleurobrachia", "1953", "AT&T", "north-eastward", "Chingachgook", "shoes", "nine", "Rashid Akmaev,", "acetylene", "don't know what \"ultracrepidarian\" means", "fiber", "a fox", "what's in a name", "Winston Rodney", "sand", "Nanjing", "Montana", "a cat and large wooden rabbit", "Louis XIV", "GILBERT & SullIVAN", "Fox Network", "the Belgae", "Joe Lieberman", "the Boston Marathon", "fibreboard", "tin", "Song of Norway", "Frida Kahlo", "walker", "\"starts in my toe & I crinkle my nose wherever it goes\"", "\"Fat man, you shoot a great game of pool.\"", "hair", "the Hearst newspaper chain", "a crustal rock", "ale", "Homo", "telephone operator", "\"When You Look Me In The Eyes\"", "Casey Jones", "The New Colossus", "yelping", "Bernard Fokke", "Sarah Fergusonthe Duchess of York", "walk surfboard", "football", "bronchoconstriction", "Forty", "a glass tube containing a mixture of neon (99.5%) and argon gas.", "northern", "Ford GT40", "Earl Long", "Neil Patrick Harris", "Rodrick", "1999", "vitamin D", "three", "albert juantorena", "R&B vocal group", "Awake", "Doctor of Philosophy", "Pakistan", "in Atlanta", "Sonia Sotomayor"], "metric_results": {"EM": 0.359375, "QA-F1": 0.42827772556390975}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false], "QA-F1": [0.052631578947368425, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4798", "mrqa_squad-validation-4455", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-10169", "mrqa_searchqa-validation-13591", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-135", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-1693", "mrqa_searchqa-validation-1994", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-11243", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-13153", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-3579", "mrqa_searchqa-validation-15750", "mrqa_searchqa-validation-15306", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-4165", "mrqa_searchqa-validation-14012", "mrqa_searchqa-validation-15632", "mrqa_searchqa-validation-3528", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-7493", "mrqa_triviaqa-validation-282", "mrqa_triviaqa-validation-6657", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5297", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-723"], "SR": 0.359375, "CSR": 0.5352822580645161, "EFR": 1.0, "Overall": 0.767641129032258}, {"timecode": 31, "before_eval_results": {"predictions": ["non-Mongol physicians", "Prospect Park", "the cornea", "the volume", "a squint", "'Apocalypse Now'", "Diners' Club Card", "Christian Dior", "(August) Wilson", "Juliet", "Notre Dame", "a huge tourist attraction", "Tate", "(Bountiful) Bligh", "Rhodes", "Edinburgh", "Swaziland", "Kevin Spacey", "Union Square", "one", "Mike Huckabee", "Queen", "headaches", "a tote", "mulberry", "(Ed) Hillary", "( Samuel) Beckett", "Rachel Carson", "Vietnam", "sports", "David Geffen", "Franklin", "Kate Middleton", "Ugly Betty", "an R", "Zechariah", "New Jersey", "Lake Ontario", "(Matt) Perry", "Marissa Jaret Winokur", "John Ford", "kismet", "canibalism", "a battery", "aluminum", "(Mathew) Brady", "Ned Kelly", "a piles of papers", "a gravitational force", "Isis", "a quiver", "Heroes", "on the two tablets", "the source of the donor organ", "seven", "Geheimrat Dr. Max", "(Duke) Ellington", "Stevie Wonder", "Ludwig van Beethoven", "March 13, 2013", "Chelsea Peretti", "two years,", "Luka Modric", "10.1"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6161458333333334}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-11182", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-3537", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-12813", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-9411", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-5737", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-1379", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-13240", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-410", "mrqa_triviaqa-validation-2878", "mrqa_triviaqa-validation-114", "mrqa_hotpotqa-validation-513", "mrqa_newsqa-validation-313", "mrqa_newsqa-validation-2123"], "SR": 0.53125, "CSR": 0.53515625, "EFR": 0.9666666666666667, "Overall": 0.7509114583333334}, {"timecode": 32, "before_eval_results": {"predictions": ["the same as the weight of the air that rushed back in", "Fresno Street and Thorne Ave", "Black Death", "Elton John", "John Stuart Mill", "Emperor Norton", "CIA", "piano", "Rickey Henderson", "Gandhi", "carotenoids", "John Grunsfeld", "a mandapa", "2015", "Galileo Descartes", "a clark", "Dust Cloth", "Rudy Giuliani", "the Free Speech Clause", "a scallop", "Thor", "Pennsylvania", "The Omega Man", "a walk-in pantry", "a barrel", "the Summer Olympics", "Hugo Chvez", "Jewel", "Hinduism", "tin", "Dirty Diana", "The Rime of the Ancient Mariner", "pine tar", "the Lincoln Tunnel", "Michael Collins", "Venus Davenport", "Los Angeles", "the east wind", "King Edward", "Labour", "the pen", "Kansas", "max Landis", "Celso Santebanes", "Hawaii", "Hilda", "France", "Sophocles", "Mark Cuban", "Thought Police", "a chest", "Central Park", "The Queen of Hearts", "Part 1", "Dana Matherson", "aeoline", "trumpet", "max beyond Thunderdome", "2.1 million", "Edward James Olmos", "Lynyrd Skynyrd", "Omar Bongo,", "South Africa", "Ignazio La Russa"], "metric_results": {"EM": 0.5, "QA-F1": 0.5811011904761905}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3434", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-10316", "mrqa_searchqa-validation-513", "mrqa_searchqa-validation-8138", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-1755", "mrqa_searchqa-validation-16331", "mrqa_searchqa-validation-12683", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6555", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-5516", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-11191", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-6404", "mrqa_searchqa-validation-1487", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-11157", "mrqa_searchqa-validation-1405", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-1310", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-7602", "mrqa_hotpotqa-validation-4767"], "SR": 0.5, "CSR": 0.5340909090909092, "EFR": 1.0, "Overall": 0.7670454545454546}, {"timecode": 33, "before_eval_results": {"predictions": ["the BBC", "\"nonself\" entities (pathogens, an allograft) trigger a destructive immune response", "a pool of blood beneath his head.", "hours", "28", "back at work", "Oxbow,", "201-262-2800", "opium", "\"I just think the case speaks for itself.\"", "during the campaign,", "Hussein\\'s Revolutionary Command Council", "drugs", "the Dalai Lama", "Myanmar at a demonstration in New Delhi, India", "The station", "protest child trafficking and shout anti-French slogans", "forgery and flying without a valid license,", "Little Rock, Arkansas,", "fuel economy", "environmental", "North Korea intends to launch a long-range missile in the near future,", "terrorism", "hardship for terminally ill patients and their caregivers", "different women coping with breast cancer in", "the North Korean regime intends to fire a missile toward Hawaii on July 4.", "police", "a cancer-causing toxic chemical.", "Roger Federer", "Brooklyn, New York,", "over 1000 square meters in forward deck space,", "CNN", "no chance", "St. Louis, Missouri.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "two years ago", "two", "a bald Bard with a small mustache and beard, and bags under his eyes.", "Symbionese Liberation Army", "acted in self defense in punching businessman Marcus McGhee.", "two tickets to Italy on Expedia.", "Colombia", "a welcoming, bright blue-purple", "resources", "1981", "Los Angeles", "16", "Pope Benedict XVI", "Sri Lanka,", "Appathurai", "$40 and a bread.", "African National Congress Deputy President Kgalema Motlanthe,", "the Ming dynasty", "George II ( George Augustus )", "2014 -- 15", "1919", "Javier Bardem", "Scotland", "Marco Da Silva", "American Wrestler", "Araminta Ross", "Mrs. Potts", "Peanuts Chocolate Candies", "the anthem"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6714172979797979}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.18181818181818182, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7999999999999999, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6585", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-438", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1981", "mrqa_naturalquestions-validation-7108", "mrqa_triviaqa-validation-6451", "mrqa_hotpotqa-validation-145", "mrqa_hotpotqa-validation-1903", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-10871", "mrqa_searchqa-validation-3588"], "SR": 0.578125, "CSR": 0.5353860294117647, "EFR": 1.0, "Overall": 0.7676930147058824}, {"timecode": 34, "before_eval_results": {"predictions": ["3", "the Koori", "weren't taking it well.", "Washington State's decommissioned Hanford nuclear site,", "Yemen,", "concerns expressed this week about a certain carrier based in Texas.", "nearly $2 billion", "is a businessman, team owner, radio-show host and author.", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Spaniard Carlos Moya", "Bahrain", "children of street cleaners and firefighters.", "Joan Rivers", "$3 billion,", "hardship for terminally ill patients and their caregivers", "Honduran", "Brazil", "three different videos", "strife in Somalia,", "Roy", "the WBO welterweight title", "relatives of the five suspects,", "Meredith Kercher.", "trying to save their client from the death penalty", "Alicia Keys", "a joint communique declaring Al-Shabaab \"a common enemy to both countries.\"", "Friday,", "a lump in Henry's nether regions", "20", "Matthew Fisher", "$1.5 million", "Tim Clark, Matt Kuchar and Bubba Watson", "40", "model of sustainability.", "glamour and hedonism", "J. Crew.", "Department of Homeland Security Secretary Janet Napolitano", "543", "The patient,", "Robert Gates", "Israel", "rural Tennessee.", "confirmed that Coleman, 42, was being treated there after being admitted on Wednesday.", "Seoul,", "Nicole", "Holding the Olympic medal she and her mom always wanted,", "next week.", "Adam Lambert", "Minerals Management Service Director Elizabeth Birnbaum", "early detection and helping other women cope with the disease.", "James Whitehouse,", "accused the charity of kidnapping the children and concealing their identities.", "a medium of introduction for the beginning of Buddhism in China, it gained imperial and courtly support", "Geoffrey Dyson Palmer", "Stephen Lang", "Dick Van Dyke", "Noreg", "Beer", "Revengers Tragedy", "1972", "Hilda Neihardt", "New York", "the hippopotamus", "Peter"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5686418876262627}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.375, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 0.0, 0.3636363636363636, 0.5, 0.125, 0.125, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1946", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-1829", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-2915", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-923", "mrqa_naturalquestions-validation-5809", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2276", "mrqa_hotpotqa-validation-3472", "mrqa_hotpotqa-validation-4378", "mrqa_searchqa-validation-16463", "mrqa_searchqa-validation-7879"], "SR": 0.515625, "CSR": 0.5348214285714286, "EFR": 1.0, "Overall": 0.7674107142857143}, {"timecode": 35, "before_eval_results": {"predictions": ["walked to the Surveyor, photographed it, and removed some parts which they returned to Earth.", "Border Reiver", "July 4,", "wine", "Nantucket", "an Islamic leadership position", "sap", "Malibu", "Sisyphus", "sound", "Australia", "Ayla", "Rudolf Hess", "Cubism", "Gettysburg", "Paul Simon", "a horseshoe", "Prospero", "Purple", "the Black Sea", "The Battle of the Little Bighorn", "The United Society of Believers in Christ's Second Appearing,", "a bellwether", "Time and Free Will", "chips", "Boxer", "The Spiderwick Chronicles", "Mabel Harding", "Las Vegas", "Acting out the Bible", "the Rose Bowl", "Degas", "the beehive", "light tunais", "Napa Valley", "Euro 2016", "Washington, D.C.", "Atlanta", "klezmer", "Japan", "The Bodyguard", "12 men", "Nancy Pelosi", "a journal", "Jupiter", "Sadat", "a sundae", "Grace Evans", "50 million", "Volitan Lionfish", "HIV", "The Brothers Karamazov", "Bonnie Aarons", "Wednesday, 5 September 1666", "pop ballad", "Ra ther a god of the desert and barren lands", "Lou Gehrig", "meaning and origin", "1949", "Aamir Khan", "My Gorgeous Life", "British", "High Court Judge Justice Davis", "Cipro, Levaquin, Avelox, Noroxin and Floxin."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6070772058823529}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true], "QA-F1": [0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4001", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-4034", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-1935", "mrqa_searchqa-validation-15861", "mrqa_searchqa-validation-12541", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14608", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-5061", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-821", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-2511", "mrqa_searchqa-validation-9342", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-12049", "mrqa_searchqa-validation-12977", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-7591", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-827"], "SR": 0.53125, "CSR": 0.5347222222222222, "EFR": 1.0, "Overall": 0.7673611111111112}, {"timecode": 36, "before_eval_results": {"predictions": ["lower-pressure boiler feed water", "Luzon", "Virginia", "a Ventured", "bullion", "Supernanny", "the Atlantic", "Catawba Wine", "a muezzin", "(Henry) Hudson", "a Peashooter", "dry ice", "Roosevelt", "Entourage", "a lionfish", "Philadelphia", "the Museum of Modern Art", "the Unicorn", "(John C.) Fremont", "Russia", "Peabodys", "Hermann Hesse", "the Taj Mittal", "the (Island) Three", "Carmen", "Margaret Mitchell", "Frollo", "Mark Knopfler", "Troilus", "(Bubbly)", "The Longest Yard", "the Sphinx", "Louis Armstrong", "Mecca", "American New Wave", "Arby\\'s Restaurant Group,", "coffee", "the Lgion", "Robert Burns", "The Incredible Hulk", "Atlanta", "the Memphis Belle", "Burkina Faso", "the Central Pacific", "Attorney General", "(Leifur) Eirksson", "a wolf", "Tony Reali", "Edith Piaf", "Ivan I", "a foreword", "( Karewa) soil", "Anthony Mayfield", "Jack Gleeson", "(Phil) Hurtt", "animals", "Massachusetts", "City of Starachowice", "(Charles) Laughton", "2009", "Democratic", "meteorologist", "$104,327,006", "\"17 Again,\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6223958333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6752", "mrqa_searchqa-validation-11176", "mrqa_searchqa-validation-5635", "mrqa_searchqa-validation-2964", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-10220", "mrqa_searchqa-validation-16500", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-8556", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-2262", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-3131", "mrqa_searchqa-validation-8958", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-4107", "mrqa_searchqa-validation-12396", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-5571", "mrqa_searchqa-validation-14328", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2026", "mrqa_triviaqa-validation-3956", "mrqa_hotpotqa-validation-2000", "mrqa_newsqa-validation-3951"], "SR": 0.578125, "CSR": 0.5358952702702703, "EFR": 1.0, "Overall": 0.7679476351351351}, {"timecode": 37, "before_eval_results": {"predictions": ["Liechtenstein", "impressionist", "John Y. Brown Jr.", "oats", "Mitt Romney", "Ivan", "Sally Field", "1927", "Egypt", "pi", "tin", "the Mississippi River", "Clark Griswold", "W", "Marriott", "Kimpton Hotel Monaco", "Canada", "the Secret", "gold", "collagen", "China", "a compound", "the cranes", "a claw", "Alzheimer", "the Colorado River", "Stephen F. Austin", "Euclid\\'s", "Eva Peron", "Cain", "Lou Grant", "X-Men", "the Louvre", "coho", "Prison Break", "Mercury", "Maine", "a sheep's milk cheese", "Meg", "the Sonnets", "first serve", "Ariel", "Peter Bogdanovich", "Henry White", "Jesus Christ", "BOAT PROPULSION", "the Cenozoic Era", "nolo contendere", "Jr. Walker", "Czech Republic", "Chicken of the Sea", "the NIRA", "John Ernest Crawford", "beta decay", "France", "Henry Henry", "Mariette", "Charles Quinton Murphy", "\"Sausage Party\"", "Australian", "the sins of the members of the church,", "$22 million", "\"17 Again,\"", "Nelson County"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6489583333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9798", "mrqa_searchqa-validation-15864", "mrqa_searchqa-validation-16789", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-10268", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-10441", "mrqa_searchqa-validation-15664", "mrqa_searchqa-validation-6003", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-1987", "mrqa_searchqa-validation-3594", "mrqa_searchqa-validation-5179", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-2766", "mrqa_searchqa-validation-10648", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-14545", "mrqa_searchqa-validation-12168", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6487", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900"], "SR": 0.59375, "CSR": 0.5374177631578947, "EFR": 1.0, "Overall": 0.7687088815789473}, {"timecode": 38, "before_eval_results": {"predictions": ["tuition fees", "Holden Caulfield", "Bill Hickok", "Leptospirosis", "a recession", "a mermaid", "Jay Silverheels", "Singapore", "a tank", "marimba", "a bear", "Sarah Marshall", "Witness", "Jack the Ripper", "3800", "Shirley Schmidt", "phylum", "Spain", "the brain", "William McMaster Murdoch", "Macbeth William Shakespeare", "comedy", "Mary Poppins", "sunfish", "Fresh Prince of Bel-Air", "Nod", "watermelon", "an idiomatic expression", "wedding", "Tommy", "Sherlock Holmes", "cotton candy", "Marie Antoinette", "Ford", "Marie Curie", "Roger Brooke Taney", "diagonals", "German", "Katamari Damacy", "Mark Twain", "Margaret Thatcher", "the Queen of Wellington", "Manganese", "forest", "Olympia", "Ritchie Valens", "Doctor Zhivago", "Brazil", "British", "Marlee Matlin", "Scrapple", "Oona Castilla Chaplin", "October 6, 2017", "John Cooper Clarke", "the different levels of importance of human psychological and physical needs", "one", "Tasmania", "the Wright brothers", "sexual activity", "Sam tick", "Sandro Bondi refused to attend", "voluntary manslaughter", "\"deep sorrow\"", "Pygmalion"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6302083333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.1, 1.0, 0.16666666666666669, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-16680", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-3282", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-4413", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-3430", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-8689", "mrqa_searchqa-validation-9146", "mrqa_searchqa-validation-1961", "mrqa_searchqa-validation-13348", "mrqa_searchqa-validation-14951", "mrqa_searchqa-validation-11444", "mrqa_searchqa-validation-2282", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-387", "mrqa_hotpotqa-validation-4013", "mrqa_newsqa-validation-630", "mrqa_newsqa-validation-600"], "SR": 0.5625, "CSR": 0.5380608974358974, "EFR": 1.0, "Overall": 0.7690304487179487}, {"timecode": 39, "before_eval_results": {"predictions": ["Brazil", "\"Boogie Woogie Bugle Boy\"", "Europe", "Jack Nicholson", "Glory", "Sweeney Todd", "The Bridge on the River Kwai", "The Fall of Constantinople", "gay", "Jefferson", "Ezra Pound", "the river", "a toothpick", "California", "Dixie", "a nonprofit institution that helps improve policy and decisionmaking", "Warren Harding", "engrave", "William", "Francis Crick", "Jay and Silent Bob", "Heath", "Abkhazia", "\"VIOLA\"", "Hawaii", "a key", "Tito", "conformation", "Ratatouille", "circadian rhythms", "Calvin Coolidge", "Mark Cuban", "Rudolph Giuliani", "eyes", "Tony Dungy", "a inn", "Andrew Johnson", "26", "Prince\\'s", "a herb", "chess", "GIGO", "Johannes Brahms", "Charleston", "Italian", "The Grapes of Wrath", "a bicentennial", "Byzantium", "Mayo", "Led Zeppelin", "a Tesla coil", "Denmark", "Tara / Ghost of Christmas Past", "on March 15, 1945", "Charles Darwin", "Old Trafford", "Spider-Man", "Honey Irani", "theme park", "the Kalahari Desert", "Pop superstar Rihanna", "Bob Dole", "Ben Kingsley", "managing his time"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6253720238095237}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-3741", "mrqa_searchqa-validation-6543", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-15434", "mrqa_searchqa-validation-6190", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-2211", "mrqa_searchqa-validation-11808", "mrqa_searchqa-validation-11929", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-5025", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-2876", "mrqa_searchqa-validation-7544", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-4134", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-4073"], "SR": 0.546875, "CSR": 0.53828125, "EFR": 1.0, "Overall": 0.769140625}, {"timecode": 40, "before_eval_results": {"predictions": ["63", "Baden-W\u00fcrttemberg", "James Weldon Johnson", "horror", "Oakdale", "Missouri", "the FAI Junior Cup", "Flaw", "alt-right", "the Drudge Report", "15,000", "Flavivirus", "Aubrey Posen", "1934", "a record of 13\u20133", "We Need a Little Christmas", "Tsavo East National Park", "the New York Islanders", "1345 to 1377", "nearly 80 years", "Jean Acker", "the Premier League", "the Gettysburg Address", "Whitney Houston", "the England national team", "The Rite of Spring", "1", "26,000", "Derek Jacobi", "Edwin Mah Lee", "1958", "1993", "American burlesque", "Afro-Russian", "Loretta Lynn", "Lancashire, England", "a B-17 Flying Fortress", "1994\u201395", "11", "the XXIV Summer Universiade", "2012", "1994", "Kansas City", "1999", "Pinellas County", "beer", "London", "the B-17 Flying Fortress bomber", "Mindy Kaling", "1988", "Leon Uris", "Erika Mitchell Leonard", "Mason Alan Dinehart", "Golde", "Sir Tom Finney", "Cameroon", "taking blood samples from patients and correctly cataloging them for lab analysis", "by military personnel to hazardous materials", "two", "Iggy Pop invented punk rock.", "a riddle", "the mayor of Casterbridge", "Leonardo DiCaprio", "a destructive ex-lover"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7332676820728291}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-1749", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-5344", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-4472", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-2151", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-3523", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-3552", "mrqa_newsqa-validation-1030", "mrqa_searchqa-validation-4643", "mrqa_naturalquestions-validation-6326"], "SR": 0.609375, "CSR": 0.540015243902439, "EFR": 1.0, "Overall": 0.7700076219512195}, {"timecode": 41, "before_eval_results": {"predictions": ["a interception", "10", "did not identify any of the dead.", "Les Bleus", "2005", "more than 4,000", "The Valley Swim Club", "an angry mob.", "normal maritime", "South Africa", "death", "an average of 25 percent", "fatally shooting a limo driver", "The Al Nisr Al Saudi", "as", "piano", "$250,000", "\"prostitute\"", "the mammoth\\'s skull", "tax", "Los Ticos", "acute stress disorder", "Russia and China", "Facebook and Google,", "through a facility in Salt Lake City, Utah,", "Manmohan Singh\\'s Congress party,", "Haiti", "Tuesday afternoon.", "Pakistan", "23 years.", "a head injury.", "Uzbekistan", "an open window that fits neatly around him", "Leo Frank", "(l-r) Paul McCartney", "it has witnessed only normal maritime traffic around Haiti,", "Mugabe", "don't have to visit laundromats", "one", "Diversity", "on-loan David Beckham claimed his first goal in Italian football.", "his son is fighting an unjust war for an America that went too far when it invaded Iraq", "\"Twilight\"", "forgery and flying without a valid license", "11", "A third beluga whale belonging to the world\\'s largest aquarium has died,", "Authorities in Fayetteville, North Carolina,", "The crash destroyed four homes and killed two people who lived in at least one of the homes", "al Qaeda", "Secretary of State Hillary Clinton", "Rihanna", "radius R of the turntable", "the right side of the heart to the lungs", "54 Mbit / s", "Gloucestershire", "B-24 Liberator", "most famous breakfast cereal mascot", "Oakdale", "Melbourne", "Guillermo del Toro", "stocks", "Monty Python and the Holy Grail", "Sweden", "FMCSA"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6183762851731602}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.7272727272727273, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.5, 0.19999999999999998, 0.5, 0.625, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-908", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-1659", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-5552", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-376", "mrqa_searchqa-validation-10945"], "SR": 0.484375, "CSR": 0.5386904761904762, "EFR": 1.0, "Overall": 0.7693452380952381}, {"timecode": 42, "before_eval_results": {"predictions": ["Accountants", "Arizona", "Zimbabwe", "Italian Serie A", "a sixth member of a Missouri family", "her dancing against a stripper's pole.", "the \" Michoacan Family,\"", "WTA Tour titles", "MDC head Morgan Tsvangirai.", "42", "\"I want to give peace to my nation,\"", "melt", "80 percent", "1979", "\"Follow the Sun,\"", "Elena Kagan", "CBS, CNN, Fox and The Associated Press.", "an auxiliary lock", "1-1", "AbdulMutallab", "Myanmar", "authorities", "his business dealings", "Filipino-American woman", "poems telling of the pain and suffering of children", "the program was made with the parents' full consent.", "Barack Obama", "The International Red Cross Committee, the U.N. High Commissioner for Refugees and UNICEF", "Moscow", "debris", "not guilty of affray", "capital murder and three counts of attempted murder", "Basel", "17", "she was awarded a Daytime Emmy Lifetime Achievement Award.", "state senators", "31 meters (102 feet)", "its nude beaches.", "how preachy and awkward cancer movies", "Florida girl", "shark River Park in Monmouth County", "three", "Islamabad", "partying", "Capitol Hill,", "\"From Terror to Nuclear bombs: The Significance of the Iranian Threat,\"", "1940's", "March 22,", "think are the best.", "at a depth of about 1,300 meters in the Mediterranean Sea.", "\"Antichrist\"", "a major fall in stock prices", "Thomas Jefferson", "Jeff East ( born October 27, 1957 )", "Orion", "brown", "Selfie", "23 March 1991", "England", "Los Alamos National Laboratory", "the Rat", "rain", "Crawford", "the Pyrenees"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6428662049755799}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.8, 0.5000000000000001, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.7692307692307693, 1.0, 0.5333333333333333, 0.4, 0.0, 1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-495", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-1683", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3073", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-932", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1772", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5989", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-920"], "SR": 0.484375, "CSR": 0.5374273255813953, "EFR": 1.0, "Overall": 0.7687136627906976}, {"timecode": 43, "before_eval_results": {"predictions": ["the north,", "the legitimacy of that race.", "88", "North Korea intends to launch a long-range missile in the near future,", "Kurt Cobain", "Former detainees", "33-year-old", "that these \"fusion teams,\" as they're being called, have come into effect.", "hardship for terminally ill patients and their caregivers,", "Jaime Andrade", "Zac Efron", "finance", "$2 billion", "pesos", "In 1937,", "The Arkansas weatherman", "Karthik Rajaram", "lifeless, naked body", "Robert Mugabe", "Jenny Sanford,", "Camp Lejeune, North Carolina", "Saturday.", "$1.5 million", "a violent government crackdown seeped out.", "could be secretly working on a nuclear weapon is a major development, but not one that should lead the U.S. to consider a military strike against the Tehran regime,", "the fact that the teens were charged as adults.", "death squad killings", "Elena Kagan", "Dangjin", "100 percent", "Saturday", "Pakistan's", "prisoners at the South Dakota State Penitentiary", "seven", "200", "Pakistan", "Seminole", "Rima Fakih", "President Robert Mugabe's", "Barack Obama", "helicopter and unmanned aerial vehicles from the White House", "U.S. Secretary of State Hillary Clinton,", "maintain an \"aesthetic environment\" and ensure public safety,", "165-room", "second", "Jund Ansar Allah", "1,500", "A receptionist with a gunshot wound in her stomach", "$50 less,", "$60 billion on America's infrastructure.", "amyotrophic Lateral Sclerosis", "Malayalam", "Harry", "1960 Summer Olympics in Rome", "Aston Villa Football Club", "small-holder farmer", "pool", "1822", "The Dressmaker", "Trilochanapala", "sugar", "a buffalo", "ruby red slippers", "a parietal lobe"], "metric_results": {"EM": 0.5, "QA-F1": 0.6167954441391941}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.5, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.15, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6153846153846154, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5714285714285715, 0.0, 1.0, 0.0, 0.33333333333333337, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-727", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1223", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-228", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-8741", "mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-4307", "mrqa_hotpotqa-validation-2278", "mrqa_searchqa-validation-11223", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-2281"], "SR": 0.5, "CSR": 0.5365767045454546, "EFR": 0.96875, "Overall": 0.7526633522727273}, {"timecode": 44, "before_eval_results": {"predictions": ["Bermuda 419 turf", "Los Angeles", "Chris Eubank Jr.", "Florida", "Benj Pasek and Paul", "Andes", "1952", "Angola", "19th", "January 28, 2016", "Araminta Ross", "Roger Staubach", "1944", "Highlands", "Robert Frost's former home in Franconia, New Hampshire", "Operation Watchtower", "Dan Crow", "\"War & Peace\"", "Hamilton County, Ohio", "\"What Are Little Boys Made Of?\"", "Berea College", "Omaha Nighthawks", "Call Me by Your Name", "Charmian Carr", "Germanic", "New York Islanders", "Amy Lysle Smart", "26,788", "the ethno-nationalist conflict in Northern Ireland known as the Troubles", "1967", "Marktown", "jus sanguinis", "Radcliffe College of Harvard University", "James A. Garfield", "Ford Motor Company", "its heart", "India", "German", "\"Charmed\"", "25 million", "\"The Snowman\"", "Ella Fitzgerald", "X-Men: God Loves, Man Kills", "Rain Man", "Interscope Records", "Robert Grosvenor", "4,000", "\"the most influential private citizen in the America of his day\"", "I'm Shipping Up to Boston", "American", "\"Britain's Got Talent\"", "central", "Australia's capital is Canberra, and its largest urban area is Sydney", "the beginning of the American colonies", "Nicola Adams", "whale-watching", "Russia", "cars have chosen their rides based on what their cars say about them.", "Steven Green", "in a hotel,", "Chaucer", "rattlesnakes", "suspicion", "healthy"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5443733623832308}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473684, 0.0, 0.4, 1.0, 0.0, 1.0, 0.7272727272727274, 1.0, 0.5, 1.0, 0.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1058", "mrqa_hotpotqa-validation-1815", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-733", "mrqa_hotpotqa-validation-4454", "mrqa_hotpotqa-validation-44", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-4325", "mrqa_hotpotqa-validation-4978", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-2355", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-5", "mrqa_hotpotqa-validation-4828", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-8356", "mrqa_naturalquestions-validation-1433", "mrqa_triviaqa-validation-3532", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2515", "mrqa_searchqa-validation-12418", "mrqa_searchqa-validation-13986", "mrqa_searchqa-validation-4414"], "SR": 0.453125, "CSR": 0.5347222222222222, "EFR": 0.9714285714285714, "Overall": 0.7530753968253968}, {"timecode": 45, "before_eval_results": {"predictions": ["Kelvin Benjamin", "murder in the beating death of a company boss who fired them.", "Indian Ocean waters", "30", "crocodile eggs", "Colorado prosecutor", "Polis", "on Saturday.", "Haiti", "in July for A Country Christmas,", "sniff out cell phones.", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Cain", "\"17 Again,\"", "intelligence community does not believe North Korea intends to launch a long-range missile in the near future,", "Raja Casablanca", "Mitt Romney", "two years ago.", "businessman", "Picasso's muse and mistress, Marie-Therese Walter.", "low-calorie meals", "Heshmatollah Attarzadeh", "the iReport form", "government", "Nine out of 10 children", "police", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "the jaws of a crocodile", "a bronze medal", "killed at least 63 people and wounded more than 200.", "Congress", "Susan Boyle", "military ID cards", "Phillip A. Myers.", "Obama's", "King Gyanendra,", "homicide by undetermined means,", "Casey Anthony, 22,", "officers at a Texas  airport", "Arnoldo Rueda Medina.", "UNICEF", "the couple's surrogate", "228", "Kerstin and two of her brothers,", "2004.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Donald Trump and Joan Rivers", "supermodel and philanthropist", "Jacob Zuma,", "in the Oaxacan countryside of southern Mexico", "Arsene Wenger", "slavery", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan ), and grandmother Mo ( Laila Morse )", "Latin liberalia studia", "women", "Johnny Mathis", "Beverly Hills Cop", "Champion Jockey", "Luca Guadagnino", "Sleepy Brown", "Maya Angelou", "February", "a jigger", "RAF bombers"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6660304607673029}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5263157894736842, 0.6666666666666666, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.7499999999999999, 0.5714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-4013", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-134", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-994", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-319", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-1360", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-4", "mrqa_searchqa-validation-2431", "mrqa_triviaqa-validation-7461"], "SR": 0.515625, "CSR": 0.5343070652173914, "EFR": 1.0, "Overall": 0.7671535326086957}, {"timecode": 46, "before_eval_results": {"predictions": ["acular", "bipartisan", "Nirvana", "\"He's the most uniquely giving, loving, happy man,\"", "without bail and will be arraigned June 25,", "12.3 million", "Mexico", "United", "Michael Arrington,", "Brett Cummins,", "Indian army troopers,", "Saturday,", "Nicole", "the legitimacy of that race.", "Adidas", "Dennis Davern,", "Africa", "American", "bartering -- trading goods and services without exchanging money", "Wednesday.", "promise to improve health and beauty.", "Chinese", "Newcastle", "Nothing But Love", "allegedly involved in forged credit cards and identity theft", "June 6, 1944,", "the Arab world", "2-1", "October 19,", "\"It was a wrong thing to say,", "Seoul,", "fuel economy and safety", "ALS6,", "eight", "Siri.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "246", "Grayback forest-", "the children of street cleaners and firefighters.", "North Korea intends to launch a long-range missile in the near future,", "a U.S. helicopter crashed in northeastern Baghdad as", "attempting illegal crossings", "The American Civil Liberties Union", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place,", "38,", "Her husband and attorney, James Whitehouse,", "blacks, Hispanics and whites", "three", "most gigantic pumpkins in the world,", "cancer,", "two", "Arnold Schoenberg", "Brooklyn, New York", "Jean Fernel", "the world of Discworld", "Japan", "fox hunting", "New York", "travel", "16,116", "\"Juno\"", "makijas", "bumblebee", "Rowan Blanchard"], "metric_results": {"EM": 0.671875, "QA-F1": 0.751905564995068}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.8, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.04761904761904762, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-89", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1764", "mrqa_triviaqa-validation-1729", "mrqa_hotpotqa-validation-2280", "mrqa_searchqa-validation-11573"], "SR": 0.671875, "CSR": 0.5372340425531915, "EFR": 1.0, "Overall": 0.7686170212765957}, {"timecode": 47, "before_eval_results": {"predictions": ["Corendon Airlines", "A Rush of Blood to the Head", "5", "Chicago", "The Wind's Twelve Quarters", "child actor", "Dennis H. Kux", "drawing the name out of a hat.", "Brett Ryan Eldredge", "I-League", "two or three", "Badfinger", "Lady Frederick Windsor", "point-coloration pattern", "1853", "1983", "Citizens for a Sound Economy", "2027 Fairmount Avenue", "1930 American Pre-Code musical film directed by John Francis Dillon and filmed entirely in Technicolor.", "5,112", "1979", "retail, office and residential", "14,677", "6'5\"", "country singer Mickey Gilley's", "Switzerland\u2013European Union relations", "a puppy", "Mexican", "December 24, 1973", "1933", "the backside", "Kristoffer Rygg", "1733\u20131811", "London Luton Airport", "the Salzburg Festival", "McComb, Mississippi", "Afghanistan", "1959", "Imelda Marcos", "Randall Boggs", "a Passion", "Boston", "the European or Eurasian cave lion", "the Royal Navy", "World War II", "Knoxville, Tennessee", "Three's Company", "Doomtree", "Labour", "\"Linda McCartney's Life in Photography\"", "Erich Maria Remarque", "September 14, 2008", "79", "Buffalo Bill", "Romania", "the James Gang", "Mt Kenya", "Aung San Suu Kyi", "Afghan National Security Forces", "Her husband and attorney, James Whitehouse,", "Cairo", "Secretariat", "lobbies", "the Bank of England"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6392609126984127}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5349", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-621", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-1466", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-2053", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-5435", "mrqa_hotpotqa-validation-5531", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-5309", "mrqa_searchqa-validation-6735", "mrqa_searchqa-validation-10434", "mrqa_triviaqa-validation-2701"], "SR": 0.5625, "CSR": 0.5377604166666667, "EFR": 1.0, "Overall": 0.7688802083333334}, {"timecode": 48, "before_eval_results": {"predictions": ["ragweed", "St Petersburg", "tuna", "an offensive", "Vulcan", "Citation", "Fawn Hall", "waive", "kenya", "Barnum & Bailey Circus", "Johnny Weissmuller", "cathode", "a torque wrench", "the gold discovery", "Marlon Brando", "Citation", "impressionist", "the University of Kentucky", "reddish", "Brussels", "Macbeth", "General Lee", "$18.2 billion", "Fyodor Dostoevsky", "Martin Luther", "Clue", "Sir Arthur Conan Doyle", "German", "Andrew Johnson", "seven", "Mike Connors", "The Legend of Tarzan", "Jim Inhofe", "sancire", "Corpus Christi", "Africa", "the ostrich", "a preamble", "a night shift", "kenya", "Desperate Housewives", "Galileo Galilei", "Canada", "Anne Hathaway", "a strike", "the bat", "West Virginia", "Thomas Jefferson", "movie theater", "Citation", "critic", "Khrushchev", "1904", "Maganlal Daiya", "Bobby Tambling", "ambidextrous", "chariots", "Humberside Airport", "more than 265 million", "100 million", "freezing gasoline prices for the rest of the year and lowering natural gas prices by 10 percent.", "a head injury.", "Pope Benedict XVI", "Charles II"], "metric_results": {"EM": 0.5, "QA-F1": 0.582040513833992}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-599", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-507", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-12540", "mrqa_searchqa-validation-11080", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-3259", "mrqa_searchqa-validation-5735", "mrqa_searchqa-validation-15736", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-4314", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-9299", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-9942", "mrqa_searchqa-validation-16389", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-14589", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-1836", "mrqa_hotpotqa-validation-2171", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-1663"], "SR": 0.5, "CSR": 0.5369897959183674, "EFR": 1.0, "Overall": 0.7684948979591837}, {"timecode": 49, "before_eval_results": {"predictions": ["the NSA", "Heisman", "Brandi Chastain", "the Colorado River", "Pamela Anderson", "carioca", "Treasure Island", "Pocahontas", "improv", "(Whizzer) White", "a guitar", "an aerosol mist", "Great American Novel", "Ferris B Mueller", "Joseph Campbell", "Margaret Mitchell", "Charles Busch", "a draft horse", "Ernest Lawrence", "a rodeo", "a fresco", "Nevil Shute", "(Ulysses) Grant", "Jesse Jackson", "Tudor", "Department of Homeland Security", "the Black Sea", "a leotard", "Bulworth", "the small intestine", "the mouthpiece", "Cuba", "Sam", "MCA", "repellents", "Manhattan", "February 2", "Leontyne Price", "a composting material", "Lauren Hutton", "Christopher Columbus", "Phil Mickelson", "(Stephen) Bradshaw", "the Castalian Spring", "Hungary", "a burnus", "Philadelphia", "peanut butter", "Edgar Allan Poe", "cork", "Lex Luthor", "food and clothing", "( Schwarzenegger) Jehnna ( Olivia d'Abo", "Master Christopher Jones", "Hebrew", "Cowdenbeath", "St Moritz", "October", "Drifting", "Ellesmere Port, United Kingdom", "an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "three", "poems", "\"Mechte Navstrechu\""], "metric_results": {"EM": 0.53125, "QA-F1": 0.6192708333333333}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.13333333333333333, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9727", "mrqa_searchqa-validation-6040", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-8249", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-10212", "mrqa_searchqa-validation-10510", "mrqa_searchqa-validation-4813", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-1364", "mrqa_searchqa-validation-8175", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-3195", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-1897", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7715", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-1028", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-3602", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1301", "mrqa_hotpotqa-validation-116"], "SR": 0.53125, "CSR": 0.536875, "EFR": 1.0, "Overall": 0.7684375}, {"timecode": 50, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1046", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1241", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-1483", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1704", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2118", "mrqa_hotpotqa-validation-2280", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2387", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2865", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-323", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3527", "mrqa_hotpotqa-validation-357", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-4145", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4370", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-4638", "mrqa_hotpotqa-validation-4853", "mrqa_hotpotqa-validation-4881", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5103", "mrqa_hotpotqa-validation-5300", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-586", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-91", "mrqa_hotpotqa-validation-97", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1714", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2635", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-3641", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7108", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7527", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7930", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8306", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-9447", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1061", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1134", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-121", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1268", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1414", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1631", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-1805", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-2158", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-228", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3186", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3381", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3483", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-363", "mrqa_newsqa-validation-3646", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3747", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3783", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3951", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-4083", "mrqa_newsqa-validation-4090", "mrqa_newsqa-validation-4135", "mrqa_newsqa-validation-423", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-511", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-622", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-712", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-974", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10042", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10175", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10501", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-10941", "mrqa_searchqa-validation-11328", "mrqa_searchqa-validation-11492", "mrqa_searchqa-validation-11686", "mrqa_searchqa-validation-1175", "mrqa_searchqa-validation-11948", "mrqa_searchqa-validation-1197", "mrqa_searchqa-validation-12123", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-12269", "mrqa_searchqa-validation-12405", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-13072", "mrqa_searchqa-validation-13226", "mrqa_searchqa-validation-13240", "mrqa_searchqa-validation-13458", "mrqa_searchqa-validation-13875", "mrqa_searchqa-validation-1393", "mrqa_searchqa-validation-13989", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-14624", "mrqa_searchqa-validation-14703", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-15062", "mrqa_searchqa-validation-15112", "mrqa_searchqa-validation-15176", "mrqa_searchqa-validation-15186", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-1530", "mrqa_searchqa-validation-15354", "mrqa_searchqa-validation-15436", "mrqa_searchqa-validation-15556", "mrqa_searchqa-validation-16418", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-16666", "mrqa_searchqa-validation-16725", "mrqa_searchqa-validation-16842", "mrqa_searchqa-validation-1695", "mrqa_searchqa-validation-205", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-219", "mrqa_searchqa-validation-2257", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-2376", "mrqa_searchqa-validation-239", "mrqa_searchqa-validation-2453", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-255", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-3011", "mrqa_searchqa-validation-306", "mrqa_searchqa-validation-3179", "mrqa_searchqa-validation-3242", "mrqa_searchqa-validation-3344", "mrqa_searchqa-validation-3394", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3496", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-3952", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-4604", "mrqa_searchqa-validation-4643", "mrqa_searchqa-validation-4650", "mrqa_searchqa-validation-4891", "mrqa_searchqa-validation-5194", "mrqa_searchqa-validation-5318", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5862", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-6162", "mrqa_searchqa-validation-6219", "mrqa_searchqa-validation-6241", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6675", "mrqa_searchqa-validation-6718", "mrqa_searchqa-validation-6764", "mrqa_searchqa-validation-6959", "mrqa_searchqa-validation-6991", "mrqa_searchqa-validation-7049", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-7379", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-7557", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7620", "mrqa_searchqa-validation-7780", "mrqa_searchqa-validation-7879", "mrqa_searchqa-validation-8503", "mrqa_searchqa-validation-8505", "mrqa_searchqa-validation-855", "mrqa_searchqa-validation-8597", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-9107", "mrqa_searchqa-validation-9296", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-945", "mrqa_searchqa-validation-9496", "mrqa_searchqa-validation-9810", "mrqa_searchqa-validation-9903", "mrqa_squad-validation-1002", "mrqa_squad-validation-10020", "mrqa_squad-validation-10100", "mrqa_squad-validation-10186", "mrqa_squad-validation-10254", "mrqa_squad-validation-10306", "mrqa_squad-validation-1146", "mrqa_squad-validation-1204", "mrqa_squad-validation-1506", "mrqa_squad-validation-1758", "mrqa_squad-validation-1906", "mrqa_squad-validation-1943", "mrqa_squad-validation-1960", "mrqa_squad-validation-2059", "mrqa_squad-validation-2225", "mrqa_squad-validation-2351", "mrqa_squad-validation-2466", "mrqa_squad-validation-2487", "mrqa_squad-validation-2530", "mrqa_squad-validation-2880", "mrqa_squad-validation-298", "mrqa_squad-validation-3265", "mrqa_squad-validation-3279", "mrqa_squad-validation-3703", "mrqa_squad-validation-3840", "mrqa_squad-validation-4047", "mrqa_squad-validation-4290", "mrqa_squad-validation-4315", "mrqa_squad-validation-4330", "mrqa_squad-validation-4353", "mrqa_squad-validation-4415", "mrqa_squad-validation-4455", "mrqa_squad-validation-4468", "mrqa_squad-validation-4517", "mrqa_squad-validation-4524", "mrqa_squad-validation-4673", "mrqa_squad-validation-4759", "mrqa_squad-validation-4812", "mrqa_squad-validation-4876", "mrqa_squad-validation-4998", "mrqa_squad-validation-5010", "mrqa_squad-validation-5170", "mrqa_squad-validation-549", "mrqa_squad-validation-5568", "mrqa_squad-validation-5581", "mrqa_squad-validation-5643", "mrqa_squad-validation-5812", "mrqa_squad-validation-5917", "mrqa_squad-validation-6106", "mrqa_squad-validation-6176", "mrqa_squad-validation-6218", "mrqa_squad-validation-6282", "mrqa_squad-validation-6547", "mrqa_squad-validation-6645", "mrqa_squad-validation-6694", "mrqa_squad-validation-670", "mrqa_squad-validation-6741", "mrqa_squad-validation-6797", "mrqa_squad-validation-6801", "mrqa_squad-validation-6842", "mrqa_squad-validation-6927", "mrqa_squad-validation-6941", "mrqa_squad-validation-7035", "mrqa_squad-validation-7069", "mrqa_squad-validation-7159", "mrqa_squad-validation-7674", "mrqa_squad-validation-7674", "mrqa_squad-validation-7757", "mrqa_squad-validation-7790", "mrqa_squad-validation-7818", "mrqa_squad-validation-7855", "mrqa_squad-validation-7937", "mrqa_squad-validation-8047", "mrqa_squad-validation-8503", "mrqa_squad-validation-8651", "mrqa_squad-validation-8733", "mrqa_squad-validation-8745", "mrqa_squad-validation-8833", "mrqa_squad-validation-8836", "mrqa_squad-validation-8896", "mrqa_squad-validation-9080", "mrqa_squad-validation-910", "mrqa_squad-validation-9170", "mrqa_squad-validation-9270", "mrqa_squad-validation-9298", "mrqa_squad-validation-9311", "mrqa_squad-validation-9398", "mrqa_squad-validation-940", "mrqa_squad-validation-9411", "mrqa_squad-validation-9543", "mrqa_squad-validation-9726", "mrqa_squad-validation-9752", "mrqa_squad-validation-9815", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1474", "mrqa_triviaqa-validation-1546", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-1611", "mrqa_triviaqa-validation-1729", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-1928", "mrqa_triviaqa-validation-1959", "mrqa_triviaqa-validation-1989", "mrqa_triviaqa-validation-210", "mrqa_triviaqa-validation-2997", "mrqa_triviaqa-validation-3020", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3821", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-42", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-492", "mrqa_triviaqa-validation-5445", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-6176", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6824", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-803", "mrqa_triviaqa-validation-993"], "OKR": 0.853515625, "KG": 0.496875, "before_eval_results": {"predictions": ["Fatih Ozmen", "the 850", "Skyscraper", "Stingray", "Norway", "Tom Jones", "VIMN Russia", "Homer Hickam, Jr.", "2015", "Hilo", "Robert Downey, Jr.", "Continental AG", "band director", "Visigoths", "Anaheim", "Reinhard Heydrich", "Big Ben", "Standard Oil", "The Longest Yard", "Chiwetel Ejiofor", "Guggenheim Partners", "19th-century", "Lady Antebellum", "HackThis Site", "George H. W. Bush", "Tottenham Hotspur", "1958", "Vixen", "Forbidden Quest", "Rymill Park", "balloon Street, Manchester", "May 1, 2011", "Santa Fe", "Social Cohesion", "Adelaide Lightning", "Operation Neptune", "Walter R\u00f6hrl", "Lonely", "ten", "Diamond White", "Ferrara", "agricultural", "Indooroopilly", "2006", "Matt Flynn", "Indian", "hamburgers", "England", "pasta", "Luigi Segre", "the legislature", "February 16, 2018", "1980", "Nacio Herb Brown", "Geoff Hurst", "Precambrian", "Mull", "his death cast a shadow over festivities", "\"The three gunshot wounds to the head included two nonfatal rounds with entry points below the chin, and one fatal shot that entered Peterson through the right side of the head,\"", "a tracheotomy", "Paul Newman", "Puccini", "John Candy", "milk and honey"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5260416666666666}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-4170", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-221", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4995", "mrqa_hotpotqa-validation-3399", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3759", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2260", "mrqa_hotpotqa-validation-3352", "mrqa_hotpotqa-validation-4406", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-5600", "mrqa_triviaqa-validation-4774", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-355", "mrqa_searchqa-validation-13015"], "SR": 0.453125, "CSR": 0.5352328431372548, "EFR": 1.0, "Overall": 0.714624693627451}, {"timecode": 51, "before_eval_results": {"predictions": ["1898", "a Native American", "The Chiltern Shakespeare Company", "1898", "Stacey Kent", "1970s", "Arthur Freed", "Elizabeth Keka\u02bbaniau La\u02bbanui", "Gothic Revival", "Buffalo", "J. Robert Oppenheimer", "George Timothy Clooney", "January 4, 1976", "237 square miles", "11,163", "an album", "its air-cushioned sole", "the White Knights of the Ku Klux Klan", "WikiLeaks", "Nine-card Brag", "Montana State University", "American rock band", "the Wikimedia Foundation", "Flashback: The Quest for Identity in the United States", "ARY Films", "1987", "dementia", "two Grammy awards in 2001 for Best Traditional Pop Vocal Album and Best Instrumental Arrangement Accompanying Vocalist(s)", "Port of Boston", "Switzerland", "Las Vegas", "1961", "Rochdale, North West England", "the Israeli Declaration of Independence", "1971", "Blue Origin", "Target Corporation", "small forward", "2012", "United States", "Robert Sargent Shriver Jr.", "35", "Mark Neary Donohue Jr.", "Peach", "Italian", "Richard Price", "Archie Andrews", "George Lawrence Mikan", "June 11, 1986", "2018\u201319 UEFA Europa League", "Magdalen College", "Kanab, Utah", "Malvolio", "the Royal Air Force", "Separate Tables", "devonian", "devonian", "near the Somali coast", "Daytime Emmy Lifetime Achievement Award.", "October 9.", "Joseph Holt", "hunter sauce", "The Quest of Erebor", "carbon"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6236556412337662}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.375, 0.5, 1.0, 0.1818181818181818, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1884", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-1055", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-2654", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4525", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-2803", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-950", "mrqa_triviaqa-validation-2289", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-6747", "mrqa_searchqa-validation-1741"], "SR": 0.5625, "CSR": 0.5357572115384616, "EFR": 1.0, "Overall": 0.7147295673076923}, {"timecode": 52, "before_eval_results": {"predictions": ["My ntonia", "Henry VIII", "lead", "the Rose Bowl", "a 747", "amber", "Denmark", "terriers", "Katrina & the Waves", "Jerusalem", "freestyle", "Great DOG'S LIFE", "Ustilaginales", "Stargate Stargate", "Lou Reed", "Stonewall Jackson", "Norway", "Emma Peel", "canvas", "eucalyptus", "The X-Files", "Frankie Muniz", "the undersea world", "the Great Lakessome", "Captain Charles Barbier", "kinetic", "Santera", "Richard Bach", "a Statue of Liberty", "quicksand", "The Return of the Native", "Welcome, You've Got Mail, \"You've got pictures,\"", "Pop-Tarts", "Minnesota", "the San Antonio River", "a cornucopia", "Bob Fosse", "Ankara", "condensation", "the Essay", "Uberti Winchester", "Chinese", "The Larry Sanders Show", "The Virgin Spring", "Like Water for Chocolate", "NigerCongo", "Applebee's", "John Tyler", "Daniel Craig", "humility", "computer programming", "Norway and at the Isle of Sheppey in England", "A footling breech", "when mixing solvents or changing their temperature", "Doctor Zhivago", "Bishopston", "The Harvest of a Quiet Eye", "Pan Am Railways", "Berthold Heinrich K\u00e4mpfert", "1961", "South America and Africa.", "It was glamorous, sexy and international.", "fake his own death by crashing his private plane into a Florida swamp.", "the Stockton & Darlington Railway"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6085851648351648}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false], "QA-F1": [0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6153846153846153, 0.30769230769230765, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-8158", "mrqa_searchqa-validation-5567", "mrqa_searchqa-validation-4668", "mrqa_searchqa-validation-7184", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-2922", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-15252", "mrqa_searchqa-validation-6879", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-6128", "mrqa_searchqa-validation-4716", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-10899", "mrqa_searchqa-validation-16650", "mrqa_searchqa-validation-4954", "mrqa_searchqa-validation-15500", "mrqa_searchqa-validation-3189", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-2965", "mrqa_triviaqa-validation-4700", "mrqa_triviaqa-validation-5698", "mrqa_hotpotqa-validation-4336", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-1004", "mrqa_triviaqa-validation-5426"], "SR": 0.515625, "CSR": 0.5353773584905661, "EFR": 1.0, "Overall": 0.7146535966981132}, {"timecode": 53, "before_eval_results": {"predictions": ["Michael Rosen", "Saint Etienne", "After Shawn's kidnapping", "to manage the characteristics of the beer's head", "early evenings to call ( in spring and summer ) and hunt for food", "an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "birch", "slide clips, slide clamps or a cross-table", "Gary Player", "Nicolas Anelka", "In his first appearance, he flirts with Meredith Grey, and Derek punches him in the face", "The set of chromosomes in a cell makes up its genome ; the human genome has approximately 3 billion base pairs of DNA arranged into 46 chromosomes", "maintains a global crowdfunding platform focused on creativity and merchandising", "warm and is considered to be the most comfortable climatic conditions of the year", "the head", "the Atlantic coast of Africa", "Madison, Wisconsin", "September 1972", "2017", "Gustav Bauer", "detritus", "the magnetic stripe `` anomalies '' on the ocean floor", "126", "Brooke Wexler", "Alice Cooper", "1961", "111", "Brazil, Turkey and Uzbekistan", "a dromedary", "13", "the five - year time jump", "a compound sentence", "Kelly Osbourne, Ian `` Dicko '' Dickson, Eddie Monk and Eddie Perfect", "the Coriolis force", "the five - year time jump", "James Rodr\u00edguez", "Kristy Swanson", "James Madison", "the NFL", "Daya Jethalal Gada", "over 74", "warning signs", "various submucosal membrane sites of the body", "noble gas", "the Department of Health and Human Services", "four distinct levels", "Pheoby Watson", "Justin Timberlake", "1966", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "September 2017", "Hercule Poirot", "Paul Gauguin", "Islamophobia", "creeks, fringing the southwest mouth of Lagos Lagoon, while protected from the Atlantic Ocean", "Martin Scorsese", "Ian Fleming", "well over 1,000 pounds).", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "off the coast", "the Northwest Territories", "Chekhov", "a robe", "the death of a pregnant soldier whose body was found Saturday morning in a motel,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5767736966028675}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.7692307692307692, 0.08333333333333333, 0.9, 1.0, 0.0, 1.0, 0.0, 0.0, 0.23076923076923078, 0.9411764705882353, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 0.4615384615384615, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.41379310344827586, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.3076923076923077, 0.4444444444444445, 1.0, 1.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-2498", "mrqa_naturalquestions-validation-10292", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-388", "mrqa_naturalquestions-validation-8483", "mrqa_triviaqa-validation-4748", "mrqa_hotpotqa-validation-3974", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-5562", "mrqa_searchqa-validation-8459", "mrqa_newsqa-validation-2516"], "SR": 0.421875, "CSR": 0.533275462962963, "EFR": 0.9459459459459459, "Overall": 0.7034224067817818}, {"timecode": 54, "before_eval_results": {"predictions": ["Oona Castilla Chaplin", "Mike Czerwien", "`` Psychomachia, '' an epic poem written in the fifth century", "Texas - style chili con carne, nachos, hard tacos and fajitas", "George Harrison", "Kanawha Rivers", "1803", "President pro tempore of the Senate", "3000 BC", "password recovery tool for Microsoft Windows", "Charlotte Thornton", "Western Australia", "Buffalo Bill", "May 3, 2005", "Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "California, Utah and Arizona", "Hem Chandra Bose", "1773", "John J. Flanagan", "1988", "judicial officer, of a lower or puisne court, elected or appointed by means of a commission ( letters patent ) to keep the peace", "at slightly different times when viewed from different points on Earth", "Jeff East", "Charlene Holt", "1969", "May 31, 2012", "Sets heart in mediastinum and limits its motion", "Alex Skuby", "Matt Monro", "12.65 m ( 41.5 ft )", "The management team", "1999", "supervillains", "the courts", "Malvolio", "Coldplay", "Arkansas", "JNCASR", "Lawrence John Wargrave", "Atlanta", "22", "Helena", "Joseph Sherrard Kearns", "Ron Hicklin Singers", "Michael Phelps", "Taron Egerton", "Joe Pizzulo", "John F. Kelly", "cylinder of glass or plastic that runs along the fiber's length", "Transvaginal ultrasonography", "741 weeks", "Zimbabwe", "Royal Albert Hall", "Sarah Palin", "Tampa", "Battle of Prome", "kitty Hawk", "John Lennon and George Harrison", "China's government accused the ship of violating Chinese and international laws during its patrols,", "city of romance, of incredible architecture and history.", "Tater Tots", "Yemen", "Q.E.D.", "the Dalton Gang"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6706532897249808}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 1.0, 0.058823529411764705, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.060606060606060615, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.25, 1.0, 1.0, 0.4, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.8571428571428572, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-440", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-7692", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-370", "mrqa_naturalquestions-validation-10598", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-940", "mrqa_newsqa-validation-3310", "mrqa_searchqa-validation-16172", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-8575"], "SR": 0.546875, "CSR": 0.5335227272727272, "EFR": 0.896551724137931, "Overall": 0.6935930152821317}, {"timecode": 55, "before_eval_results": {"predictions": ["Leona Stevenson ( Barbara Stanwyck ) is the spoiled, bedridden daughter of wealthy businessman James Cotterell ( Ed Begley )", "Matt Monro", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "Aristotle", "Reproductive system", "Peter Andrew Beardsley MBE", "the American frigate USS Chesapeake", "Celtic", "Columbia River Gorge", "the Northeast Monsoon", "2013", "Jim Photoglo", "biosphere ( living and organic material ), such as forests and animals, and the materials that can be obtained from them", "2015", "Neil Young", "the closing of the atrioventricular valves and semilunar valves, respectively", "minimum viable product that addresses and solves a problem or need that exists", "London", "Robert Cappucci", "Hem Chandra Bose", "Ernest Rutherford", "depolarization of the cardiac muscle begins at the sinus node", "a strong, weight transferral synovial plane joint", "png HTTP / 1.1", "the Brewster family, descended from the Mayflower", "1 mile ( 1.6 km )", "pop ballad", "8 December 1985", "during meiosis", "2005", "Arnold Schoenberg", "an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "Orographic lift", "the books of Exodus and Deuteronomy", "Scarlett Johansson", "Holiday Inn Hotels & Resorts", "Benzodiazepines", "Steve Valentine", "John J. Flanagan", "dead stratified squamous, keratinized epithelial cells", "2007", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "10,605", "Tom Thornton", "Sebastian Vettel", "Alamodome in San Antonio, Texas", "Meg Optimus", "biological taxonomy", "the national or royal anthem in a number of Commonwealth realms, their territories, and the British Crown Dependencies", "pathology", "Tevin Campbell", "My Big Fat Gypsy Wedding", "Roger Casement", "James Garner", "Oxford, UK", "Colonel Patrick John Mercer, OBE", "Robert Matthew Hurley", "urged more help for military members, especially for those returning from war.", "five", "The TNT series", "rastislav", "Madonna", "Eiffel Tower", "Aaron Hall"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5386869465528147}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.7096774193548387, 1.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1290322580645161, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.09523809523809522, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-4470", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-9047", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-4149", "mrqa_naturalquestions-validation-46", "mrqa_triviaqa-validation-1894", "mrqa_hotpotqa-validation-5371", "mrqa_hotpotqa-validation-2296", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-104", "mrqa_searchqa-validation-1762", "mrqa_searchqa-validation-14136", "mrqa_hotpotqa-validation-3771"], "SR": 0.46875, "CSR": 0.5323660714285714, "EFR": 0.9705882352941176, "Overall": 0.7081689863445378}, {"timecode": 56, "before_eval_results": {"predictions": ["Gerald Ford", "World War II", "Harishchandra", "16", "1877", "1999", "Old Trafford", "Tami Lynn", "U + 2234 \u2234 therefore ( HTML & # 8756 ; &there4 ; )", "the United States", "Max", "April 13, 2018", "Jenna Boyd", "Spencer Treat Clark", "Sedimentary rock", "Theodore Roosevelt", "Nepal", "Jurriaen Aernoutsz", "4 September 1936", "hydrogen", "1940", "Authority", "April 1st", "DJ Lance Rock", "noon Eastern Time", "Francisco Pizarro", "a habitat", "Ben Faulks", "Lady Gaga", "negatively affect a person's personal, work, or school life, as well as sleeping, eating habits, and general health", "1989", "Liam Cunningham", "Lorenzo Lamas", "Walter Pauk", "1980", "septum", "Buddhism", "the forex market", "Frankie Laine", "Sir Ernest Rutherford", "Nigel Lythgoe", "late - night", "gastrocnemius muscle", "Art Carney", "introduced and elaborated as early as in 1651 by Thomas Hobbes in his Leviathan, though with a somewhat different meaning ( similar to the meaning used by the British associationists )", "March 26, 1973", "1986", "on location", "President Lyndon Johnson", "the formation of two endocardial tubes which merge to form the tubular heart", "a Nativity scene", "1840", "2007", "Branson", "first baseman", "Tumi Holdings", "Loch Shiel", "Ozzy Osbourne", "Polo because \"it was the sport of kings.", "Terra Firma", "ego", "Nova Scotia", "Isaac Newton", "love Letter"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7506690428187404}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19047619047619047, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.32258064516129037, 1.0, 1.0, 1.0, 1.0, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-305", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-7486", "mrqa_hotpotqa-validation-3278", "mrqa_hotpotqa-validation-5421", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2096"], "SR": 0.6875, "CSR": 0.5350877192982456, "EFR": 0.95, "Overall": 0.7045956688596491}, {"timecode": 57, "before_eval_results": {"predictions": ["the Old French tailleur ( `` cutter '' ) in France", "Boston - by - the - Sea, Massachusetts", "The Golden Gate Bridge", "a numeric scale used to specify the acidity or basicity of an aqueous solution", "a house edge of between 0.5 % and 1 %", "the Infamy Speech of US President Franklin D. Roosevelt", "the coffee shop Monk's", "Fats Waller", "the January 2017 patch", "Ozzie Smith", "Mark Jackson", "1983", "Resident Commissioner", "by January 2018", "all land - living organisms, both alive and dead, as well as carbon stored in soils", "September 30", "pigs", "Gerald Ford", "September 8, 2017", "1998", "political ideology", "Spektor", "converging lenses", "the nucleus", "1973", "October 12, 2017", "Hans Raffert 1988 -- 1992", "a key signature at the beginning to designate the pitches that make up that scale", "a thirty - second call to one of a number of friends ( who provide their phone numbers in advance )", "to refer to a god of the Ammonites, as well as Tyrian Melqart", "P.V. Sindhu", "Carpenter", "Asuka", "Wilt Chamberlain", "Scorpions", "Uzbekistan", "UNESCO / ILO Recommendation concerning the Status of Teachers", "a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "Rich Mullins", "during prenatal development in the central part of each developing bone", "skeletal muscle", "Kix Brooks", "Ireland's Johnny Logan", "Felicity Huffman", "In 1908", "Sir Henry Cole", "the fictional town of West Egg on prosperous Long Island", "Eukarya", "commemorating fealty and filial piety", "Luigi Pirandello", "Russ Conway", "liver", "Heineken International", "\"Irish Chekhov\"", "Charlie Wilson", "Gov. Mark Sanford,", "Lance Cpl. Maria Lauterbach", "step up.", "the Prohibition era", "Joe Louis", "Richard Cory", "Mayan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6168681805726244}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false], "QA-F1": [0.5, 0.75, 1.0, 0.14285714285714288, 0.7272727272727273, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.2222222222222222, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6956521739130436, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.8823529411764706, 0.0, 0.9600000000000001, 1.0, 0.33333333333333337, 0.6666666666666666, 0.25, 0.5714285714285715, 1.0, 0.6666666666666666, 0.3, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8858", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-9251", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-9687", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4497", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-3697", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-3189", "mrqa_hotpotqa-validation-572", "mrqa_hotpotqa-validation-4873", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-2524", "mrqa_searchqa-validation-6103", "mrqa_searchqa-validation-5902"], "SR": 0.453125, "CSR": 0.5336745689655172, "EFR": 0.9142857142857143, "Overall": 0.6971701816502464}, {"timecode": 58, "before_eval_results": {"predictions": ["William Wyler", "Mercedes -Benz G - Class", "1986", "Idaho", "July 18, 2013", "Deuteronomy", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "1979 -- 80", "a patronymic surname", "lithium - ion batteries", "the Reverse - Flash", "Los Angeles, California", "the thirteen American colonies", "circular", "Lady Arbuthnot's Chamber", "Nebuchadnezzar", "Eddie Murphy", "17 - year - old", "1923", "membranes that envelop the brain and spinal cord", "Seattle, Washington", "October 1898", "Yosemite National Park", "Ewan McGregor", "LED illuminated display", "roasted turkey", "1917", "January 2004", "Christy Plunkett ( Anna Faris )", "smen", "Mount Sinai", "Macon Blair", "the genome", "each state's DMV, which is required to drive", "Convention was founded with the dual purpose of abolishing the monarchy and drafting a new constitution", "four", "a divergent tectonic plate boundary", "Steve Russell", "to either peace between two entities ( especially between man and God or between two countries ), or to the well - being, welfare or safety of an individual or a group of individuals", "New York University", "into the intermembrane space", "Northeast Monsoon or Retreating Monsoon", "began on 13 February", "276", "the early 1960s", "President Yahya Khan", "Thespis", "Madison Square Park in Manhattan", "Wednesday, 5 September 1666", "2003", "Zuzu & Zaza Zebra", "endometriosis", "1960", "Justin Trudeau", "2006", "Walldorf", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\"", "on pipelines and hostage-taking", "Peppermint oil, soluble fiber, and antispasmodic", "U.S. Navy", "a ferry", "Leland Stanford", "Mexico City", "Nepal"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6046989669687037}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.92, 0.8, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.6666666666666666, 0.0, 0.0, 0.2857142857142857, 0.8, 0.5, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.5641025641025641, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3571428571428571, 0.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-7342", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-3760", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-645", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-10561", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2428", "mrqa_searchqa-validation-10231"], "SR": 0.453125, "CSR": 0.5323093220338984, "EFR": 0.9142857142857143, "Overall": 0.6968971322639226}, {"timecode": 59, "before_eval_results": {"predictions": ["a regulatory site", "Jason Marsden", "Ireland", "Vicente Fox", "Daryl Sabara", "February 6, 2005", "Justin Timberlake", "Abiotic", "9 ( VIIII )", "a useless, time - wasting activity", "coach", "February 2011", "Audrey II", "Tim Russert", "Masha Skorobogatov", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "January 2018", "George Strait", "climate on the Earth", "Herman Hollerith", "94 by 50 feet", "gears that can be changed to allow a wide range of vehicle speeds, and also in the differential, which contains the final drive to provide further speed reduction at the wheels", "Gibraltar", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "January 12, 2017", "The Miracles", "to provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "Marie Fredriksson", "Long Island", "1988", "the British Empire", "a castle", "Sarah Brightman", "the Devastator", "the final episode of the series", "2010", "Ram Nath Kovind", "Abid Ali Neemuchwala", "August 9, 1945", "the 1950s", "Napoleon", "XXXX", "December 25", "Made a decision to turn our will and our lives over to the care of God as we understood Him", "Italian architect and art theorist Leon Battista Alberti", "January 2, 1971", "J. Presper Eckert and John William Mauchly's ENIAC", "diastema", "July 21, 1861", "Brooklyn, New York", "Efren Manalang Reyes", "Jokers Wild", "Chicago", "Dijon", "Lucas Stephen Grabeel", "15,024", "actress and model", "the results by the medical examiner's office,", "teenage", "Thursday,", "Vietnam", "the bass", "Richard", "Vinny"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6506765139658475}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.6415094339622641, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 0.2580645161290323, 1.0, 1.0, 0.18181818181818182, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.28571428571428575, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7208", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-8689", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3553", "mrqa_triviaqa-validation-34", "mrqa_hotpotqa-validation-3979", "mrqa_hotpotqa-validation-4797", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2984", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-8465", "mrqa_triviaqa-validation-7581"], "SR": 0.5625, "CSR": 0.5328125, "EFR": 0.9642857142857143, "Overall": 0.7069977678571429}, {"timecode": 60, "before_eval_results": {"predictions": ["people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "1998", "there were approximately 400 -- 500 Native Americans in the United States Marine Corps whose primary job was the transmission of secret tactical messages", "Gilbert building", "Tom Brady", "James Rodr\u00edguez", "a computer maintenance utility included in Microsoft Windows designed to free up disk space on a computer's hard drive", "1837", "a writ of certiorari", "silk floss tree", "Ferm\u00edn Francisco de Lasu\u00e9n", "Fats Waller", "honey", "79", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T )", "49 cents", "Jason Lee", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Dody Goodman", "Mahatma Gandhi", "France to the people of the United States", "the eighth season", "Erica Rivera", "John Young", "Russia", "2019", "Charles Perrault", "1990", "James `` Jamie '' Dornan", "the left coronary artery", "Sam Waterston", "Brazil, Bolivia, Paraguay and Argentina", "Nicklaus", "1957", "Pink Floyd", "ummat al - Islamiyah", "Brazil", "Parashara ( c. 400 -- c. 500 AD )", "Domhnall Gleeson", "the Guaran\u00ed peoples of South America, who called it ka'a he'\u00ea ( `` sweet herb '' )", "agriculture", "St. John's, Newfoundland and Labrador", "from the Greek \u0392\u03bf\u03ce\u03c4\u03b7\u03c2, Bo\u014dt\u0113s, meaning `` herdsman '' or `` plowman '' ( literally, `` ox - driver '' ; from \u03b2\u03bf\u1fe6\u03c2", "plant anatomy", "the bloodstream or surrounding tissue following surgery, disease, or trauma", "1923", "1871 A.D.", "an episode typically ends as a cliffhanger showing the first few moments of Sam's next leap ( along with him again uttering `` Oh, boy! '' on discovering his situation )", "Brooklyn Heights, New York, at 10 Stigwood Avenue", "1931", "early known period on the alluvial plain", "\"The closest approach to the original sound\"", "Peter Sellers", "Colonel Thomas Andrew \u201cTom\u201d Parker (Born Andreas Cornelis van Kuyk)", "Atlantic Ocean", "mistress of the Robes", "Australian Electoral Division", "Kris Allen", "Kurt Cobain's", "\"Empire of the Sun,\"", "Hector Berlioz", "The Killing Fields", "the Endeavour", "News of the World tabloid."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6938455783856192}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true, true, false], "QA-F1": [0.48275862068965514, 0.0, 0.10526315789473684, 1.0, 0.0, 1.0, 0.9142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.5, 1.0, 1.0, 1.0, 0.08000000000000002, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.2857142857142857, 1.0, 0.7272727272727272, 0.9285714285714286, 0.5, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.6666666666666666, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-9371", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-9085", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-9058", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-2476", "mrqa_hotpotqa-validation-3716", "mrqa_newsqa-validation-1282"], "SR": 0.5625, "CSR": 0.5332991803278688, "EFR": 0.8571428571428571, "Overall": 0.6856665324941452}, {"timecode": 61, "before_eval_results": {"predictions": ["March 21, 2016", "to form a higher alkane", "Carlos Tevez", "Jason Marsden", "New Mexico", "In 1974", "Poems : Series 1", "William the Conqueror", "March 2, 2016", "2018", "five", "December 19, 1971", "James Rodr\u00edguez", "Oceania", "Zoe Badwi", "Mickey Rourke", "John Donne", "1977", "David Gahan", "Emma Watson", "Acts passed by the Congress of the United States and its predecessor, the Continental Congress", "2018", "7 July", "2010", "4.25 inches ( 108 mm )", "Judi Dench", "November 27, 2017", "159", "Chris Rea", "between $10,000 and $30,000", "Kelly Reno", "Ozzie Smith", "8 December 1985", "in the 18th century", "Thomas Jefferson", "Ian McKellen", "the cat in the hat knows a lot about space movie", "Brad Dourif", "counter clockwise", "Joanne Wheatley", "vice president", "a combination of the rise of literacy, technological advances in printing, and improved economics of distribution", "Donna Mills", "the 1994 season", "Matt Flinders", "the parthenogenic reproduction of an adult New Mexico whiptail", "the major contributor and the associated free software philosophy", "the efferent nerves that directly innervate muscles", "1773", "The Union", "American country music duo Brooks & Dunn", "\"Maljanne\"", "south america", "The Pilgrim's Progress", "Bourbon County", "Argentina", "Bohemia", "Sen. Barack Obama", "Sri Lanka's Tamil rebels", "Osama bin Laden's sons", "(Jack) London", "Arthur C. Clarke", "the Koran", "whooping cough"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6398320256132756}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.24000000000000002, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 0.32, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.8, 1.0, 0.25, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-6554", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-7819", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-7147", "mrqa_naturalquestions-validation-7513", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-8452", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-9772", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-1190", "mrqa_naturalquestions-validation-4444", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-3340", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-1069", "mrqa_hotpotqa-validation-5483", "mrqa_newsqa-validation-648", "mrqa_triviaqa-validation-4519"], "SR": 0.546875, "CSR": 0.5335181451612903, "EFR": 0.9310344827586207, "Overall": 0.7004886505839822}, {"timecode": 62, "before_eval_results": {"predictions": ["the television series, The Lone Ranger for one season from 1952 until 1953", "beneath the liver", "Rudy Clark", "Abbot Suger", "Yuzuru Hanyu", "Tim Russert", "the Roman Empire", "for toys or doorbell installations", "muscle contraction", "in positions Arg15 - Ile16 and produces \u03c0 - Chymotrypsin", "the northernmost point on the Earth", "Alex Drake", "Eduardo", "1868 war veterans", "1971", "Leo Arnaud", "Emmanuelle Chriqui", "Carlos Alan Autry Jr.", "16 March 2018", "Hollywood, Los Angeles, California", "Merry Clayton ( born December 25, 1948 )", "a bronze statue designed by Thomas Crawford ( 1814 -- 1857 ) that, since 1863, has crowned the dome of the U.S. Capitol building in Washington, D.C.", "judges", "1936", "Eric Clapton", "Djokovic", "James Hutton", "1922", "2017", "a scythe", "to connect the CNS to the limbs and organs", "Leonard Bernstein", "Toronto and locations in Canada and the United States using Canadian - built Bombardier Dash - 8 Q 400 turboprop aircraft", "listing the telephone numbers of individuals and families who have requested that telemarketers not contact them", "September 2017", "October 2012", "Kaley Christine Cuoco ( / \u02c8ke\u026ali \u02c8kwo\u028ako\u028a / KAY - lee KWOH - koh", "2013", "Dido", "Isekai wa Sum\u0101tofon to Tomo ni", "the final scene of the fourth season", "Phillip Paley", "2004", "Jakkur, Bangalore, India", "New Orleans going north through Chicago and to New York", "the port of Nueva Espa\u00f1a to the Spanish coast", "10.5 %", "the Great Plains and U.S. Interior Highlands region", "White House Executive Chef", "the International Border ( IB )", "lead vocalist Bart Millard", "a married woman", "Thabo Mbeki", "Midnight Cowboy", "Austrian", "heavy metal", "Selden", "Muslim Eid-ul-Adha", "the day before.", "said the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "Chastity", "the Entente Council", "Chief Oshkosh Monument", "River Welland"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5901242565305066}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.5333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.25, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.5, 0.16666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5185185185185185, 1.0, 0.07692307692307691, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.9714285714285714, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4954", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2900", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-9658", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6970", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-8628", "mrqa_naturalquestions-validation-8734", "mrqa_naturalquestions-validation-7639", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-10077", "mrqa_triviaqa-validation-7273", "mrqa_hotpotqa-validation-529", "mrqa_hotpotqa-validation-5848", "mrqa_newsqa-validation-1307", "mrqa_newsqa-validation-1292", "mrqa_newsqa-validation-1285", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-9194", "mrqa_searchqa-validation-9115", "mrqa_hotpotqa-validation-1201"], "SR": 0.484375, "CSR": 0.5327380952380952, "EFR": 0.9696969696969697, "Overall": 0.7080651379870131}, {"timecode": 63, "before_eval_results": {"predictions": ["Meri", "1998", "the closing of the atrioventricular valves and semilunar valves, respectively", "the Farrow / Previn / Allens", "sacroiliac joint", "Definition of the problems", "Cuernavaca", "the development of electronic computers", "Employers", "Balaam ( Numbers 22 : 28 )", "Bhupendranath Dutt", "Charlotte of Mecklenburg - Strelitz", "April 13, 2018", "the kitchen", "Annie Potts", "Jakkur, Bangalore, India", "in a thousand years", "2001", "the European economy had collapsed", "brothers Henry", "Ben Findon", "Incudomalleolar joint", "Terry Reid", "an active supporter of the League of Nations", "the Kennedy Space Center ( KSC ) in Florida", "enabled business applications to be developed with Flash", "Forbes Burnham", "Saturday", "Isekai wa Sum\u0101tofon to Tomo ni", "the tsar's Moscow residence", "its members", "Alicia Vikander", "over 300,000", "February 27, 2015", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "115", "ten times", "Lori Rom", "a Czech word, robota", "Arlen `` The Chief '' Bitterbuck", "Cameron Fraser", "Austin and Pflugerville", "1933", "the misuse", "four", "about 25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "Utah, Arizona, Wyoming, and Oroville, California", "Jack Barry", "Hugo Weaving", "the heraldic crest carved in the lintel on St. Ignatius'family home in Azpeitia, Spain", "Lana Del Rey", "The Matterhorn", "a song", "jockey", "The Pentagon", "Pisgah National Forest", "Johnnie Ray", "Robert Mugabe", "Capitol Hill,", "discuss water shortages in the major Tigris and Euphrates rivers,", "impressionism", "the Pussycat Dolls", "tuberculosis", "May 4"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5255573533739653}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.2222222222222222, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.33333333333333337, 0.0, 0.1818181818181818, 1.0, 0.761904761904762, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-8460", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-8063", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2023", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-225", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-3995", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-3801", "mrqa_triviaqa-validation-6825", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-4240", "mrqa_newsqa-validation-198"], "SR": 0.421875, "CSR": 0.531005859375, "EFR": 0.918918918918919, "Overall": 0.6975630806587838}, {"timecode": 64, "before_eval_results": {"predictions": ["Agra", "2018\u201319 UEFA Europa League group stage", "FIFA Women's World Cup", "Dan Brandon Bilzerian", "Len Wiseman", "Viglen Ltd", "1896", "Randall Boggs", "Detroit, Michigan,", "Roots: The Saga of an American Family", "the network's National Football League and Major League Baseball coverage", "Boston", "local South Australian and Australian produced content", "Hindi", "Ronald Wilson Reagan", "Los Angeles", "Ben Johnston", "Nia Temple Sanchez", "Vanessa Anne Hudgens", "Liga MX", "Amber Laura Heard", "Peter Seamus O'Toole", "March 8, 1942", "American alternative rock band R.E.M.", "January 30, 1930", "Doctor", "the Irish Government's Health Service Executive", "James Weldon Johnson", "Wilmington, North Carolina, United States", "1979", "Taylor Swift", "Cher", "Kew Gardens", "7 January 1936", "Towards the Sun", "The Braes of Balquhither", "Westminster system", "I. helicon", "For Love Alone", "October 4, 1970", "King of Valois", "Sam Waterston", "Transporter 3", "2000", "Gauteng province", "Vietnam War", "Bill Walton", "Darling River", "Brian Keith Bosworth", "140 million", "English", "Teri Garr", "the employer", "1965 -- 66", "Wyoming", "Wee Jimmy Krankie and his father", "England or English culture", "would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "black, red or white, and women in the impoverished city are concerned that they will not be able to purchase clothing that conforms to the order,", "trying to prevent attempted defections as the country goes through a tumultuous transition,", "Billy Corgan", "Ellicott City", "6 to 8", "Southport, North Carolina"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6639260912698413}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 0.8, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.8, 0.5, 0.888888888888889, 1.0, 1.0, 0.8333333333333333, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.06666666666666667, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-257", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-5143", "mrqa_hotpotqa-validation-2632", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-4781", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3342", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2442", "mrqa_hotpotqa-validation-4810", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-1171", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2777", "mrqa_searchqa-validation-9071", "mrqa_searchqa-validation-16474"], "SR": 0.53125, "CSR": 0.5310096153846153, "EFR": 1.0, "Overall": 0.7137800480769231}, {"timecode": 65, "before_eval_results": {"predictions": ["T. D. Lee", "animated series \"Archer\"", "Albert", "September 30, 2017", "339,520", "New York Giants", "the Swiss tourism boom", "Eliot Cutler", "the 1946 Winecoff Hotel fire", "Odense Boldklub", "Stephen of Blois", "Scott Eastwood", "Gweilo", "Tufts College", "Amedeo", "1936", "The Wu-Tang Clan", "For Love Alone", "a midtempo hip hop ballad", "hard rock", "G\u00e9rard Depardieu", "managed by the U.S. Army Counter Intelligence Corps during World War II", "Las Vegas", "Appleby-in-Westmorland", "from 1345 to 1377", "Indiana University", "the first four James Bond films", "Syracuse", "Kings Point, New York", "former Chicago Bears", "The Gang", "a hamlet", "Port Clinton", "November 20, 1942", "Wayne Conley", "Armidale, New South Wales", "Faith", "turns out to be a terrible date", "the Celtics", "Supernatural", "CHRoccan clementine", "eight", "1867", "Sippin' on Some Syrup", "Jim Harrison", "James II", "Arabella Churchill", "Lester", "two Grammy awards", "The S7", "2017", "Qutab Ud - Din - Aibak", "September 2000", "the Thirteen Colonies", "France", "kibbutznik", "The Muffin Man", "President George Bush", "250,000", "Vernon Forrest", "Yonkers", "blown", "the supernatural", "Dan Aykroyd"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5528760302197802}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false], "QA-F1": [0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.25, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.15384615384615385, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-4493", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-3263", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1435", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-5825", "mrqa_hotpotqa-validation-3246", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-1614", "mrqa_hotpotqa-validation-1542", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-2978", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-10202", "mrqa_triviaqa-validation-5776", "mrqa_triviaqa-validation-2994", "mrqa_newsqa-validation-2677", "mrqa_newsqa-validation-368", "mrqa_searchqa-validation-7440", "mrqa_searchqa-validation-3848", "mrqa_naturalquestions-validation-1925"], "SR": 0.4375, "CSR": 0.529592803030303, "EFR": 1.0, "Overall": 0.7134966856060606}, {"timecode": 66, "before_eval_results": {"predictions": ["Captain Mark Phillips", "Sheffield Wednesday", "Paraguay", "126 mph", "Absalom", "Terry Hall", "December", "Anthony Joshua", "George, Prince of Wales", "Zsa Zsa Gabor", "ambilevous", "Louis Daguerre", "Philip Plait", "krank", "a living architect or architects whose built work demonstrates a combination of those qualities of talent, vision, and commitment", "Guy the Gorilla", "near infrared", "moresby", "orange", "kursk", "pyrotechnic", "kung fu", "Annie Lennox", "goose", "Olympics", "Typhon", "Syria", "Wyoming", "Professor Brian Cox", "Benjamin Franklin", "Albert Finney", "Scotland", "24", "George Washington James Monroe Thomas Jefferson Abraham Lincoln", "ellice", "Meta", "luxe capital of Europe", "northern Skye", "a double basses", "The Spice Girls", "Mr Loophole", "Istanbul", "drinking song", "Texas", "Erik Aunapuu", "the Yalta Conference", "Rajasthan", "African violet", "\u0411\u0430\u043b\u0438", "glee", "stonton", "Ratonhnhak\u00e9 : ton and Haytham Kenway", "Djokovic", "1912", "fennec fox", "1927", "Nikolai Trubetzkoy", "Vernon Forrest,", "Linda Hogan", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "Andrew Wyeth", "virus", "Stephen Alan Weinberg", "X"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5570913461538461}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.15384615384615385, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-2689", "mrqa_triviaqa-validation-4454", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2232", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-6075", "mrqa_triviaqa-validation-3729", "mrqa_triviaqa-validation-4157", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-4036", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-6436", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-6457", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-4970", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-3214", "mrqa_naturalquestions-validation-3922", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-491", "mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-9098", "mrqa_searchqa-validation-7189", "mrqa_naturalquestions-validation-6584"], "SR": 0.484375, "CSR": 0.5289179104477613, "EFR": 1.0, "Overall": 0.7133617070895524}, {"timecode": 67, "before_eval_results": {"predictions": ["$50 less", "Thailand", "France", "tranquil beaches,", "took on water", "Schalke", "Secretary of State", "Obama", "21 percent", "Fernando Caceres", "an Italian and six Africans dead.", "no evidence", "America's Cup", "Cambodian territory", "Bahrain", "voluntary manslaughter", "Jenny Sanford", "Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "Miami Beach, Florida,", "\"Percy Jackson & The Olympians,\"", "contraband cell phones", "two contestants.", "Fiona Mac Keown", "the Southern Baptist Convention", "South Africa", "former U.S. secretary of state.", "tried to fake his own death by crashing his private plane into a Florida swamp.", "8", "Thursday and Friday to the end of her tour on June 17 and 18,", "helicopters and boats, as well as vessels from other agencies,", "terrorize is a crime,", "two tickets to Italy", "Oxbow, a town of about 238 people,", "the FAA received no reports of ground strikes or interference with aircraft in flight,", "21-year-old", "Jacob Zuma,", "toffelmakaren.", "former Procol Harum bandmate Gary Brooker", "response to a civil disturbance call,", "Pew Research Center", "face transplant", "Kenyan and Somali", "30,000", "1983", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "North Korea", "Jobs", "Garth Brooks", "40-year-old", "Facebook and Google,", "1983", "Carolyn Sue Jones", "Hold On", "a central place in Christian eschatology", "Phil Mickelson", "Dumbo", "Yardbirds", "1969", "\"$10,000 Kelly,\"", "\"Estadio de L\u00f3pez Cort\u00e1zar\"", "jurassic", "(John) Wayne", "director", "how they would learn to be nonviolent in any relationship"], "metric_results": {"EM": 0.625, "QA-F1": 0.6988362332112332}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.2857142857142857, 0.8461538461538461, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-1093", "mrqa_naturalquestions-validation-833", "mrqa_searchqa-validation-5451", "mrqa_searchqa-validation-2492", "mrqa_naturalquestions-validation-9387"], "SR": 0.625, "CSR": 0.5303308823529411, "EFR": 1.0, "Overall": 0.7136443014705882}, {"timecode": 68, "before_eval_results": {"predictions": ["license plate \"BADBUL,\"", "2050,", "Molotov cocktails, rocks and glass.", "after Michael Jackson's death in the Holmby Hills, California,", "German Chancellor Angela Merkel", "The son of Gabon's former president", "to put a lid on the marking of Ashura this year.", "their homes in Bhola for the Muslim festival of Eid al-Adha.", "off Somalia's coast.", "in time as another American icon's wheels come off.", "AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro on Sunday.", "President Barack Obama", "the Southern Baptist Convention", "in body bags on the roadway near the bus,", "Thursday,", "an American who entered the country illegally from China on Christmas Eve.", "2000", "at least 300", "Thursday,", "hot and humid", "Israeli", "the drama of the action in-and-around the golf course", "2008", "root out terrorists within its borders.", "25", "the finding of \"Zed,\" a Columbian mammoth", "Ciudad Juarez, across the border", "105-year", "Michael Schumacher", "Steve Jobs", "Jenny Sanford", "a remote part of northwestern Montana", "genocide", "forged credit cards and identity theft", "Bailey, Colorado,", "U.S. Justice Department", "Unseeded Frenchwoman Aravane Rezai", "two weeks", "\"How I Met Your Mother,\"", "British", "Six", "he was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.", "a bank", "the most important race facing the country is the \"race for the future... and it won't be won with a president who is stuck in the past,\"", "Barnes & Noble", "14", "Michael Arrington", "well over 1,000 pounds", "halt fighting between Somali forces and Islamic insurgents.", "his past and his future", "Mombasa, Kenya,", "a loanword of the Visigothic word guma `` man ''", "Taron Egerton as Johnny", "Italy", "Hard Times", "purpurea", "Nellie Melba", "The King of Hollywood", "1979", "the backside", "Sweden", "cambogia", "Pablo Picasso", "improved the speed of encryption of communications at both ends in front line operations during World War II"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5496058692323936}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [0.5, 0.4, 1.0, 0.0, 1.0, 0.0, 0.8, 0.18181818181818182, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.19512195121951217, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3470", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-3611", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-1997", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-3287", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-430", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-2426", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-4976", "mrqa_triviaqa-validation-4494", "mrqa_hotpotqa-validation-5465", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-10753"], "SR": 0.453125, "CSR": 0.5292119565217391, "EFR": 1.0, "Overall": 0.7134205163043479}, {"timecode": 69, "before_eval_results": {"predictions": ["human", "Tim Russert", "on the microscope's stage", "P.V. Sindhu", "Nick Kroll", "April 1917", "Australia's Sir Donald Bradman", "two - stroke engines and chain drive", "revenge", "Kevin Sumlin", "about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "Professor Eobard Thawne", "Hathi Jr", "a liquid crystal on silicon ( LCoS ) ( based on an LCos chip from Himax ), field - sequential color system, LED illuminated display", "Spektor", "The Star Spangled Banner", "Bill Russell", "Luke Luke 18 : 1 - 8", "July 2010", "a protocol ( http )", "343 m / s in air", "1997", "Carol Worthington", "September 6, 2019", "1972", "1902", "rootlets", "down to the ground", "Battle of Antietam", "US 211 in Thornton Gap", "Clarence Anglin", "Andrew Garfield", "throughout the right atrium", "1980s", "Pasek & Paul", "in a 1920 play R.U.R. by the Czech writer, Karel \u010capek", "epidemiology", "2013", "Billie", "piety", "Daniel Suarez", "The White House Executive Chef", "Mali", "25 years", "the bank's own funds", "The Abbott and Costello Show", "Waylon Jennings", "libretto", "the Rolling Stones", "Sun Tzu", "Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluated", "eucalyptus", "inflation", "Christies Foxhunters", "John M. Dowd", "December 17, 1974", "The Northrop P-61 Black widow", "26", "The woman", "as soon as 2050,", "West Point", "Paul Bunyan", "thyroid", "1965"], "metric_results": {"EM": 0.546875, "QA-F1": 0.664816362150921}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 0.0, 0.8181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.888888888888889, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9411764705882353, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-182", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-10529", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-221", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-8374", "mrqa_triviaqa-validation-6937"], "SR": 0.546875, "CSR": 0.5294642857142857, "EFR": 0.9655172413793104, "Overall": 0.7065744304187193}, {"timecode": 70, "before_eval_results": {"predictions": ["the President", "Walter Pauk", "Madison", "Brevet Colonel Robert E. Lee", "a specific task", "January 2, 1971", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "St. Louis Cardinals", "Bonhomme Carnaval", "1792", "Longline", "Sebastian Vettel", "Reginald Jeeves", "China", "2017", "Upstate New York", "Carol Ann Susi", "a stem", "Nala", "Billy Gibbons of ZZ Top", "P.V. Sindhu", "Anglican services", "the closing of the atrioventricular valves and semilunar valves, respectively", "Friedman Billings Ramsey", "the NFL", "14.69278 \u00b0 N 17.44667 \u00b0 W \ufeff / 14.68278", "1 January 1904", "recover many kinds of passwords using methods such as network packet sniffing, cracking various password hashes by using methodssuch as dictionary attacks, brute force and cryptanalysis attacks", "part - time", "by week 4 of development", "contemporary Earth", "somatic cell nuclear transfer ( SCNT )", "The UN General Assembly", "Benzodiazepines", "two", "David Ben - Gurion", "as far west as Las Vegas", "the 7th century", "St. Theodosius Russian Orthodox Cathedral", "Ray Charles", "a jazz funeral without a body", "2004", "May 31, 2012", "Michael Edwards", "Malware", "a person was once associated, in a relationship, marriage, or once talked to", "Beorn", "North Dakota", "John F. Kennedy", "around 100,000", "1967", "Rajasthan", "Sodor", "eye", "44,300", "2008", "Frisian", "Long Island convenience store", "\"You saw the joy that the British had, that the Americans had, and saw them here through their representatives celebrating and acting as if we Zimbabwe are either an extension of Britain or... America.", "11", "bones", "LEWIS CARROLL", "Thailand", "500-room"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6579901384359105}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4999999999999999, 0.35294117647058826, 0.9454545454545454, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.09523809523809522, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-7579", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-2146", "mrqa_hotpotqa-validation-2098", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-3943", "mrqa_searchqa-validation-6656", "mrqa_searchqa-validation-7551"], "SR": 0.5625, "CSR": 0.5299295774647887, "EFR": 0.9285714285714286, "Overall": 0.6992783262072435}, {"timecode": 71, "before_eval_results": {"predictions": ["German Poet Alexander Pope", "Diana Vickers", "Tina Turner", "Woodrow Wilson", "Striding Edge ridge", "photographer", "clown", "the Titanic", "benevento", "Hadrian", "Madagascar", "Barbizon school", "Renzo Piano", "Manet", "john johnson", "Challenger", "Edinburgh City Football Club", "Lacock Abbey", "Clive Cussler", "Canada", "'Hansel and Gretel' cottage", "Honda", "Greenock", "ABBA", "sonja Henie", "eight", "Lord Snooty", "Greyfriars Bobby", "Rudolf Hess", "koninklijke Vlaamse Academie van Belgi\u00eb voor Wetenschappen en Kunsten", "Stieg Larsson", "Facebook Stories", "1957", "cromlech", "steel", "Rotherham United", "Joseph Priestley", "greyhound, gazelle hound or tazi", "tennis", "Periodic Table", "CameroonCameroon", "a region of SW Asia between the lower and middle reaches of the Tigris and Euphrates rivers", "Timothy Carroll", "Spanish", "armstrong", "Patience", "Chubby Checker", "Tim Roth", "smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity, and simplified setup of more complex communications such as Wi-Fi", "Salvador Dal\u00ed", "Camellia", "San Francisco", "After strong sales into the 90s", "Brooke Wexler", "2004 Nokia Sugar Bowl", "aging issues", "January", "one of Europe's most experienced providers of carbon offsets,", "hiring veterans as well as job training for all service members leaving the military.", "$2.6 million", "the Civil War", "in the inverse relationship exhibited by price/earnings ratios and the rate of inflation in the past.", "a monoplane", "UFC Fight Pass"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6320427389705882}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.23529411764705882, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0625, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3529411764705882, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-127", "mrqa_triviaqa-validation-6465", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-5576", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5665", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-4791", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-452", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-4282", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-5816", "mrqa_naturalquestions-validation-8965", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-1758", "mrqa_searchqa-validation-11196", "mrqa_searchqa-validation-15899"], "SR": 0.59375, "CSR": 0.5308159722222222, "EFR": 1.0, "Overall": 0.7137413194444445}, {"timecode": 72, "before_eval_results": {"predictions": ["bobby Darin", "Thames", "Altamont Speedway", "The Jetsons", "26 miles", "talus fracture", "jellyfish", "manoah", "Connecticut", "daedalus", "gary clifton", "uranus", "a goad", "Miles Morales", "14", "radars", "Queen Elizabeth II", "tonto", "hippocampus", "Frank Miller", "tennis", "gollancz", "Atlantic", "mori", "chatsworth house", "giorgio armani", "paris", "eyelid", "chainsaws", "augusta", "August 25", "taurine", "augusta", "Venezuela", "Southwest Airlines", "SUNSET BOULEVARD", "b Bill Gibson", "Derwent", "sesame", "Laos", "West Ham", "General Henri-Philippe Petain", "Oliver Barrett IV", "Miami", "Bill Haley", "bolognese", "1768", "Joan Rivers", "gezahegn Abera", "The Fridge", "Ghana", "Near East", "observing the magnetic stripe `` anomalies '' on the ocean floor", "2001", "Easy", "seven", "Karl Johan Schuster", "the U.S. Holocaust Memorial Museum", "Robert Barnett", "Diego Milito", "Rembrandt Harmenszoon van Rijn", "Dumbo the Flying elephant", "1580s North Carolina", "pythons"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4920272435897436}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.4, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-408", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-6757", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5979", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2423", "mrqa_triviaqa-validation-7364", "mrqa_triviaqa-validation-7682", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-1703", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-2048", "mrqa_triviaqa-validation-3916", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-9987", "mrqa_hotpotqa-validation-3333", "mrqa_hotpotqa-validation-1534", "mrqa_hotpotqa-validation-2017", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2755", "mrqa_searchqa-validation-2901", "mrqa_searchqa-validation-7110", "mrqa_searchqa-validation-4706", "mrqa_searchqa-validation-4802"], "SR": 0.40625, "CSR": 0.529109589041096, "EFR": 1.0, "Overall": 0.7134000428082192}, {"timecode": 73, "before_eval_results": {"predictions": ["Scottish national team", "Speedway World Championship", "The Bears", "\"Time\"", "Babylon", "1501", "The Shins", "11,791", "Eliot Cutler", "Manchester", "Hellenism", "The Ansonia Hotel", "Washington", "Helen Mirren", "racehorse breeder", "Schutzstaffel", "Eddie Albert", "The Bye Bye Man", "Chevron Corporation", "ragby", "Indianapolis", "paintings", "Premier League", "Sleepy Hollow", "Jane Mayer", "Obafemi Akinwunmi Martins", "Knowlton School", "143,007", "Philadelphia", "33", "hostess of \"wheel of Fortune\"", "1957", "mathematician", "king Duncan", "The St Andrews Agreement", "Royal College of Music", "4145 ft above mean sea level", "Japan Airlines Flight 123", "near North Chicago, in Lake County, Illinois", "HBO miniseries \"Empire Falls\"", "2013", "Michael Phelps", "major intersections", "schoolteacher", "People v. Turner", "Bill Ponsford", "Faisal Qureshi", "one", "Johnny cage", "Mike Holmgren", "Gauteng", "Herman Hollerith", "6 -- 14 July", "parashiyot", "paramita", "1881", "philipps", "the defendants, who were charged with stealing the personal credit information of thousands of American and European consumers, are allegedly members of five organized crime rings with ties to Europe, Asia, Africa and the Middle East.", "Jobs", "Buddhism", "blintz", "Texas Chainsaw Massacre", "Joe DiMaggio", "Caster Semenya"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6788974521396396}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true], "QA-F1": [0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0625, 0.6, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2702702702702703, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-1851", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-680", "mrqa_hotpotqa-validation-821", "mrqa_hotpotqa-validation-2716", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-1002", "mrqa_naturalquestions-validation-3546", "mrqa_triviaqa-validation-1757", "mrqa_triviaqa-validation-3539", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3692"], "SR": 0.578125, "CSR": 0.5297719594594594, "EFR": 0.9629629629629629, "Overall": 0.7061251094844845}, {"timecode": 74, "before_eval_results": {"predictions": ["Jesuits", "ribonucleic acid", "ketchup", "a igloo", "the house fly", "Timberland", "\"Dancing with the Stars\"", "Burma", "Latvia", "a spleen", "Auf Wiedersehen", "rely", "statues", "wine", "an esophagus", "Super Bowl VI", "the Bible", "the twist", "mme tussaud", "Biscay", "a dragon", "March", "Ferdinand Magellan", "Verbal Kint", "a brothel", "an oblate spheroid", "The Aviator", "Gioachino Rossini", "Mexico", "a tail", "Nashville", "The 7 Wonders of the World", "the Starfighter", "Billy Crystal", "wrinkles", "LaSalle", "Quebec", "Freemason", "The Drew Carey Show", "Fiji Islands", "Moonlighting", "Corpus Christi", "a friend", "Ruth Bader Ginsburg", "American anticommunist hysteria", "Bay of Bengal", "in vitro fertilisation", "Diogenes", "pastries", "a chocolate milk drink", "the Electric Company", "September 24, 2012", "Roger Dean Stadium", "March 31, 2013", "\"Lady Madonna\"", "Celsius", "jeremy irons", "January", "Jennifer Grey", "Donald Wayne Johnson", "demolishing American third seed Venus Williams in the final of the Sony Ericsson Open in Miami on Saturday.", "4,000", "Princess Diana", "Melbourne, Victoria,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.57109375}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-4940", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-10572", "mrqa_searchqa-validation-6347", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-13654", "mrqa_searchqa-validation-4454", "mrqa_searchqa-validation-1613", "mrqa_searchqa-validation-6459", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-2220", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-15633", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-3466", "mrqa_searchqa-validation-4143", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-85", "mrqa_searchqa-validation-14509", "mrqa_searchqa-validation-5114", "mrqa_searchqa-validation-7111", "mrqa_searchqa-validation-16566", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-4512", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-976", "mrqa_naturalquestions-validation-5096", "mrqa_triviaqa-validation-6455", "mrqa_newsqa-validation-801", "mrqa_hotpotqa-validation-403"], "SR": 0.46875, "CSR": 0.5289583333333333, "EFR": 1.0, "Overall": 0.7133697916666667}, {"timecode": 75, "before_eval_results": {"predictions": ["eleven", "Randy VanWarmer", "October 2012", "Sylvester Stallone", "between 1765 and 1783", "The Miracles", "1900", "a site for genetic transcription that is segregated from the location of translation in the cytoplasm, allowing levels of gene regulation that are not available to prokaryotes", "Geothermal gradient", "AD 1600", "1963", "The Satavahanas", "Jos\u00e9 Mart\u00ed", "they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "16 August 1975", "shortwave radio", "28 July 1914", "St. Pauli Special Dark", "908 mbar ( hPa ; 26.81 inHg )", "North Atlantic Ocean", "February 7, 2018", "October 2000", "The Lutheran Church of Sweden", "commemorating fealty and filial piety", "on the fictional Iron River Ranch, Colorado", "everyone on board", "American singer - songwriter - actress Debbie Gibson", "Lula", "31 January 1934", "Camp Green Lake", "the southeastern United States of unresolved taxonomic identity", "the gastrocnemius", "Daniel A. Dailey", "the winter solstice", "President Yahya Khan", "Ramanaa", "function like an endocrine organ, and dysregulation of the gut flora has been correlated with a host of inflammatory and autoimmune conditions", "Kyla Coleman", "Bill Patriots", "September 1972", "Tim Passmore", "Garbi\u00f1e Muguruza", "Spanish / Basque origin", "Lilian Bellamy", "4.37 light - years ( 1.34 pc )", "Shirley Partridge", "from handheld subscriber equipment, placing a call to Dr. Joel S. Engel of Bell Labs, his rival", "Saint Etienne", "1 lakh people '' or `` 1 lakh of people '' ; `` 200 lakh rupees '' ; '' rupees 10 lakhs '' ; or `` 5 lakhs of rupee ''", "Chuck Noland", "many forested parts of the world", "arithmetic", "arrows", "red squirrels", "Robert Curley", "Maria von Trapp", "Skatoony", "President Felipe Calderon", "low-calorie", "0-0 draw", "the Romanov Dynasty", "Mexico", "the coyote", "Majid Movahedi,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.616839968007751}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.9863013698630138, 1.0, 0.0, 0.6, 0.6666666666666666, 0.6, 0.05714285714285714, 1.0, 1.0, 1.0, 1.0, 0.11764705882352942, 0.0, 0.5, 0.0, 0.5, 0.0, 0.6, 1.0, 1.0, 0.4, 0.8, 0.0, 0.3333333333333333, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.4, 0.0, 0.0, 0.1, 1.0, 0.8333333333333333, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-8156", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-7133", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-5481", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-8747", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-1705", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-178", "mrqa_hotpotqa-validation-543", "mrqa_newsqa-validation-4076", "mrqa_searchqa-validation-12989", "mrqa_newsqa-validation-1646"], "SR": 0.453125, "CSR": 0.5279605263157895, "EFR": 0.9714285714285714, "Overall": 0.7074559445488722}, {"timecode": 76, "before_eval_results": {"predictions": ["$677.4 million in North America and $1.528 billion in other countries, for a worldwide total of $2.187 billion", "the fourth season", "Christopher Lloyd", "senators", "robbery", "West Egg on prosperous Long Island in the summer of 1922", "treaty of Utrecht of 1713", "Authority", "Jughead Jones", "American rock band Los Lonely Boys", "ecological regions", "cakes", "Kiss", "18 September to 31 October", "Julie Adams", "After a visit by Adolf Hitler, Bruno's father is promoted to Commandant, and the family has to move to `` Out - With '' because of the orders of `` The Fury ''", "Michael G. Hutton", "January 2004", "Zoe Badwi", "Hank J. Deutschendorf II", "Tennesseeitans", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins, and the lyrics to most of the suite's sections consist of his thoughts about her and their imminent breakup", "southern Turkey", "to encounter antigens passing through the mucosal epithelium", "Ashoka", "Spanish / Basque origin", "a contemporary drama in a rural setting", "1916", "Billie Jean King", "a sweet alcoholic drink made with rum, fruit juice, and syrup or grenadine", "2014 -- 15", "October 28, 2007", "Laura Vallejo", "an anembryonic gestation", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "Matt Monro", "a Nativity scene", "IV", "UMBC", "Saphira hatches from the stone, which was really an egg", "September 2017", "a feminine form of the Hebrew Yohannan, `` God forgave / God gratified ''", "British group Ace", "Spike", "regulatory site", "When the others arrive", "Atreus", "InterContinental Hotels Group", "peninsula", "Jason Flemyng", "the Isthmus of Corinth", "Norman Mailer", "vickers Vimy", "EMI", "Part I", "17 October 2006", "biochemist and academic Dr. Alberto Taquini", "$1.5 million.", "San Diego,", "CNN.com", "jazz", "echidna", "Forrest Gump", "31 meters (102 feet) long and 15 meters (49 feet) wide,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6693947157557354}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true], "QA-F1": [0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 0.7368421052631577, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.375, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.4444444444444445, 0.6153846153846153, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-5640", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-921", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-2873", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-2067", "mrqa_triviaqa-validation-6055", "mrqa_hotpotqa-validation-1166", "mrqa_newsqa-validation-3170", "mrqa_searchqa-validation-14736"], "SR": 0.5625, "CSR": 0.5284090909090908, "EFR": 0.9285714285714286, "Overall": 0.6989742288961039}, {"timecode": 77, "before_eval_results": {"predictions": ["Pegasus", "As You Like It", "Apollo 11", "Live and Let Die", "Giuliano Bugiardini", "metal", "a pulsar", "seth", "Honda", "Mannie Charles", "Adolf Hitler", "It's True", "the 2010 FIFA World Cup", "Elizabeth I", "14th", "Italy", "1960s", "mel Brooks", "France", "chlorophyll", "Paul Dukas", "Iceland", "Uranus", "rum", "apples", "Arbroath", "Roddy Doyle", "colink Babka, Jay Silvester and Ludvik Danek", "Something of Life and Death", "human voice", "Beatrix Potter", "magpie", "comets", "skating", "the Phillies", "Raul Castro", "Space Oddity", "Scotland", "Red Admiral", "Illinois", "green", "\u201c Splash\u201d", "South Africa", "menorah", "Good Will Hunting", "Smeagol", "otters", "John McCarthy", "John Mortimer", "Kellogg\u2019s Special K", "line code", "Sturnidae ( starlings and mynas )", "Liam Cunningham", "Brobee", "Madrid", "Philadelphia Eagles, Denver Broncos, Berlin Thunder, Las Vegas Outlaws and Ottawa Renegades", "E22", "security breach", "at checkposts and military camps in the Mohmand agency,", "Mashhad", "St Bernard", "France", "Barnard College", "the equator,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6207589285714286}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.09523809523809525, 0.0, 1.0, 0.0, 0.4, 0.5, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-958", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-693", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-2258", "mrqa_triviaqa-validation-7621", "mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-2314", "mrqa_naturalquestions-validation-5687", "mrqa_hotpotqa-validation-844", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-2404", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-4403", "mrqa_searchqa-validation-4278", "mrqa_newsqa-validation-3978"], "SR": 0.5625, "CSR": 0.5288461538461539, "EFR": 1.0, "Overall": 0.7133473557692308}, {"timecode": 78, "before_eval_results": {"predictions": ["French", "Granada", "gustavus III", "April", "Al Pacino", "Mohanda Karamchand", "by increasing the number of arcs", "Mr. golding", "a nerve cell cluster  or a group of nerve cell bodies located in the autonomic nervous system", "vitamin B3", "director of the Security Service", "hard", "Funchal", "hobbits", "spaghetti harvest", "Ireland", "\"passport\"", "Marcel Duchamp", "the Quatermass Experiment", "Mumbai", "a statue", "1875", "a raven", "hound", "Susie", "duke of chicago", "$x^2", "Narendra Modi", "villa wahnfried", "quentin tarantino", "Argentine", "hard", "Kitzb\u00fchel", "Tunisia", "barbara armrell", "steppes", "Romania", "brindisi", "Muriel", "Emeril Lagasse", "Belle Epoque Casino", "darrin", "bialystock", "Israel\u2019s Holocaust memorial", "Eva Herzigov\u00e1", "june berkowitz", "Ireland", "hit movie, 'The Ipcress File'", "hobbits", "Germany", "Ireland", "the anterolateral system", "magnetic stripe `` anomalies '' on the ocean floor", "the ruling city of the Northern Kingdom of Israel, Samaria", "Tudor music and English folk-song", "Martin Joseph O'Malley", "1992", "sculptures", "Sunday's", "al Qaeda,", "The Old Man and the Sea", "Edward of Carnarvon", "Luxembourg", "there were no radar outages and said it had not lost contact with any planes during the computer glitch."], "metric_results": {"EM": 0.359375, "QA-F1": 0.424562324929972}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.35294117647058826, 0.6666666666666666, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1306", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-4300", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-5135", "mrqa_triviaqa-validation-5551", "mrqa_triviaqa-validation-5714", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-5434", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-1150", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-6121", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-3345", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-6986", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2959", "mrqa_triviaqa-validation-6490", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-6999", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-2699", "mrqa_naturalquestions-validation-7511", "mrqa_hotpotqa-validation-4500", "mrqa_newsqa-validation-2233", "mrqa_searchqa-validation-7161", "mrqa_searchqa-validation-6534", "mrqa_newsqa-validation-904"], "SR": 0.359375, "CSR": 0.5267009493670887, "EFR": 0.975609756097561, "Overall": 0.70804026609293}, {"timecode": 79, "before_eval_results": {"predictions": ["Astor", "Addis Ababa", "peacock", "france", "HMS amethyst", "nacion nadinagar", "tomato", "Kyoto Protocol", "Fancy Dress Shop", "Bull Moose Party", "headpan", "Jake La Motta", "resistance of an unknown resistor", "hitler", "South Africa", "indigestion", "discretion", "eva croker", "Apprentice", "ernie pyle", "Corinth Canal", "Ede & Ravenscroft", "Iceland", "ascot", "peaches", "giuseppe kenner", "Sicilian Mafia", "doe", "Duncan I", "UKIP", "argentina", "South Sudan", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Darby and Joan", "franny", "Julian WikiLeaks", "IT Crowd", "nastase", "carters", "giuseppe berkowitz", "Mr. Curtis", "Terms of Endearment", "China", "augustus", "1790", "port of argent", "chamomile", "driving Miss Daisy", "orchid", "Hilary Swank", "Aberdeen", "latitude 90 \u00b0 North", "18th century", "nine hours", "18 minutes", "China", "Sri Lanka Freedom Party", "Steamboat Bill, Jr.", "hostile war zones,", "Rodong Sinmun", "theology", "Cyd Charisse", "sanctions", "February"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6259374999999999}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3717", "mrqa_triviaqa-validation-3508", "mrqa_triviaqa-validation-2739", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-1338", "mrqa_triviaqa-validation-2468", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-2987", "mrqa_triviaqa-validation-2354", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-2200", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-944", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-1797", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-9875", "mrqa_hotpotqa-validation-306", "mrqa_newsqa-validation-3862", "mrqa_searchqa-validation-2116"], "SR": 0.5625, "CSR": 0.5271484375, "EFR": 1.0, "Overall": 0.7130078125000001}, {"timecode": 80, "before_eval_results": {"predictions": ["Wisconsin", "Eleanor Roosevelt", "senators", "2", "in the dress shop", "Robert Gillespie Adamson IV", "Colon Street", "off the rez", "Jason Momoa", "1969", "Tim Passmore", "2003", "at 5 : 7 -- 8", "the Canadian Rockies continental divide east to central Saskatchewan", "H CO", "Miami Heat", "began on March 29, 2018", "four", "manifestation of God's presence as perceived by humans according to the Abrahamic religions", "Emmanuelle Chriqui", "British Indian Association", "autopistas", "set to 0.05 ( 5 % ), implying that it is acceptable to have a 5 % probability of incorrectly rejecting the null hypothesis", "Tom Burlinson, Red Symons and Dannii Minogue", "The controlled synthesis of materials as thin films ( a process referred to as deposition ) is a fundamental step in many applications", "Tulsa, Oklahoma", "Kristy Swanson", "Joanna Moskawa", "Santa Fe, New Mexico", "Tbilisi", "genome", "North Atlantic Ocean", "people", "United Nations", "as part of their basic clothing bag issue when they enter the army during initial entry training", "2026", "318", "Director of the Federal Bureau of Investigation", "Michael Crawford", "\u03b5\u1f30\u03c2 \u03c4\u1f78 \u1f44\u03bd\u03bf\u03bc\u03b1 \u03c4\u03bf\u1fe6 \u03ba\u03b1\u03c4\u03c1\u1f78\u03c2", "Kida", "February 10, 2017", "Staci Keanan", "Brooklyn, New York", "1837", "Lagaan", "1996", "Los Lonely Boys", "appearances", "the foreign exchange market ( FX )", "The Hustons", "Sunday Post", "Karl Pilkington", "peking", "1860", "music video direction for Katy Perry's \"Born to Die\"", "Buck Owens", "\"Up\" mixes allegory with adventure and dumb imaginative exuberance.", "\"Empire of the Sun,\"", "off east  Africa", "modificre", "molly ringwald", "faerie", "a skull and crossbones"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6920067836494646}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.10526315789473684, 0.0, 0.1875, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 0.09090909090909093, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-4206", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-715", "mrqa_triviaqa-validation-7286", "mrqa_hotpotqa-validation-5141", "mrqa_newsqa-validation-4104", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-1024", "mrqa_searchqa-validation-11768", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-691"], "SR": 0.59375, "CSR": 0.5279706790123457, "EFR": 0.9230769230769231, "Overall": 0.6977876454178539}, {"timecode": 81, "before_eval_results": {"predictions": ["Sun Tzu", "shanghai", "Cleopatra", "nuclear tests", "harmony, protection, eternity and infinity", "a pizza", "Sarah Jessica Parker", "Long Island Sound", "Hawaii", "Fauvism", "Auguste D.", "Marcia Clark", "Jenny Industrial Revolution", "a period of development in the uterus from conception until birth; pregnancy.", "ravens", "J.R. Tolkien", "James Franco", "the Blue Ridge Mountain range", "Georgia", "a mixture of iron oxide and aluminum oxide", "Buddha", "Apple", "Thomas R. Gray", "catfish", "A Chorus Line", "Frommer\\'s", "Atonement", "a feeling of sadness about something that you did or did not do", "de Havilland", "Virginia", "Oneonta College", "a small breed", "Louisiana", "Matthew Vassar", "Japan", "a table", "The Police", "Air France", "mccartney", "Heracles", "trudge", "The Moody Blues", "Albert Camus", "Volvo", "Rhode Island", "yodeling", "the Indian Ocean", "a hypodermic needle", "Marat", "a nanosecond", "Didelphodon vorax", "Mason Alan Dinehart", "plays a key role in chain elongation in fatty acid biosynthesis and polyketide biosynthesis", "Stuart Craig, set designer for all of the previous Harry Potter films, returned for the final two parts", "2010", "arborea", "Madagascar", "Thomas William Hiddleston", "Estadio Victoria", "Allerdale", "Zimbabwean President Robert Mugabe", "70,000", "Israel", "Owsley Stanley"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5610119047619047}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6955", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-13181", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-6876", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-1201", "mrqa_searchqa-validation-4338", "mrqa_searchqa-validation-1527", "mrqa_searchqa-validation-12500", "mrqa_searchqa-validation-9661", "mrqa_searchqa-validation-6884", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-11009", "mrqa_searchqa-validation-9895", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-13846", "mrqa_searchqa-validation-5489", "mrqa_searchqa-validation-2254", "mrqa_searchqa-validation-13394", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-506", "mrqa_searchqa-validation-6250", "mrqa_searchqa-validation-1975", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-5526", "mrqa_triviaqa-validation-7122", "mrqa_hotpotqa-validation-980", "mrqa_hotpotqa-validation-868", "mrqa_newsqa-validation-3390"], "SR": 0.46875, "CSR": 0.5272484756097561, "EFR": 0.9705882352941176, "Overall": 0.7071454671807749}, {"timecode": 82, "before_eval_results": {"predictions": ["the Nautilus", "the Hopi", "China", "Pope John Paul II", "the Yangtze River", "Gnarls Barkley", "the Parthenon", "(Mary) Caroline Richards", "Marilyn Monroe", "souvlaki", "Richard III", "the bald eagle", "the Louvre", "29,333 square feet", "the Galapagos", "(Alan) Bobbe", "the Black Sox", "lynx", "Grenadine", "Constantine", "the Aleutian", "alchemy", "nouveau", "the Autobahn", "Anglo-Saxon", "the quail", "curtsy", "lacrosse", "Toronto", "grave", "King David", "(Vitamin) B1", "Hiawatha", "Indiana Jones", "Michigan", "Blue", "freelance", "Philadelphia", "Goodyear", "the hobbit", "the Red Sox", "(William) Claude) Dukenfield", "Yale University", "Graceland", "the Caspian Sea", "point", "Lee Marvin", "Prince Edward Island", "Westminster Abbey", "Superbad", "the Granite State", "1885", "$2.187 billion", "On the west", "belgian", "acai", "the Benedictine Order", "the sexual, romantic or emotional attraction towards people regardless of their sex or gender identity", "authorship", "getaway driver", "Drew Kesse,", "the pregnancy.", "eight-week", "1999"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6607638888888889}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-9065", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-15986", "mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-4928", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-7058", "mrqa_searchqa-validation-9159", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-12190", "mrqa_searchqa-validation-13324", "mrqa_searchqa-validation-2798", "mrqa_searchqa-validation-14340", "mrqa_searchqa-validation-13625", "mrqa_searchqa-validation-1956", "mrqa_searchqa-validation-11580", "mrqa_searchqa-validation-6810", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-16140", "mrqa_searchqa-validation-9427", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5308", "mrqa_triviaqa-validation-5499", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-475", "mrqa_newsqa-validation-3331", "mrqa_hotpotqa-validation-943"], "SR": 0.578125, "CSR": 0.5278614457831325, "EFR": 1.0, "Overall": 0.7131504141566266}, {"timecode": 83, "before_eval_results": {"predictions": ["George Washington", "The Office", "Jesus", "penguins", "vrai", "Napoleon Bonaparte", "A.J. Foyt", "a vulture", "Nantucket", "Ebony", "Trinity College", "Algeria", "Joseph Haydn", "(Stephen) Cheney", "the black market", "a number", "Saturday Night Fever", "the (ASIA)", "Sicilian pizza", "turtles", "the Empire State Building", "White blood cells", "a trifle", "florida", "Qubec", "Larry McMurtry", "Kellogg\\'s", "Helen of Troy", "a hood", "W=Fd", "Napoleon", "wood", "the Arctic", "Ben & Jerry", "Rigoletto", "Tim Tebow", "schizophrenia", "Catherine of Aragon", "the Standard Oil Co", "Pancho Gonzales", "the Aleutians", "the Mormon", "Lady Jane Grey", "Tommy Tutone", "the crescent moon", "Iraq", "grasshopper", "Copernicus", "poblano chile peppers", "William Safire", "Leonardo da Vinci", "London", "Charlton Heston", "Andrea Brooks", "the fallopian tube", "Some Like It Hot", "(N7/P7)", "\"Casablanca\"", "T. R. M. Howard", "Parlophone", "9:20 p.m. ET Wednesday.", "the U.N. aid agency", "1995", "four"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6296875}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-4701", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-12923", "mrqa_searchqa-validation-11419", "mrqa_searchqa-validation-2764", "mrqa_searchqa-validation-7924", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-897", "mrqa_searchqa-validation-5131", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1070", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-7466", "mrqa_searchqa-validation-8128", "mrqa_searchqa-validation-12014", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-699", "mrqa_searchqa-validation-12489", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-1989", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-112", "mrqa_hotpotqa-validation-3182", "mrqa_newsqa-validation-537"], "SR": 0.53125, "CSR": 0.5279017857142857, "EFR": 1.0, "Overall": 0.7131584821428572}, {"timecode": 84, "before_eval_results": {"predictions": ["the Czech Republic", "Catherine of Aragon", "Judas", "Windsor, Ontario", "Douglas", "comrade", "The Great Gatsby", "a fox", "sexuality", "baseball", "Solomon", "John McEnroe", "a bicycle", "Johnson", "Jericho", "a push", "Alexander Solzhenitsyn", "Tom Clonan", "Mexico", "Easter", "John Denver", "Hurricane Katrina", "Paris", "leeches", "the American Revolution", "the Philippines", "St. Mark", "Eragon", "The Beatles", "Louisiana", "Mexico", "jolly Roger", "engrave", "Daisy Miller", "The Princess Victoria", "a Y chromosome", "a ship", "Ulukau", "a wolf", "Jamestown", "Jerry Maguire", "the Ross Sea", "oyster", "The Jungle Book", "Raph J. Wilson Stadium", "Zimbabwe", "a bow", "Patty Duke", "I", "Hoffmann", "a calico", "Frankie Muniz", "season two", "A complex sentence", "40", "Mars", "Nowhere Boy", "August 1973", "a simple iron boar crest adorns the top of this helmet associating it with the Benty Grange Farm in the civil parish of Monyash in the English county of Derbyshire", "Richa Sharma", "Carrefour - saw many dead bodies and injured along the way -", "financial gain,", "in a Nazi concentration camp,", "Agent Mark Steinberg"], "metric_results": {"EM": 0.625, "QA-F1": 0.6839978448275861}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2758620689655173, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2392", "mrqa_searchqa-validation-5876", "mrqa_searchqa-validation-109", "mrqa_searchqa-validation-6222", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9592", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-11433", "mrqa_searchqa-validation-9119", "mrqa_searchqa-validation-271", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-3347", "mrqa_searchqa-validation-1469", "mrqa_naturalquestions-validation-9752", "mrqa_triviaqa-validation-2049", "mrqa_hotpotqa-validation-1226", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-3759"], "SR": 0.625, "CSR": 0.5290441176470588, "EFR": 1.0, "Overall": 0.7133869485294119}, {"timecode": 85, "before_eval_results": {"predictions": ["Boston", "Tigger Pooh", "Italian", "Eggs Benedict", "a mausoleum", "Ayn Rand", "Brahman", "\"Rough Stone Rolling\"", "The Sweet", "Tiger Woods", "the Amazon", "Harry Houdini", "Falconer", "Queen Latifah", "( Ezra) Cornell", "Strawberry Fields", "The Hague", "Geena Davis", "pharmacy", "(Bobby) Dust Bowl", "the NFL", "\" Jimmy\" Doolittle", "pneuma", "Shakespeare", "Oscar De La Hoya", "ABBA", "the League of Nations", "Marlee Matlin", "(the NIV)", "the X-Files", "The Dreaming Youth", "the high IQ society", "Edward Hopper", "oratorios", "steak", "a snake", "high chairs", "Salt Lake City", "Italy", "a watermelon", "the North Atlantic Treaty Organization", "Sparta", "the Sunday New York Times", "anode", "(TNG)", "Teachers Hall of Fame", "the Bicentennial", "the Cherokee", "the dermis", "the Texas Rangers", "fluoxetine", "H CO ( equivalently OC ( OH ) )", "Middle Eastern alchemy", "Brooklyn, New York", "Eton College", "Leeds", "on the banks of the San Antonio River", "Dwight D. Eisenhower", "Battleship", "\"Shake It Off\"", "ketamine.", "moved out of her rental house because it is facing foreclosure", "Why he's more American than a German,", "Santiago Ram\u00f3n y Cajal"], "metric_results": {"EM": 0.515625, "QA-F1": 0.56875}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-4035", "mrqa_searchqa-validation-43", "mrqa_searchqa-validation-14183", "mrqa_searchqa-validation-7391", "mrqa_searchqa-validation-5581", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-14331", "mrqa_searchqa-validation-378", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-6109", "mrqa_searchqa-validation-15693", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-13169", "mrqa_searchqa-validation-610", "mrqa_searchqa-validation-1894", "mrqa_searchqa-validation-16201", "mrqa_searchqa-validation-16198", "mrqa_searchqa-validation-7363", "mrqa_searchqa-validation-3434", "mrqa_searchqa-validation-12882", "mrqa_searchqa-validation-11773", "mrqa_searchqa-validation-10056", "mrqa_searchqa-validation-14355", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-2999", "mrqa_hotpotqa-validation-5190", "mrqa_newsqa-validation-2692", "mrqa_naturalquestions-validation-4103"], "SR": 0.515625, "CSR": 0.5288880813953488, "EFR": 1.0, "Overall": 0.7133557412790699}, {"timecode": 86, "before_eval_results": {"predictions": ["Happy feet", "a sprint", "a real animal", "Joseph", "Chicago", "Aphrodite", "Cannery Row", "Palatine", "California", "the Mississippi", "Alpha", "Quebec City", "nacreous", "the Texas Chainsaw Massacre", "the rotunda", "a Medal of Honor", "Manet", "Plutarch", "Mediolanum", "William Shakespeare", "Shropshire", "a kidney", "Afghanistan", "satin", "Lady Godiva", "Job", "Vasco da Gama", "Millard", "chino", "Finnegans Wake", "alamu", "the black market", "professor of higher education", "earthquakes", "Maastricht", "Delilah", "synapses", "croissant", "Rocky Down Mexico Way", "the lungs", "fuchsia", "metacarpal", "a pool", "Warsaw", "a parsnips", "the Mercury Seven", "Taiwan", "Gettysburg", "the NCAA", "trout", "\"Boat to China\"", "1959", "the first season of NCIS", "$75,000", "zenia", "fifteen", "stonemason\\'s Yard", "Agent Carter", "Orson Welles", "Manhattan, New York City", "56,", "Secretary of State", "Bright Automotive,", "James Hogg"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6680803571428571}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1883", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-16459", "mrqa_searchqa-validation-14805", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-861", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-9319", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-5239", "mrqa_searchqa-validation-6883", "mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-2279", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-2311", "mrqa_searchqa-validation-9618", "mrqa_naturalquestions-validation-9595", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-5762", "mrqa_hotpotqa-validation-24"], "SR": 0.59375, "CSR": 0.5296336206896552, "EFR": 1.0, "Overall": 0.7135048491379311}, {"timecode": 87, "before_eval_results": {"predictions": ["Tim Russert", "the Alfonsists", "Michael Crawford", "Sonu Nigam", "Pat McCormick", "Louis Mountbatten", "David Ben - Gurion", "April 6, 1917", "protect the genome from lethal chemical and physical agents", "1 US dollar worth close to 5,770 guaranies", "Geoffrey Zakarian", "$72", "Mary Elizabeth ( Margaret Hoard )", "Scott Schwartz", "Paris", "American indie pop band Foster the People", "Florida", "Husrev Pasha", "Patrick Warburton", "Alan Shearer", "Robert Duvall", "$66.5 million", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "slavery", "The Osmonds", "a political pamphlet", "Taiwan", "Anakin Jedi", "Jeff East", "one", "Thomas Lennon", "Jesse Triplett", "Kevin Garnett", "a star", "Uzbekistan", "Selena Gomez", "Washington", "the 2nd century", "offensive", "1998", "shared", "foreign investors", "Louis XVIII", "the inverted - drop - shaped icon that marks locations in Google Maps", "James Ray", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "2004", "The User State Migration Tool ( USMT )", "Robber baron", "December 20, 1951", "Crick", "Mount Aconcagua", "B&B", "1924", "Eugene Levy", "zona glomerulosa of the adrenal cortex", "Nicole Kidman", "last summer.", "work", "fifth", "banker", "an eyelid", "the Cubs", "a thief"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6260243243802898}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7741935483870968, 1.0, 1.0, 0.25, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9825", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-6021", "mrqa_naturalquestions-validation-72", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-5638", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-2008", "mrqa_naturalquestions-validation-5305", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-5041", "mrqa_naturalquestions-validation-8763", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-9712", "mrqa_triviaqa-validation-1386", "mrqa_triviaqa-validation-3335", "mrqa_hotpotqa-validation-3321", "mrqa_hotpotqa-validation-397", "mrqa_newsqa-validation-1117", "mrqa_searchqa-validation-14144", "mrqa_searchqa-validation-14176", "mrqa_triviaqa-validation-4676"], "SR": 0.53125, "CSR": 0.5296519886363636, "EFR": 0.9666666666666667, "Overall": 0.706841856060606}, {"timecode": 88, "before_eval_results": {"predictions": ["Cologne, Germany,", "Philip Markoff", "a bag", "Federer", "Galveston, Texas,", "Diego Milito's", "And you hold your breath, one bare breast at a time is tightly compressed between two flat panels and X-rayed.", "\"The suspects were plotting to attack two Shiite mosques, police stations, and a Norwegian telecommunications company in Punjab,", "Salt Lake City, Utah,", "normal maritime", "Form Design Center", "to make space for two ocean wind farms -- taking up 2 percent of the state's waters -- without angering fishing industries, killing whales or harming ecosystems.", "Rocky Ford brand cantaloupes", "\"The oceans are kind of the last frontier for use and development,\"", "In the last four weeks, authorities arrested three men with suicide vests who were plotting to carry out the attacks,", "\"We want to reset our relationship and so we will do it together.'\"", "club managers,", "Long Island", "90", "FBI", "Reggae legend Lucky Dube,", "the Kurdish militant group in Turkey", "At least 14", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Kerstin Fritzl,", "Defense of Marriage Act", "Europe,", "for not doing more since taking office.", "immediate release into the United States of 17 Chinese Muslims", "Greeley, Colorado,", "Festival Foods", "people were throwing Molotov cocktails, rocks and glass.", "Since drugs are funding the insurgency, NATO has a self-interest in supporting Afghan forces in destroying drug labs, markets and convoys,\"", "Daniel Radcliffe", "more than 1.2 million", "\"It was a wrong thing to say, something that we both acknowledge,\"", "12.3 million", "Krishna Rajaram,", "North Korea", "Patrick McGoohan,", "saying Chaudhary's death was warning to management.", "Jund Ansar Allah", "state senators", "2,000 euros ($2,963)", "Anil Kapoor", "And the Obama administration needs to think of \"victory\" not only in the short term and from a purely anti-terrorism perspective, but also in consideration of the people who have lived and will continue to live in those lands.", "the Yemeni port city of Aden", "The federal officers' bodies", "\"Up\"", "central business district of Bangkok", "journalists and the flight crew", "By petition for a writ of certiorari, filed by a party to a case that has been decided by one of the United States courts of appeals or by the United United States Court of Appeals for the Armed Forces", "pigs", "James Corden", "Norway", "Hamlet", "The Hague Convention", "Chris Hemsworth", "Viscount Cranborne", "England and Ireland", "beef", "Sleyman", "the rainforest", "Ramadan"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5815393339731946}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 0.07407407407407407, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.06666666666666667, 0.1, 1.0, 0.6666666666666666, 0.3157894736842105, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0851063829787234, 0.0, 1.0, 1.0, 0.33333333333333337, 0.7272727272727273, 0.1621621621621622, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-834", "mrqa_newsqa-validation-1361", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-1534", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-594", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-798", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-2192", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-928", "mrqa_naturalquestions-validation-7950", "mrqa_hotpotqa-validation-3169", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-3763"], "SR": 0.46875, "CSR": 0.5289676966292135, "EFR": 1.0, "Overall": 0.7133716643258428}, {"timecode": 89, "before_eval_results": {"predictions": ["Pease Air National Guard Base", "Kim So-hyun", "president", "Comedy Film Nerds", "9\u201310 March 1945", "2011", "John D Rockefeller", "during the early 1970s", "Asiana Town", "R&B", "Rockland County", "Manitowoc County", "south-east", "1967", "alcoholic drinks", "FCA", "Chrysler", "Australia", "chimpanzee", "\"Traumnovelle\" (\"Dream Story\"", "Joshua Rowley", "Robert Digges Wimberly Connor", "Rage Against the Machine", "Rolling Stones", "Baden-W\u00fcrttemberg, Germany", "2001 NBA All-Star Game", "\"Rated R\"", "95 AD", "1614", "French", "\"The Manhunter from Mars\"", "Mondays", "Michael Jordan", "I, (Annoyed Grunt)-bot", "HSBC Main Building", "1944", "Kalokuokamaile", "17 October 2006", "melodic hard rock", "Irish Parliamentary Party", "Anne Fletcher", "1822", "Mulberry", "Suspiria", "BBC Focus", "Kansas\u2013Nebraska Act of 1854", "Scandinavian design", "Buck Owens and the Buckaroos", "Big Machine Records", "postal delivery", "Flaw", "October 27, 2016", "1972", "the initiator must go through an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "Daniel Boone", "elbow", "Tigris", "Kgalema Motlanthe,", "CNN's \"Piers Morgan Tonight\"", "misdemeanor assault charges", "Florida", "The Partridge Family", "Mickey Spillane", "housing, business and infrastructure repairs,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6726934523809525}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false], "QA-F1": [0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.8, 0.8, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-574", "mrqa_hotpotqa-validation-247", "mrqa_hotpotqa-validation-1702", "mrqa_hotpotqa-validation-3795", "mrqa_hotpotqa-validation-3458", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5233", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-2672", "mrqa_hotpotqa-validation-837", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-3832", "mrqa_naturalquestions-validation-1728", "mrqa_triviaqa-validation-7701", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-3369"], "SR": 0.5625, "CSR": 0.5293402777777778, "EFR": 0.9642857142857143, "Overall": 0.7063033234126984}, {"timecode": 90, "before_eval_results": {"predictions": ["Sharon Sheeley", "Ardeth Bay", "2009", "singer", "Pakistan", "1754", "\"Na Na\"", "Bundesliga club VfL Wolfsburg", "ady", "David Villa", "Adrian Peter McLaren", "2013", "a schoolmaster", "Cleopatra", "Leon Marcus Uris", "Knoxville, Tennessee", "cancer", "Kim Yoon-seok and Ha Jung-woo", "The Lion Guard", "25 November 2015", "Craig William Macneill", "January 14,", "2,664", "Tamil", "Objectivism", "Chicago", "Gatwick Airport", "Riot Act", "The Gold Coast", "January 30, 1930", "Soma", "October 29, 1985", "35,124", "Tercera Divisi\u00f3n \u2013 Group 4", "Sir Seretse Goitsebeng Maphiri Khama, GCB, KBE", "Scandinavian design", "Mike Pence", "Barack Obama's Cabinet", "Flexible-fuel", "Bulgarian", "1949", "Trappist beer", "\"Waiting for Guffman\"", "Presbyterian Church (USA)", "138,535 people", "Ry\u016bky\u016b", "1972", "Stern-Plaza in Potsdam", "Life Is a Minestrone", "Columbia Records", "The Spiderwick Chronicles", "Jewel Akens", "gravitation", "ensures consistency within a document and across multiple documents and enforces best practice in usage and in language composition, visual composition, orthography and typography", "Mexico", "Ann Widdecombe", "Jennifer Eccles", "\"TSA has reviewed the procedures themselves and agrees that they need to be changed,\"", "sailing", "Robert", "dolls", "Diane Cilento", "CO2", "Walgreens"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6312196823406062}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.30434782608695654, 0.0, 1.0, 1.0, 0.2564102564102564, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3064", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-1539", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-3907", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-2734", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2717", "mrqa_hotpotqa-validation-1896", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-598", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-1457", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-4050", "mrqa_triviaqa-validation-4655", "mrqa_newsqa-validation-386", "mrqa_searchqa-validation-4335", "mrqa_searchqa-validation-10988", "mrqa_searchqa-validation-11743", "mrqa_searchqa-validation-10146"], "SR": 0.53125, "CSR": 0.5293612637362637, "EFR": 0.9666666666666667, "Overall": 0.7067837110805861}, {"timecode": 91, "before_eval_results": {"predictions": ["Close Encounters of the Third Kind", "TouchTunes", "the Jaguar", "the Gateway Mall", "Friday", "Sabino Canyon", "Bloom", "Babe Ruth", "the Hallmark Channel", "Arkansas", "Vince Lombardi", "Virgo", "contemporary", "Coyote", "pastoral", "Tito Puente", "Hydrogen", "Ben Jonson", "malignant disease", "Margaret, Countess of Snowdon", "Salt Lake City", "San Francisco", "the 1940s", "Mary Baker Eddy", "Bank One Corp.", "the College of William & Mary", "the Wright Brothers", "Badminton", "John Deere", "lauren", "Pontiac", "Reptiles", "Georgia", "Key lime pie", "Lettuce", "the Hodgepodge", "bumblebee", "Savannah", "Rickey Henderson", "parquet", "Alice Walker", "F Troop", "Russia", "Lincoln", "Eva Pern", "Port Royal", "a key", "Ghost", "the Basilica Cathedral of Lima", "Mesopotamia", "Jean-Paul Marat", "Cetshwayo", "Bay of Montevideo", "a bank, drawn on the bank's own funds and signed by a cashier", "jingle", "Bobby Brown", "bATH", "1.5 million", "Macomb County", "Kristoffer Rygg", "boats are expected to arrive in Veracruz, Mexico,", "Monday night.", "23", "biographer"], "metric_results": {"EM": 0.5, "QA-F1": 0.6195075757575758}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-6087", "mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-7846", "mrqa_searchqa-validation-5927", "mrqa_searchqa-validation-13146", "mrqa_searchqa-validation-13929", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-595", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-10498", "mrqa_searchqa-validation-955", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-10442", "mrqa_searchqa-validation-3322", "mrqa_searchqa-validation-1574", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-16068", "mrqa_searchqa-validation-12942", "mrqa_searchqa-validation-6896", "mrqa_naturalquestions-validation-3303", "mrqa_triviaqa-validation-7696", "mrqa_hotpotqa-validation-2255", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-979", "mrqa_hotpotqa-validation-4539"], "SR": 0.5, "CSR": 0.5290421195652174, "EFR": 1.0, "Overall": 0.7133865489130435}, {"timecode": 92, "before_eval_results": {"predictions": ["Luzon", "Brancusi", "Quantico, Virginia", "the East", "William Shakespeare", "Ende (disease)", "yurt", "Alaska", "Sputnik", "Richmond, Virginia", "the period", "Java", "a baritone", "Blanche DuBois", "(1) the flag", "vulcanization", "Wuthering Heights", "Ali", "September 20, 1934", "3-4 Box Set", "Frederick Forsyth", "Chesterfield, Virginia", "a chipmunk", "Josephine", "chocolate", "a warrant", "Rossini", "Oman", "Lapland", "Tom", "Roman Polanski", "Joan Didion", "USS", "Baltimore", "the Bay of Bengal", "Button Gwinnett", "Clinton", "Terrific", "geology", "six sides", "Olympia", "Ship of Fools", "Ghost Town Streets", "yang", "blood", "Margaret Mitchell", "Frances", "Vin Diesel", "cremation", "the French & Indian War", "manic", "central Saskatchewan", "lighter", "a scuffle with the Beast Folk", "(Judi) Dench", "british", "portaenau", "Nairobi, Kenya", "Love Streams", "My Beautiful Dark Twisted Fantasy", "between June 20 and July 20.", "Michael Krane,", "Virgin America", "AIDS and HIV"], "metric_results": {"EM": 0.515625, "QA-F1": 0.595486111111111}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1244", "mrqa_searchqa-validation-16790", "mrqa_searchqa-validation-10078", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-11106", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6502", "mrqa_searchqa-validation-173", "mrqa_searchqa-validation-13847", "mrqa_searchqa-validation-6877", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-3320", "mrqa_searchqa-validation-20", "mrqa_searchqa-validation-4824", "mrqa_searchqa-validation-15704", "mrqa_searchqa-validation-15802", "mrqa_searchqa-validation-10543", "mrqa_searchqa-validation-315", "mrqa_searchqa-validation-14800", "mrqa_searchqa-validation-15780", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-6507", "mrqa_naturalquestions-validation-6707", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-6867"], "SR": 0.515625, "CSR": 0.5288978494623655, "EFR": 1.0, "Overall": 0.7133576948924731}, {"timecode": 93, "before_eval_results": {"predictions": ["Mesopotamia", "Gettysburg College", "Tim McGraw", "provides the public with financial information about a nonprofit organization", "Lulu", "Telma Hopkins", "Mel Gibson", "2017", "drivers who were 2016 Pole Award winners, former Clash race winners,, former Daytona 500 pole winners who competed full - time in 2016, and drivers who qualified for the 2016 Chase", "17 December 1968", "At the north end, a portion of the early route through Pacific Grove begins at the intersection of Del Monte Blvd and Esplanade Street", "In a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form, he made the hair more `` wild '' and covered Frieza's body in red fur", "Audrey II", "January 2017", "NIRA", "1922", "Julie Kavner", "Justin Timberlake", "American production duo The Chainsmokers", "13 May 1787", "The Province", "a father, Paul Monti, whose son, Medal of Honor recipient Jared, was killed in Afghanistan while trying to save a fellow soldier", "Seattle, Washington, site of the Century 21 Exposition, the 1962 World's Fair", "honey bees", "Article 1, Section 2, Clause 3 of the United States Constitution", "Bill Irwin", "Napoleon", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "September 27, 2017", "Fusajiro Yamauchi", "March 31 to April 8, 2018", "Tbilisi, Capital of Georgia", "Tiffany Adams Coyne", "Uranus", "high rates of inflation and hyperinflation", "1939", "Richard Masur", "Maria Hall", "the Philippines", "Sauron", "Lana Del Rey", "position", "159", "The Third Five - year Plan", "KU", "has just arrived from Puerto Rico for her arranged marriage to Chino, a friend of Bernardo's", "activates a relay which will handle the higher current load", "A patent is a set of exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time in exchange for detailed public disclosure of an invention", "commemorating fealty and filial piety", "in the stems and roots of certain vascular plants", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "euro", "Robin Hood\\'s A Holy Grail", "Snowshoe", "February 13, 1946", "Crystal Dynamics", "Congo River", "Jason Chaffetz", "Angels & Demons", "humiliate herself by standing next to a story,\"", "Khrushchev", "Julie Andrews", "the Headless Horseman", "Leo Frank,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6349082221657653}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6206896551724138, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428572, 0.0, 0.7499999999999999, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4615384615384615, 1.0, 0.2, 0.0909090909090909, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-2862", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4540", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-8026", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-9264", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8159", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3305", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-3374"], "SR": 0.515625, "CSR": 0.5287566489361701, "EFR": 0.9354838709677419, "Overall": 0.7004262289807824}, {"timecode": 94, "before_eval_results": {"predictions": ["direct scattering and inverse scattering", "Thon MarialMaker", "Battle of Chester", "youngest TV director ever", "19 February 1927, Halifax", "on the shore,", "playback singer, director, writer and producer", "L\u00edneas A\u00e9reas", "English", "National Basketball Development League", "Neville Chamberlain", "Boulder High School in Boulder, Colorado.", "Revengers Tragedy", "Japan", "rural", "8", "Larry Alphonso Johnson Jr.", "Gabriel Jesus Iglesias", "August 28, 1774", "Pantone Matching System (PMS)", "Las Vegas Boulevard", "intelligent design", "Barbara Ryan Coleman", "Jack Elam", "Adelaide Botanic Garden, Hutt Street, and Victoria Park", "Naruto Uzumaki", "Kansas", "nearly 80 years", "Chevrolet Corvette Stingrays", "\"thirtysomething\"", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Tom Tykwer", "German political and military leader as well as one of the most powerful figures in the Nazi Party (NSDAP) that ruled Germany from 1933 to 1945.", "In the UK, he was awarded nine platinum album certifications, eleven gold and eight silver, releasing eleven number-one albums.", "Drowning Pool", "in various games typically found within a casino, ranging from card to slot machines.", "the Food and Agriculture Organization", "swing fox", "Bharat Ratna", "Cesar Millan", "Eurasia", "Beauty and the Beast", "Bardney", "Holinshed\\'s Chronicles", "August 9, 2017", "Bangalore University", "1 April 1985", "Australian", "Bonkyll Castle", "February 5, 2015", "William Shakespeare", "trident", "alveolar bone", "Dortmund - Ems Canal", "h Hugh Quarshie", "george germany", "Tokyo", "Utah Valley Regional Medical Center,", "Madonna", "Fareed Zakaria", "Easter Island", "Eli Whitney", "Today Show", "25"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6520089285714286}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [0.8333333333333333, 0.4, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.47619047619047616, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-839", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-5255", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2344", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-4558", "mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-854", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-1206", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-2510", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-2703", "mrqa_triviaqa-validation-4660", "mrqa_triviaqa-validation-514"], "SR": 0.546875, "CSR": 0.5289473684210526, "EFR": 1.0, "Overall": 0.7133675986842105}, {"timecode": 95, "before_eval_results": {"predictions": ["250 million copies", "Ben Ainslie", "September 22, 1978", "The Golden Egg", "Scott Mosier", "1950", "Roy Warren Spencer", "1484", "Xfinity channel 803", "World of Wonder", "London Review of Books", "Russian", "James Coburn", "July 25", "Hong Kong Mak\u00e9l\u00e9l\u00e9", "radio presenter", "Northern Lights", "coca wine", "Mach number", "Jordan Ridgeway", "Maine", "Encore Las Vegas", "\"Baa, Baa, Black sheep\"", "It's Always Sunny in Philadelphia", "John Francis Kelly", "Madeleine L' Engle", "1972", "President John F. Kennedy", "paracyclist", "Mandarin", "Kevin Spacey", "Deputy Vice-Chancellor", "Song Il-gon", "ortal Kombat X", "Mickey Mouser", "Grammy Award", "right-hand", "Sheen Michaels Entertainment", "Sela Ann Ward", "seal hunting", "Houston Rockets", "DI Humphrey Goodman", "Daphnis et Chlo\u00e9", "Nebraska Cornhuskers", "Metro-Goldwyn-Mayer", "P.O.S", "My Backyard", "Sun Woong", "American professional boxer", "Aloe Vera of America", "creeks", "1992", "the Kingdom of Great Britain", "Roger Dean Stadium", "cirrocumulus", "Compiegne", "La traviata", "April.", "\"The Da Vinci Code\"", "Dogpatch Labs", "iceberg", "fuel cell", "Queen Victoria", "Venus Williams"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6630952380952381}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true], "QA-F1": [0.8, 1.0, 0.5, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-74", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-5147", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-1059", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-5809", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-5447", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-4196", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5665", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-3301", "mrqa_hotpotqa-validation-5071", "mrqa_hotpotqa-validation-62", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-3773", "mrqa_naturalquestions-validation-360", "mrqa_newsqa-validation-3838", "mrqa_searchqa-validation-14503"], "SR": 0.5625, "CSR": 0.529296875, "EFR": 1.0, "Overall": 0.7134375000000001}, {"timecode": 96, "before_eval_results": {"predictions": ["an obsessed and tormented king", "1927", "16,116", "The 2012 Summer Olympics", "at the end of the 18th century", "1942", "The Highwaymen", "Estadio de L\u00f3pez Cort\u00e1zar", "Syracuse", "Kim Jong-hyun", "the Bears", "Gillian Leigh Anderson", "the alternative rock band R.E.M.", "Ice Princess", "The Summer Olympic Games", "Oldham County", "1896", "Oracle Corporation", "143,007", "severe acute respiratory syndrome (SARS)", "5.3", "chocolate-colored", "Norman Graham Hill", "1908", "Neneh Mariann Karlsson", "Eminem", "\"Love Streams\"", "In a Better World", "the Shropshire Union Canal", "Easy", "The Killer", "2015", "Dutch", "Lowestoft, Suffolk", "Trey Parker", "\"Pimp My Ride\"", "Big 12 Conference", "Hillsborough County", "Dancing with the Stars", "wooden Indian", "John Francis Kelly", "early Romantic period", "Prescription Drug User Fee Act", "the Sun", "Bhushan Patel", "1692", "navigation by river", "The Wu-Tang Clan", "\"Kids\"", "Mortal Kombat", "Kew Gardens", "third season", "May 2016", "Kristy Swanson", "colonel", "Conan Doyle", "right", "put a lid on the marking of Ashura", "Pakistan's", "homicide", "bread pudding", "leather", "the cornea", "Eleanor Roosevelt"], "metric_results": {"EM": 0.75, "QA-F1": 0.8853805916305917}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-3344", "mrqa_hotpotqa-validation-4011", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1401", "mrqa_hotpotqa-validation-2482", "mrqa_triviaqa-validation-2789"], "SR": 0.75, "CSR": 0.5315721649484536, "EFR": 0.875, "Overall": 0.6888925579896907}, {"timecode": 97, "before_eval_results": {"predictions": ["Eddie Redmayne", "Caucasus", "David Bowie", "horace Lindrum", "Granada", "Treaty of Brest-Litovsk", "Karl Marx", "Paramounts", "(Mary) Monroe", "fish", "1957", "1919", "transsexual", "Fred Astaire", "germany", "Scotland Yard detective", "Inverness, Aberdeen, Glasgow and Edinburgh", "fair", "winnie Mae", "Rudyard Kipling", "1921", "The Full Monty", "Desdemona", "avocado", "Frans Hals", "Syriza", "Ford", "soybeans", "Cole Porter", "1826", "W W Jacobs", "Parthenon", "Paddy Doherty", "Thomas Aquinas", "Dubonnet Rouge Aperitif", "an elephant", "Tigran Petrosyan", "i", "Westminster Abbey", "Canada", "Flavio Briatore", "Edward VII", "Tombstone", "germany", "Mr. Men and Little Miss", "worcester Cathedral", "Mars", "December 7, 1941", "ear", "Kerri Comaneci", "Neil Armstrong", "Ant & Dec", "John Ernest Crawford", "March 1930", "Gillian Anderson", "Electronic Attack Squadron 135", "95 AD", "170", "\"It's you against the world and the CBT will bring out a dark side you might not have even realized you possessed\"", "Carol Fowler", "awe-inspiring", "the Caspian Sea", "(Francisco) Pizarro", "1922 to 1991"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6197916666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5066", "mrqa_triviaqa-validation-4459", "mrqa_triviaqa-validation-3538", "mrqa_triviaqa-validation-7293", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6630", "mrqa_triviaqa-validation-7326", "mrqa_triviaqa-validation-5204", "mrqa_triviaqa-validation-2651", "mrqa_triviaqa-validation-1602", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-3730", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3800", "mrqa_triviaqa-validation-1547", "mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-229", "mrqa_hotpotqa-validation-2058", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-4053", "mrqa_searchqa-validation-8143", "mrqa_searchqa-validation-15233", "mrqa_naturalquestions-validation-7080"], "SR": 0.546875, "CSR": 0.5317283163265306, "EFR": 0.9310344827586207, "Overall": 0.7001306848170303}, {"timecode": 98, "before_eval_results": {"predictions": ["sent an e-mail to reporters Wednesday with the subject line \"Vice presidential...\"", "Afghanistan", "digging at the site", "Several suspects are believed to have engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags", "his health and about a comeback.", "poems", "then-Sen. Obama", "Gloria Allred,", "581", "The Everglades, known as the River of Grass,", "Herman Cain", "Brett Cummins,", "3-0", "The remains of Cologne's archive building", "Mobile County Circuit Judge Herman Thomas", "celebrities", "The Kurdish Gas City is projected to generate job opportunities for nearly 200,000 Iraqi citizens in infrastructure, industrial projects, support services and other business activities.", "Phillip A. Myers.", "I showed up at WAHR with the record that would play my theme song, \"Swingin' Down the Lane.\"", "share personal information.", "London and Buenos Aires", "Sheik Mohammed Ali", "Iraqi Prime Minister Nouri al-Maliki", "Egypt", "\"It was incredible. We've had so much rain, and yet today it was beautiful,\"", "\"The Little Couple,\"", "WBO welterweight title", "Austin, Texas,", "17-month", "a monthly allowance,", "Manmohan Singh's", "\"I'm certainly not nearly as good of a speaker as he is.\"", "for death squad killings carried out during his rule in the 1990s.", "100 meter", "sniff out cell phones.", "Fayetteville, North Carolina,", "Bill Haas", "Consumer Reports", "28", "step up.\"", "42 years old", "since 1983.", "improve health and beauty.", "almost 100", "Alberto Espinoza Barron's", "Derek Mears", "\"I want to get the job done. We have identified a problem -- let's go solve it together,\"", "the fastest circumnavigation of the globe in a powerboat -- and now Earthrace is for sale.", "$106,482,500", "18th", "Haeftling,", "on the table", "Asuka", "Bart Millard", "niger nismo", "stone arch bridges", "Jane Austen", "the Marx Brothers film", "Indian", "World War I,", "(department) manager", "70th", "W. Somerset Maugham", "leicestershire"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5782852472819577}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.9210526315789475, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.4444444444444445, 0.7272727272727272, 0.0, 0.07692307692307693, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2105263157894737, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.6666666666666666, 1.0, 0.1, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2182", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-2272", "mrqa_newsqa-validation-3008", "mrqa_newsqa-validation-1336", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3323", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2745", "mrqa_newsqa-validation-2541", "mrqa_naturalquestions-validation-2024", "mrqa_triviaqa-validation-5314", "mrqa_triviaqa-validation-3928", "mrqa_triviaqa-validation-5307", "mrqa_hotpotqa-validation-3326", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-14191"], "SR": 0.484375, "CSR": 0.53125, "EFR": 0.9090909090909091, "Overall": 0.6956463068181818}, {"timecode": 99, "UKR": 0.71484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1002", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-136", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-153", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-1746", "mrqa_hotpotqa-validation-1834", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2075", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-2132", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2255", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-24", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2463", "mrqa_hotpotqa-validation-2489", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2652", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-277", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2844", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-3145", "mrqa_hotpotqa-validation-3278", "mrqa_hotpotqa-validation-3289", "mrqa_hotpotqa-validation-3301", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-3713", "mrqa_hotpotqa-validation-3753", "mrqa_hotpotqa-validation-377", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3996", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4169", "mrqa_hotpotqa-validation-4378", "mrqa_hotpotqa-validation-4435", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5221", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5283", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5630", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-585", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-969", "mrqa_naturalquestions-validation-10077", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-10693", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-1329", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-228", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2459", "mrqa_naturalquestions-validation-2471", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-269", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3033", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4697", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-5761", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-6408", "mrqa_naturalquestions-validation-6426", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-7967", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-8329", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8741", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-9208", "mrqa_naturalquestions-validation-9219", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9595", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-9987", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1475", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1744", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-189", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2379", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2993", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-3077", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-3118", "mrqa_newsqa-validation-3124", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3464", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3925", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-4029", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-506", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-678", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-965", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-10146", "mrqa_searchqa-validation-10231", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-10527", "mrqa_searchqa-validation-10763", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10879", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-11077", "mrqa_searchqa-validation-11089", "mrqa_searchqa-validation-11111", "mrqa_searchqa-validation-11151", "mrqa_searchqa-validation-11196", "mrqa_searchqa-validation-11599", "mrqa_searchqa-validation-11976", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12092", "mrqa_searchqa-validation-12660", "mrqa_searchqa-validation-12942", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13182", "mrqa_searchqa-validation-13352", "mrqa_searchqa-validation-13625", "mrqa_searchqa-validation-13654", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-1371", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14001", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14519", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-14705", "mrqa_searchqa-validation-14740", "mrqa_searchqa-validation-14770", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-14805", "mrqa_searchqa-validation-15045", "mrqa_searchqa-validation-15157", "mrqa_searchqa-validation-15235", "mrqa_searchqa-validation-15394", "mrqa_searchqa-validation-15659", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-15843", "mrqa_searchqa-validation-15883", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-16140", "mrqa_searchqa-validation-16335", "mrqa_searchqa-validation-16515", "mrqa_searchqa-validation-1655", "mrqa_searchqa-validation-16644", "mrqa_searchqa-validation-16751", "mrqa_searchqa-validation-16786", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-1741", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1897", "mrqa_searchqa-validation-2116", "mrqa_searchqa-validation-2215", "mrqa_searchqa-validation-2228", "mrqa_searchqa-validation-2392", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2801", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-3087", "mrqa_searchqa-validation-334", "mrqa_searchqa-validation-3347", "mrqa_searchqa-validation-3469", "mrqa_searchqa-validation-3496", "mrqa_searchqa-validation-3567", "mrqa_searchqa-validation-3760", "mrqa_searchqa-validation-3825", "mrqa_searchqa-validation-386", "mrqa_searchqa-validation-4023", "mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-4512", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4808", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-5625", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-5669", "mrqa_searchqa-validation-5733", "mrqa_searchqa-validation-5906", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-629", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-6616", "mrqa_searchqa-validation-6736", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-69", "mrqa_searchqa-validation-6941", "mrqa_searchqa-validation-7139", "mrqa_searchqa-validation-7166", "mrqa_searchqa-validation-7440", "mrqa_searchqa-validation-746", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7753", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-8239", "mrqa_searchqa-validation-826", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8293", "mrqa_searchqa-validation-8374", "mrqa_searchqa-validation-8383", "mrqa_searchqa-validation-8459", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-861", "mrqa_searchqa-validation-8702", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8761", "mrqa_searchqa-validation-8933", "mrqa_searchqa-validation-9119", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9682", "mrqa_searchqa-validation-9752", "mrqa_searchqa-validation-9942", "mrqa_squad-validation-10026", "mrqa_squad-validation-10227", "mrqa_squad-validation-112", "mrqa_squad-validation-1204", "mrqa_squad-validation-1454", "mrqa_squad-validation-1758", "mrqa_squad-validation-1759", "mrqa_squad-validation-2225", "mrqa_squad-validation-2365", "mrqa_squad-validation-2466", "mrqa_squad-validation-2784", "mrqa_squad-validation-3080", "mrqa_squad-validation-3110", "mrqa_squad-validation-3130", "mrqa_squad-validation-3581", "mrqa_squad-validation-3632", "mrqa_squad-validation-4259", "mrqa_squad-validation-457", "mrqa_squad-validation-4621", "mrqa_squad-validation-4770", "mrqa_squad-validation-5010", "mrqa_squad-validation-5651", "mrqa_squad-validation-5784", "mrqa_squad-validation-5913", "mrqa_squad-validation-6166", "mrqa_squad-validation-6694", "mrqa_squad-validation-6789", "mrqa_squad-validation-6947", "mrqa_squad-validation-7214", "mrqa_squad-validation-7269", "mrqa_squad-validation-7521", "mrqa_squad-validation-7547", "mrqa_squad-validation-7596", "mrqa_squad-validation-7848", "mrqa_squad-validation-8052", "mrqa_squad-validation-8151", "mrqa_squad-validation-8733", "mrqa_squad-validation-8830", "mrqa_squad-validation-9233", "mrqa_squad-validation-930", "mrqa_squad-validation-9311", "mrqa_squad-validation-962", "mrqa_squad-validation-9816", "mrqa_squad-validation-9859", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1216", "mrqa_triviaqa-validation-124", "mrqa_triviaqa-validation-1450", "mrqa_triviaqa-validation-1547", "mrqa_triviaqa-validation-1552", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-1923", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-2200", "mrqa_triviaqa-validation-2208", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2505", "mrqa_triviaqa-validation-2668", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-3190", "mrqa_triviaqa-validation-3226", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3400", "mrqa_triviaqa-validation-3434", "mrqa_triviaqa-validation-3471", "mrqa_triviaqa-validation-3707", "mrqa_triviaqa-validation-3796", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-4365", "mrqa_triviaqa-validation-4385", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-4660", "mrqa_triviaqa-validation-4737", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-4890", "mrqa_triviaqa-validation-5158", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5361", "mrqa_triviaqa-validation-5457", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5820", "mrqa_triviaqa-validation-5832", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-5891", "mrqa_triviaqa-validation-6239", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-6329", "mrqa_triviaqa-validation-642", "mrqa_triviaqa-validation-6540", "mrqa_triviaqa-validation-6636", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-6729", "mrqa_triviaqa-validation-6985", "mrqa_triviaqa-validation-7031", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7219", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7350", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-958"], "OKR": 0.826171875, "KG": 0.4875, "before_eval_results": {"predictions": ["Mount Rainier, Washington", "Douglas Jackson", "Austral L\u00edneas A\u00e9reas", "2", "Craig William Macneill", "USS Essex", "8,648", "three", "Jeffrey Adam \"Duff\" Goldman", "Minnesota, United States", "most performed song of all time", "Oregon Ducks football", "Arkansas", "the 2011 Pulitzer Prize in General Nonfiction", "Golden Gate", "\"Grandmasters\"", "Broadcasting House in London", "a mystery after one of her teachers goes missing", "sitcom \"Barney Miller\"", "Lily Hampton", "41st President of the United States", "Big Machine Records", "constant support from propaganda campaigns.", "\"The Heirs\" (2013) and \"Fight for My Way\" ( 2017)", "Saturday Night Live", "religious", "Tumi Holdings, Inc.", "Black Ravens", "commercial", "Suspiria", "Silvia Navarro", "22,500", "Warsaw, Poland", "Nelson County, Kentucky", "Krusty the Clown", "25 million", "Cleopatra VII Philopator", "James G. Kiernan", "the MC5", "James City County, and Newport News in the Hampton Roads region of Virginia", "Arab", "Linda Ronstadt", "United Kingdom", "August 19, 2013", "the Americas and the entire South American temperate zone", "\"The Omega Man\"", "five-time", "Steve Coogan", "13 May 2018", "Kevin Spacey", "Stalybridge Celtic", "The term was first used in tennis", "Frank Zappa", "1991", "red", "Fred Trueman", "Scotland", "Chesley \"Sully\" Sullenberger", "Mario Balotelli", "Friday,", "the Lifeboat", "a kilobytes", "a banker", "Benazir Bhutto"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7008136828449328}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.888888888888889, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8, 1.0, 0.3333333333333333, 0.25, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.6666666666666666, 0.05128205128205128, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-5774", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-1798", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-3197", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-2156", "mrqa_naturalquestions-validation-10325", "mrqa_triviaqa-validation-7095", "mrqa_newsqa-validation-2470", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-14144"], "SR": 0.5625, "CSR": 0.5315624999999999, "EFR": 1.0, "Overall": 0.7120156249999999}]}