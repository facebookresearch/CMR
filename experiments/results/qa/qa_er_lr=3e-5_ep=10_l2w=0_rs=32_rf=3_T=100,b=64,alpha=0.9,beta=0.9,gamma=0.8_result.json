{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.9,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5390, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["a combination of anthrax and other pandemics", "Children in Need", "July 2013", "4 August 1915 until November 1918", "three hundred years", "Cultural imperialism", "caning", "three to five", "weak labor movements", "a school or other place of formal education", "agricola", "Denmark, Iceland and Norway", "colonizing empires", "removed some parts", "Los Angeles Times", "Richard Lindzen", "nineteenth-century cartographic techniques", "1903", "Japan", "international metropolitan region", "United States", "ash leaf", "the problem of multiplying two integers", "an official school sport", "Hong Kong", "Book of Common Prayer", "until 1796", "full independent prescribing authority", "democracy", "a mainline Protestant Methodist denomination", "Michael Eisner", "Slipback", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Jerusalem", "pH or available iron", "Bart Starr", "the disbelieving (Kafir) colonial powers", "cryptomonads", "on Fresno's far southeast side", "four", "Demaryius Thomas", "faith", "William Hartnell's poor health", "Annual Conference Order of Elders", "Any member", "Thomas Reid and Dugald Stewart", "Kurt Vonnegut", "Paul Revere", "Warszawa", "the instance", "he sent missionaries", "fourteen", "Zhongtong", "Del\u00fc\u00fcn Boldog", "Rev. Paul T. Stallsworth", "market", "73", "20.8%", "live", "free", "inequality", "260 kilometres", "The Daleks", "a Latin translation of the Qur'an"], "metric_results": {"EM": 0.84375, "QA-F1": 0.86171875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-1891", "mrqa_squad-validation-1766", "mrqa_squad-validation-9918", "mrqa_squad-validation-4662", "mrqa_squad-validation-2372", "mrqa_squad-validation-3119", "mrqa_squad-validation-3130", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-2289"], "SR": 0.84375, "CSR": 0.84375, "EFR": 1.0, "Overall": 0.921875}, {"timecode": 1, "before_eval_results": {"predictions": ["canceled", "photooxidative damage", "Britain", "He began investigating what he referred to as radiant energy", "Ps. 31:5", "five", "supporting applications such as on-line betting, financial applications", "Josh Norman", "DuMont", "24", "Dutch Cape Colony", "Buckland Valley", "The Curse of the Daleks", "lecture theatre", "progressivity", "drawn by the convenience of the railroad and worried about flooding", "Roman", "mid-18th century", "WatchESPN", "co-chair", "Mike Carey", "Dave Logan", "Sweynforkbeard", "starch", "1% to 3%", "European People's Party", "15 February 1546", "DNA results may be flawed", "northern China", "Institute for Policy Studies", "Port of Long Beach", "Pannerdens Kanaal", "The Rankine cycle", "proplastids", "Teenage Mutant Ninja Turtles: Out of the Shadows", "strong sedimentation", "elect and appoint bishops", "prime ideals", "lower incomes", "near their current locations", "Catholicism", "cartels", "Titian", "Pattern recognition receptors", "1275", "5 to 15 years", "August 1967", "Arabic numerals", "3:08", "Jamukha", "England", "EastEnders", "A fundamental error", "quantum", "water", "c1180", "heart disease, chronic pain, and asthma", "end of the Pleistocene", "He was an only son, and he was born six", "It's the only NBA team name that uses a state nickname in place of a city", "In 1879 the existing settlement was incorporated and named Crookston, after... drove the first spike of the St. Paul & Pacific Railroad, the first railroad in Minnesota", "Many who had believed in Spiritualism wrote most pathet- ically", "What separates a Cyberpunk setting from a futuristic setting?... set in a lawless subculture of an oppressive society dominated by computer technology", "additional GI Bill that expands education benefits for veterans who have served since the 9/11 attacks"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7440559280814215}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 0.0, 0.125, 0.0, 0.0, 0.10526315789473684, 0.46153846153846156]}}, "before_error_ids": ["mrqa_squad-validation-7291", "mrqa_squad-validation-1500", "mrqa_squad-validation-5835", "mrqa_squad-validation-7307", "mrqa_squad-validation-2226", "mrqa_squad-validation-8558", "mrqa_squad-validation-694", "mrqa_squad-validation-1092", "mrqa_squad-validation-8597", "mrqa_squad-validation-4999", "mrqa_squad-validation-8927", "mrqa_squad-validation-3165", "mrqa_squad-validation-4528", "mrqa_squad-validation-9145", "mrqa_searchqa-validation-16816", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-541", "mrqa_newsqa-validation-160"], "SR": 0.6875, "CSR": 0.765625, "EFR": 0.95, "Overall": 0.8578125}, {"timecode": 2, "before_eval_results": {"predictions": ["night", "his animosity toward each other", "Jan Andrzej Menich", "49\u201315", "10", "infrequent rain", "Chicago Theological Seminary", "upper sixth", "man-rating", "the oil shock", "Thomas Edison", "Children of Earth", "WTRF-TV", "picture thinking", "1066", "BBC 1", "one", "two", "Over 61", "Genghis Khan", "an innate force of impetus", "24\u201310", "Newcastle", "1887", "the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day", "torn down", "punts", "\u00a320,980", "2011", "Khuruldai", "SAP Center", "NBA", "1724 to 1725", "Two thirds", "the courts of member states", "Jim Gray", "Fort Beaus\u00e9jour", "Queen Victoria and Prince Albert", "education", "oxyacetylene welding", "war, famine, and weather", "the Wesel-Datteln Canal", "TLC", "on the south side of the garden", "novel", "friendly and supportive", "Eero Saarinen", "Newton", "41", "he may have intercepted Marconi's European experiments in July 1899", "\"The Lodger\"", "1954", "the Sacred Grounds Cafe", "the French word Fondre", "the Green Hornet", "the scrum-half", "\"not just for dancing\"", "Kingston", "the Old French and Latin words meant \"bloody, blood-colored\"", "New Hampshire", "the Tennessee Valley Authority", "the American Kennel Club", "1 April 1985", "Ford Motor Company"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7690393518518519}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1512", "mrqa_squad-validation-236", "mrqa_squad-validation-4015", "mrqa_squad-validation-2920", "mrqa_squad-validation-1941", "mrqa_squad-validation-3947", "mrqa_squad-validation-5525", "mrqa_squad-validation-6393", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-6374", "mrqa_searchqa-validation-9403", "mrqa_hotpotqa-validation-1297"], "SR": 0.703125, "CSR": 0.7447916666666667, "EFR": 1.0, "Overall": 0.8723958333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["1474", "average teacher salaries", "mother-of-pearl", "Elizabeth", "rule", "four", "San Joaquin Light & Power Building", "1972", "three", "books, films, radio, TV, music, live theater, comics and video games", "behavioral and demographic data", "the Conservatives", "north", "the Legislative Assembly", "African-American", "few British troops", "12.5 acres", "technical problems and flight delays", "the US Supreme Court", "trust God's word", "zeta function", "those who proceed to secondary school or vocational training", "139th out of 176 total countries", "eight", "kinetic friction force", "1526", "1939", "1986", "Black's Law Dictionary", "November 28, 1995", "the head of government of a country", "ten", "1 a.m.", "Department of State Affairs", "occupational stress", "a rolling circle mechanism", "San Jose", "7.8%", "three", "Bainbridge's", "WBT (1110 AM)", "cellular respiration", "Giuliano da Sangallo", "2009", "that the individual circumstances of a patient justify waiting lists, and this is also true in the context of the UK's National Health Service.", "BBC HD", "Brough Park in Byker", "Genoa", "the Common Core State Standards", "Chickamauga", "a reddish-brown horse", "the National Center for Physical Acoustics", "Gaius Maecenas", "Christopher Tolkien", "Sweden", "the Student loan Scheme", "a miserably tedious mess", "the Palais Garnier", "the Chicago White Stockings", "John James Osborne", "Nineteen Eighty-Four", "the Barbizon school", "Harry Potter", "a mansard roof"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7447916666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9810", "mrqa_squad-validation-6809", "mrqa_squad-validation-4462", "mrqa_squad-validation-5456", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-2022", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9548", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-412"], "SR": 0.71875, "CSR": 0.73828125, "retrieved_ids": ["mrqa_squad-train-70560", "mrqa_squad-train-78436", "mrqa_squad-train-80525", "mrqa_squad-train-50984", "mrqa_squad-train-20341", "mrqa_squad-train-12586", "mrqa_squad-train-30354", "mrqa_squad-train-80373", "mrqa_squad-train-49561", "mrqa_squad-train-77640", "mrqa_squad-train-71028", "mrqa_squad-train-32931", "mrqa_squad-train-774", "mrqa_squad-train-2645", "mrqa_squad-train-73859", "mrqa_squad-train-30262", "mrqa_squad-validation-9145", "mrqa_squad-validation-236", "mrqa_squad-validation-1891", "mrqa_squad-validation-2226", "mrqa_squad-validation-7687", "mrqa_squad-validation-8558", "mrqa_squad-validation-694", "mrqa_squad-validation-7574", "mrqa_squad-validation-1512", "mrqa_squad-validation-3130", "mrqa_squad-validation-1941", "mrqa_searchqa-validation-11770", "mrqa_squad-validation-6393", "mrqa_squad-validation-2289", "mrqa_hotpotqa-validation-1297", "mrqa_squad-validation-8927"], "EFR": 0.9444444444444444, "Overall": 0.8413628472222222}, {"timecode": 4, "before_eval_results": {"predictions": ["1873", "Because everyday clothing from previous eras has not generally survived", "July 1969", "six", "a liturgical setting of the Lord's Prayer", "$5 million", "toxic", "2.666 million", "service", "violence", "Parish Church of St Andrew", "1262", "New Orleans' Mercedes-Benz Superdome", "April 1523", "radiometric isotopes", "Wesleyan Holiness Consortium", "26", "Suleiman the Magnificent", "James Bryant Conant", "2010", "Chartered", "an imposed selective breeding version of eugenics", "15 May 1525", "lupus erythematosus", "Education", "cholera", "Tuesday afternoon", "Pickawillany", "plan the physical proceedings, and to integrate those proceedings with the other parts", "Cybermen", "graduate and undergraduate students elected to represent members from their respective academic unit", "16", "a theory of everything", "Lucas\u2013Lehmer", "Level 3 Communications", "Ilkhanate", "1685", "19", "economically", "general and complete disarmament", "electromagnetic theory", "killed in a horse-riding accident", "the ark", "opera buffa", "Okinawa", "a verse", "organ", "potato pancake", "Casper", "Tarsus", "a luxury department store", "Woody Allen", "Jane Austen", "Walter Cronkite", "Treasure Island", "Death Watch", "Kerry Moosman", "Liqueur Devoille", "white", "Blended", "in the 1960s", "metal Bridge", "Alistair Grant", "they had arrested Samson D'Souza, 29, to make it look like they were making progress in the case"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6842447916666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-2346", "mrqa_squad-validation-3543", "mrqa_squad-validation-8403", "mrqa_squad-validation-6791", "mrqa_squad-validation-4932", "mrqa_squad-validation-455", "mrqa_squad-validation-10506", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-4742", "mrqa_newsqa-validation-2983"], "SR": 0.671875, "CSR": 0.725, "EFR": 0.9523809523809523, "Overall": 0.8386904761904761}, {"timecode": 5, "before_eval_results": {"predictions": ["an ash leaf", "75,000 to 100,000 people", "1970s", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "The individual is the final judge of right and wrong", "Hendrix v Employee Insurance Institute", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "SAP Center", "one-eighth", "Video On Demand content", "extended structure", "principle of equivalence", "pump water out of the mesoglea", "closed system", "21 to 11", "crustal rock", "formalize a unified front in trade and negotiations with various Indians", "two", "the public PAD service Telepad", "a separate condenser", "to the North Sea", "Cam Newton", "requiring his arrest", "John Mayow", "state or government schools", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "45,000 pounds", "Gottfried Fritschel", "third most abundant chemical element", "39", "(Romana (Mary Tamm and Lalla Ward)", "metals", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "C\u00e9loron threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "200 horsepower (150 kilowatts)", "at Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "a UNESCO World Heritage Site", "Frederick II the Great", "the ball", "(G) Richard Wagner", "(G) ( ( (I) (K) Parker", "New Netherland", "Monrovia", "umpires", "Taiwan", "Omaha", "Beniamino", "the Nez Perce", "Gershwin", "New Funk And Wagnalls", "Oprah Winfrey", "sewing machines", "(Jack Bauer)", "Inchon", "February 29", "beetles", "Alabama", "Bennington", "Giorgio Armani", "the mint moved from London to a new 38 acres ( 15 ha ) plant in Llantrisant, Wales", "study insects and their relationship to humans, other organisms, and the environment", "Squam Lake", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region", "the District of Columbia National Guard"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5944341908772056}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.23529411764705882, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.07692307692307693, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.1111111111111111, 0.962962962962963, 0.0, 0.9600000000000001, 1.0, 1.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.15384615384615385, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-3040", "mrqa_squad-validation-6975", "mrqa_squad-validation-9640", "mrqa_squad-validation-2976", "mrqa_squad-validation-973", "mrqa_squad-validation-10214", "mrqa_squad-validation-8551", "mrqa_squad-validation-9320", "mrqa_squad-validation-6614", "mrqa_squad-validation-3559", "mrqa_squad-validation-639", "mrqa_squad-validation-7719", "mrqa_squad-validation-9489", "mrqa_squad-validation-10141", "mrqa_squad-validation-1441", "mrqa_squad-validation-10274", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-7010", "mrqa_naturalquestions-validation-866", "mrqa_triviaqa-validation-3868", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1289"], "SR": 0.484375, "CSR": 0.6848958333333333, "EFR": 0.9696969696969697, "Overall": 0.8272964015151515}, {"timecode": 6, "before_eval_results": {"predictions": ["The Central Region", "Fred Singer", "north", "for Lutheran views", "Bible", "water pump", "874.3 square miles", "Gender pay gap", "a Scottish Parliament", "science fiction", "a background check and psychiatric evaluation", "Super Bowl XX", "Queen Bees", "the study of rocks", "Roger Goodell", "to avoid being targeted by the boycott", "(circa 1964\u20131965)", "a guru", "continental sculptors", "Judith Merril", "the connection id in a table", "Von Miller", "weekly screenings of all available classic episodes", "type III secretion system", "10,000", "12 May 1191", "The Three Doctors", "1870 to 1939", "Ealy", "Seven Days to the River Rhine", "ten", "New Orleans", "when the oxygen concentration is too high", "to punish Christians by God, as agents of the Biblical apocalypse that would destroy the antichrist, whom Luther believed to be the papacy, and the Roman Church", "the global village", "Sun City, Arizona", "Freeport, Maine", "a dolphin", "auctions", "Liberty Island", "next of kin", "Matt Lauer", "Lenin", "Abilene, Kansas", "Amtrak", "the Pioneer Log House", "The Pianist", "Patty Duke", "the king", "a Mackintosh", "Richard Cory", "Homer J. Simpson", "South Africa", "the greyhound", "Beany and Cecil", "Eastern Nevada", "Trenton", "nickel", "H.L.A. Hart", "tap dance", "margarita", "prostate cancer", "DNA's structure", "Pyrenees"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7369212962962963}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29629629629629634, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7449", "mrqa_squad-validation-5589", "mrqa_squad-validation-2564", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-11888", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-14720", "mrqa_naturalquestions-validation-9273", "mrqa_triviaqa-validation-2363", "mrqa_triviaqa-validation-4255"], "SR": 0.671875, "CSR": 0.6830357142857143, "retrieved_ids": ["mrqa_squad-train-38673", "mrqa_squad-train-22478", "mrqa_squad-train-13731", "mrqa_squad-train-33096", "mrqa_squad-train-6616", "mrqa_squad-train-20832", "mrqa_squad-train-85650", "mrqa_squad-train-38857", "mrqa_squad-train-35864", "mrqa_squad-train-48897", "mrqa_squad-train-46879", "mrqa_squad-train-38675", "mrqa_squad-train-5463", "mrqa_squad-train-66527", "mrqa_squad-train-11113", "mrqa_squad-train-66629", "mrqa_searchqa-validation-13077", "mrqa_squad-validation-7687", "mrqa_searchqa-validation-3735", "mrqa_squad-validation-3119", "mrqa_squad-validation-4999", "mrqa_squad-validation-3130", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-33", "mrqa_squad-validation-6975", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-12876", "mrqa_triviaqa-validation-412", "mrqa_squad-validation-6809"], "EFR": 1.0, "Overall": 0.8415178571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["Mercedes-Benz Superdome", "Works Council Directive", "the Court of Justice", "United Kingdom", "Brooklyn", "1569", "Computational complexity theory", "models", "Death wish Coffee", "Pittsburgh Steelers", "McManus", "Gemini", "Mick Mixon", "Northern Europe and the Mid-Atlantic", "Africa", "X-ray imaging", "corporal punishment", "1 October 1998", "Marconi successfully transmitted the letter S from England to Newfoundland, terminating Tesla's relationship with Morgan", "LOVE Radio", "The Holocene", "Hasar, Hachiun, and Tem\u00fcge", "between AD 0\u20131250", "(various allied groups from Central Asia and the western end of the empire", "civil disobedients", "oil was priced in dollars, oil producers' real income decreased", "Chuck Howley", "holy catholic (or universal) church", "competition", "1516", "decrease in wages", "Prudhoe Bay", "a cat's eye", "cigar", "William Godwin", "Lucy Hayes", "ribonucleic acid", "a huge family", "Eight Is Enough", "Madrid", "Bacall", "The Name of the Rose", "Thomas Paine", "a sea quahogs", "the Silver Surfer", "G4", "Karl Shapiro", "Julius Caesar", "malaria", "a blonde", "Hairspray", "Johann Wolfgang von Goethe", "masks", "the Oneida Community", "a seaplane", "Sherman Antitrust Act", "Hafnium", "Grace Zabriskie", "Harold Bierman", "Winnie the Pooh", "Ryder Russell", "a breathing part of history", "Joe Harn", "dismissal"], "metric_results": {"EM": 0.65625, "QA-F1": 0.698846850613155}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-490", "mrqa_squad-validation-1467", "mrqa_squad-validation-6759", "mrqa_squad-validation-3718", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-86", "mrqa_naturalquestions-validation-519", "mrqa_triviaqa-validation-6277", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-2246", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-689"], "SR": 0.65625, "CSR": 0.6796875, "EFR": 0.9545454545454546, "Overall": 0.8171164772727273}, {"timecode": 8, "before_eval_results": {"predictions": ["the 1970s", "an electrical exhibition at Madison Square Garden.", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Lucas Horenbout", "its safaris, diverse climate and geography, and expansive wildlife reserves", "Silk Road", "The Sinclair Broadcast Group", "8", "1.6 kilometres", "deportation of the French-speaking Acadian population from the area", "Ryan Seacrest", "his last statement", "buildings, infrastructure and industrial", "a broken arm", "August 10, 1948", "not having a residence permit", "Cheyenne", "large dumbbell-shaped chloroplasts", "him to return to his side", "Kevin Harlan", "up to 30%", "The Open Championship golf and The Wimbledon tennis tournaments", "when the oxygen concentration is too high", "Anglican tradition's Book of Common Prayer", "the Golden Gate Bridge", "Diarmaid MacCulloch", "inferior", "2015", "a raincoat mae of waterproof heavy-duty cotton drill or poplin, wool gabardine", "(Weil's disease)", "a little blue engine", "a \"dwarf planet\"", "tango", "a cave", "bamboos", "Pied Piper", "Octavia", "Tudor Arghezi", "a garden.", "ginseng", "coffee", "Depeche Mode", "pepsiweb.org", "a device called a generator or IPG", "Pat Sajak", "a hippopotamus.", "1492", "madding", "(M Mikhail) Baryshovo.", "Mars", "the Boston Massacre Trials", "a bee", "a Hardmode gun.", "Milan", "Maycommemorates the Mexican army's", "( Peggy Lee)", "Carl Sagan.", "February 2011", "General Paulus", "John Ford", "Cirque du Soleil", "a donor molecule to an acceptor molecule.", "Sylvester Stallone", "The Mongol - led Yuan dynasty ( 1271 -- 1368 )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6036791196741855}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 0.5, 0.16666666666666666, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1456", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-6402", "mrqa_squad-validation-8864", "mrqa_squad-validation-6115", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-11086", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-10604", "mrqa_naturalquestions-validation-7733", "mrqa_newsqa-validation-2133", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-6321"], "SR": 0.515625, "CSR": 0.6614583333333333, "EFR": 0.967741935483871, "Overall": 0.8146001344086021}, {"timecode": 9, "before_eval_results": {"predictions": ["the Metropolitan Police Authority", "Francis Marion", "all \"trading rules\" that are \"enacted by Member States\"", "the first Block II CSM and LM in a dual mission", "Genghis Khan", "five", "governmental", "the Great Yuan", "Mario Addison", "immune system adapts its response during an infection to improve its recognition of the pathogen", "more than 70", "movements of nature, movements of free and unequal durations", "1850s", "2000", "Bruno Mars", "electrical arc light based illumination", "megaprojects", "James Lofton", "gurus, mullahs, rabbis", "by limiting aggregate demand", "five", "Danny Lane", "5,500,000", "an adjustable spring-loaded valve", "classical position variables", "The Left Hand of Darkness", "(Nevil) Gondorff", "George Jetson", "Deus", "an arboretum", "an artistic gymnastics apparatus", "President William McKinley", "PSP", "Daphne du Maurier", "Turkey", "antonism", "an identifying trait", "Daughters of the American Revolution", "The Talkers TenTM", "Paul Bennett", "Mercury and Venus", "Tokyo", "an entry-level restaurant job or a labor-saving appliance", "an animal", "The Pentagon", "an important staple food", "I Love You", "China", "Gone With the Wind", "A Delicate Balance", "Nancy Reagan", "grasshopper", "Lord Baden-Powell", "Pyrrhus", "The Miracle Worker", "the pancreas", "the mid-1990s", "the Hudson Bay", "Dr Ichak Adizes", "Melpomene", "Boston Bruins", "James Lofton", "an estimate from the Nielsen Company.", "crimes he committed and the age at which he committed the crimes, [a 108-year sentence] was dramatically outside the norm for sentencing."], "metric_results": {"EM": 0.484375, "QA-F1": 0.5540482954545454}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3118", "mrqa_squad-validation-4329", "mrqa_squad-validation-4068", "mrqa_squad-validation-6185", "mrqa_squad-validation-6757", "mrqa_squad-validation-8046", "mrqa_squad-validation-825", "mrqa_squad-validation-6680", "mrqa_squad-validation-664", "mrqa_squad-validation-1290", "mrqa_squad-validation-1849", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-2768", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-3127", "mrqa_naturalquestions-validation-4124", "mrqa_hotpotqa-validation-5831", "mrqa_hotpotqa-validation-3949", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1271"], "SR": 0.484375, "CSR": 0.64375, "retrieved_ids": ["mrqa_squad-train-76111", "mrqa_squad-train-60507", "mrqa_squad-train-7669", "mrqa_squad-train-56765", "mrqa_squad-train-78141", "mrqa_squad-train-82671", "mrqa_squad-train-13312", "mrqa_squad-train-30914", "mrqa_squad-train-84493", "mrqa_squad-train-41324", "mrqa_squad-train-50440", "mrqa_squad-train-13164", "mrqa_squad-train-31606", "mrqa_squad-train-13778", "mrqa_squad-train-36460", "mrqa_squad-train-77423", "mrqa_squad-validation-8597", "mrqa_newsqa-validation-491", "mrqa_triviaqa-validation-2363", "mrqa_squad-validation-10011", "mrqa_squad-validation-4662", "mrqa_squad-validation-10214", "mrqa_triviaqa-validation-4255", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1162", "mrqa_squad-validation-7574", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-13077", "mrqa_searchqa-validation-9403", "mrqa_newsqa-validation-160"], "EFR": 1.0, "Overall": 0.821875}, {"timecode": 10, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-9273", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-689", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10308", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-10604", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10925", "mrqa_searchqa-validation-10964", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11139", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11944", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12302", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-125", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-12876", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-14838", "mrqa_searchqa-validation-14884", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15554", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-15847", "mrqa_searchqa-validation-15915", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16911", "mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-1992", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2252", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2617", "mrqa_searchqa-validation-2752", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3060", "mrqa_searchqa-validation-3102", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-37", "mrqa_searchqa-validation-3735", "mrqa_searchqa-validation-3887", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4004", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-414", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4888", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5128", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5679", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-5915", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-695", "mrqa_searchqa-validation-6962", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-7852", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8570", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-8590", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8715", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10010", "mrqa_squad-validation-10011", "mrqa_squad-validation-10061", "mrqa_squad-validation-10092", "mrqa_squad-validation-10125", "mrqa_squad-validation-10137", "mrqa_squad-validation-10140", "mrqa_squad-validation-10141", "mrqa_squad-validation-10214", "mrqa_squad-validation-10218", "mrqa_squad-validation-10273", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10306", "mrqa_squad-validation-10338", "mrqa_squad-validation-10380", "mrqa_squad-validation-10387", "mrqa_squad-validation-10433", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1055", "mrqa_squad-validation-1079", "mrqa_squad-validation-1082", "mrqa_squad-validation-1092", "mrqa_squad-validation-1118", "mrqa_squad-validation-1122", "mrqa_squad-validation-1125", "mrqa_squad-validation-117", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1207", "mrqa_squad-validation-1215", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1347", "mrqa_squad-validation-1404", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1467", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-1640", "mrqa_squad-validation-1641", "mrqa_squad-validation-1662", "mrqa_squad-validation-167", "mrqa_squad-validation-172", "mrqa_squad-validation-1725", "mrqa_squad-validation-1766", "mrqa_squad-validation-1841", "mrqa_squad-validation-1849", "mrqa_squad-validation-19", "mrqa_squad-validation-192", "mrqa_squad-validation-1921", "mrqa_squad-validation-1936", "mrqa_squad-validation-1955", "mrqa_squad-validation-1983", "mrqa_squad-validation-2059", "mrqa_squad-validation-2066", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2149", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2209", "mrqa_squad-validation-2226", "mrqa_squad-validation-2235", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-236", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2374", "mrqa_squad-validation-2387", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2441", "mrqa_squad-validation-2442", "mrqa_squad-validation-2472", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2550", "mrqa_squad-validation-2552", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2640", "mrqa_squad-validation-2656", "mrqa_squad-validation-272", "mrqa_squad-validation-2748", "mrqa_squad-validation-2765", "mrqa_squad-validation-2783", "mrqa_squad-validation-2831", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2926", "mrqa_squad-validation-2942", "mrqa_squad-validation-2949", "mrqa_squad-validation-2973", "mrqa_squad-validation-2976", "mrqa_squad-validation-3022", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3118", "mrqa_squad-validation-3119", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-3168", "mrqa_squad-validation-3215", "mrqa_squad-validation-3355", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3417", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3508", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3663", "mrqa_squad-validation-3699", "mrqa_squad-validation-3718", "mrqa_squad-validation-3779", "mrqa_squad-validation-3947", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-3959", "mrqa_squad-validation-4001", "mrqa_squad-validation-4068", "mrqa_squad-validation-4101", "mrqa_squad-validation-4144", "mrqa_squad-validation-42", "mrqa_squad-validation-4329", "mrqa_squad-validation-4452", "mrqa_squad-validation-4462", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4633", "mrqa_squad-validation-4633", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-477", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-4797", "mrqa_squad-validation-4829", "mrqa_squad-validation-4841", "mrqa_squad-validation-490", "mrqa_squad-validation-4932", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5099", "mrqa_squad-validation-518", "mrqa_squad-validation-5185", "mrqa_squad-validation-5296", "mrqa_squad-validation-5309", "mrqa_squad-validation-5348", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5451", "mrqa_squad-validation-5456", "mrqa_squad-validation-5470", "mrqa_squad-validation-5498", "mrqa_squad-validation-5513", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-560", "mrqa_squad-validation-5616", "mrqa_squad-validation-565", "mrqa_squad-validation-5724", "mrqa_squad-validation-5727", "mrqa_squad-validation-5765", "mrqa_squad-validation-5771", "mrqa_squad-validation-5804", "mrqa_squad-validation-5824", "mrqa_squad-validation-5830", "mrqa_squad-validation-5852", "mrqa_squad-validation-588", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6086", "mrqa_squad-validation-6097", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6156", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6334", "mrqa_squad-validation-6354", "mrqa_squad-validation-639", "mrqa_squad-validation-6393", "mrqa_squad-validation-6402", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6594", "mrqa_squad-validation-6609", "mrqa_squad-validation-6614", "mrqa_squad-validation-664", "mrqa_squad-validation-6680", "mrqa_squad-validation-6714", "mrqa_squad-validation-6757", "mrqa_squad-validation-6759", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-6869", "mrqa_squad-validation-6881", "mrqa_squad-validation-6917", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-703", "mrqa_squad-validation-704", "mrqa_squad-validation-7051", "mrqa_squad-validation-7081", "mrqa_squad-validation-7090", "mrqa_squad-validation-7128", "mrqa_squad-validation-7202", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7412", "mrqa_squad-validation-7424", "mrqa_squad-validation-7431", "mrqa_squad-validation-7439", "mrqa_squad-validation-7473", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7653", "mrqa_squad-validation-7665", "mrqa_squad-validation-7687", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7733", "mrqa_squad-validation-774", "mrqa_squad-validation-7772", "mrqa_squad-validation-7785", "mrqa_squad-validation-7794", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7836", "mrqa_squad-validation-7837", "mrqa_squad-validation-784", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7934", "mrqa_squad-validation-7951", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8033", "mrqa_squad-validation-8056", "mrqa_squad-validation-8067", "mrqa_squad-validation-8097", "mrqa_squad-validation-8115", "mrqa_squad-validation-8136", "mrqa_squad-validation-8149", "mrqa_squad-validation-8196", "mrqa_squad-validation-825", "mrqa_squad-validation-828", "mrqa_squad-validation-8294", "mrqa_squad-validation-8400", "mrqa_squad-validation-8403", "mrqa_squad-validation-8412", "mrqa_squad-validation-8436", "mrqa_squad-validation-8442", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8568", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-862", "mrqa_squad-validation-8657", "mrqa_squad-validation-8683", "mrqa_squad-validation-8689", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-8923", "mrqa_squad-validation-8927", "mrqa_squad-validation-8939", "mrqa_squad-validation-8981", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9145", "mrqa_squad-validation-919", "mrqa_squad-validation-9205", "mrqa_squad-validation-9234", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-9320", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-937", "mrqa_squad-validation-9489", "mrqa_squad-validation-9533", "mrqa_squad-validation-9559", "mrqa_squad-validation-9581", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9731", "mrqa_squad-validation-9810", "mrqa_squad-validation-9822", "mrqa_squad-validation-985", "mrqa_squad-validation-9869", "mrqa_squad-validation-9870", "mrqa_squad-validation-9910", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-412", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-7474"], "OKR": 0.908203125, "KG": 0.47578125, "before_eval_results": {"predictions": ["Mike Figgis", "around 1.7 billion years ago", "southern", "technical problems and flight delays", "the fact (Fermat's little theorem)", "Virgin Media", "Tesla would be killed through overwork", "Times Square Studios", "Philip Webb and William Morris", "to service to the neighbor in the common, daily vocations of this perishing world", "Amtrak San Joaquins", "refusing to make a commitment", "regulations and directives", "in the possession of already-wealthy individuals or entities", "26", "\"physical control or full-fledged colonial rule\"", "30 July 1891", "Bible", "Lower Lorraine", "parish churches", "kinetic friction", "relatively large protein complexes about 40 nanometers across", "a photoelectric cell", "Peggy", "a mycelium", "Memoirs of a Geisha", "stability control", "a pistol", "Gothic Names", "Aluminium", "country", "Cenozoic", "Niger", "Reddi-wip", "Jeopardy", "tea", "Larry Fortensky", "oxygen", "Shakira", "Aimee Semple McPherson", "Hawaii", "Time & 1936", "The Jeffersons", "Tony Soprano", "The Crucible", "Muhammad Ali", "Impressionists", "Willa Cather", "Aida", "The Strange & Curious Tale of the Last True Hermit", "the Burgundy wine region", "constitutional liberties and vio- lating their", "a screw", "zero", "Australian & New Zealand", "Vermont", "Neela Montgomery", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "Hal Ashby", "John Ford", "119", "The Ballade", "a skilled hacker", "Sonia Sotomayor"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6188519021739131}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9178", "mrqa_squad-validation-9023", "mrqa_squad-validation-2455", "mrqa_squad-validation-9734", "mrqa_squad-validation-8839", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-1747", "mrqa_searchqa-validation-13939", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-9725", "mrqa_naturalquestions-validation-5297", "mrqa_triviaqa-validation-862", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-2708"], "SR": 0.515625, "CSR": 0.6321022727272727, "EFR": 1.0, "Overall": 0.7610298295454545}, {"timecode": 11, "before_eval_results": {"predictions": ["the study of rocks", "imperialist", "A plant cell which contains chloroplasts", "provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "allowing the lander spacecraft to be used as a \"lifeboat\" in the event of a failure of the command ship", "Doctor Who", "Maria Sk\u0142odowska-Curie", "1978", "2000", "Cargill Meat Solutions and Foster Farms", "25 May 1521", "253", "concrete", "anti-colonial movements", "Lampea", "75%", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "oppidum Ubiorum", "studio 5 at the City Road complex", "1.7 million", "August 4, 2000", "interrogation tactics that Obama has prohibited through an executive order.", "free", "Bob Dole", "1959", "cyberattack", "three men with suicide vests who were plotting to carry out the attacks, said Interior Minister Rehman Malik.", "137", "the Christmas Dinner & Show", "Opryland", "Asashoryu", "Conway", "How I Met Your Mother", "as adults", "the insurgency", "China", "war", "war funding without the restrictions congressional Democrats vowed to put into place since the 2006 midterm elections in large part to balance the Pentagon's books", "San Simeon, California", "said that the women are never pitted against one another in the media", "The Rev. Alberto Cutie", "attack by a spurned suitor.", "military trials for some Guant Bay detainees.", "opium", "Obama's race in 2008.", "his company Polo", "Egypt", "Arabic, French and English", "a minor league baseball team in that stadium.", "seven", "Honduras", "Manila, Philippines", "war still poses for Americans despite a third strike in two days involving local politicians and political institutions in Baghdad.", "chaos and horrified reactions after the July 7, 2005, London transit bombings were shown to jurors Thursday in the trial of three men charged with conspiracy in the case.", "warren Meehan", "middle of the 15th century", "1966", "J. S. Bach", "Groucho Marx", "Fitzroya cupressoides", "Stephanie Plum", "Sweeney Todd", "Andorra", "The Rise and Fall of Eliza Harris"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5516371425474687}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.07407407407407407, 0.0, 0.07407407407407407, 1.0, 0.3333333333333333, 0.10256410256410256, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 0.16, 1.0, 0.0, 0.6666666666666666, 0.0, 0.14285714285714288, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3805", "mrqa_squad-validation-7659", "mrqa_squad-validation-9298", "mrqa_squad-validation-5465", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-267", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3151", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-5492", "mrqa_hotpotqa-validation-5394", "mrqa_searchqa-validation-10090"], "SR": 0.484375, "CSR": 0.6197916666666667, "EFR": 1.0, "Overall": 0.7585677083333333}, {"timecode": 12, "before_eval_results": {"predictions": ["San Jose State", "Halo", "rocketry and manned spaceflight, including avionics, telecommunications, and computers", "136", "55.1%", "Mandatory Committees", "main porch", "Warren Buffett", "3.55 inches", "Doctor Who", "Prime", "Council of Industrial Design", "The Open Championship golf and The Wimbledon tennis tournaments", "781", "Andr\u00e9s Marzal De Sax", "contemporary accounts were exaggerations", "3,792,621", "Chinggis Khaan International Airport", "23 years.", "between Pyongyang and Seoul", "Jason Chaffetz", "Draquila -- Italy Trembles.", "Chinese", "recovery from last spring's tornado, severe storms and flooding in Jasper County and in Joplin.", "two", "CNN", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi,", "Suwardi,", "Muhammad Ali, Kareem Abdul-Jabbar and the Persian poet Mawlana Jalal al-Din Rumi,", "U.S. senators", "viviro, the god of death and disaster, and they were featured on one side of a New Zealand five-cent coin that was phased out in 2006.", "Muslim", "California, Texas and Florida,", "Lillo Brancato Jr.", "Argentina", "Three searches", "creation of an Islamic emirate in Gaza,", "Garacad, Somalia", "The United Nations is calling on NATO to do more to stop the Afghan opium trade after a new survey showed how the drug dominates Afghanistan's economy.", "Pope Benedict XVI", "his grandfather was a \"Zionist\" and \"a zealous supporter of the usurper entity, and a prominent member of a number of Zionist hate organizations.\"", "He told the newspaper she came to her decision based on the combination of the interrogation techniques, their duration and the effect on al-Qahtani's health.", "Apple employees", "green-card warriors", "Haiti", "\"Stagecoach\" (John Ford, 1939)", "test-launched a rocket capable of carrying a satellite,", "Nieb\u00fcll", "Del Potro.", "the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state,", "Seoul,", "John Wayne", "Pakistan", "seven", "Johan Persson and Martin Schibbye", "Fix You", "David Hoak", "Ytterby", "George III", "Philadelphia, Pennsylvania", "Alien Resurrection", "vivicia", "Moscow", "A dressage horse performing at his peak levels will be calm, supple, and in complete harmony"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6306632921147723}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.10810810810810811, 0.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.6666666666666666, 0.3157894736842105, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2044", "mrqa_naturalquestions-validation-4193", "mrqa_hotpotqa-validation-5014", "mrqa_searchqa-validation-266", "mrqa_searchqa-validation-9605"], "SR": 0.546875, "CSR": 0.6141826923076923, "retrieved_ids": ["mrqa_squad-train-72905", "mrqa_squad-train-67870", "mrqa_squad-train-83323", "mrqa_squad-train-6496", "mrqa_squad-train-82000", "mrqa_squad-train-8355", "mrqa_squad-train-29503", "mrqa_squad-train-68509", "mrqa_squad-train-26570", "mrqa_squad-train-21075", "mrqa_squad-train-50208", "mrqa_squad-train-86180", "mrqa_squad-train-73965", "mrqa_squad-train-29675", "mrqa_squad-train-57069", "mrqa_squad-train-3180", "mrqa_searchqa-validation-6737", "mrqa_searchqa-validation-15312", "mrqa_searchqa-validation-11704", "mrqa_squad-validation-4462", "mrqa_squad-validation-825", "mrqa_searchqa-validation-9116", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-13459", "mrqa_newsqa-validation-2246", "mrqa_searchqa-validation-13939", "mrqa_squad-validation-1849", "mrqa_naturalquestions-validation-519", "mrqa_squad-validation-3130", "mrqa_squad-validation-664", "mrqa_searchqa-validation-15748"], "EFR": 0.9655172413793104, "Overall": 0.7505493617374005}, {"timecode": 13, "before_eval_results": {"predictions": ["before World War I", "war, famine, and weather", "Gryphon", "March 2003", "Elders", "Jon Culshaw", "CD4", "1995", "2014", "multi-stage centrifugal pumps", "salvation", "6.4 nanometers", "WJRT-TV and WTVG", "1939", "Treaty on the Functioning of the European Union", "the City of Edinburgh Council", "Osama", "Muslim Palestine.", "Hearst Castle.", "CNN's \"Larry King Live.\"", "Laura Ling and Euna Lee,", "Quebradillas.", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "Culhane,", "\"Let me here tell you something about myself and my biography, in which there is a benefit and a lesson,\"", "iPhone 4S news", "Pakistan's largest city of Karachi.", "John McCain", "Johannesburg", "2006", "Iran's nuclear program.", "North Korea", "Sunday,", "\"This is not something that anybody can reasonably anticipate,\"", "Haeftling,", "i report form", "Kurt Cobain", "Nkepile M abuse", "\"happy ending\" to the case.", "San Diego", "tie salesman", "At least 40", "$1,500", "25", "137", "suppress the memories and to live as normal a life as possible;", "Copts", "poor", "Ewan McGregor", "The Louvre", "27-year-old", "165", "\"It was incredible. We've had so much rain, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick.", "16,801", "Ali", "Kansas", "October", "music", "Connie", "Lusitania", "the Earth", "Coronation Street", "Turkey"], "metric_results": {"EM": 0.453125, "QA-F1": 0.563702876984127}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.9333333333333333, 0.5, 0.0, 0.0, 0.6, 0.0, 0.0, 1.0, 0.8, 1.0, 0.25, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7555555555555554, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5911", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2204", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-2202", "mrqa_hotpotqa-validation-1028", "mrqa_searchqa-validation-2338", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2251"], "SR": 0.453125, "CSR": 0.6026785714285714, "EFR": 0.9714285714285714, "Overall": 0.7494308035714285}, {"timecode": 14, "before_eval_results": {"predictions": ["Thomas Reid and Dugald Stewart", "between September and November 1946", "$2.50 per AC horsepower royalty", "1990s", "organic", "Stagg Field", "2010", "Reuben Townroe", "the Black Death", "a water pump", "high growth rates", "roads, bridges and large plazas", "two", "non-Mongol physicians", "ABC International", "Zuma", "Bangladesh's southern Bhola district", "At least 88 people had been hurt,", "bankruptcies", "Inter Milan", "98", "glaciers in the European Alps may melt as soon as 2050,", "based on race or its understanding of what the law required it to do.", "The Ski Train", "severe", "Naples home.", "top designers, such as Stella McCartney,", "Col. Elspeth Cameron-Ritchie,", "homicide", "\"surge\" strategy he implemented last year.", "the port remains shut down, and desperately needed aid cannot be unloaded quickly.\"", "onstage demos.", "International Polo Club", "impeachment", "Kearny, New Jersey", "Thessaloniki and Athens,", "New York-based Human Rights Watch", "september", "gang rape", "The remaining 240 patients will be taken to hospitals in other provinces by Sunday,", "killing rampage.", "genocide, crimes against humanity, and war crimes.", "The oldest documented bikinis", "Fullerton, California,", "Ma Khin Khin Leh,", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "repeal of the military's \"don't ask, don't tell\" policy,", "Consumer Reports", "The National Restaurant Association,", "Sheikh Abu al-Nour al-Maqdessi,", "an independent homeland", "The Everglades,", "six-year veteran of the museum's security staff,", "\"It's more likely that lightning would cause a fire or punch a hole through the aircraft structure,\"", "ninth w\u0101", "Magnavox Odyssey", "The Lone Ranger", "robin", "Russell Humphreys", "The Guest", "\"Time of Your Life", "september", "The Oakland Raiders relocation to Las Vegas", "6 January 793"], "metric_results": {"EM": 0.5, "QA-F1": 0.5386982808857809}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4908", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-886", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-667", "mrqa_triviaqa-validation-2022", "mrqa_hotpotqa-validation-1239", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-4863"], "SR": 0.5, "CSR": 0.5958333333333333, "EFR": 1.0, "Overall": 0.7537760416666666}, {"timecode": 15, "before_eval_results": {"predictions": ["tropical", "90%", "1966", "Turkey", "Ollie Treiz", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "organisms", "libertarian", "the late 1870s", "Death wish Coffee", "quality of a country's institutions and high levels of education", "proportionally to the number of votes received in the second vote of the ballot", "North", "Mohammed Mohsen Zayed,", "\"still trying to absorb the impact of this week's stunning events,\"", "President Obama", "Friday,", "CNN affiliate WFTV.", "mysterious scene Sunday before a polo match", "The station", "sculptures", "along the equator between South America and Africa.", "the 725-mile Veracruz regatta", "200.", "the ancient Greek site of Olympia,", "Patrick McGoohan,", "his parents", "$627,", "27-year-old's", "Virgin America", "know what's important in life,", "gossip Girl", "Ketchum, Idaho.", "at my undergrad alma mater, Wake Forest,", "Sporting Lisbon", "his company Polo", "the defending champions were held to a 1-1 draw at Stoke City.", "1998.", "Jean Van de Velde", "overturned about 5:15 p.m. Saturday,", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "Secretary of State Hillary Clinton,", "will look at how the universe formed by analyzing particle collisions.", "10 below", "\"She was focused so much on learning that she didn't notice,\"", "Haiti.", "\"Dancing With the Stars.\"", "1 million", "\"I can tell you, there are definitely going to be more ships in that area in the next 24 or 48 hours, because there are two more sailing to it right now,\"", "1.2 million", "club managers,", "\"We say to the people of Gaza, give more resistance and we will be with you in the field, and know that our victory in kicking out the invaders is your victory as well,", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "his mother.", "pigs", "Matt Flinders", "Isar", "East of Eden", "Sam Bettley", "33-member", "a boat", "sticky liquid", "Oxfordshire", "Krusty Krab"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6794037176274019}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.88, 1.0, 0.5454545454545454, 0.9090909090909091, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.33333333333333337, 1.0, 0.33333333333333337, 0.9473684210526316, 0.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 1.0, 1.0, 0.14545454545454548, 0.2666666666666667, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-543", "mrqa_squad-validation-9528", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-4126", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-1844", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3088", "mrqa_searchqa-validation-11087", "mrqa_searchqa-validation-5504", "mrqa_triviaqa-validation-5573"], "SR": 0.5625, "CSR": 0.59375, "retrieved_ids": ["mrqa_squad-train-2974", "mrqa_squad-train-22191", "mrqa_squad-train-1386", "mrqa_squad-train-67955", "mrqa_squad-train-27909", "mrqa_squad-train-81708", "mrqa_squad-train-48585", "mrqa_squad-train-65584", "mrqa_squad-train-9958", "mrqa_squad-train-18974", "mrqa_squad-train-41996", "mrqa_squad-train-28593", "mrqa_squad-train-38515", "mrqa_squad-train-42581", "mrqa_squad-train-34114", "mrqa_squad-train-12296", "mrqa_newsqa-validation-1101", "mrqa_searchqa-validation-9403", "mrqa_newsqa-validation-2708", "mrqa_squad-validation-3718", "mrqa_squad-validation-4932", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-2133", "mrqa_searchqa-validation-11427", "mrqa_searchqa-validation-9733", "mrqa_newsqa-validation-3911", "mrqa_squad-validation-9918", "mrqa_searchqa-validation-8715", "mrqa_squad-validation-4861", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-2863"], "EFR": 1.0, "Overall": 0.7533593749999999}, {"timecode": 16, "before_eval_results": {"predictions": ["np\u2261n (mod p)", "adjustable spring-loaded valve", "George Low", "Synthetic aperture radar", "A fundamental error", "recant his writings", "geologic, topographic, and natural ecosystem", "one can include arbitrarily many instances of 1 in any factorization", "136", "union membership", "Larger Catechism", "The European Court of Justice", "two", "Martin \"Al\" Culhane,", "Robert Park", "with Lebanese heritage,", "fatally shooting a limo driver", "Holley Wimunc.", "1918-1919.", "Ben Kingsley", "U.S. Holocaust Memorial Museum,", "Texas and Oklahoma to points east,", "Asashoryu's", "Mary Phagan,", "William Lynch", "to do its own immigration enforcement because the federal government is asleep at the switch,", "software magnate", "U.S. senators who couldn't resist taking the vehicles for a spin.", "Ninety-two percent", "Larry Ellison,", "Taher Nunu", "Obama", "Karen Floyd", "U.S. Chamber of Commerce", "Kim Il Sung died", "Daniel Nestor,", "Caylee Anthony,", "because its facilities are full.", "25 dead", "more than 200.", "that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown.", "they recently killed eight Indians whom the rebels accused of collaborating with the Colombian government,", "rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.", "South Africa", "Seoul,", "Haiti", "The United States", "he intends to apologize for his behavior.", "Daytime Emmy Lifetime Achievement Award", "Republican", "\"Gandhi,\"", "Eleven", "Hugo Chavez", "Four bodies", "translocation Down syndrome", "starch", "(the UK)", "Diptera", "the 100th anniversary of the first \"Tour de France\" bicycle race", "is a reference to the BBC teletext service Ceefax", "cartilage", "Johannes Brahms", "the 17th century", "Orson Welles"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6688924433409728}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09090909090909091, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3975", "mrqa_squad-validation-2788", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-1392", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-1442", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-697", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-334", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-4760", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-4478"], "SR": 0.609375, "CSR": 0.5946691176470589, "EFR": 1.0, "Overall": 0.7535431985294118}, {"timecode": 17, "before_eval_results": {"predictions": ["trade liberalisation", "14th century", "lymphocytes or an antibody-based humoral response", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "a multi-cultural city", "the father of the house when in his home", "John Fox", "US$1,000,000", "Annual Conference", "Colonel Monckton,", "thermodynamic", "\"Russian Madonna\" singer Valeriya, and London-based Russian art collector Nonna Materkova", "the FBI.", "helping to plan the September 11, 2001, terror attacks,", "\"People have lost their homes, their jobs, their hope,\"", "he was in good health, contrary to media reports he was diagnosed with skin cancer.", "Saturn owners", "iTunes,", "Seoul", "a remote part of northwestern Montana", "a delegation of American Muslim and Christian leaders", "South Africa", "wants a judge to order the pop star's estate to pay him a monthly allowance,", "after they ambushed a convoy carrying supplies for NATO forces in southern Afghanistan,", "Amsterdam, in the Netherlands,", "seven", "Iran test-launched a rocket capable of carrying a satellite,", "Lousiana", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "2008,", "the FBI.", "as many as 250,000", "the release of the four men", "Jake Garner", "question people if there's reason to suspect they're in the United States illegally.", "more than 4,000", "allegations that a dorm parent mistreated students at the school.", "Pakistan", "Columbia, Illinois,", "\"I'm just getting started.\"", "a older generation", "heavy flooding and scattered debris.", "Oxbow,", "Asashoryu", "Florida Everglades.", "Deputy Treasury Secretary", "Dubai", "Alfredo Astiz,", "a ban on inflatable or portable signs and banners on public property.", "Tim Clark, Matt Kuchar and Bubba Watson", "15,000", "President Bush of a failure of leadership at a critical moment in the nation's history.", "corruption", "Terrell Owens", "Rajendra Prasad", "Hartford,", "Ginger Rogers", "five", "Marine Corps", "Garfield", "a \"pickpocket\"", "seven", "a vigorous deciduous tree", "point-contact transistors"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6755645624477862}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.33333333333333337, 0.19999999999999998, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.4210526315789474, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7535", "mrqa_squad-validation-6559", "mrqa_squad-validation-8749", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-427", "mrqa_searchqa-validation-16210", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-2925"], "SR": 0.5625, "CSR": 0.5928819444444444, "EFR": 1.0, "Overall": 0.7531857638888889}, {"timecode": 18, "before_eval_results": {"predictions": ["Lower Lorraine", "Westchester", "humid subtropical", "American Sign Language", "Fort Caroline", "specialty drugs", "Doctor of Theology", "Christ", "The Prince of P\u0142ock", "multi-stage centrifugal pumps", "\"God Only Knows,", "40", "Sax Rohmer,", "Aug. 24, 1572", "fraractice", "\"Mocha Dick,\"", "Tiriac", "Jezebel", "Jeffrey Archer", "General Paulus", "Anne Boleyn", "Golda Meir", "a fur hat", "Jonas Bernanke", "Thai", "Parsley", "Japan", "Runic", "plutonium", "Guy Pearce", "blancmange", "fraadina", "frattage", "recorder", "fravelin throw", "ThriXXX,", "Austria", "Isambard Kingdom Brunel", "Edward Lear", "Jamaica", "Francis Ford", "Royal Dutch Shell", "Beyonce", "Microsoft", "Otto II", "frathanides", "The Battle of the Three Emperors", "southern Pacific Ocean,", "Trimdon,", "Midnight Cowboy", "Dada", "FIFA World Cup 2010", "Southwest Airlines", "Afghanistan", "Thomas M disableitch", "Rudolf H\u00f6ss", "3 May 1958", "Tom Hanks", "off Somalia's coast.", "canibalism", "(the) Brewers", "Ford Motor Company", "Banff", "a calves"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5447916666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6390", "mrqa_squad-validation-2008", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-1735", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-3824", "mrqa_naturalquestions-validation-4731", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-9943", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-3267"], "SR": 0.484375, "CSR": 0.587171052631579, "retrieved_ids": ["mrqa_squad-train-33262", "mrqa_squad-train-36206", "mrqa_squad-train-28260", "mrqa_squad-train-15977", "mrqa_squad-train-6975", "mrqa_squad-train-69617", "mrqa_squad-train-85455", "mrqa_squad-train-58107", "mrqa_squad-train-15554", "mrqa_squad-train-1935", "mrqa_squad-train-47909", "mrqa_squad-train-40089", "mrqa_squad-train-49134", "mrqa_squad-train-13187", "mrqa_squad-train-74535", "mrqa_squad-train-55222", "mrqa_searchqa-validation-15748", "mrqa_searchqa-validation-5955", "mrqa_squad-validation-7574", "mrqa_triviaqa-validation-2925", "mrqa_newsqa-validation-2634", "mrqa_squad-validation-8551", "mrqa_searchqa-validation-9109", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-2179", "mrqa_searchqa-validation-16625", "mrqa_squad-validation-8046", "mrqa_newsqa-validation-1073", "mrqa_searchqa-validation-198", "mrqa_newsqa-validation-3313", "mrqa_searchqa-validation-1162", "mrqa_squad-validation-1441"], "EFR": 0.9393939393939394, "Overall": 0.7399223734051036}, {"timecode": 19, "before_eval_results": {"predictions": ["2.2 inches", "tentilla", "Sky Q Silver", "\"ash tree\"", "24 September 2007", "2007", "34\u201319", "1991", "Canada", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands, thoracic aorta and the pulmonary artery", "Tony Blair", "The Flintstones", "9-1-1", "Jonathan Swift", "South Sudan", "Maria Bueno", "dill", "Frankie Laine", "1992,", "Thor", "bulgaria", "Goosnargh", "(1967\u20131969)", "dna structure", "Montr\u00e9al", "dassler Brothers", "murray", "Rocky and Bullwinkle", "Ray Winstone", "al-Qaida", "Poland", "Indiana Jones", "John Philip Sousa", "Hyde Park Corner", "Sydney", "Alabama", "Jura", "armoured car", "a finger", "a meteoroid", "Norman Brookes", "bobbyjo", "ilita", "Bodhidharma", "dolly", "Albert Reynolds", "gaff", "Baltic Sea", "Singapore", "auster", "yellow", "murray", "vespa", "Squamish", "13 September 2011", "Theme Park World", "Cape Cod", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "10", "867-5309", "quizlet", "a medium", "the small intestine", "Buddha"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5666666666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4634", "mrqa_squad-validation-8598", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-7777", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7743", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-5317", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4323", "mrqa_searchqa-validation-10359"], "SR": 0.53125, "CSR": 0.584375, "EFR": 0.9666666666666667, "Overall": 0.7448177083333333}, {"timecode": 20, "UKR": 0.791015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5049", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-3545", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1612", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3016", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3665", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3797", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3881", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3949", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-605", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10297", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10823", "mrqa_searchqa-validation-10883", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-1162", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-12547", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13844", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14734", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15795", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-198", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3932", "mrqa_searchqa-validation-3960", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4674", "mrqa_searchqa-validation-4910", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5349", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5456", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-6011", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6722", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7043", "mrqa_searchqa-validation-7384", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-8721", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-9403", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10125", "mrqa_squad-validation-10218", "mrqa_squad-validation-10252", "mrqa_squad-validation-10274", "mrqa_squad-validation-10280", "mrqa_squad-validation-10287", "mrqa_squad-validation-10307", "mrqa_squad-validation-10380", "mrqa_squad-validation-10395", "mrqa_squad-validation-10433", "mrqa_squad-validation-1049", "mrqa_squad-validation-10494", "mrqa_squad-validation-10506", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-1122", "mrqa_squad-validation-1177", "mrqa_squad-validation-1206", "mrqa_squad-validation-1215", "mrqa_squad-validation-1329", "mrqa_squad-validation-1347", "mrqa_squad-validation-1407", "mrqa_squad-validation-1456", "mrqa_squad-validation-1548", "mrqa_squad-validation-1587", "mrqa_squad-validation-1615", "mrqa_squad-validation-1661", "mrqa_squad-validation-167", "mrqa_squad-validation-1753", "mrqa_squad-validation-19", "mrqa_squad-validation-1983", "mrqa_squad-validation-2009", "mrqa_squad-validation-204", "mrqa_squad-validation-2072", "mrqa_squad-validation-2088", "mrqa_squad-validation-2095", "mrqa_squad-validation-2102", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2286", "mrqa_squad-validation-2289", "mrqa_squad-validation-2346", "mrqa_squad-validation-2353", "mrqa_squad-validation-2365", "mrqa_squad-validation-2372", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-2476", "mrqa_squad-validation-25", "mrqa_squad-validation-253", "mrqa_squad-validation-2560", "mrqa_squad-validation-2564", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2684", "mrqa_squad-validation-2762", "mrqa_squad-validation-2833", "mrqa_squad-validation-2844", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2932", "mrqa_squad-validation-2949", "mrqa_squad-validation-2976", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3461", "mrqa_squad-validation-3493", "mrqa_squad-validation-3543", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3681", "mrqa_squad-validation-3699", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-3955", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4308", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-4489", "mrqa_squad-validation-4502", "mrqa_squad-validation-452", "mrqa_squad-validation-455", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4594", "mrqa_squad-validation-4619", "mrqa_squad-validation-4633", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4664", "mrqa_squad-validation-4694", "mrqa_squad-validation-4736", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4782", "mrqa_squad-validation-4829", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-4999", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5178", "mrqa_squad-validation-5302", "mrqa_squad-validation-5311", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-5418", "mrqa_squad-validation-543", "mrqa_squad-validation-5451", "mrqa_squad-validation-5465", "mrqa_squad-validation-5470", "mrqa_squad-validation-5528", "mrqa_squad-validation-5570", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5617", "mrqa_squad-validation-5706", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5911", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6097", "mrqa_squad-validation-6185", "mrqa_squad-validation-6206", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-6354", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6569", "mrqa_squad-validation-6572", "mrqa_squad-validation-6680", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-6975", "mrqa_squad-validation-703", "mrqa_squad-validation-7051", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7243", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-7462", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-763", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-773", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-7964", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8115", "mrqa_squad-validation-813", "mrqa_squad-validation-8136", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-828", "mrqa_squad-validation-8337", "mrqa_squad-validation-8436", "mrqa_squad-validation-850", "mrqa_squad-validation-8575", "mrqa_squad-validation-8597", "mrqa_squad-validation-8683", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-8864", "mrqa_squad-validation-9017", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-919", "mrqa_squad-validation-9198", "mrqa_squad-validation-9227", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9559", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9617", "mrqa_squad-validation-9640", "mrqa_squad-validation-9734", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1319", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2681", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-3006", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-4742", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-644", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7624", "mrqa_triviaqa-validation-7777"], "OKR": 0.865234375, "KG": 0.40703125, "before_eval_results": {"predictions": ["chromalveolate", "pathogens", "1525\u201332", "a few", "solution", "2011", "random noise", "Wardenclyffe", "Fix", "preston", "Washington Post", "prefecture", "Steve Biko", "leather", "blister beetle", "acute", "nellig mag-OH NO!", "a salt", "lew hool and the Gang", "lew murray", "oiver TWIST 1968", "Lone Ranger", "Bolton", "Hawaii", "wyarevitch", "le Roy", "junk planet", "Hartford", "majesty", "King George III", "lew h. Seward", "river Severn", "cairn", "nell Nimoy", "preston", "lewston", "Jesse Garon Presley", "Kopassus", "lithium", "40", "duchess of devonshire", "leotard-wearing", "white", "China", "Salt Lake City,", "cepheus", "Capricorn", "match Rugby", "Sergio Garcia", "meadow brown", "lewitted, bald man", "The Savoy", "Steve Jobs", "habitat", "2 %", "729", "Twitch", "right-wing extremist groups.", "Rocky Ford brand cantaloupes", "Heartbreak Hotel", "leopard", "Wes Craven", "Australian", "\"$10,000 Kelly,\""], "metric_results": {"EM": 0.40625, "QA-F1": 0.44635416666666666}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8756", "mrqa_squad-validation-6470", "mrqa_squad-validation-2513", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-4912", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-6527", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3820", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-4453", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-4152", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-7635", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-875", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-4158", "mrqa_searchqa-validation-10273", "mrqa_hotpotqa-validation-2205"], "SR": 0.40625, "CSR": 0.5758928571428572, "EFR": 1.0, "Overall": 0.7278348214285715}, {"timecode": 21, "before_eval_results": {"predictions": ["Edison Medal", "Extension", "bourgeois", "confrontational", "The San Francisco Bay Area", "gold", "Chinese", "Surrey", "Telstar", "wED", "Buzz Aldrin", "titus", "Niger", "Backgammon", "Instagram", "Home alone", "Columbus", "t.S. Eliot", "Venus", "Bob Marley & the Wailers", "Crusades", "nicky Henderson", "a mansard roof", "jagger", "Danae", "tchaikovsky", "Socrates", "selenium", "Stephen King", "chestnut, and sometimes has white markings on its face", "Catskill Mountains", "dogs", "wirings", "fluid", "Jordan", "jerry huggins", "London", "a chainsaw", "Poland", "EGBDF", "united states", "dill", "eukaristia", "100 years", "apple", "Washington, D.C.", "Piccadilly", "tundra", "Melbourne, Victoria, Australia", "norman thistle", "Tangled", "Vincent Motorcycle Company", "Melissa Duck", "inner core", "novella", "The Prodigy", "Jack White", "Michelle Rounds", "21-year-old", "gertrude Stein", "Daytona", "nick reiner", "Mickey's PhilharMagic", "hiphop"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5885416666666666}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-170", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-7334", "mrqa_triviaqa-validation-5909", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-5681", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-7539", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-550"], "SR": 0.5625, "CSR": 0.5752840909090908, "retrieved_ids": ["mrqa_squad-train-2872", "mrqa_squad-train-64651", "mrqa_squad-train-985", "mrqa_squad-train-16893", "mrqa_squad-train-35308", "mrqa_squad-train-59650", "mrqa_squad-train-64827", "mrqa_squad-train-65720", "mrqa_squad-train-62090", "mrqa_squad-train-74937", "mrqa_squad-train-1317", "mrqa_squad-train-66675", "mrqa_squad-train-24529", "mrqa_squad-train-3056", "mrqa_squad-train-77933", "mrqa_squad-train-76908", "mrqa_searchqa-validation-3369", "mrqa_hotpotqa-validation-3843", "mrqa_newsqa-validation-983", "mrqa_squad-validation-694", "mrqa_searchqa-validation-11451", "mrqa_squad-validation-7527", "mrqa_squad-validation-7574", "mrqa_newsqa-validation-2249", "mrqa_searchqa-validation-3478", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-3406", "mrqa_hotpotqa-validation-2600", "mrqa_newsqa-validation-3053", "mrqa_squad-validation-1456", "mrqa_newsqa-validation-3965"], "EFR": 1.0, "Overall": 0.7277130681818182}, {"timecode": 22, "before_eval_results": {"predictions": ["The Times newspaper", "being drafted into the Austro-Hungarian Army", "about 63,517", "faith in Christ", "Ticonderoga Point", "a seal", "Season 4", "Tyrion", "1972 -- 81", "Randy Goodrum", "October 1980", "james garner", "Central and South regions", "Muguruza", "Missi Hale", "alternating biannually with the World Cup of Hockey", "California beach intercut with scenes of them driving an orange campervan", "Gregor Mendel", "Baltimore, Maryland", "The United States is the only Western country currently applying the death penalty", "Battle of Antietam and Lincoln's Emancipation Proclamation", "loghouse, church, and a number of houses", "left atrium and ventricle", "Brooklyn, New York", "1560s", "Davos", "Prince James", "New Orleans", "The Granite Mountain Hotshots were a group within the department whose mission was to fight wildfires", "U.S. service members", "March 16, 2018", "Narendra Modi", "Sohrai", "an explosion", "a pop and R&B ballad", "Annette", "The season was ordered in May 2017", "james garner", "ABC", "eukaryotic cells", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome )", "Henry Purcell", "Thomas Edison", "Hellenism", "1964", "Jack Nicklaus", "Jenny Slate", "between 8.7 % and 9.1 %", "\"The phrase tippecanoe and Tyler Too ''", "37.7", "1954", "1922 to 1991", "neil helfgott", "james", "Ethiopia", "Mountain West Conference", "Sydney", "yasiin Bey", "look at how the universe formed by analyzing particle collisions.", "Pastor Paula White", "combat veterans", "eliot", "Antarctica", "cherry bombs"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5186343744181445}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.19999999999999998, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 0.14285714285714288, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.6451612903225806, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.37499999999999994, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2919", "mrqa_squad-validation-2373", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-2037", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-6089", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-7080", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-6854", "mrqa_hotpotqa-validation-1873", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-4157", "mrqa_searchqa-validation-9687"], "SR": 0.40625, "CSR": 0.5679347826086957, "EFR": 1.0, "Overall": 0.7262432065217392}, {"timecode": 23, "before_eval_results": {"predictions": ["Andrew Alper", "DeMarcus Ware", "life on Tyneside,", "vicious and destructive", "60%", "girls", "in the 1980s", "scrolls dating back to the 12th and 13th centuries", "almost 3,000", "`` Skid Row ''", "T'Pau", "Millerlite", "comedy web television series", "Universal Pictures and Focus Features", "LED illuminated display", "committed and effective Sultans", "when each of the variables is a perfect monotone function of the other", "Mangal Pandey", "North Carolina", "in the eye", "IBM", "Felicity Huffman", "Djokovic", "84 season", "the United States economy first went into an economic recession", "Wales and Yorkshire", "1979 / 80", "Pyeongchang County, Gangwon Province, South Korea", "Sanchez Navarro", "the nerves and ganglia outside the brain and spinal cord", "Nalini Negi", "very important", "in the Southern United States, and has been sold as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "Jodie Foster", "the head of state", "May 18, 2018", "10 May 1940", "Sally Field", "King Willem - Alexander", "`` It Ain't Over'til It's Over ''", "Massillon, Ohio", "African - Americans", "the eighth and farthest known planet from the Sun in the Solar System", "the RAF, Fighter Command had achieved a great victory in successfully carrying out Sir Thomas Inskip's 1937 air policy of preventing the Germans from knocking Britain out of the war", "c. 8000 BC", "New York City", "German and Italian", "20 July 2015", "Coroebus of Elis", "Tami Lynn", "New York Giants quarterback Phil Simms", "1", "Nepal", "Elton John", "lung cancer", "Pakistan", "Sam Raimi", "7 October 1978", "bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "natural disasters", "Alabama", "wiki", "gaffer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6213210978835979}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 0.0, 0.0, 1.0, 0.2857142857142857, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.07407407407407407, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9047619047619047, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_naturalquestions-validation-9032", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3593", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-2547", "mrqa_newsqa-validation-692", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-8291"], "SR": 0.515625, "CSR": 0.5657552083333333, "EFR": 0.9354838709677419, "Overall": 0.712904065860215}, {"timecode": 24, "before_eval_results": {"predictions": ["ca. 22,000\u201314,000 yr BP", "Many people in the city have Scottish or Irish ancestors", "German creedal hymn \"Wir glauben all an einen Gott\" (\"We All Believe in One True God\")", "April 20", "Tanzania", "March 29", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia", "1928", "the ruling city of the Northern Kingdom of Israel, Samaria", "in northern China", "Missouri River", "Harry", "September 21, 2017", "Austria - Hungary", "Robert Gillespie Adamson IV", "1950, 1955, 1956, 1974, 1975, 1985, 2000", "May 3, 2005", "David Hemmings as Nigel", "Vijaya Mulay", "a global cruise line that was founded in Italy", "1977", "Cody Fern", "22 November 1970", "Reveille", "2007", "Camping World Stadium", "Aldis Hodge", "US $11,770", "Hans Zimmer, Steve Mazzaro & Missi Hale", "to form a higher alkane", "based on God from eternity are all His works", "Kimberlin Brown", "British - American rock band Fleetwood Mac", "a single, very long DNA helix on which thousands of genes are encoded", "in either Tagalog or English", "American rock band R.E.M.", "a blend of ground beef and other ingredients", "Juliet", "a semi-independent State of Vietnam", "July 25, 2017", "Rachel Kelly Tucker", "September 24, 2012", "rocks and minerals", "to encounter antigens passing through the mucosal epithelium", "Super Bowl LII", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "its vast territory was divided into several successor polities", "St. Theodosius Russian Orthodox Cathedral", "a hooker and addict", "Kingsholm Stadium and Sandy Park", "Ahmad ( Real ) selected Doll", "a `` skin - changer ''", "a best known as the lead singer and lyricist of the rock band Led Zeppelin", "beetle", "Copenhagen", "Super Bowl XXIX", "a Soviet and Russian actor and film director", "Elbow River", "41,", "Fareed Zakaria", "Afghan National Security Forces", "a Satanic image in English witch", "a Welch rabbit", "the International Committee of the Red Cross"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5718344974078341}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.19999999999999998, 0.33333333333333337, 1.0, 1.0, 0.8, 0.6875000000000001, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.8571428571428571, 0.7741935483870968, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_squad-validation-5042", "mrqa_squad-validation-2416", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-6864", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-3362", "mrqa_searchqa-validation-13806", "mrqa_searchqa-validation-1833", "mrqa_searchqa-validation-11809"], "SR": 0.4375, "CSR": 0.5606249999999999, "retrieved_ids": ["mrqa_squad-train-73097", "mrqa_squad-train-15380", "mrqa_squad-train-33932", "mrqa_squad-train-6447", "mrqa_squad-train-33002", "mrqa_squad-train-75365", "mrqa_squad-train-8321", "mrqa_squad-train-37147", "mrqa_squad-train-86548", "mrqa_squad-train-17563", "mrqa_squad-train-48189", "mrqa_squad-train-30836", "mrqa_squad-train-53230", "mrqa_squad-train-1520", "mrqa_squad-train-57179", "mrqa_squad-train-81111", "mrqa_newsqa-validation-1016", "mrqa_searchqa-validation-3222", "mrqa_triviaqa-validation-170", "mrqa_newsqa-validation-3267", "mrqa_naturalquestions-validation-6321", "mrqa_newsqa-validation-3965", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4363", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-11704", "mrqa_squad-validation-6975", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-7536", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-697"], "EFR": 1.0, "Overall": 0.72478125}, {"timecode": 25, "before_eval_results": {"predictions": ["exceeds any given number", "8:10 p.m.", "about 5 nanometers across, arranged in rows 6.4 nanometers apart", "1894", "the means of production", "Atlanta, Georgia", "Thunder Road", "Acid rain", "Bette Midler", "gathering money from the public", "the pyloric valve", "james garner", "Julia Ormond", "synovial", "The Satavahanas", "March 16, 2018", "Hathi Jr", "by capillary action", "twice", "Asuka", "in the pachytene stage of prophase I of meiosis during a process called synapsis", "Hathi Jr", "the Lower Mainland in Vancouver", "electronic computers", "notorious Welsh pirate Edward Kenway, grandfather and father of Assassin's Creed III protagonist and antagonist Ratonhnhak\u00e9 : ton and Haytham Kenway", "Madison, Wisconsin, United States", "Germany", "2018", "1981", "USS Chesapeake", "Luke Skywalker and Kylo Ren", "a transformiation, change of mind, repentance, and atonement", "The reservation nourishes the historically disadvantaged castes and tribes, listed as Scheduled Castes and Scheduled Tribes by the Government of India", "Harishchandra", "The Intolerable Acts", "31 January 1934", "Cairo, Illinois", "Hedwig", "Lee Mack", "without deviating from basic strategy", "Burnham Beeches in Buckinghamshire", "1898", "Frank Morris", "April 1st", "9.7 m ( 31.82 ft ) and 9 t ( 20,000 lb )", "the Northeast Monsoon", "Michael Crawford", "1930s", "Thomas Mundy Peterson", "her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries", "How I Met Your Mother", "The Parlement de Bretagne", "Steve Davis", "phosphorus", "Spencer Perceval", "Scotland", "the Chief of the Operations Staff of the Armed Forces High Command (Oberkommando der Wehrmacht)", "Jack Kilby", "Cpl. Richard Findley,", "Venezuela", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "a mid-sized city located in Stark County, Northeast Ohio in the State of Ohio, USA.", "jopardy", "new Orleans"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5518054529123902}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.28571428571428575, 0.33333333333333337, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.17142857142857143, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.3636363636363636, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.09090909090909093, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.35294117647058826, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1583", "mrqa_squad-validation-8869", "mrqa_squad-validation-7514", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-7021", "mrqa_triviaqa-validation-5467", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3902", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-3191", "mrqa_searchqa-validation-1563", "mrqa_searchqa-validation-15996"], "SR": 0.4375, "CSR": 0.5558894230769231, "EFR": 0.9722222222222222, "Overall": 0.7182785790598291}, {"timecode": 26, "before_eval_results": {"predictions": ["A deterministic Turing machine", "State Route 99", "those who already hold wealth", "vector quantities", "the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Thomas Alva Edison", "Andy Serkis", "England", "virtual reality simulator", "the five - year time jump", "seven years earlier on Christmas Eve", "September 6, 2019", "an integral membrane protein that builds up a proton gradient across a biological membrane", "18", "Jack Nicklaus", "two installments", "Spanish missionaries, ranchers and troops", "Sedimentary rock", "a 2010 United States federal law requiring all non-U.S. ('foreign') financial institutions", "the Western world", "Vicente Fox", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "Ben Rosenbaum", "Zilphia Horton", "Richard Stallman", "Santa Monica", "South Asia", "December 15, 2017", "Ed Sheeran", "Johnson", "the liver and kidneys", "the lumbar cistern", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "the 1932 Games", "Geoffrey Zakarian", "Tommy James and the Shondells", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Bonnie Aarons", "April 13, 2018", "Jay Baruchel", "De Wayne Warren", "2004", "A rear - view mirror ( or rearview mirror )", "the New World", "1986", "The terrestrial biosphere", "1937", "2017", "Beijing", "the court from its members for a three - year term", "convert single - stranded genomic RNA into double - stranded cDNA which can integrate into the host genome", "Thomas Edison", "United Nations Month", "1 through 75", "Famous Players-Lasky Corporation", "Tiffany & Company", "Al Gore", "villanelle", "a man's lifeless, naked body", "a man's lifeless, naked body", "four months ago,", "magnesium", "Captain Christopher Newport", "rotunda"], "metric_results": {"EM": 0.453125, "QA-F1": 0.583498228884219}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 0.0, 1.0, 0.5283018867924527, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.9, 0.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.2222222222222222, 0.6976744186046512, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10320", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1696", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1974", "mrqa_triviaqa-validation-1173", "mrqa_triviaqa-validation-667", "mrqa_triviaqa-validation-86", "mrqa_hotpotqa-validation-2141", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-464", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11530"], "SR": 0.453125, "CSR": 0.5520833333333333, "EFR": 0.8857142857142857, "Overall": 0.7002157738095238}, {"timecode": 27, "before_eval_results": {"predictions": ["voluminous literature on the subject", "Dane", "Albert C. Outler", "Henry Young Darracott Scott, also of the Royal Engineers", "Seminole Tribe", "about 12 million", "Tuesday in Los Angeles.", "The pilot, whose name has not yet been released,", "the estate with its 18th-century sights, sounds, and education facility.", "Mubarak", "22-year-old", "southern port city of Karachi,", "Brian David Mitchell,", "NASCAR", "the repeal of the military's \"don't ask, don't tell\" policy", "leftist Workers' Party.", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "step up", "helping to plan the September 11, 2001,", "tried to fake his own death by crashing his private plane into a Florida swamp.", "october out of eleven,\"", "Little Rock military recruiting center.", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "part of the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "blew up an ice jam Wednesday evening south of  Bismarck,", "Michelle Rounds", "telephone survey", "not speak", "African National Congress Deputy President Kgalema Motlanthe,", "Turkish, Iraqi and Syrian ministers met in Ankara on Thursday to discuss water shortages in the major Tigris and Euphrates rivers, which run through all three countries.", "Bill Stanton", "humans", "Herman Thomas", "kuranyi's 15th league goal of the season -- after just two minutes -- gave Schalke the three points with a 1-0 win in Hamburg", "a lightning strike", "Deputy Treasury Secretary", "St. Louis, Missouri,", "Texas", "two weeks after Black History Month", "the Somali capital of Mogadishu.", "Tom Hanks", "outside his house in Najaf's Adala neighborhood", "11th year in a row", "the last surviving British soldier from World War I", "Rocky Ford brand cantaloupes", "Both men were hospitalized and expected to survive,", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "22", "Andre Ward", "Abdullah Gul,", "1979", "Stephen Vance", "Lauren Blumenfeld", "Jughead Jones", "Sarah Josepha Hale", "1997", "violinist.com", "to stop a video or step backwards through your selections", "House of Fraser", "Reginald Engelbach", "Al Capone", "october", "shrimp", "cnidarians"], "metric_results": {"EM": 0.375, "QA-F1": 0.4995383503875352}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.7499999999999999, 0.4, 0.2222222222222222, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.5, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.6666666666666666, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6816", "mrqa_squad-validation-5270", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3453", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-619", "mrqa_newsqa-validation-778", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-2233", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1604", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-5640", "mrqa_triviaqa-validation-4531", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-3394", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-3554"], "SR": 0.375, "CSR": 0.5457589285714286, "retrieved_ids": ["mrqa_squad-train-59190", "mrqa_squad-train-75313", "mrqa_squad-train-34251", "mrqa_squad-train-84800", "mrqa_squad-train-50007", "mrqa_squad-train-46519", "mrqa_squad-train-67882", "mrqa_squad-train-82728", "mrqa_squad-train-35295", "mrqa_squad-train-76545", "mrqa_squad-train-27931", "mrqa_squad-train-71024", "mrqa_squad-train-22971", "mrqa_squad-train-70098", "mrqa_squad-train-19166", "mrqa_squad-train-29514", "mrqa_newsqa-validation-4098", "mrqa_squad-validation-7719", "mrqa_naturalquestions-validation-5928", "mrqa_triviaqa-validation-4580", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-627", "mrqa_squad-validation-9489", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-3555", "mrqa_newsqa-validation-240", "mrqa_triviaqa-validation-45", "mrqa_newsqa-validation-3911", "mrqa_triviaqa-validation-4912", "mrqa_squad-validation-2976", "mrqa_naturalquestions-validation-2605", "mrqa_searchqa-validation-13657"], "EFR": 1.0, "Overall": 0.7218080357142858}, {"timecode": 28, "before_eval_results": {"predictions": ["Beyonc\u00e9 and Bruno Mars", "Nepali", "German", "Sheikh Sharif Sheikh Ahmed", "Africa", "two Manchester, England shows", "Rod Blagojevich", "gasoline", "Denver, Colorado.", "Dolgorsuren Dagvadorj,", "not", "Zac Efron", "Picasso's muse and mistress, Marie-Therese Walter.", "Deputy Treasury Secretary", "drowned", "Kurt Cobain", "the Iranian consulate,", "The Casalesi Camorra clan", "President Clinton.", "he regretted describing her as \"wacko.\"", "Adenhart", "Carnival", "education", "environmental", "2009", "problems with the way Britain implements European Union employment directives.", "France's famous Louvre museum", "More than 15,000", "He won it with an organization that even opponents called brilliant.", "0-0 draw", "Spaniard", "the National Guard reallocate reconnaissance helicopters and robotic surveillance craft", "$249", "Amsterdam, in the Netherlands,", "Juan Martin Del Potro.", "his wife,", "the finding of \"a whole new treasure hoard of fossils\"", "Nazi Germany", "Sharon Bialek", "Kurdish militant group in Turkey", "military veterans", "41,", "the job bill's controversial millionaire's surtax", "Sabina Guzzanti", "Booches Billiard Hall,", "More than 15,000", "21 percent", "China", "Najaf.", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "Haitians", "Bobby Jindal", "necessary, but not sufficient", "Italian pignatta", "1973", "football", "rage", "Parkinson's disease", "ten", "Disha Patani", "Anah\u00ed", "Labour", "The Passing of Arthur", "witchcraft"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6929124694749694}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.8333333333333334, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.20512820512820515, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.7142857142857143, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-4207", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10680", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-4573", "mrqa_hotpotqa-validation-2876", "mrqa_searchqa-validation-11053", "mrqa_searchqa-validation-15007"], "SR": 0.5625, "CSR": 0.5463362068965517, "EFR": 0.9642857142857143, "Overall": 0.7147806342364532}, {"timecode": 29, "before_eval_results": {"predictions": ["Systemic acquired resistance", "quarterback Broncos quarterback Denver Broncos", "teach by rote", "treats as a way to introduce those unfamiliar with a vegan diet to some of the flavorful foods they can eat.", "\"Dance Your Ass Off.\"", "Charles H.W. Bush", "his business dealings for possible securities violations", "Royal Navy servicemen who have been helping the Iraqis to protect oil fields around the port town of Umm Qasr,", "Jacob Zuma,", "Simon Cowell.", "great jazz music and a very cheerful crowd.", "\"falling space debris,\"", "Obama's", "three", "Monday night", "prison inmates.", "Franklin, Tennessee,", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "the coalition", "sexual assault on a child.", "Brian David Mitchell,", "Saturn", "football", "getting by without U.S. taxpayer money.", "Republican", "only normal maritime traffic", "Dean Martin, Katharine Hepburn and Spencer Tracy", "lining up for vitamin injections that promise to improve health and beauty.", "the area was sealed off, so they did not know casualty figures.", "twice.", "The EU naval force", "chairman of the House Budget Committee,", "top designers, such as Stella McCartney,", "about 5:20 p.m.", "think themselves as an \"extermination\" force that works as the armed front \"of the people and for the people.\"", "Darrel Mohler", "Casalesi clan", "Obama and McCain camps", "Sen. Barack Obama", "heavy brush,", "more than 30", "Empire of the Sun", "30-minute", "11 healthy eggs", "Laura Ling and Euna Lee,", "a paragraph about the king and crown prince", "second time since the 1990s", "Monday,", "won 1-0 victory against underdogs Los Ticos in Cairo after the Black Satellites qualified for the final for the third time with a 3-2 victory against suspension-hit Hungary in the first semi.", "a small child", "reached an agreement late Thursday", "6-4 loss,", "Don Valley Parkway / Highway 403 Junction in Toronto", "they each supported major regional wars known as proxy wars", "late January or early February", "Galileo Galilei", "The Colossus of Rhodes", "paper sales company", "chancellor of Austria", "Indianola, Mississippi", "Wayne County, Michigan", "Diff'rent Strokes", "emperor of Japan.", "Charles Parker and the Vicious Circle"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4929558100414078}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.14285714285714285, 0.4, 0.0, 0.25, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.608695652173913, 0.0, 0.5000000000000001, 1.0, 0.0, 0.5, 0.0, 0.0, 0.8, 0.0, 0.8571428571428571, 0.28571428571428575]}}, "before_error_ids": ["mrqa_squad-validation-627", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-8441", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-6435", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-1681", "mrqa_searchqa-validation-9488", "mrqa_searchqa-validation-1614", "mrqa_searchqa-validation-2389"], "SR": 0.359375, "CSR": 0.5401041666666666, "EFR": 1.0, "Overall": 0.7206770833333334}, {"timecode": 30, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1296", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4271", "mrqa_hotpotqa-validation-4803", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5831", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1385", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3352", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3853", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4124", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4628", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-5411", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6321", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8014", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9824", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1057", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-1185", "mrqa_newsqa-validation-1224", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1542", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2190", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2528", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2592", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2624", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2836", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3234", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3685", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4023", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-557", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-621", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-673", "mrqa_newsqa-validation-741", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10359", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11352", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11875", "mrqa_searchqa-validation-12038", "mrqa_searchqa-validation-12312", "mrqa_searchqa-validation-12462", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12750", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13459", "mrqa_searchqa-validation-13476", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13899", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-1453", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-15224", "mrqa_searchqa-validation-15804", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2214", "mrqa_searchqa-validation-2338", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2863", "mrqa_searchqa-validation-2871", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3222", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-3369", "mrqa_searchqa-validation-3478", "mrqa_searchqa-validation-3720", "mrqa_searchqa-validation-4057", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-5056", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-541", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-5728", "mrqa_searchqa-validation-5762", "mrqa_searchqa-validation-5785", "mrqa_searchqa-validation-5963", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6264", "mrqa_searchqa-validation-6638", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-6992", "mrqa_searchqa-validation-7564", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-8574", "mrqa_searchqa-validation-86", "mrqa_searchqa-validation-8658", "mrqa_searchqa-validation-9605", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9777", "mrqa_squad-validation-10011", "mrqa_squad-validation-10014", "mrqa_squad-validation-10218", "mrqa_squad-validation-10249", "mrqa_squad-validation-10274", "mrqa_squad-validation-10307", "mrqa_squad-validation-10489", "mrqa_squad-validation-10494", "mrqa_squad-validation-1086", "mrqa_squad-validation-1092", "mrqa_squad-validation-111", "mrqa_squad-validation-1177", "mrqa_squad-validation-1215", "mrqa_squad-validation-1490", "mrqa_squad-validation-1587", "mrqa_squad-validation-1641", "mrqa_squad-validation-1661", "mrqa_squad-validation-1753", "mrqa_squad-validation-204", "mrqa_squad-validation-2088", "mrqa_squad-validation-217", "mrqa_squad-validation-2190", "mrqa_squad-validation-2192", "mrqa_squad-validation-2226", "mrqa_squad-validation-2283", "mrqa_squad-validation-2286", "mrqa_squad-validation-2353", "mrqa_squad-validation-2372", "mrqa_squad-validation-2373", "mrqa_squad-validation-2395", "mrqa_squad-validation-2411", "mrqa_squad-validation-2421", "mrqa_squad-validation-25", "mrqa_squad-validation-2622", "mrqa_squad-validation-2656", "mrqa_squad-validation-2762", "mrqa_squad-validation-2857", "mrqa_squad-validation-304", "mrqa_squad-validation-3040", "mrqa_squad-validation-3130", "mrqa_squad-validation-3168", "mrqa_squad-validation-3382", "mrqa_squad-validation-3393", "mrqa_squad-validation-3508", "mrqa_squad-validation-3559", "mrqa_squad-validation-3654", "mrqa_squad-validation-3699", "mrqa_squad-validation-3796", "mrqa_squad-validation-3941", "mrqa_squad-validation-3955", "mrqa_squad-validation-3975", "mrqa_squad-validation-4015", "mrqa_squad-validation-4162", "mrqa_squad-validation-4382", "mrqa_squad-validation-4398", "mrqa_squad-validation-4452", "mrqa_squad-validation-4550", "mrqa_squad-validation-457", "mrqa_squad-validation-4585", "mrqa_squad-validation-4619", "mrqa_squad-validation-4634", "mrqa_squad-validation-466", "mrqa_squad-validation-4694", "mrqa_squad-validation-4753", "mrqa_squad-validation-4763", "mrqa_squad-validation-4764", "mrqa_squad-validation-4774", "mrqa_squad-validation-4782", "mrqa_squad-validation-490", "mrqa_squad-validation-4933", "mrqa_squad-validation-494", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-5003", "mrqa_squad-validation-5014", "mrqa_squad-validation-5029", "mrqa_squad-validation-5071", "mrqa_squad-validation-5302", "mrqa_squad-validation-5360", "mrqa_squad-validation-5370", "mrqa_squad-validation-5377", "mrqa_squad-validation-538", "mrqa_squad-validation-543", "mrqa_squad-validation-5465", "mrqa_squad-validation-5528", "mrqa_squad-validation-5589", "mrqa_squad-validation-5616", "mrqa_squad-validation-5806", "mrqa_squad-validation-5824", "mrqa_squad-validation-5824", "mrqa_squad-validation-5852", "mrqa_squad-validation-5956", "mrqa_squad-validation-5961", "mrqa_squad-validation-5995", "mrqa_squad-validation-6058", "mrqa_squad-validation-6082", "mrqa_squad-validation-6151", "mrqa_squad-validation-6206", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6349", "mrqa_squad-validation-641", "mrqa_squad-validation-6557", "mrqa_squad-validation-6572", "mrqa_squad-validation-6792", "mrqa_squad-validation-6809", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-704", "mrqa_squad-validation-719", "mrqa_squad-validation-7281", "mrqa_squad-validation-7291", "mrqa_squad-validation-7307", "mrqa_squad-validation-7330", "mrqa_squad-validation-7462", "mrqa_squad-validation-7527", "mrqa_squad-validation-7608", "mrqa_squad-validation-7622", "mrqa_squad-validation-7659", "mrqa_squad-validation-7665", "mrqa_squad-validation-7719", "mrqa_squad-validation-7729", "mrqa_squad-validation-7751", "mrqa_squad-validation-7785", "mrqa_squad-validation-7822", "mrqa_squad-validation-7829", "mrqa_squad-validation-7837", "mrqa_squad-validation-7855", "mrqa_squad-validation-7908", "mrqa_squad-validation-7964", "mrqa_squad-validation-7990", "mrqa_squad-validation-8046", "mrqa_squad-validation-8056", "mrqa_squad-validation-8204", "mrqa_squad-validation-8210", "mrqa_squad-validation-8216", "mrqa_squad-validation-8269", "mrqa_squad-validation-828", "mrqa_squad-validation-8558", "mrqa_squad-validation-8568", "mrqa_squad-validation-8597", "mrqa_squad-validation-87", "mrqa_squad-validation-883", "mrqa_squad-validation-9019", "mrqa_squad-validation-9054", "mrqa_squad-validation-9110", "mrqa_squad-validation-9135", "mrqa_squad-validation-9145", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9334", "mrqa_squad-validation-9365", "mrqa_squad-validation-9379", "mrqa_squad-validation-957", "mrqa_squad-validation-9603", "mrqa_squad-validation-9640", "mrqa_squad-validation-973", "mrqa_squad-validation-9870", "mrqa_squad-validation-9918", "mrqa_squad-validation-9993", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1245", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1524", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-1945", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-199", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2716", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2925", "mrqa_triviaqa-validation-2972", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-3662", "mrqa_triviaqa-validation-3725", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-391", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-4567", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4721", "mrqa_triviaqa-validation-4772", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-528", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-544", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5592", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-6199", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7233", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-7426", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-79"], "OKR": 0.87890625, "KG": 0.4890625, "before_eval_results": {"predictions": ["Super Bowl XX", "undermining the communist ideology", "67.9", "communication", "Wendell, North Carolina", "Queen Mary II", "Pula Arena", "caribes", "Google Doodles", "ionization energy", "HIV", "a chela", "1942's issue No. 1", "The Last Starfighter", "(to) Prone", "the House of Romanov", "a mirror", "fermentation", "Thomas Becket", "Morocco", "Little Red Riding Hood", "distressing", "The Simpsons Movie", "Clara Barton", "Hawaii", "Minnesota", "a bad one", "Han Solo", "Gutzon Borglum", "Catherine of Aragon", "Paris", "Festa di San Marco", "Oklahoma", "Salman Rushdie", "the United Nations", "Tycho Brahe", "an American sitcom that aired on CBS from February 8, 1974 to August 1, 1979", "the Four C's", "elephants", "cloister", "Mail to the Chief", "(Punjab) financial", "Idiot's DOS", "Clue", "Heath", "(Lovely Rita) Rita", "President Woodrow Wilson", "Pentachlorophenol", "tornado", "Omaha, Nebraska", "The Greatest Gift", "Mayflower", "Vienna", "Zachary John Quinto", "March 16, 2018", "gda\u0144sk", "Bobby Kennedy", "Mercury", "Jello Biafra drew on Nardwuar's face with a marker pen", "Nivetha Thomas", "1975", "four people believed to be illegal immigrants", "CEO of an engineering and construction company", "maintain an \"aesthetic environment\" and ensure public safety,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6067234848484848}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13142", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-13853", "mrqa_searchqa-validation-2171", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-7112", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-9915", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-396", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-3112", "mrqa_searchqa-validation-5510", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-15778", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-2226", "mrqa_searchqa-validation-14425", "mrqa_searchqa-validation-1317", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-13593", "mrqa_searchqa-validation-5879", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-1834", "mrqa_newsqa-validation-1432"], "SR": 0.515625, "CSR": 0.5393145161290323, "retrieved_ids": ["mrqa_squad-train-25450", "mrqa_squad-train-37821", "mrqa_squad-train-19193", "mrqa_squad-train-17790", "mrqa_squad-train-51177", "mrqa_squad-train-62190", "mrqa_squad-train-79859", "mrqa_squad-train-30570", "mrqa_squad-train-42742", "mrqa_squad-train-42069", "mrqa_squad-train-74343", "mrqa_squad-train-78005", "mrqa_squad-train-80423", "mrqa_squad-train-8215", "mrqa_squad-train-58471", "mrqa_squad-train-45688", "mrqa_hotpotqa-validation-2600", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-3423", "mrqa_naturalquestions-validation-9715", "mrqa_squad-validation-543", "mrqa_squad-validation-6680", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-16210", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-294", "mrqa_hotpotqa-validation-5831", "mrqa_searchqa-validation-9488", "mrqa_newsqa-validation-373", "mrqa_naturalquestions-validation-5411", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-11451"], "EFR": 1.0, "Overall": 0.7306754032258065}, {"timecode": 31, "before_eval_results": {"predictions": ["vocational subjects", "Lenin", "the quotient", "Eli", "hail", "the Sierra Nevada", "Florida", "the Hippocratic Oath", "Queen Latifah", "a Golden Retriever", "Shropshire", "the Aegean Sea", "nails", "a bogey", "Sinclair Lewis", "Crocodile", "mutton", "Christmas", "the Chesapeake Bay", "Mao Zedong", "World War I", "John Alden", "a Colombian conscientious objector", "the Trans Alaska Pipeline", "trout", "the 13th", "country", "Carl Bernstein", "a buffalo", "America", "Istanbul", "Blue Horse", "glare", "\"Rehab\"", "the Golden Hind", "Administrative Professionals Week", "the Nile", "Van Halen", "the black bear", "dams", "Djibouti", "pyrite", "Cyclone", "Ted Morgan", "cashmere", "Princess Diana", "spilled milk", "grasshopper", "carat", "Robin Hood", "the chalk cliffs", "J! Archive - Show #4100,", "September 29, 2017", "Franklin and Wake counties", "December 1800", "Nicolas Sarkozy", "Cleveland", "a quarter", "Rabies", "Environmental Protection Agency", "Robert Gibson", "Mogadishu", "45 minutes, five days a week.", "400 years"], "metric_results": {"EM": 0.625, "QA-F1": 0.7211834733893557}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11971", "mrqa_searchqa-validation-15383", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-9398", "mrqa_searchqa-validation-15099", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-15945", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-9137", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-8756", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-14198", "mrqa_naturalquestions-validation-4359", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-2760", "mrqa_hotpotqa-validation-1298", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-4100"], "SR": 0.625, "CSR": 0.5419921875, "EFR": 1.0, "Overall": 0.7312109375}, {"timecode": 32, "before_eval_results": {"predictions": ["30", "the neuro immune system", "prone", "Madrid", "the Declaration of Independence", "Jackie Moon", "tornado", "The Taj Mahal", "the plantain", "fried", "John", "Liverpool", "The Andy Griffith Show", "Nassau", "the Mediterranean", "Fahrenheit", "Janet Reno", "the Spanish American War", "Seinfeld", "steroids", "Atlantic City", "\"Who is John Galt?\"", "republicans", "Iraq", "the taro", "Sans Souci", "Frozone", "Pyotr Ilyich Tchaikovsky", "Malle Babbe", "the Stone Age", "\"Some Things Bear Fruit\"", "Billy Pilgrim", "Louis XIV", "Cain's offering was not accepted by God,", "The Prince of Wales", "the Sacred Heart", "whiskers", "a lighter", "Elmer", "the Cretaceous", "Peggy Fleming", "Panama", "the electron", "Sweden", "Castle Rock", "fuchsia", "the Mediterranean", "republicans", "The Fabulous Baker Boys", "\" Buzz\" Windrip", "Daphne du Maurier", "\"Airplane\"", "King Willem - Alexander", "the New England Patriots", "comprehend and formulate language", "Damon Albarn", "Mazovia", "Ken Burns", "the Pennacook", "Flashback", "Manchester United", "the Yemeni port city of Aden", "into the Atlantic Ocean.", "four decades"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5143871753246754}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6489", "mrqa_searchqa-validation-4246", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8752", "mrqa_searchqa-validation-15871", "mrqa_searchqa-validation-1946", "mrqa_searchqa-validation-6763", "mrqa_searchqa-validation-10799", "mrqa_searchqa-validation-2403", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-8360", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-16917", "mrqa_searchqa-validation-2029", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-7229", "mrqa_searchqa-validation-9024", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-15491", "mrqa_searchqa-validation-8080", "mrqa_searchqa-validation-11372", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-13787", "mrqa_searchqa-validation-16407", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-4806", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-305", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-2782"], "SR": 0.4375, "CSR": 0.5388257575757576, "EFR": 1.0, "Overall": 0.7305776515151516}, {"timecode": 33, "before_eval_results": {"predictions": ["intuition", "spiritual teachers", "echinos", "poker", "Salmon", "the South African Airways", "the Bronze Age", "Sulphur Island", "Thomas Merton", "ex-wife", "the phantom", "Rodeo Drive", "The Pink Panther", "47.3 years", "donut", "basalt", "deor", "German", "volcanoes", "Audrey Hepburn", "Chicago", "dolomite", "Alaska", "birds", "Columbia University", "Punky Night", "Sexuality", "Greece", "the Inca Empire", "contagious", "Vin Diesel", "the mob", "New Mexico", "the French Revolution", "a Purple Heart", "Arkansas", "the 7090 mainframe", "Colette", "the tsuba", "Return To Sender", "Jean Lafitte", "the Komodo dragon", "Italian", "Churchill", "Knit", "Atonement", "money", "Damascus", "kung", "Innsbruck", "Noah", "SeaWorld", "donor hair numbers from the back of the head are insufficient", "Article Two", "Newcastle United", "Genghis Khan.", "Roy Rogers", "African violet", "the Great Northern Railway", "25 October 1921", "East Germany", "The Orchid Thief", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "diprivan,"], "metric_results": {"EM": 0.5, "QA-F1": 0.5598958333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9258", "mrqa_searchqa-validation-12775", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-7499", "mrqa_searchqa-validation-6305", "mrqa_searchqa-validation-11208", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-2912", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-1863", "mrqa_searchqa-validation-9506", "mrqa_searchqa-validation-11961", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-13178", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-9415", "mrqa_searchqa-validation-10622", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3194", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-10032", "mrqa_triviaqa-validation-7627", "mrqa_hotpotqa-validation-5707", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3614"], "SR": 0.5, "CSR": 0.5376838235294117, "retrieved_ids": ["mrqa_squad-train-72510", "mrqa_squad-train-83774", "mrqa_squad-train-47549", "mrqa_squad-train-41515", "mrqa_squad-train-51985", "mrqa_squad-train-80851", "mrqa_squad-train-35065", "mrqa_squad-train-29099", "mrqa_squad-train-37255", "mrqa_squad-train-48598", "mrqa_squad-train-29945", "mrqa_squad-train-67919", "mrqa_squad-train-24306", "mrqa_squad-train-69055", "mrqa_squad-train-18381", "mrqa_squad-train-86537", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4200", "mrqa_newsqa-validation-1008", "mrqa_searchqa-validation-14601", "mrqa_triviaqa-validation-2296", "mrqa_naturalquestions-validation-3348", "mrqa_searchqa-validation-2214", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-3557", "mrqa_hotpotqa-validation-1298", "mrqa_triviaqa-validation-412", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-1425"], "EFR": 0.9375, "Overall": 0.7178492647058824}, {"timecode": 34, "before_eval_results": {"predictions": ["three-dimensional", "cortisol and catecholamines", "Moon River", "Mighty Joe Young", "Robert the Devil", "the West India Company", "Hans Christian Andersen", "luffa sponge", "Hershey's", "a snail", "a crossword", "Muhammad Ali", "Dove", "the Supreme Court", "the north magnetic pole", "Putin", "thunderstorms", "Kennebunkport", "a satellite", "Black Death", "gregorodon", "Earhart", "Hoover Dam", "Panty Raid", "French", "cricket", "kami and Buddhas", "the \"NYPD Blue\"", "Tonto", "the chinchilla family", "white", "Flying to Africa", "a keypunch", "the Amazons", "The Fugitive", "Indonesia", "blak-smith", "Harpers Ferry", "computer vision", "lilac", "a letter of the Roman alphabet", "Tampa", "zinc", "Shakespeare's Men", "Leo", "first anniversary", "nautilus", "salaam", "Bigfoot", "Juris Doctorate", "buy back the option", "The Thing", "Sebastian Lund", "Maravich", "Kusha", "Mars", "Captain America", "Black Tuesday", "South America", "1998", "Picric acid", "Nineteen", "housing, business and infrastructure repairs", "Siri"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6383928571428572}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13921", "mrqa_searchqa-validation-9204", "mrqa_searchqa-validation-700", "mrqa_searchqa-validation-14868", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14554", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-8094", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-5783", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-12254", "mrqa_searchqa-validation-1088", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-1930"], "SR": 0.546875, "CSR": 0.5379464285714286, "EFR": 0.9655172413793104, "Overall": 0.7235052339901478}, {"timecode": 35, "before_eval_results": {"predictions": ["Nairobi, Mombasa and Kisumu", "one", "How I Met Your Mother", "the two-state solution", "violet", "protective shoes", "forgery and flying without a valid license,", "Kurdistan Workers' Party", "the underprivileged.", "end of a biology department faculty meeting", "Malawi", "\"fusion teams,\"", "Her husband and attorney, James Whitehouse,", "shut down buses, subways and trolleys that carry almost a million people daily.", "Muslim", "Muslim festival", "the IAAF", "kishan Kumar,", "Christian", "death of cardiac arrest", "\"Drugs not only poison people, but they poison economies and governments,", "rural Tennessee.", "The BBC", "Plymouth Rock", "$55.7 million", "seven", "Karen Floyd", "Expedia", "Kenneth Cole", "\"wider relationship\"", "Harvard Law School.", "Zilla Torg.", "July", "down a steep embankment in the Angeles National Forest", "cards", "Amy Bishop Anderson,", "\"The Little Couple,\"", "her family", "job training", "State Department", "two years", "stopping militant rocket fire", "Diego Maradona", "21-year-old", "bartering", "Rawalpindi", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola", "Leo Frank", "Fort Lauderdale", "Zipassana", "Russian bombers", "President George Bush", "independently in different parts of the globe", "c. 497 / 6 -- winter 406 / 5 BC", "a garden in the Government House at New Delhi", "Vito Corleone", "Caribbean", "Valletta", "The Eisenhower Executive Office Building", "Premier League club Tottenham Hotspur and the England national team", "February 22, 1968", "Lupercale", "gas", "\"12 Years a Slave\""], "metric_results": {"EM": 0.4375, "QA-F1": 0.5347000071184853}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.36363636363636365, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.8717948717948718, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.14285714285714288, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-707", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-2104", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-2270", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-939", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-2677", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-800", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-3265", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-5633"], "SR": 0.4375, "CSR": 0.53515625, "EFR": 1.0, "Overall": 0.72984375}, {"timecode": 36, "before_eval_results": {"predictions": ["the General Conference", "resources that could sustain future exploration of the moon and beyond.", "\"Nothing But Love\"", "Itawamba County School District", "Vernon Forrest,", "without bail", "a paragraph about the king and crown prince", "death of cardiac arrest", "$1.5 million.", "\"Top Gun\"", "to step up.", "glass shards", "one", "Jaipur", "President-elect Barack Obama", "after a plane crash on April 6, 1994", "the Democratic VP candidate", "March 3.", "34", "Los Angeles, California.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "President-elect Barack Obama", "Immigration Minister Eric Besson", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "suicides", "Facebook and Google,", "Asashoryu", "Henrik Stenson", "Seoul", "seeking help", "Kevin Evans", "Some truly mind-blowing structures", "the FARC", "Dan Brown", "The pilot,", "Paul McCartney and Ringo Starr", "Booches Billiard Hall,", "air support.", "\"She was focused so much on learning that she didn't notice,\"", "in a Starbucks", "finance", "Monday.", "he was diagnosed with skin cancer.", "last month,", "around there,", "more than 5,600", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "21 percent", "Yoko Ono Lennon, Olivia Harrison and Ringo Starr", "$20 million to $30 million,", "five masked men dressed in black appear on the video,", "the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them", "about six to seven million", "10 years", "Jeffrey Archer", "a palla", "Jason Robards", "Flatbush Zombies", "Crane Wilbur", "Venice", "a bagpipe", "reconnaissance", "Kevin Durant", "Fix You"], "metric_results": {"EM": 0.453125, "QA-F1": 0.609640960550887}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.8235294117647058, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 0.25, 1.0, 0.0, 1.0, 0.0, 0.888888888888889, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.8, 0.0, 0.0, 0.5, 0.1, 0.0, 0.0, 0.5714285714285715, 0.0, 0.6799999999999999, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3903", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2139", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-3655", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2792", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-960", "mrqa_triviaqa-validation-3908", "mrqa_triviaqa-validation-111", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-2975", "mrqa_searchqa-validation-1127"], "SR": 0.453125, "CSR": 0.5329391891891893, "retrieved_ids": ["mrqa_squad-train-44077", "mrqa_squad-train-27542", "mrqa_squad-train-10185", "mrqa_squad-train-68292", "mrqa_squad-train-72206", "mrqa_squad-train-64657", "mrqa_squad-train-84799", "mrqa_squad-train-80918", "mrqa_squad-train-82757", "mrqa_squad-train-2480", "mrqa_squad-train-23527", "mrqa_squad-train-5179", "mrqa_squad-train-34452", "mrqa_squad-train-15055", "mrqa_squad-train-73705", "mrqa_squad-train-32250", "mrqa_newsqa-validation-2638", "mrqa_searchqa-validation-2871", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-267", "mrqa_triviaqa-validation-4019", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3764", "mrqa_hotpotqa-validation-2731", "mrqa_hotpotqa-validation-5444", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3880", "mrqa_searchqa-validation-4945", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5034"], "EFR": 1.0, "Overall": 0.729400337837838}, {"timecode": 37, "before_eval_results": {"predictions": ["inside hospitals and clinics", "Ricardo Valles de la Rosa,", "three", "Sunni Arab and Shiite tribal leaders", "the iconic Hollywood headquarters of Capitol Records,", "Kgalema Motlanthe,", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "1994", "Belfast, Northern Ireland", "Herman Cain", "U.S. filmmakers", "Clarkson", "CEO of an engineering and construction company", "London's Heathrow airport", "40 lashings", "breathe through her nose, smell, eat solid foods and drink out of a cup,", "9 million", "the soldiers", "NATO fighters", "low-calorie", "1,500", "Grayback forest-firefighters", "authorizing killings and kidnappings by paramilitary death squads.", "10 a.m.", "Bergdahl, 23, was captured June 30 from Paktika province in southeastern Afghanistan,", "some of the best stunt ever pulled off", "Brian Smith.", "U.S. District Judge Ricardo Urbina", "Swansea Crown Court,", "Virgin America", "The Kirchners", "3,000 kilometers", "strangled his wife in his sleep while dreaming that she was an intruder walked free from court Friday after the case against him was withdrawn,", "nuclear", "Iran's parliament speaker", "No 4,", "\"services to film, theater and the arts and to activism for equal rights for the gay and lesbian community.\"", "chosen their rides based on what their cars say", "10", "artificial intelligence.", "There's no chance", "10 percent", "April 13,", "Juri Kibuishi,", "London", "Obama", "16", "Ralph Lauren", "$10 billion", "2,800", "three", "David Ben - Gurion", "Kiss", "maintenance fees", "Ben Affleck", "Noises Off", "aeoline", "Nazi concentration camps", "Delilah Rene", "Tampa Bay Storm", "Pope John Paul II", "art deco", "Invisible Man", "Pembrokeshire Coast National Park"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6473517198381329}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.8, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.36363636363636365, 1.0, 0.2608695652173913, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.08695652173913043, 0.19999999999999998, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6319", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2294", "mrqa_newsqa-validation-2196", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2779", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2935", "mrqa_naturalquestions-validation-633", "mrqa_triviaqa-validation-7160", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-3381"], "SR": 0.515625, "CSR": 0.532483552631579, "EFR": 0.967741935483871, "Overall": 0.72285759762309}, {"timecode": 38, "before_eval_results": {"predictions": ["events and festivals", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "the downing of two Blackhawk helicopters", "U.S. Holocaust Memorial Museum", "Ennis, County Clare", "At least 33 people", "2007", "heavy turbulence", "Sophia Stellatos.", "Nashville", "Brett Cummins,", "Rod Blagojevich,", "Diego Maradona", "40", "Miguel Cotto", "\"Draquila -- Italy Trembles.\"", "he acted in self defense in punching businessman Marcus McGhee.", "Libreville, Gabon.", "September 23,", "1980", "Haiti,", "Hanin Zoabi", "Archbishop Desmond Tutu", "84-year-old", "John Kiriakou.", "President Bill Clinton", "humans", "the island's dining scene", "chairman of the House Budget Committee", "broadband television network.", "President Robert Mugabe's", "he rejected the option of committing more forces for an undefined mission of nation-building without any deadlines.", "more than 30", "Brown and her family", "133", "it would", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "Italian Serie A title", "drug words to rituals", "fled Zimbabwe and found his qualifications mean little as a refugee.", "mental health and recovery.", "pesos", "consumer confidence", "a one-shot victory in the Bob Hope Classic", "Russian flights were carried out in strict accordance with international rules governing airspace above neutral waters, and that the aircraft did not violate the borders of other states.", "Pervez Musharraf", "two", "first grand Slam,", "MS Columbus", "a murderousic killer who preys on a group of young people at the fictitious Camp Crystal Lake.", "The local Republican Party", "2006", "1834", "endocytosis", "piano", "Scafell Pike", "caffeine", "the University of Keele University", "9,984", "Smithfield, Rhode Island,", "a vacuum", "Donna Rice Hughes", "a albatross", "actress"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6307069983653042}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.10526315789473685, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16, 0.5, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.10256410256410257, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0625, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-1882", "mrqa_newsqa-validation-821", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3203", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-10355", "mrqa_triviaqa-validation-3468", "mrqa_hotpotqa-validation-3644", "mrqa_searchqa-validation-7185", "mrqa_hotpotqa-validation-3314"], "SR": 0.515625, "CSR": 0.532051282051282, "EFR": 0.9032258064516129, "Overall": 0.709867917700579}, {"timecode": 39, "before_eval_results": {"predictions": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "$10 billion", "\"People have lost their homes, their jobs, their hope,\"", "her husband", "Peshawar", "to renew registration until the manufacturer's fix has been made.", "30,000", "last week", "ties", "Addis Ababa,", "then-Sen. Obama", "Uighurs,", "Leo Frank,", "Vivek Wadhwa,", "fears the problem is much larger than just the TVA.", "Harlem,", "the fact that the teens were charged as adults.", "\"a crusade\" and \"Islamofascism\"", "a dress from an American designer.", "Saturday,", "alleviation of their pain", "Robert", "suicides", "Songs penned by Harrison included \"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "serious consequences for Haiti,", "fighting charges of Nazi war crimes for well over two decades. He was extradited from the United States to Israel,", "talk show queen Oprah Winfrey.", "They're big, strong, and fierce", "1,000 pounds", "two satellites", "the most gigantic pumpkins in the world,", "onto the college campus.", "Sunni Arab and Shiite tribal leaders", "three", "$50", "on more than a week at the country's third-largest oil refinery,", "1,300 meters in the Mediterranean Sea.", "phone calls or by text messaging,", "Pakistan", "he arrives he will be captured,", "the family, which remains united and strong despite the \"tremendous hardship,\" will release more information soon. He did not elaborate.", "fluoroquinolone", "to follow up with ICE to ensure that detainees are not drugged unless there is a medical reason to do so.", "Empire of the Sun", "digging", "1000 square meters", "President Obama", "North Korea,", "Kingman Regional Medical Center,", "Henrik Stenson", "Rev. Alberto Cutie", "2001", "786 -- 802", "31 March 2018", "Muhammad Ali", "tallest building in the world", "81st", "football", "the Secret Intelligence Service", "75 mi southeast", "chef salads", "grasshopper", "the Knesset", "Secretary of the Interior"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5796975521118666}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 1.0, 0.4799999999999999, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.13333333333333333, 1.0, 0.3333333333333333, 0.7142857142857143, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1702", "mrqa_naturalquestions-validation-9953", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-667", "mrqa_searchqa-validation-11207", "mrqa_searchqa-validation-6954"], "SR": 0.46875, "CSR": 0.53046875, "retrieved_ids": ["mrqa_squad-train-82918", "mrqa_squad-train-70396", "mrqa_squad-train-73227", "mrqa_squad-train-51576", "mrqa_squad-train-36953", "mrqa_squad-train-32601", "mrqa_squad-train-5811", "mrqa_squad-train-80915", "mrqa_squad-train-3866", "mrqa_squad-train-43408", "mrqa_squad-train-70473", "mrqa_squad-train-18003", "mrqa_squad-train-72068", "mrqa_squad-train-17716", "mrqa_squad-train-5638", "mrqa_squad-train-31592", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7852", "mrqa_triviaqa-validation-3815", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3131", "mrqa_searchqa-validation-11208", "mrqa_naturalquestions-validation-5739", "mrqa_squad-validation-3165", "mrqa_searchqa-validation-1914", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-293", "mrqa_searchqa-validation-9398", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-7058", "mrqa_searchqa-validation-10116", "mrqa_triviaqa-validation-4639"], "EFR": 1.0, "Overall": 0.72890625}, {"timecode": 40, "UKR": 0.759765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4056", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10688", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6561", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7886", "mrqa_naturalquestions-validation-8164", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1377", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3114", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3247", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-3313", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3764", "mrqa_newsqa-validation-3795", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-704", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-754", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-92", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10090", "mrqa_searchqa-validation-10116", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11451", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11710", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13616", "mrqa_searchqa-validation-13745", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-14783", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-1843", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2386", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3127", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3644", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5522", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7396", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8770", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10011", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1092", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1512", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1771", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1936", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2059", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2416", "mrqa_squad-validation-2476", "mrqa_squad-validation-2613", "mrqa_squad-validation-2640", "mrqa_squad-validation-2788", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3068", "mrqa_squad-validation-3283", "mrqa_squad-validation-3317", "mrqa_squad-validation-3407", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4398", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5765", "mrqa_squad-validation-5778", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-677", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7202", "mrqa_squad-validation-7243", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7729", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7772", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8196", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-850", "mrqa_squad-validation-851", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8864", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-957", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9954", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-1706", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-395", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-5863", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7281", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.828125, "KG": 0.48671875, "before_eval_results": {"predictions": ["1985", "doctors", "more than 20 times during the 1992 campaign.", "Austin Wuennenberg,", "in a canyon in the path of the blaze", "machine guns and two silencers", "Matthew Fisher", "CNN's", "NATO", "Lieberman", "meter reader", "the Gulf", "petionville, Haiti,", "Iranian consulate,", "Basel", "Pyongyang and Seoul", "\"I don't know how I ever got through this whole program. I'm sure my mom was there with me,\"", "Kurt Cobain's", "using recreational drugs", "1983", "19-12 victory", "Egypt.", "Rima Fakih", "delivers a big speech", "Ripken will be inducted into the Baseball Hall of Fame in July.", "Justicialist Party, or PJ by its Spanish acronym,", "Rancho La Brea,", "The Falklands, known as Las Malvinas in Argentina,", "86", "future relations between the Middle East and Washington.", "cell phones", "six", "2004.", "Egypt", "U.S. security coordinator and chief of the Office of Military Cooperation.", "19-year-old", "two-seater sports cars.", "the Taliban", "\"Perfidia,\" \"Walk -- Don't Run\" and \"Hawaii Five-O\"", "melt", "Communist Party of Nepal (Unified Marxist-Leninist)", "the journalists and the flight crew will be freed,", "Haitians", "Sri Lanka's", "telling CNN his comments had been taken out of context.", "summer", "Rev. Alberto Cutie", "since 1983.", "lack of a cause of death and the absence of any soft tissue", "the content of the speech,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "Afghanistan", "Tsetse can be distinguished from other large flies by two easily observed features", "1957", "Jack Ruby", "The Altamont Speedway Free Festival", "Trainspotting", "Nicol Williamson", "500,000 copies", "Latin American culture", "Jake Farris", "a true story", "Stranger in a Strange Land", "Nippon Professional Baseball"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6084736305589975}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.05714285714285714, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.13793103448275862, 0.9411764705882353, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.2857142857142857, 1.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0625, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1635", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2330", "mrqa_naturalquestions-validation-2901", "mrqa_hotpotqa-validation-622", "mrqa_hotpotqa-validation-4027", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-4535", "mrqa_hotpotqa-validation-5556"], "SR": 0.484375, "CSR": 0.5293445121951219, "EFR": 0.9393939393939394, "Overall": 0.7086695653178123}, {"timecode": 41, "before_eval_results": {"predictions": ["historians", "Adam Lambert", "in a Nazi concentration camp,", "Los Angeles,", "security officer Stephen Johns reportedly opened the door for the man police say was his killer.", "A Brazilian supreme court judge", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "KBR", "the same drama that pulls in the crowds", "across Greece", "a monthly allowance,", "U.S. Navy", "video cameras with them as they negotiate their way in the 111th Congress, both inside and outside Washington.", "Bastian Schweinsteiger", "he believed he was about to be attacked himself.", "the Brundell family", "near the municipal building of Abu Ghraib in western Baghdad", "The Al Nisr Al Saudi", "two years ago.", "Appathurai", "a missing sailor whose five Texas A&M University crew mates", "The FBI's Baltimore field office", "Tuesday", "Honduran", "curfew in Jaipur", "Pakistani city of Lahore.", "Robert", "he exercised in a park in a residential area of Mexico City,", "16", "Pixar's \"Toy Story\"", "Stora Nygatan 25a)", "the Russian air force,", "an Italian and six Africans", "three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs) Sunday in a heist police characterized as \"spectacular.\"", "an auxiliary lock", "German Chancellor Angela Merkel", "2,700-acre sanctuary", "Missouri.", "the Dalai Lama", "Ketamine", "Haleigh Cummings,", "at least two and a half hours.", "Bobby Darin,", "Queen Elizabeth's birthday", "Monday.", "Hakeemullah Mehsud", "kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.", "an obscure story of flowers", "Kuranyi's", "Kris Allen,", "World Wide Village,", "2", "Supplemental oxygen", "Iran", "Harley", "Tom Mix", "George Washington", "lion", "German", "Forbes", "black magic or of dealings with the devil", "prostate cancer", "Orlando", "The Italian Agostino Bassi"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5283675250900861}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 0.0, 0.923076923076923, 0.4, 0.4878048780487806, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-1334", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-706", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-5973", "mrqa_hotpotqa-validation-3343", "mrqa_searchqa-validation-15278", "mrqa_searchqa-validation-59", "mrqa_searchqa-validation-13584", "mrqa_naturalquestions-validation-8733"], "SR": 0.421875, "CSR": 0.5267857142857143, "EFR": 0.972972972972973, "Overall": 0.7148736124517374}, {"timecode": 42, "before_eval_results": {"predictions": ["non-Mongol physicians", "product-market fit", "Freddie Highmore", "Elvis Presley", "divergent tectonic", "Stefanie Scott", "Tanvi Shah", "Kida", "1991", "Sam Waterston", "Bobby Beathard", "Palmer Williams Jr. as Floyd", "Chicago metropolitan area", "Coldplay", "$5.4 trillion", "3,000 metres ( 9,800 ft )", "Ann Gillespie", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "the opisthodomus", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "Albert Einstein", "1994", "Fats Waller", "Accounting Standards Board ( ASB )", "2012", "Bette Midler", "push the food down the esophagus", "Walter Mondale", "Nick Sager", "Sweden had been an active supporter of the League of Nations and most of Sweden's political energy in the international arena had been directed towards the preservation of", "the 18th century", "Graham McTavish", "1962", "Julie Adams", "Odoacer", "J. S Seton - Karr", "one", "Neal Dahlen", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Brobee", "January 15, 2007", "John Garfield as Al Schmid", "absorption of water from the soil by the root", "10 logarithm of the molar concentration, measured in units of moles per liter, of hydrogen ions", "geophysicists", "Billy Colman", "360", "the 75th Golden Globe Awards", "Alice Cooper", "Bart Millard", "Sven Goran Eriksson", "the Marshall Plan", "Botany Bay", "1932", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Valerie Stowe", "\"The Screening Room\"", "model", "\"I remember growing up in the Middle East, influenced, enjoying his music, waiting for his albums,\"", "a surrogate", "salt", "Jersey Joe", "consumer confidence"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6206385583230157}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.5, 0.4, 1.0, 0.9523809523809523, 0.8, 0.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.06896551724137931, 0.16, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.18181818181818182, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2414", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-4225", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4294", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1351"], "SR": 0.53125, "CSR": 0.5268895348837209, "retrieved_ids": ["mrqa_squad-train-21256", "mrqa_squad-train-22352", "mrqa_squad-train-45779", "mrqa_squad-train-69625", "mrqa_squad-train-35348", "mrqa_squad-train-41884", "mrqa_squad-train-37805", "mrqa_squad-train-18636", "mrqa_squad-train-16510", "mrqa_squad-train-9097", "mrqa_squad-train-60153", "mrqa_squad-train-510", "mrqa_squad-train-30382", "mrqa_squad-train-29412", "mrqa_squad-train-70952", "mrqa_squad-train-959", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-383", "mrqa_triviaqa-validation-7635", "mrqa_triviaqa-validation-2431", "mrqa_triviaqa-validation-4742", "mrqa_searchqa-validation-9109", "mrqa_squad-validation-9178", "mrqa_searchqa-validation-10060", "mrqa_newsqa-validation-2544", "mrqa_naturalquestions-validation-10015", "mrqa_naturalquestions-validation-5640", "mrqa_searchqa-validation-13556", "mrqa_hotpotqa-validation-4354", "mrqa_newsqa-validation-1508", "mrqa_naturalquestions-validation-4359", "mrqa_searchqa-validation-1843"], "EFR": 0.9666666666666667, "Overall": 0.7136331153100776}, {"timecode": 43, "before_eval_results": {"predictions": ["confrontational", "A witness", "34", "Miami Beach, Florida,", "eight surgeons", "Somalia's piracy problem was fueled by environmental and political events.", "Cash for Clunkers", "Justine Henin", "Haiti,", "California-based Current TV", "It is I, the chief executive officer, the one on the very top,", "Kevin Kuranyi", "Tim Clark, Matt Kuchar and Bubba Watson", "Columbia", "Omar Bongo,", "love the outdoors, particularly if they have a garden to eat from,", "mother.", "Madrid's Barajas International Airport", "1940's", "tax", "pizza", "people have chosen their rides based on what their cars say", "up three", "Chinese", "Passers-by", "\"He hears what I'm saying, but there's just no coming through,\"", "not guilty", "Larry Ellison,", "Mexican military", "Sporting Lisbon", "The Kirchners", "she expects to face public scrutiny following her accusations.", "July 1999,", "CNN's", "\"weighing all options necessary to protect his client.\"", "London's O2 arena,", "90", "Col. Elspeth Cameron-Ritchie,", "(Zhanar) Tokhtabayeba,", "his parents", "nearly 28 years", "(3 degrees Fahrenheit),", "Claude Monet", "Bodyguard Trevor Rees", "Consumer Reports", "capital flows", "nine-wicket", "Iowa,", "Plymouth Rock", "a member of the band for more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Michael Schumacher", "the freedom of the press, the right to peaceably assemble, or to petition for a governmental redress of grievances", "Medicare", "Julia Roberts", "line-coded", "Harry Bailley", "The Muffin Man", "Clovis I", "Roots: The Saga of an American Family", "Almeda Mall", "a greek cheese", "FRAM", "Ross Ice Shelf", "Bonita Melody Lysette"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5881740196078431}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.058823529411764705, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.09999999999999999, 1.0, 0.6, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2401", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2392", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-3990", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6285", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-1439", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-5199", "mrqa_searchqa-validation-1615", "mrqa_triviaqa-validation-7164"], "SR": 0.53125, "CSR": 0.5269886363636364, "EFR": 0.9666666666666667, "Overall": 0.7136529356060606}, {"timecode": 44, "before_eval_results": {"predictions": ["Grey Street", "Stratfor,", "269,000", "August 4, 2000", "Sunday", "Why he's more American than a German,", "Wilhelmina Kids,", "Rawalpindi", "poor.\"", "40", "700", "Mandi Hamlin", "breast cancer.", "Alfredo Astiz,", "$5.5 billion", "Her husband and attorney, James Whitehouse,", "3.5", "Thailand", "rural Tennessee.", "39,", "question people if there's reason to suspect they're in the United States illegally.", "Derek Mears", "Sunday,", "Stuttgart", "27", "45 minutes,", "14 years", "Chesley \"Sully\" Sullenberger", "unclear what, if any, action might be taken against the mother.", "South Africa,", "The Obama campaign has kept the details on both the timing and selection of the running mate under wraps.", "John and Elizabeth Calvert", "The Bronx County District Attorneys Office", "her mom,", "a federal judge in Mississippi", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "he fears a desperate country with a potential power vacuum that could lash out.", "123 pounds of cocaine and 4.5 pounds of heroin,", "3-0", "70,000 or so", "citizenship", "Manuel Mejia Munera", "2,700-acre", "his comments", "two weeks after Black History Month", "smiley", "Wanda Eileen Barzee", "pro-democracy activists", "Kim Jong Un", "3,000 kilometers (1,900 miles),", "The agency wants to ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "late - September through early January", "Europe", "Asia", "piscina", "The Bible", "Douglas MacArthur", "PlayStation 4", "ITV", "bullfighting", "dragon", "Galileo Galilei", "Carson McCullers", "fearful"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6473827798663325}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 0.42857142857142855, 1.0, 0.22222222222222224, 0.0, 0.19999999999999998, 0.5, 0.0, 1.0, 1.0, 0.15789473684210525, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1554", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2773", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1233", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-1065", "mrqa_naturalquestions-validation-1187", "mrqa_triviaqa-validation-7376", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-3192", "mrqa_searchqa-validation-10445", "mrqa_searchqa-validation-10531", "mrqa_triviaqa-validation-3284"], "SR": 0.578125, "CSR": 0.528125, "EFR": 1.0, "Overall": 0.720546875}, {"timecode": 45, "before_eval_results": {"predictions": ["sports tourism", "0-0", "Aung San Suu Kyi", "led the weekend box office, grossing $55.7 million during its first weekend.", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Al-Shabaab,", "treadmill", "North Korea", "Piers Morgan", "Mary Phagan,", "well over two decades.", "100,000", "drowned in the Pacific Ocean", "more than a million residents who have been displaced by fighting in Somalia,", "9-1", "drama of the action in-and-around the golf course", "poems telling of the pain and suffering of children just like her.", "\"mentally deranged person steeped in the inveterate enmity towards the system\" in the North.", "15-year-old's", "100% of its byproducts which supplies 80% of the operation energy", "it really like to be a new member of the world's most powerful legislature?", "participate in Iraq's government.", "The Rosie Show,\"", "helicopters and unmanned aerial vehicles", "racial intolerance.", "\"Reusable Lessons - Step onto the campus of a school that's a model of sustainability.", "Rolling Stone.", "walk on ice in Alaska.", "Ralph Lauren", "\"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "82", "\"It would not make sense for Pyongyang to make such a move after going through official channels with its plans,", "\"a striking blow to due process and the rule of law.\"", "\"So that you come for another. We will be waiting for you here.\"", "$250,000 for Rivers' charity: God's Love We Deliver.", "Elizabeth Birnbaum", "three", "once on New Year's Day and once in June,", "Lindsey Vonn", "Sunday", "Rwanda", "cancer", "Jose Manuel Zelaya", "10:30 p.m. October 3,", "Monterrey,", "200", "\"The boat, the designers said, could make life just like at home on a personal estate for its owner.", "\"They were all cordial and cooperative, and were cited with speed racing, a class A traffic violation that can command a fine of $627,", "Brian Mabry", "\"I think I killed somebody.\"", "Sunday", "December 2, 2013, and the third season concluded on October 1, 2017", "1,236 km ( 768 mi ) south of Cape Sable Island, Nova Scotia ; and 1,578 km ( 981 mi ) north of Puerto Rico", "Christopher Lloyd", "Nero", "Ethiopia", "Chile and Argentina", "River Shiel", "7 miles", "Burnley", "O. Henry", "Douglas Fairbanks, Jr.", "P.M.S. Blackett", "a system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6679019764957265}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 0.42857142857142855, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.33333333333333337, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2116", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-2418", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-3879", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-3476", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-3407", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-4771", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-3547", "mrqa_searchqa-validation-9553", "mrqa_naturalquestions-validation-952"], "SR": 0.546875, "CSR": 0.5285326086956521, "retrieved_ids": ["mrqa_squad-train-26408", "mrqa_squad-train-72056", "mrqa_squad-train-7837", "mrqa_squad-train-33224", "mrqa_squad-train-11089", "mrqa_squad-train-84003", "mrqa_squad-train-27439", "mrqa_squad-train-74376", "mrqa_squad-train-71645", "mrqa_squad-train-28764", "mrqa_squad-train-51865", "mrqa_squad-train-67585", "mrqa_squad-train-59723", "mrqa_squad-train-64852", "mrqa_squad-train-68217", "mrqa_squad-train-59322", "mrqa_triviaqa-validation-6854", "mrqa_squad-validation-7719", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-39", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2261", "mrqa_naturalquestions-validation-3285", "mrqa_triviaqa-validation-6632", "mrqa_hotpotqa-validation-2237", "mrqa_searchqa-validation-6122", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2221", "mrqa_hotpotqa-validation-2731", "mrqa_searchqa-validation-12357", "mrqa_squad-validation-10141"], "EFR": 0.9655172413793104, "Overall": 0.7137318450149925}, {"timecode": 46, "before_eval_results": {"predictions": ["a political role for Islam", "a Serie A game at Roma", "al Fayed's", "opium", "maintain an \"aesthetic environment\" and ensure public safety,", "Tuesday", "science fiction", "the Beatles", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "23", "near Ciudad Juarez,", "former U.S. secretary of state.", "Sri Lanka,", "Communist", "Gainsbourg", "DBG,", "Ike", "The ACLU", "41,", "Tuesday", "withdrawing most U.S. forces by the end of his current term,", "The local Republican Party", "Taliban", "debris", "8,", "new materials", "a president who understands the world today, the future we seek and the change we need.", "Djibouti,", "in the mouth.", "a full garden and pool, a tennis court, or several heli-pads.", "Alfredo Astiz,", "WILL MISS YOU! WE LOVE YOU MICHAEL!!!\"", "14 years", "1979", "100", "100% of its byproducts", "prostate cancer,", "EU naval force", "the highest ranking former member of Saddam Hussein's regime still at large,", "Michelle Obama", "a fight outside of an Atlanta strip club", "\"People have lost their homes, their jobs, their hope,\"", "Afghanistan", "bodies and heads", "Seoul.", "\"The most affecting thing about this whole wheelchair for children is when the parents realize the gift that is being given to their children and they reach out to hug you.\"", "Muqtada al-Sadr", "a house party", "Ozzy Osbourne", "100 vessels", "$81,88010", "Hungary", "more than 80 tank\u014dbon volumes", "Ben Findon, Mike Myers and Bob Puzey", "Boxing Day", "Hemingway", "123", "Ellie Kemper", "President's Volunteer Service Award", "nursery rhyme", "the equatorial plane", "St. Mary's", "Holly", "Lundy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6792275508308117}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9565217391304348, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.7499999999999999, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9519", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-3826", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-154", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-4199", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-7206", "mrqa_triviaqa-validation-5184", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-5346", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-1315", "mrqa_searchqa-validation-12477"], "SR": 0.53125, "CSR": 0.5285904255319149, "EFR": 1.0, "Overall": 0.720639960106383}, {"timecode": 47, "before_eval_results": {"predictions": ["\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "\"The Cycle of Life,\"", "\"a striking blow to due process and the rule of law.", "make the new truck safer, but also could make it more expensive to repair after a collision.", "200", "Alexey Pajitnov,", "1959.", "lightning strike", "off", "at least 18 federal agents and two soldiers", "$17,000", "\"The oceans are kind of the last frontier for use and development,\"", "Animal Planet", "Caster Semenya", "a mammoth", "$3 billion,", "Les Bleus", "Samoa", "more than 100.", "The controversial technique that simulates drowning -- and which President Obama calls torture", "Roy", "hardship for terminally ill patients and their caregivers,", "100 percent", "near Garacad, Somalia,", "a gift to the Obama girls from Sen. Ted Kennedy.", "Long Island", "arrested, arraigned and jailed,", "Damon Bankston", "Fayetteville, North Carolina,", "hand-painted", "\"bleaching\"", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "Bob Bogle,", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "Deputy Treasury Secretary", "an Italian and six Africans", "the captain of a nearby ship", "warning", "London and Buenos Aires", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "Ripken's latest project is a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the Difference,\"", "\"We essentially closed the wheelhouse doors. I went to the port side, and I looked out up at the derrick. That's when I see the mud coming out of the top", "food, music, culture and language of Latin America", "former Procol Harum bandmate Gary Brooker", "No 4,", "Tuesday", "she's in love, thinks maybe it's a good thing she thought Rounds was straight.", "Miguel Cotto", "Zac Efron", "The plane", "269,000", "rear - view mirror", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "the most recent Super Bowl champions", "Turkey", "czarevitch", "auk", "Portland, OR", "from 1993 to 1996", "Minette Walters", "in 5 seconds", "(Dwight) Yoakam", "photoelectric", "April 13, 2018"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6484248498005614}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.13333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.8571428571428571, 0.0, 0.0, 0.19047619047619047, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.0, 0.4, 0.8, 0.07692307692307691, 0.962962962962963, 1.0, 1.0, 1.0, 0.0, 0.12121212121212123, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-551", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2053", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-4441", "mrqa_searchqa-validation-12326", "mrqa_naturalquestions-validation-177"], "SR": 0.546875, "CSR": 0.5289713541666667, "EFR": 0.9310344827586207, "Overall": 0.7069230423850575}, {"timecode": 48, "before_eval_results": {"predictions": ["racial intolerance.", "the U.S. intelligence community does not believe North Korea intends to launch a long-range missile in the near future,", "Vonn", "Salt Lake City, Utah,", "Lana Clarkson", "Wake Forest,", "Jero is making old, new again in Japan.", "Los Angeles", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "L'Aquila earthquake,", "a judge to order the pop star's estate to pay him a monthly allowance,", "\"They had the live jackets... they were staying in one house and they had hidden the suicide jackets not far from that (house) in the hills,\"", "peppermint oil, soluble fiber, and antispasmodic drugs", "crashed his private plane into a Florida swamp.", "David Beckham", "Aryan Airlines Flight 1625", "ketamine.", "Kris Allen,", "body was found Saturday morning in a hotel,", "in a 4-1 Serie A win at Bologna on Sunday", "Haiti's", "suppress the memories and to live as normal a life as possible;", "1981,", "Colombia's", "Bill Gates", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Bob Bogle,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "Iran test-launched a rocket capable of carrying a satellite,", "$279", "his brother to surrender.", "helping to plan the September 11, 2001,", "pilot", "in the fashionable Western Harbor neighborhood is another spot to hit for of-the-moment style.", "it really like to be a new member of the world's most powerful legislature?", "in Arabic, Russian and Mandarin that led police to 86 suspects in a series of raids that started Tuesday,", "NATO fighters", "Michelle Obama", "three men with suicide vests who were plotting to carry out the attacks,", "$250,000", "the WBO welterweight title from Miguel Cotto on a 12th round technical knockout in Las Vegas.", "Courtney Love,", "Chinese President Hu Jintao", "Bahrain", "54", "Anil Kapoor", "murder", "African National Congress", "walk", "Carl", "maintain an \"aesthetic environment\" and ensure public safety,", "Oklahoma", "the Behavioral Analysis Unit", "BeBe Winans", "John Major", "her old friend", "Bangladesh", "four", "rhyme", "Edward R. Murrow", "in collocation", "Rabbi Small", "Cheers", "Shep Meyers"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5968975672240334}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.7692307692307693, 0.6666666666666666, 0.75, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7058823529411764, 0.06451612903225806, 0.2, 0.631578947368421, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.8, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0909090909090909, 1.0, 1.0, 0.15384615384615385, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-212", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4107", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-4079", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-7105", "mrqa_searchqa-validation-12017", "mrqa_searchqa-validation-11020"], "SR": 0.46875, "CSR": 0.5277423469387755, "retrieved_ids": ["mrqa_squad-train-70510", "mrqa_squad-train-47821", "mrqa_squad-train-9157", "mrqa_squad-train-33583", "mrqa_squad-train-66481", "mrqa_squad-train-69030", "mrqa_squad-train-78718", "mrqa_squad-train-44085", "mrqa_squad-train-47809", "mrqa_squad-train-55418", "mrqa_squad-train-47939", "mrqa_squad-train-11697", "mrqa_squad-train-29171", "mrqa_squad-train-53720", "mrqa_squad-train-77844", "mrqa_squad-train-7454", "mrqa_hotpotqa-validation-4791", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2220", "mrqa_naturalquestions-validation-10032", "mrqa_newsqa-validation-2298", "mrqa_naturalquestions-validation-8355", "mrqa_searchqa-validation-8976", "mrqa_searchqa-validation-59", "mrqa_newsqa-validation-3015", "mrqa_naturalquestions-validation-5966", "mrqa_squad-validation-3543", "mrqa_hotpotqa-validation-400", "mrqa_newsqa-validation-1185", "mrqa_searchqa-validation-940", "mrqa_squad-validation-1941", "mrqa_naturalquestions-validation-64"], "EFR": 1.0, "Overall": 0.7204703443877551}, {"timecode": 49, "before_eval_results": {"predictions": ["delegation of American Muslim and Christian leaders", "\"an Afghan patriot\"", "35,000.", "curfew", "Muslim revolutionary named Malcolm X", "Four", "its nude beaches.", "The Falklands,", "Pyongyang and Seoul", "in Japan", "Somalia's coast.", "Haiti", "current and historic conflict zones,", "cancerous tumor.", "Brett Cummins,", "\"It was a wrong thing to say,", "his former caddy,", "David McKenzie", "\"If we're going to revise our policies here,", "Daniel Radcliffe", "\"The Da Vinci Code,\"", "exotic sports cars", "the secrets of Freemasonry", "al Qaeda,", "Jared Polis", "the state's first lady,", "\"I think if I had known that she was gay, I wouldn't have been brave enough to talk to her,\"", "Bob Bogle,", "$55.7 million", "Rwanda declared a cease-fire", "$60 million", "4,000 credit cards and the company's \"private client\" list,", "95.", "At least 33", "Carrousel du Louvre,", "137", "bartering", "Austin Wuennenberg,", "wanted to change the music on the CD player", "\"momentous discovery\"", "Bob Bogle,", "Mitt Romney", "a plaque at the home of his great-grandfather", "Wednesday,", "15-year-old", "almost 100 vessels", "Matthew Fisher,", "to the southern city of Naples", "\"brain hacking\"", "Saturday", "Both women", "Andy Serkis", "late 1989 and 1990", "in Davos", "Scania", "Richard Attenborough", "an eclipse", "\"novel with a key\",", "London", "Oklahoma", "Kevin Nealon", "Christianity", "(Kim) Wynette", "Joseph Sherrard Kearns"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7095008290320791}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.2, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4, 0.0, 0.8, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.25, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-283", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2982", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2646", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-4033", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-6297"], "SR": 0.59375, "CSR": 0.5290625, "EFR": 1.0, "Overall": 0.720734375}, {"timecode": 50, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-86", "mrqa_hotpotqa-validation-864", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1653", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-333", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4338", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9559", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1952", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2050", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2428", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2477", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2875", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11450", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11495", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12317", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12357", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13028", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2175", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2394", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5757", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_searchqa-validation-9943", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1213", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1725", "mrqa_squad-validation-1742", "mrqa_squad-validation-1849", "mrqa_squad-validation-1891", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-2938", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5470", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-6548", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-693", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-719", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7951", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8683", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9528", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-997", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2250", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-3699", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4336", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-5659", "mrqa_triviaqa-validation-5771", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.830078125, "KG": 0.5, "before_eval_results": {"predictions": ["Palestinian-Israeli issue", "Fareed Zakaria", "11", "July 1999,", "the actor who created one of British television's most surreal thrillers,", "Haiti.", "May 4", "Turkey,", "11", "Shanghai,", "\"Den of Spies\"", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "Cash for Clunkers", "19-year-old", "This will be the second", "Islamabad,", "March 8", "her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "Michoacan state,", "celebrity-studded gala and a three-day party.", "CEO of an engineering and construction company", "Sunni Arab and Shiite tribal leaders", "the 101st Airborne Division,", "U.S. Holocaust Memorial Museum", "The Human Rights Watch organization", "10 municipal police officers", "strong work ethic", "12", "Arabic, French and English,", "40", "South Africa.", "L'Aquila", "\"Body Works\"", "North Korea,", "at least 27", "satirical award Friday night at an awards dinner in Shanghai,", "the Netherlands,", "burned over 65 percent of his body after being set on fire,", "45 minutes, five days a week.", "the 45-year-old future president", "Madonna", "shoot down the satellite", "posting a $1,725 bail,", "Bill,", "more than 78,000 parents of children ages 3 to 17.", "Apple", "London's", "\"fusion teams,\"", "martial arts,", "Jennifer Arnold and husband Bill Klein,", "Operation Pipeline Express.", "Orwell", "Guwahati", "winter solstice", "Frenchman", "haggis", "daisy", "1853", "Musicology", "1902", "the Alaska territory", "\"Twelfth Night\"", "trenchcoat", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6850057060994561}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9743589743589743, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.7777777777777777, 0.6666666666666666, 1.0, 1.0, 1.0, 0.923076923076923, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-239", "mrqa_triviaqa-validation-7329", "mrqa_triviaqa-validation-1015", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-2863", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-16778"], "SR": 0.5625, "CSR": 0.529718137254902, "EFR": 1.0, "Overall": 0.7274280024509804}, {"timecode": 51, "before_eval_results": {"predictions": ["Sheikh Ali Mahmud Ragi,", "\"disagreements\" with the Port Authority of New York and New Jersey,", "in Auckland,", "my recent 12-day trip to Iran to film a public-television show.", "at least nine", "Kgalema Motlanthe,", "mental health and recovery.", "204,000", "Arizona", "Kenyan and Somali", "Capt. Angelo Nieves,", "Diego Maradona", "London", "in the Willamette Valley to the Pacific coast.", "rural Tennessee.", "Fakih", "as", "14", "Former Mobile County Circuit Judge", "18", "Abdullah Gul,", "April 13,", "Washington Redskins fan and loved to travel,", "Kindle Fire", "Vicente Carrillo Leyva", "Dolgorsuren Dagvadorj,", "said they would not be making any further comments, citing the investigation.", "41,", "his daughter Sonam, sister-in-law Sridevi", "two years,", "cell phones.", "forgery and flying without a valid license,", "Larry Ellison,", "digging", "Wednesday.", "the pirates", "the estate", "Isabella", "March 22,", "Hamas,", "3,000 kilometers (1,900 miles),", "September 21.", "$2,000.", "a U.S. helicopter crashed in northeastern Baghdad as", "served in the military,", "air support.", "\"Draquila", "11th year in a row.", "200", "Seminole", "morphine sulfate oral solution 20 mg/ml.", "16.5 quadrillion BTUs", "Charlton Heston", "administrative supervision over all courts and the personnel thereof", "Thu\u1eadn Thi\u00ean", "Gauleiters", "The Landlord\\'s Game", "in New York City", "Kentucky, Virginia, and Tennessee", "1999", "feijo", "Mountain Dew", "Whopper", "Japan"], "metric_results": {"EM": 0.625, "QA-F1": 0.6504493464052288}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-631", "mrqa_naturalquestions-validation-7457", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-2485", "mrqa_hotpotqa-validation-2623", "mrqa_searchqa-validation-12036"], "SR": 0.625, "CSR": 0.5315504807692308, "retrieved_ids": ["mrqa_squad-train-76479", "mrqa_squad-train-25492", "mrqa_squad-train-18970", "mrqa_squad-train-36173", "mrqa_squad-train-81538", "mrqa_squad-train-37470", "mrqa_squad-train-5256", "mrqa_squad-train-14107", "mrqa_squad-train-8305", "mrqa_squad-train-62565", "mrqa_squad-train-25591", "mrqa_squad-train-32708", "mrqa_squad-train-8662", "mrqa_squad-train-36238", "mrqa_squad-train-44618", "mrqa_squad-train-14832", "mrqa_squad-validation-5525", "mrqa_newsqa-validation-43", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-13326", "mrqa_newsqa-validation-2558", "mrqa_triviaqa-validation-6827", "mrqa_newsqa-validation-1102", "mrqa_naturalquestions-validation-5034", "mrqa_newsqa-validation-463", "mrqa_triviaqa-validation-7627", "mrqa_hotpotqa-validation-2379", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-154", "mrqa_searchqa-validation-11867", "mrqa_searchqa-validation-946", "mrqa_newsqa-validation-3628"], "EFR": 1.0, "Overall": 0.7277944711538462}, {"timecode": 52, "before_eval_results": {"predictions": ["1.2 million", "Ben Roethlisberger", "death", "St. Louis, Missouri.", "Honduran President Jose Manuel Zelaya", "mother.", "$2 billion in disaster assistance for parts of the Midwest that have been hit by record floods.", "No. 4", "\"It's like having one of our own kids in this situation.\"", "U.S. security coordinator", "Ashley \"A.J.\" Jewell,", "\"We are deeply saddened and shocked by this tragic loss.", "Department of Homeland Security Secretary Janet Napolitano", "Too many glass shards left by beer drinkers in the city center,", "any abuse that occurred in his diocese.", "United", "planned attacks in the southern port city of Karachi,", "\"falling space debris,\"", "Michael Schumacher", "Sen. Barack Obama", "Rolling Stone", "Marianela Galli", "\"We don't see at this point any indication of an individual out in the neighborhoods committing additional crimes or homicides,", "Kingman Regional Medical Center,", "bronze medal", "Long Island", "5,600", "the Pew Research Center held favorable views of America,", "Sharon Bialek", "prisoners", "two", "humans", "Muslim", "New York appeals court", "Kevin Evans", "near the Somali coast", "$24,000-30,000", "2008,", "killing rampage.", "\"Twilight\" book series.", "trading goods and services without exchanging money", "not guilty", "Dennis Davern,", "Obama and McCain", "The flooding was so fast that the thing flipped over,\"", "five", "Princess Diana", "Dubai", "June 6, 1944,", "the bill", "$40,000 a year for school,", "she who brings victory", "the sex organs", "Aidan Gallagher", "Rebecca Adlington", "Buckinghamshire", "10", "high-ranking", "2007", "The entity", "The Suite Life of Zack & Cody", "(Nevil) Fonda", "launch one ship", "light skin and blue"], "metric_results": {"EM": 0.46875, "QA-F1": 0.57845193001443}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 0.4444444444444445, 1.0, 0.16666666666666666, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5454545454545454, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-3791", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1539", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-815", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-3052", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-5499", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-2481", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-118", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-9831", "mrqa_naturalquestions-validation-6214"], "SR": 0.46875, "CSR": 0.5303655660377358, "EFR": 1.0, "Overall": 0.7275574882075471}, {"timecode": 53, "before_eval_results": {"predictions": ["a \"happy ending\" to the case.", "pregnant soldier", "any charge of threatening behavior.", "Argentina", "Ferraris, a Lamborghini and an Acura NSX", "Laurean killed Lauterbach", "1983", "the simple puzzle video game,", "\"Dancing With the Stars.\"", "African National Congress", "Greece", "morphine sulfate oral solution 20 mg/ml.", "Lance Cpl. Maria Lauterbach", "US Airways Flight 1549", "he failed to return home,", "Jiverly Wong,", "Ireland", "Gaslight Theater.", "punish participants in this week's bloody mutiny,", "Mildred,", "Sunday's", "help nations trapped by hunger and extreme poverty,", "$10 billion", "prosecutors of buckling under pressure from the ruling party.", "April 22.", "Mitt Romney", "twice.", "seeking help", "Mary Phagan,", "pesos", "judge", "Herman Cain,", "60 euros", "$60 billion on America's infrastructure.", "Revolutionary Armed Forces of Colombia,", "Kurt Cobain's", "the BBC", "Islamabad", "the UK", "Roy", "give detainees greater latitude in selecting legal representation", "some one-liners", "Vernon Forrest,", "Tomas Olsson,", "1983.", "Nafees A. Syed,", "Sunday", "a share in the royalties", "the U.S.-Mexico border", "in a canyon in the path of the blaze Thursday.", "number of calls,", "Pre-evaluation, strategic planning, operative planning", "Anatomy", "seven", "Henry Higgins", "shoes", "Herbert Lom,", "Battle of Prome", "Union Hill section of Kansas City, Missouri", "Jean- Marc Vall\u00e9e", "dice", "the American League's all-star squad", "Tom Osborne", "Kwame Nkrumah"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7578933747412009}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2525", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-2523", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3062", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-3555", "mrqa_newsqa-validation-3970", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-9078", "mrqa_triviaqa-validation-7280", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-2323", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-11037"], "SR": 0.671875, "CSR": 0.5329861111111112, "EFR": 1.0, "Overall": 0.7280815972222222}, {"timecode": 54, "before_eval_results": {"predictions": ["$249", "diabetes and hypertension,", "The aviation industry", "many different", "eight", "last week,", "separately on multiple corruption charges.", "Joan Rivers", "\"Watchmen's\"", "not just to the islands,", "NATO's Membership Action Plan, or MAP,", "Bangladesh", "250,000", "complicated and deeply flawed man", "scored his sixth Test century", "\"to be one of his four children and know that is there for the world to see,", "\"build a fortress around America; to stop trading with other countries, shut down immigration, and rely on old industries.\"", "voluntary manslaughter", "dancing", "South Africa", "The noose incident", "the world's poorest children.", "propofol,", "Catholic church sex abuse scandal,", "head injury.", "500 feet down an embankment", "Marxist guerrillas", "World War I", "Rwanda", "U.N. High Commissioner for Refugees", "Jenny Sanford,", "African National Congress Deputy President Kgalema Motlanthe,", "58 minutes.", "come here,", "CNN", "Jobs", "bribing other wrestlers to lose bouts,", "his comments", "Juan Martin Del Potro.", "Tehran,", "gasoline", "Thirty to 40", "country music superstar", "President Obama", "Tuesday in Los Angeles.", "Wayne Michaels", "The UNHCR recommended against granting asylum,", "Al-Shabaab,", "Michael Jackson", "planning processes are urgently needed", "Molotov cocktails, rocks and glass.", "The Golden State Warriors are an American professional basketball team based in the San Francisco Bay Area in Oakland, California", "March 29, 2018", "quartz or feldspar", "Kursk nuclear submarine", "squash", "Caroline Garcia", "Caesars Entertainment Corporation", "Premier League club Manchester United", "London Review of Books", "Eudora Welty", "Richard Nixon", "the sousaphone", "National Lottery"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5858671171171171}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8333333333333333, 0.0, 0.16216216216216214, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.4, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-2789", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1325", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-1280", "mrqa_newsqa-validation-3863", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-4170", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-655", "mrqa_triviaqa-validation-2064", "mrqa_triviaqa-validation-5969", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-65", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-7251", "mrqa_hotpotqa-validation-5604"], "SR": 0.484375, "CSR": 0.5321022727272727, "retrieved_ids": ["mrqa_squad-train-32889", "mrqa_squad-train-77812", "mrqa_squad-train-9448", "mrqa_squad-train-61561", "mrqa_squad-train-17875", "mrqa_squad-train-69773", "mrqa_squad-train-68935", "mrqa_squad-train-45927", "mrqa_squad-train-1721", "mrqa_squad-train-18900", "mrqa_squad-train-24100", "mrqa_squad-train-58984", "mrqa_squad-train-68778", "mrqa_squad-train-67846", "mrqa_squad-train-12255", "mrqa_squad-train-65802", "mrqa_naturalquestions-validation-10353", "mrqa_searchqa-validation-7229", "mrqa_hotpotqa-validation-1028", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2590", "mrqa_triviaqa-validation-6649", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1514", "mrqa_searchqa-validation-16653", "mrqa_newsqa-validation-2313", "mrqa_newsqa-validation-4037", "mrqa_newsqa-validation-2739", "mrqa_searchqa-validation-1843", "mrqa_newsqa-validation-162", "mrqa_naturalquestions-validation-2901"], "EFR": 1.0, "Overall": 0.7279048295454545}, {"timecode": 55, "before_eval_results": {"predictions": ["The entertainer, whose real name is Clifford Harris,", "without the", "Mexico", "A group of men push a giant snowball across Kensington Gardens, west London", "five", "customers are lining up for vitamin injections that promise", "Portuguese water dog", "actor", "\"We want to reset our relationship and so we will do it together.'\"", "the 11th century Preah Vihear temple", "general astonishment", "June 6, 1944,", "a lightning strike", "81st minute", "Sen. Barack Obama", "money or other discreet aid", "people have chosen their rides based on what their", "Sri Lanka's Tamil rebels", "Pakistani territory", "Golfer Tiger Woods", "200 human bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "The 19-year-old woman", "Nearly eight in 10", "The paper said the trip had caused fury among some in the military who saw", "the 3rd District of Utah.", "Golfer Tiger Woods", "organizing the distribution of wheelchairs,", "The initial reaction was shock, quickly followed by speculation about what was going to happen next,\"", "\"She was focused so much on learning that she didn't notice,\"", "The space agency says the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "punish participants in this week's bloody mutiny,", "piracy incident", "Pacific Ocean territory of Guam", "Robert Park", "Djibouti,", "an assortment of ailments, some not too serious, but others that are potentially deadly.", "Six", "Bahrain", "delivers a big speech", "Facebook and Google,", "Sheikh Sharif Sheikh Ahmed", "2006", "18th", "March 24,", "The father of Haleigh Cummings,", "a senior at Stetson University studying computer science.", "Saturday,", "NATO fighters", "\"Empire of the Sun\"", "New Zealand", "a model of sustainability.", "The Jewel of the Nile", "summer", "79", "dumfries House", "Squeeze", "golf", "Montagues and Capulets", "Atlas ICBM", "Walt Disney World Resort", "Frank Sinatra", "mass", "a snout beetle", "Neville Chamberlain"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6346963713369964}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4799999999999999, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.375, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1245", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-144", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-307", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-2810", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-2923", "mrqa_newsqa-validation-3064", "mrqa_naturalquestions-validation-10114", "mrqa_triviaqa-validation-3763", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2685", "mrqa_searchqa-validation-11933", "mrqa_triviaqa-validation-920"], "SR": 0.578125, "CSR": 0.5329241071428572, "EFR": 1.0, "Overall": 0.7280691964285715}, {"timecode": 56, "before_eval_results": {"predictions": ["Tuesday", "Dr. Cade", "those traveling near the Somali coast", "\"To My Mother\"", "billboards with an image of the burning World Trade Center", "2.5 million", "almost 100", "137", "1,500", "a series of monthly meals", "Blagojevich,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "\"still trying to absorb the impact of this week's stunning events.\"", "terrorism.", "Trevor Rees,", "the most-wanted man in the world", "the Carrousel du Louvre,", "three men with suicide vests who were plotting to carry out the attacks,", "don't have to visit laundromats", "101", "Tim Masters,", "approximately 600 square miles of south-central Washington,", "shows the world that you love the environment and hate using fuel,\"", "The apartment building collapsed together with two other buildings on March 3.", "11", "Henrik Stenson", "CEO of an engineering and construction company with a vast personal fortune.", "Milan", "strife in Somalia,", "cancerous tumor.", "provided Syria and Iraq 500 cubic meters of water a second,", "Abdullah Gul,", "dead", "11th year in a row.", "the journalists and the flight crew will be freed,", "state senators who will decide whether to remove him from office", "national telephone", "the thoroughness of the officers involved", "the AR-15 and two other rifles and left the cabin.", "Ben Roethlisberger", "Larry Ellison,", "Newcastle", "228", "\"Golden City,\"", "gasoline", "the county jail in Spanish Fork,", "Swansea Crown Court,", "physicist Steven Chu", "the Dominican Republic", "militants", "Monday", "a Celtic people living in northern Asia Minor", "a diastema ( plural diastemata )", "to manage the characteristics of the beer's head", "clement", "Cambridge", "mercury", "13 October 1958", "bassline (subgenre of UK garage)", "omnisexuality", "(Invisibility)", "Zachary Taylor", "a sci-fi series", "(M Marilyn) Monroe"], "metric_results": {"EM": 0.53125, "QA-F1": 0.649321050171234}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.2222222222222222, 0.9411764705882353, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.625, 1.0, 1.0, 0.25, 1.0, 0.2727272727272727, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.7692307692307692, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-1712", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-3219", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-6999", "mrqa_triviaqa-validation-2291", "mrqa_hotpotqa-validation-2826", "mrqa_searchqa-validation-10329", "mrqa_searchqa-validation-15020", "mrqa_triviaqa-validation-3538"], "SR": 0.53125, "CSR": 0.5328947368421053, "EFR": 0.9666666666666667, "Overall": 0.7213966557017544}, {"timecode": 57, "before_eval_results": {"predictions": ["producing rock music with a country influence.", "African National Congress", "Expedia.", "Molotov cocktails, rocks and glass.", "\"Sesame Street's\"", "5,600", "the European Commission", "three", "using recreational drugs", "0-0 draw", "air support.", "Christopher Savoie", "Alina Cho", "not speak", "\"Draquila", "al Qaeda,", "U.S. Chamber of Commerce", "Carol Browner", "U.N. Security Council resolution", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "actor", "\"We tortured (Mohammed al-) Qahtani,\"", "an empty water bottle down the touchline", "a U.S. helicopter crashed in northeastern Baghdad as", "children of street cleaners and firefighters.", "Marie-Therese Walter.", "an acid attack", "Congress", "southern city of Naples", "her most important work is her charity, the Happy Hearts Fund.", "The model", "South Africa", "Somali,", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "the French consulate in the oil-rich city of Port-Gentil, on the country's coast.", "Michael Schumacher", "consumer confidence", "Golfer", "Longo-Ciprelli", "Fernando Caceres", "iPods", "a treadmill", "violation of a law that makes it illegal to defame, insult or threaten the crown.", "HSH Nordbank Arena", "$40 and a loaf of bread.", "tennis", "No. 1", "Republican Gov. Jan Brewer.", "Boundary County, Idaho, which borders Canada and abuts the area where the attack took place.", "securities", "$150 billion", "the Berlin School of experimental", "Michael Crawford", "the beginning", "coconuts", "Fenn Street School", "the ear canal", "Australian", "Argentinian", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "rap", "inducere", "Harvard Law School", "129,007"], "metric_results": {"EM": 0.625, "QA-F1": 0.7175662878787878}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5833333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9523809523809523, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.0909090909090909, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-950", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-2824", "mrqa_newsqa-validation-536", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3677", "mrqa_naturalquestions-validation-4112", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-2114", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-1502", "mrqa_searchqa-validation-9174", "mrqa_searchqa-validation-3718"], "SR": 0.625, "CSR": 0.5344827586206897, "retrieved_ids": ["mrqa_squad-train-68173", "mrqa_squad-train-86234", "mrqa_squad-train-79081", "mrqa_squad-train-11277", "mrqa_squad-train-55065", "mrqa_squad-train-68867", "mrqa_squad-train-70722", "mrqa_squad-train-15504", "mrqa_squad-train-23377", "mrqa_squad-train-70870", "mrqa_squad-train-29312", "mrqa_squad-train-52076", "mrqa_squad-train-72703", "mrqa_squad-train-70634", "mrqa_squad-train-36386", "mrqa_squad-train-78087", "mrqa_searchqa-validation-13657", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-562", "mrqa_searchqa-validation-8976", "mrqa_triviaqa-validation-3232", "mrqa_naturalquestions-validation-5564", "mrqa_newsqa-validation-4033", "mrqa_hotpotqa-validation-2205", "mrqa_newsqa-validation-3176", "mrqa_searchqa-validation-15278", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-3284", "mrqa_naturalquestions-validation-6451", "mrqa_searchqa-validation-8582", "mrqa_searchqa-validation-7185", "mrqa_newsqa-validation-4062"], "EFR": 0.9583333333333334, "Overall": 0.7200475933908046}, {"timecode": 58, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "Summer", "\"The missile defense system is not aimed at Russia,\"", "Six", "success for charities in the Harlem neighborhood.", "\u00a320 million ($41.1 million) fortune", "40 militants and six Pakistan soldiers", "it will be the first thing you shot.", "Arthur E. Morgan III,", "Jason Chaffetz", "\"a very thorough, 78-page decision by the district court\"", "Casey Anthony,", "The Ski Train", "bronze medal in the women's figure skating final,", "No 4, the highest ever position", "People Against Switching Sides (PASS)", "\"If they are not secure, I don't have a great deal of confidence that the rest of our critical infrastructure on the electric grid is secure,\"", "\"A chicken soaked in the rain,\"", "President Obama.", "Jacob Zuma,", "December 7, 1941", "help rebuild the nation's highways, bridges and other public-use facilities.", "18", "the Southeast,", "\"Up,\"", "disposable income", "capture that fascinating transformation that takes place when carving a pumpkin.", "school,", "a motor scooter", "learn in safer surroundings.", "$50 less,", "J.Crew", "$106.5 million", "Nearly eight in 10", "credit card", "he was one of 10 gunmen who attacked several targets in Mumbai", "Akio Toyoda", "in July", "success with high-tech,", "\"black box\" label warning", "that drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "cancer awareness", "Virgin America", "Sharp-witted. Direct. In control. Loyal.", "I sound like a basket case. It's funny with acting -- we all wear masks in our normal life.", "Kenyan and Somali", "opium trade", "in 1980,", "a man had been stoned to death by an angry mob.", "Africa", "the most-wanted man in the world", "left - sided heart failure", "The kidnapper tells Shawn to tell `` Abigail '' that he loved her", "Devastator", "Madness", "james stewart", "vice-admiral", "George Lawrence Mikan, Jr.", "Kait Parker", "Centre-du-Qu\u00e9bec area", "the Nguyen", "doughboy", "Motto: United We Stand, Divided We Fall", "professor henry higgins"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6917728084823673}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true], "QA-F1": [0.4444444444444445, 1.0, 0.4, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.9600000000000001, 0.0, 1.0, 0.0, 0.05714285714285714, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.923076923076923, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-2534", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2740", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2193", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-2175", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-505", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-3611", "mrqa_hotpotqa-validation-2951", "mrqa_searchqa-validation-3774"], "SR": 0.546875, "CSR": 0.5346927966101696, "EFR": 0.896551724137931, "Overall": 0.7077332791496201}, {"timecode": 59, "before_eval_results": {"predictions": ["his business dealings for possible securities violations", "1913.", "$40 and a bread.", "five days.", "U Win Tin,", "543", "Knox and her Italian former boyfriend, Raffaele Sollecito,", "dinosaur kin.", "four", "64,", "\"a whole new treasure trove of fossils\"", "at least two and a half hours.", "shark River Park", "improve the environment", "\"first dog\"", "The park bench facing Lake Washington", "More than 15,000", "\"Teen Patti\" (\"Card Game\")", "Muslim countries,", "\"Piers Morgan Tonight\"", "Illness", "Basel", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "Strategic Arms Reduction Treaty and nonproliferation.", "sumo wrestling", "10 below", "\"the biggest exodus from the troubled Somali capital since the Ethiopian intervention in 2007.\"", "recall", "Roy", "VBS.TV", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Marxist guerrillas", "Greeley, Colorado,", "at least seven", "NATO's International Security Assistance Force", "Jacob Zuma,", "Palestinian Islamic Army,", "toxic smoke from burn pits", "Fullerton, California,", "opened considerably higher Tuesday", "34", "3,000", "Workers'", "helicopters and unmanned aerial vehicles", "dual nationality", "1959,", "the Muslim north of Sudan", "at least 18 federal agents and two soldiers have been", "Bahrain", "33", "Kenneth Cole", "the Devastator", "Brazil", "Theodore Roosevelt", "vice-admiral", "Phillies", "Jiles Perry (JP) Richardson Jr,", "Greek-American", "feats of exploration", "he is often considered the \"godfather\" of U.S.-Mexico border cartels", "Monarch", "the American Repertory Theater (ART)", "Kansas City", "Audi"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6122109661172161}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false], "QA-F1": [0.6, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-742", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-1331", "mrqa_triviaqa-validation-105", "mrqa_triviaqa-validation-2582", "mrqa_hotpotqa-validation-4241", "mrqa_searchqa-validation-2313", "mrqa_searchqa-validation-156", "mrqa_hotpotqa-validation-2473"], "SR": 0.546875, "CSR": 0.5348958333333333, "EFR": 0.9655172413793104, "Overall": 0.7215669899425288}, {"timecode": 60, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2323", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4030", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-92", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10060", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10255", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-2629", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3381", "mrqa_naturalquestions-validation-3555", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-454", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5002", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6451", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-8787", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-916", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1040", "mrqa_newsqa-validation-1055", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1069", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-1116", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1176", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1423", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-145", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1501", "mrqa_newsqa-validation-1522", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1536", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1619", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1690", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1812", "mrqa_newsqa-validation-1815", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1895", "mrqa_newsqa-validation-190", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1921", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1966", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-2013", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2038", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2143", "mrqa_newsqa-validation-2164", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2284", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-230", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2403", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2438", "mrqa_newsqa-validation-2465", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2502", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2562", "mrqa_newsqa-validation-257", "mrqa_newsqa-validation-2578", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2909", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3112", "mrqa_newsqa-validation-3131", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-3156", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-3299", "mrqa_newsqa-validation-3317", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3360", "mrqa_newsqa-validation-3370", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3436", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3633", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3704", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3722", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3823", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3849", "mrqa_newsqa-validation-3876", "mrqa_newsqa-validation-3885", "mrqa_newsqa-validation-3886", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4038", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4088", "mrqa_newsqa-validation-4098", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-543", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-570", "mrqa_newsqa-validation-578", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-737", "mrqa_newsqa-validation-765", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-979", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-100", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-10045", "mrqa_searchqa-validation-10193", "mrqa_searchqa-validation-10233", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-10790", "mrqa_searchqa-validation-1085", "mrqa_searchqa-validation-11002", "mrqa_searchqa-validation-11050", "mrqa_searchqa-validation-11375", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11770", "mrqa_searchqa-validation-12117", "mrqa_searchqa-validation-12313", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12409", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13434", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13556", "mrqa_searchqa-validation-13852", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13951", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14405", "mrqa_searchqa-validation-15158", "mrqa_searchqa-validation-15412", "mrqa_searchqa-validation-15749", "mrqa_searchqa-validation-16053", "mrqa_searchqa-validation-16282", "mrqa_searchqa-validation-16605", "mrqa_searchqa-validation-16886", "mrqa_searchqa-validation-16908", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-1791", "mrqa_searchqa-validation-2260", "mrqa_searchqa-validation-2462", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-252", "mrqa_searchqa-validation-2963", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3982", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-4972", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6297", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-6420", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6954", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7022", "mrqa_searchqa-validation-7132", "mrqa_searchqa-validation-7411", "mrqa_searchqa-validation-7418", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8776", "mrqa_searchqa-validation-9109", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-9687", "mrqa_searchqa-validation-9725", "mrqa_squad-validation-10494", "mrqa_squad-validation-1055", "mrqa_squad-validation-1268", "mrqa_squad-validation-1384", "mrqa_squad-validation-1490", "mrqa_squad-validation-1529", "mrqa_squad-validation-1615", "mrqa_squad-validation-167", "mrqa_squad-validation-1742", "mrqa_squad-validation-1941", "mrqa_squad-validation-204", "mrqa_squad-validation-2095", "mrqa_squad-validation-2283", "mrqa_squad-validation-2387", "mrqa_squad-validation-2613", "mrqa_squad-validation-2857", "mrqa_squad-validation-2865", "mrqa_squad-validation-3040", "mrqa_squad-validation-3317", "mrqa_squad-validation-3456", "mrqa_squad-validation-3493", "mrqa_squad-validation-3790", "mrqa_squad-validation-3941", "mrqa_squad-validation-3954", "mrqa_squad-validation-4241", "mrqa_squad-validation-4402", "mrqa_squad-validation-4452", "mrqa_squad-validation-457", "mrqa_squad-validation-4633", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4841", "mrqa_squad-validation-4933", "mrqa_squad-validation-5029", "mrqa_squad-validation-5185", "mrqa_squad-validation-5222", "mrqa_squad-validation-5311", "mrqa_squad-validation-543", "mrqa_squad-validation-5479", "mrqa_squad-validation-57", "mrqa_squad-validation-5804", "mrqa_squad-validation-5961", "mrqa_squad-validation-6121", "mrqa_squad-validation-6147", "mrqa_squad-validation-6241", "mrqa_squad-validation-6470", "mrqa_squad-validation-664", "mrqa_squad-validation-6792", "mrqa_squad-validation-6869", "mrqa_squad-validation-694", "mrqa_squad-validation-7022", "mrqa_squad-validation-7064", "mrqa_squad-validation-7338", "mrqa_squad-validation-7443", "mrqa_squad-validation-7494", "mrqa_squad-validation-7546", "mrqa_squad-validation-7733", "mrqa_squad-validation-7747", "mrqa_squad-validation-7908", "mrqa_squad-validation-7918", "mrqa_squad-validation-7964", "mrqa_squad-validation-809", "mrqa_squad-validation-8115", "mrqa_squad-validation-8204", "mrqa_squad-validation-8204", "mrqa_squad-validation-8216", "mrqa_squad-validation-8412", "mrqa_squad-validation-8495", "mrqa_squad-validation-8551", "mrqa_squad-validation-8558", "mrqa_squad-validation-8923", "mrqa_squad-validation-9087", "mrqa_squad-validation-9178", "mrqa_squad-validation-9227", "mrqa_squad-validation-9581", "mrqa_squad-validation-9775", "mrqa_squad-validation-9910", "mrqa_squad-validation-9944", "mrqa_squad-validation-9993", "mrqa_squad-validation-9996", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-1839", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-2481", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-3097", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-354", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-3931", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4580", "mrqa_triviaqa-validation-5467", "mrqa_triviaqa-validation-6001", "mrqa_triviaqa-validation-6050", "mrqa_triviaqa-validation-6282", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6309", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-6649", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-7672", "mrqa_triviaqa-validation-795"], "OKR": 0.837890625, "KG": 0.4421875, "before_eval_results": {"predictions": ["183", "Carson", "fastest circumnavigation of the globe in a powerboat", "the Airbus A330-200 encountered heavy turbulence about 02:15 a.m. local time Monday", "Paul McCartney and Ringo Starr", "ballots", "transit bombings", "2000,", "Martin \"Al\" Culhane,", "normal maritime", "\"Even though I moved a slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "the Baha'i International Community to the United Nations,", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf, police said.", "from 18 years to life in prison", "Clinton", "Matthew Chance", "34", "five victims", "Herman Cain,", "\"She was focused so much on learning that she didn't notice,\"", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "Vertikal-T,", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Michael Brewer,", "December 1", "don't have to visit laundromats because they enjoy the luxury of a free", "death squad killings", "Ozzy Osbourne", "it is not just $3 billion of new money into the economy.", "\"Steamboat Bill, Jr.\"", "Omar Bongo,", "\"happy ending\" to the case.", "Obama and McCain camps", "Africa", "in a hotel, police said.", "the only goal of the game", "France", "100,000 workers", "U.S. security coordinator", "North Korea intends to launch a long-range missile in the near future,", "Nasser Medical Institute in Cairo,", "1991-1993,", "to a civil disturbance call,", "images of the small girl being sexually assaulted.", "Iran's parliament speaker", "Deputy Treasury Secretary", "\"Operation Crank Call,\"", "Islamabad", "Williams' body", "Kris Allen,", "ConAgra Foods plant", "Lalo Schifrin", "1982", "Billy Idol", "false negatives", "( Telegraph) Cameron", "every ten years since 1801", "five", "\"The Dragon\"", "1994", "a magnolia", "the same", "Jupiter", "mural"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7162548934582391}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.975609756097561, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.4, 0.4210526315789474, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.3333333333333333, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1538", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-3930", "mrqa_newsqa-validation-1711", "mrqa_newsqa-validation-3950", "mrqa_naturalquestions-validation-4329", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5502", "mrqa_triviaqa-validation-1114", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-16357"], "SR": 0.59375, "CSR": 0.5358606557377049, "retrieved_ids": ["mrqa_squad-train-53428", "mrqa_squad-train-8435", "mrqa_squad-train-77283", "mrqa_squad-train-35303", "mrqa_squad-train-40666", "mrqa_squad-train-48349", "mrqa_squad-train-507", "mrqa_squad-train-69227", "mrqa_squad-train-69879", "mrqa_squad-train-56638", "mrqa_squad-train-83368", "mrqa_squad-train-12580", "mrqa_squad-train-17109", "mrqa_squad-train-13958", "mrqa_squad-train-12395", "mrqa_squad-train-64886", "mrqa_naturalquestions-validation-8972", "mrqa_triviaqa-validation-45", "mrqa_searchqa-validation-409", "mrqa_triviaqa-validation-4152", "mrqa_searchqa-validation-16971", "mrqa_newsqa-validation-1016", "mrqa_squad-validation-10320", "mrqa_squad-validation-1290", "mrqa_triviaqa-validation-7563", "mrqa_triviaqa-validation-7160", "mrqa_newsqa-validation-3313", "mrqa_searchqa-validation-7251", "mrqa_searchqa-validation-14425", "mrqa_triviaqa-validation-2541", "mrqa_newsqa-validation-2901", "mrqa_triviaqa-validation-7164"], "EFR": 1.0, "Overall": 0.707328381147541}, {"timecode": 61, "before_eval_results": {"predictions": ["re-impose order", "South Dakota State Penitentiary", "$8.8 million", "Friday,", "11th year in a row.", "Russian concerns that the defensive shield could be used for offensive aims.", "The leftist guerilla group,", "a baseball bat", "six", "a book.", "Venezuela", "Kerstin", "$1.45 billion", "Iranian consulate,", "Apple Inc.", "Janet Napolitano", "Malawi", "Daniel Radcliffe", "the privileged ethnicity,", "\"Steamboat Bill, Jr.\"", "Explosives are set off in the Missouri River", "\"The Sopranos,\"", "artificial intelligence.", "sculptures by famous artists.", "Shanghai", "the BBC's central London offices", "giving up their tour buses, as well as their road crew and traveling with their own equipment.", "an engineering and construction company", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "ties", "\"procedure on her heart,\"", "civilians,", "Award for a fourth consecutive year for outstanding performance by a female actor in a drama series for her role as Deputy Chief Brenda Johnson.", "9:20 p.m. ET Wednesday.", "tallest building,", "\"Zed,\" a Columbian mammoth", "Spc. Megan Lynn Touma,", "1979", "three out of four questioned say that things are going well for them personally.", "island's dining scene", "fascinating transformation that takes place when carving a pumpkin.", "prisoners at the South Dakota State Penitentiary", "Intensifying", "More than 15,000", "Princess Diana", "\"Zed,\" a Columbian mammoth", "Lavau's accident and the one involving the dead driver are under investigation.", "hiring veterans as well as job training for all service members leaving the military.", "his fleet of trucks used to pick up cargo.", "UK", "bipartisan", "tomato puree has a thicker consistency and a deeper flavour than sauce", "in skeletal muscle and the brain", "1985 -- 1993", "Dublin", "Goldfinger", "lidice", "Columbia", "Wynonna Judd", "to be identified as transgender", "the Italian regime", "razorback", "Canada", "Bolton"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6963516467192938}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3529411764705882, 0.3, 1.0, 1.0, 0.18181818181818182, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-3682", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-2853", "mrqa_naturalquestions-validation-2943", "mrqa_naturalquestions-validation-1155", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1868", "mrqa_hotpotqa-validation-5251", "mrqa_searchqa-validation-16084", "mrqa_searchqa-validation-4753"], "SR": 0.609375, "CSR": 0.537046370967742, "EFR": 1.0, "Overall": 0.7075655241935485}, {"timecode": 62, "before_eval_results": {"predictions": ["Gorakhpur Junction", "Colman", "the Michael Douglas film, The Jewel of the Nile, the sequel to the hit blockbuster film, Romancing the Stone", "Nodar Kumaritashvili", "three", "constitutional monarchy", "sperm and ova", "Michael Buffer", "greater than 14", "16,801 students", "Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "Book of Exodus", "the 1820s", "Tigris and Euphrates rivers", "third", "Andrew Garfield", "The Fixx", "digestion of proteins, by activating digestive enzymes, and making ingested proteins unravel so that digestive enzymes break down the long chains of amino acids", "2010", "0.30 in ( 7.6 mm )", "March 8, 2018", "Camping World Stadium in Orlando, Florida", "George Harrison", "Kristy Swanson", "Chairman of the Monetary Policy Committee", "mathematical model", "James Martin Lafferty", "Kenny Anderson", "agriculture", "The vas deferens is connected to the epididymis above the point of blockage", "Anglo - Norman French waleis", "the early 20th century", "Omar Khayyam", "Uralic", "the human immunoglobulin heavy chain region contains 2 Constant ( C\u03bc and C\u03b4 ) gene segments", "Universal Pictures, which holds the library of predecessor companies DreamWorks Animation and Classic Media, and who in turn with copyright holder Ward Productions", "Tbilisi, Capital of Georgia", "on dry lake beds", "autopistas, or tolled ( quota ) highways", "the Vital Records Office of the states, capital district, territories and former territories", "outside cultivated areas", "Frank Theodore `` Ted '' Levine", "IIII", "hydrolysis reaction", "The Maginot Line", "Gustav Bauer", "James Watson and Francis Crick", "the person compelled to pay for reformist programs", "card verification value", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Sondheim", "melbourne", "Noppawan Lertcheewakarn of Thailand", "Afghanistan", "Todd McFarlane", "Massachusetts", "one", "\"significant skeletal remains\"", "the forward's lawyer", "a full tropical garden", "the wine", "the mouth", "locoweed", "December 1974"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6288653273809524}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.92, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.16666666666666669, 0.4, 0.0, 0.33333333333333337, 0.625, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1, 0.2857142857142857, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2946", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-9571", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-6182", "mrqa_naturalquestions-validation-8026", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4043", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-9755", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-6610", "mrqa_newsqa-validation-1699", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11479"], "SR": 0.53125, "CSR": 0.5369543650793651, "EFR": 1.0, "Overall": 0.7075471230158731}, {"timecode": 63, "before_eval_results": {"predictions": ["Lady Agnes", "the Coriolis force", "1776", "1994", "Roger Dean Stadium", "James Brown", "Everywhere", "1 mile ( 1.6 km )", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "Jessica Sanders", "Article 1, Section 2, Clause 3", "The two men, Lex Luger and Rick Rude, have held the championship for a continuous reign of one year ( 365 days ) or more", "ancient Greece", "Foreign minister Hermann M\u00fcller and colonial minister Johannes Bell", "annuity", "Mark Lowry", "the murder of Lord William Russell, a provincial doctor, Robert Blake Overton, wrote to Scotland Yard suggesting checking for fingerprints", "the list of judges of the Supreme Court of India, the highest court in the Republic of India", "c. 1000 AD", "a bow bridge", "Dick Rutan and Jeana Yeager", "an stretch of Forsyth Street at the foot of the Manhattan Bridge in the Little Fuzhou neighborhood within Manhattan's Chinatown", "March 4, 1789", "King Saud University", "Hugo Weaving", "the biblical Book of Exodus", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Bart Howard", "to transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "Sean O' Neal", "Toby Kebbell", "the end of 1066", "James", "Stefanie Scott", "amino acids glycine and arginine", "the book and architecture", "Stephen A. Douglas", "the Dolby Theatre in Hollywood, Los Angeles, California", "the 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII,", "a fictional South American country", "during meiosis", "July -- October 2012", "Andy Serkis", "the priests and virgins", "1560s", "twice", "Border Collie", "Gwendoline Christie", "September 19 - 22", "provinces along the Yangtze River and in provinces in the south", "humid subtropical climate", "1989", "furniture", "danzig", "Marjorie McGinnis", "the Electorate", "fourth-ranking", "Anne Frank,", "Sunday,", "123 pounds of cocaine and 4.5 pounds of heroin,", "Twilight Zone:", "\"Mulholland Drive,\"", "No Child Left Behind", "part of the proceeds"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5938237457056742}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6153846153846153, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.8387096774193549, 0.0, 1.0, 1.0, 0.8571428571428571, 0.3137254901960785, 1.0, 0.15384615384615383, 0.4, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 1.0, 0.21052631578947367, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6153846153846153, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3756", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-9782", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-171", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1725", "mrqa_naturalquestions-validation-9876", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-9961", "mrqa_triviaqa-validation-5913", "mrqa_triviaqa-validation-2963", "mrqa_hotpotqa-validation-862", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2386", "mrqa_searchqa-validation-7843"], "SR": 0.453125, "CSR": 0.53564453125, "retrieved_ids": ["mrqa_squad-train-10845", "mrqa_squad-train-46602", "mrqa_squad-train-72854", "mrqa_squad-train-9248", "mrqa_squad-train-9998", "mrqa_squad-train-40960", "mrqa_squad-train-1164", "mrqa_squad-train-44053", "mrqa_squad-train-55318", "mrqa_squad-train-41602", "mrqa_squad-train-45870", "mrqa_squad-train-70201", "mrqa_squad-train-14630", "mrqa_squad-train-81651", "mrqa_squad-train-72688", "mrqa_squad-train-17101", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-3474", "mrqa_hotpotqa-validation-1050", "mrqa_newsqa-validation-3741", "mrqa_hotpotqa-validation-1127", "mrqa_newsqa-validation-3970", "mrqa_squad-validation-6809", "mrqa_naturalquestions-validation-142", "mrqa_triviaqa-validation-4760", "mrqa_naturalquestions-validation-5564", "mrqa_searchqa-validation-13853", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-1087", "mrqa_newsqa-validation-294"], "EFR": 0.9428571428571428, "Overall": 0.6958565848214286}, {"timecode": 64, "before_eval_results": {"predictions": ["the winter solstice", "19 July 1990", "senators", "Rex Harrison", "a manufacturing operation", "Turducken", "Patrick Warburton", "Judas Iscariot", "in 1936", "the President of the United States", "administrative supervision over all courts and the personnel thereof", "James Fleet", "The Seattle Center", "Yuzuru Hanyu", "Tracy McConnell", "Kenny Rogers", "the part of the gastrointestinal tract between the stomach and the large intestine", "Action Jackson", "Thomas Alva Edison", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Tom Brady", "Rumplestiltskin", "Sylvester Stallone", "from 35 to 40 hours per week", "Naomi", "`` There is one body and one Spirit just as you were called to the one hope that belongs to your call one Lord, one faith, one baptism, one Father of all, who is over all and through all and in all", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "December 25", "Louis XV", "Waylon Jennings", "In 2012", "In first, the sound films which included synchronized dialogue, known as `` talking pictures '', or `` talkies '', were exclusively shorts", "`` Far Away ''", "John C. Reilly", "100,000", "Richard Masur", "5", "Johnny Cash", "consistency", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "John C. Reilly", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Saint Peter", "King Saud University", "the presence of correctly oriented P waves", "Brenda", "In The surgical knife purpose - made for the occasion is on display in the Mus\u00e9e d'histoire de la m\u00e9decine", "Cyanea capillata", "Bonnie Lipton", "2002", "Tom Brady", "Dawn French", "translator", "Ut\u00f8ya", "125 lb (57 kg)", "Old World fossil representatives", "1964", "The National Infrastructure Program,", "North Korea", "\"E! News\"", "carbon fiber", "current congressmen", "The Greatest Show on Earth", "Catherine"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7039039942900237}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.14545454545454545, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0909090909090909, 1.0, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4389", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-1818", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-9436", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-4309", "mrqa_newsqa-validation-3451", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16408", "mrqa_triviaqa-validation-3010"], "SR": 0.640625, "CSR": 0.5372596153846154, "EFR": 1.0, "Overall": 0.7076081730769231}, {"timecode": 65, "before_eval_results": {"predictions": ["Mel Gibson", "`` The Crossing ''", "2016", "Jocelyn Flores", "1956", "November 25, 2002", "zinc", "Pebe Sebert", "Thomas Chisholm", "Higher density regions of the interstellar medium form clouds, or diffuse nebulae", "Lesley Gore", "Paul", "book series", "Radiotelegraphy", "ingredients", "George III's German - born wife, Charlotte of Mecklenburg - Strelitz", "December 1, 2009", "four", "com TLD", "Neil Young", "Ren\u00e9 Verdon", "Melanie Martinez", "the Director of National Intelligence", "Liam Cunningham", "Elliot Scheiner", "a cylinder of glass or plastic that runs along the fiber's length", "Ace", "Goths", "H CO", "StubHub Center", "the Maryland Senate's actions", "Jaydev Shah", "Dougie MacLean", "Glenn Close", "Egypt, the only part of the country located in Asia", "Germanic elements `` hrod '' meaning renown", "1888", "Nashville, Tennessee", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "performance marker", "following the 2017 season", "The Seattle Center, including the Seattle Center Monorail and the Space Needle", "Columbia River Gorge", "Setsuko Thurlow", "John Joseph Patrick Ryan", "1912", "Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16, and Acts 1 : 13 )", "Ric Flair", "Around 1200", "southern hemisphere", "2010", "Adam Werritty", "the Jets", "\u201cThe Seven Year Itch\u201d", "Kim Jong-hyun", "Edward II", "Harrods", "\"What of my friends have put in at least a couple hours,\"", "job training", "Coleman, 42, was being treated there after being admitted on Wednesday.", "Nixon", "Great Expectations", "cathode", "No Surprises"], "metric_results": {"EM": 0.5, "QA-F1": 0.5953322718947719}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6153846153846153, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428572, 1.0, 1.0, 0.5714285714285715, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.6666666666666666, 0.8, 1.0, 0.16666666666666669, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-10057", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-5515", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9386", "mrqa_triviaqa-validation-1851", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4129", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1827", "mrqa_hotpotqa-validation-1697"], "SR": 0.5, "CSR": 0.5366950757575757, "EFR": 0.96875, "Overall": 0.7012452651515152}, {"timecode": 66, "before_eval_results": {"predictions": ["a substitute good", "October 1980", "IX for nine o'clock", "Edgar Lungu", "Drew Barrymore", "Massachusetts", "tourneys or slow wheels", "harmful for the one whose envy inflicts it on others as well as for the sufferer", "W. Edwards Deming", "Jackie Robinson", "a measure of the rate at which soil is able to absorb rainfall or irrigation", "Kathy Najimy", "Nicole Gale Anderson", "Jethalal Gada", "a transformative change of heart ; especially : a spiritual conversion", "smoking", "Richard Crispin Armitage", "Mahalangur Himal sub-range of the Himalayas", "professor in Half - Blood Prince", "mid-ocean ridges, where new oceanic crust is formed through volcanic activity and then gradually moves away from the ridge", "In 1837", "late - September through early January", "in 1991, with L.A. Reid and Babyface during sessions for the Dangerous album, but didn't make the final cut", "Joseph Sherrard Kearns", "The Union's forces", "On 1 September 1939", "a loop", "Carroll O'Connor", "West Egg on prosperous Long Island", "Subject to the advice and consent role of the U.S. Senate, the President of the United States negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "certified question or proposition of law from one of the United States Courts of Appeals", "after World War II, ending the Empire of Japan's 35 - year rule over Korea in 1945", "Guwahati", "the west - facing core of the crescent on Salamis Bay, which opens into the Saronic Gulf", "Todd Griffin", "October 29, 2015", "The Pir Panjal Railway Tunnel", "16", "~ 3.5 million years old from Idaho, USA", "federal government", "Tigris and Euphrates rivers", "the executive, consisting of the President ; and the judicial, consists of the Supreme Court and other federal courts", "in the year 2026", "Holly Marie Combs", "utopian novels of H.G. Wells, including A Modern Utopia ( 1905 ) and Men Like Gods ( 1923 )", "Michael Crawford", "password recovery tool for Microsoft Windows", "Indo - Pacific distribution", "Tokyo / Helsinki", "moral", "Lana Del Rey", "NBA", "a greyhound, gazelle hound or tazi", "anne", "Northwest Mall", "\"Supergirl\"", "Field Marshal Lord Gort", "\"It didn't matter if you were 60, 40 or 20 like I am.", "gun", "between government soldiers and Taliban militants in the Swat Valley.", "Odysseus", "cajun", "Boy Scouts of America", "three"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5450860380535787}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.8, 0.2857142857142857, 0.11764705882352941, 0.125, 1.0, 0.2285714285714286, 1.0, 0.0, 0.36363636363636365, 0.0, 0.7272727272727273, 0.0, 1.0, 0.0, 0.8, 0.0, 0.5263157894736842, 1.0, 0.058823529411764705, 0.05555555555555555, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.1, 1.0, 0.3636363636363636, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1198", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9063", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-397", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-1848", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-4847", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-989", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-2458", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2240", "mrqa_searchqa-validation-16540", "mrqa_searchqa-validation-4320"], "SR": 0.40625, "CSR": 0.5347481343283582, "retrieved_ids": ["mrqa_squad-train-34590", "mrqa_squad-train-24279", "mrqa_squad-train-58964", "mrqa_squad-train-42436", "mrqa_squad-train-10530", "mrqa_squad-train-68040", "mrqa_squad-train-7852", "mrqa_squad-train-57629", "mrqa_squad-train-64277", "mrqa_squad-train-84912", "mrqa_squad-train-54858", "mrqa_squad-train-59062", "mrqa_squad-train-29239", "mrqa_squad-train-23648", "mrqa_squad-train-59389", "mrqa_squad-train-48327", "mrqa_naturalquestions-validation-3593", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-9078", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-2187", "mrqa_searchqa-validation-7499", "mrqa_triviaqa-validation-5322", "mrqa_naturalquestions-validation-4124", "mrqa_searchqa-validation-11020", "mrqa_naturalquestions-validation-5452", "mrqa_newsqa-validation-1598", "mrqa_searchqa-validation-4535", "mrqa_naturalquestions-validation-8610", "mrqa_searchqa-validation-10060", "mrqa_naturalquestions-validation-7226", "mrqa_newsqa-validation-1325"], "EFR": 0.9736842105263158, "Overall": 0.7018427189709349}, {"timecode": 67, "before_eval_results": {"predictions": ["the year 2026", "Egypt", "1904", "1885", "July 2010", "Clarence Darrow", "John B. Watson", "Spanish", "Anna Murphy", "follows a child with Treacher Collins syndrome trying to fit in", "when the forward reaction proceeds at the same rate as the reverse reaction", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "Ceramic art", "December 1, 2017", "Erica Rivera", "McFerrin", "Donald Trump", "Matt Flinders", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Philadelphia", "Sir Ronald Ross", "Georgia", "Domhnall Gleeson", "CeCe Drake", "March 11, 2016", "November 6, 2017", "Thomas Mundy Peterson", "Brutus", "from boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round", "consistency", "Nucleotides", "acts as a primer, by polymerizing the first few glucose molecules, after which other enzymes take over", "James Intveld", "Michael Jackson and Lionel Richie", "Amybeth McNulty", "saecula saeculorum", "jerry Slugworth", "into the intermembrane space", "February 25, 2004", "the breast or lower chest of beef or veal", "each state'sDM, which is required to drive", "Dr. Hartwell Carver", "two", "following the 2017 season", "Arunachal Pradesh", "Charles R Ranch, County Road 24", "a work of social commentary", "his brother", "Washington", "The euro", "Ferm\u00edn Francisco", "Aslan", "Richmond", "drinking song", "the tissues of the outer third of the vagina", "Bergen", "Cartoon Network", "\"She was focused so much on learning that she didn't notice,\"", "change course", "a federal judge in Mississippi", "a skunk", "Russia", "Tommy Hilfiger", "a pitcher"], "metric_results": {"EM": 0.53125, "QA-F1": 0.642214619101229}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.25, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.2666666666666667, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7058823529411764, 0.39999999999999997, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4593", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-5932", "mrqa_hotpotqa-validation-4194", "mrqa_triviaqa-validation-2358"], "SR": 0.53125, "CSR": 0.5346966911764706, "EFR": 1.0, "Overall": 0.7070955882352942}, {"timecode": 68, "before_eval_results": {"predictions": ["2016", "B.R. Ambedkar", "Lalo Schifrin", "Gwendoline Christie", "Rockwell", "Chris Sarandon", "Olivia Olson", "21 June 2007", "Peter Klaven ( Paul Rudd ), a Los Angeles real estate agent", "Janie ( G. Hannelius ) and Willamina `` Will ''", "4 January 2011", "her brother, Brian", "Elizabeth Dean Lail", "Ashoka and Kalinga Ashoka ( son of Bindusara )", "Omar Khayyam ( Persian pronunciation : ( x\u00e6j\u02c8j\u0251\u02d0m )", "keep the leaves in the light and provide a place for the plant to keep its flowers and fruits", "British Columbia, Canada", "the government - owned Panama Canal Authority", "Johnny Cash", "before the first year begins", "personnel directors", "Davos", "Neil Patrick Harris", "1946", "Joel", "stems and roots of certain vascular plants", "either late 2018 or early 2019", "R.E.M.", "Jewish audiences", "chemical element with symbol I and atomic number 53", "the Ark of the Covenant", "Luther Ingram", "September 29, 2017", "Joseph Sherrard Kearns", "Kelly Reno", "US - grown fruit ( grown by its cooperative members primarily in Polk County, Florida )", "Iran", "the beginning of the Wizarding World shared media franchise", "Jikji", "the Prince - Electors", "1799", "Kid Creole & The Coconuts", "a dysphemic vocalisation in the Second Temple period of a theonym based on the root ``k `` king ''", "December 2, 2013, and the third season concluded on October 1, 2017", "an official document permitting a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "Toto", "social commentary, and condemns rural depopulation and the pursuit of excessive wealth", "1770 BC", "Australia's Sir Donald Bradman", "Roman Reigns", "Rocky Dzidzornu", "Sikhism", "guitar", "September 27 1825", "Miracle", "1 km east of the village of Closeburn, and 2 km south-east of Thornhill, in Dumfries and Galloway, south-west Scotland", "the Crab Orchard Mountains", "President Obama and Britain's Prince Charles", "NATO fighters", "19, standing 6'2\", with his auburn hair pulled back in a queue.", "a lighthouse", "cradle song", "E.E. Cummings", "Elizabeth Birnbaum"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6332929082384025}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.25, 0.5714285714285715, 0.5, 1.0, 0.3846153846153846, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.14285714285714288, 0.0, 0.8837209302325582, 1.0, 1.0, 0.8, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.2727272727272727, 0.0, 0.26666666666666666, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-400", "mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3097", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-9054", "mrqa_naturalquestions-validation-10402", "mrqa_naturalquestions-validation-8209", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-3425", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2653", "mrqa_hotpotqa-validation-5586", "mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-3345", "mrqa_searchqa-validation-14960", "mrqa_searchqa-validation-13013"], "SR": 0.484375, "CSR": 0.5339673913043479, "EFR": 0.9393939393939394, "Overall": 0.6948285161396575}, {"timecode": 69, "before_eval_results": {"predictions": ["Thawne", "Old Trafford", "The Intolerable Acts", "skeletal muscle and the brain", "libretto", "up to 13 individuals", "1947", "the St. Louis Cardinals", "Andy Serkis", "Panning", "September 21, 2017", "Bob Dylan", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "the sidewalk between Division Street and East Broadway", "Garbi\u00f1e Muguruza", "HTTP / 1.1", "eagles", "eleven", "10.5 %", "Roger Dean Stadium", "`` Blood is the New Black ''", "Otis Timson", "four", "Benjamin Franklin", "a routing table, or routing information base ( RIB )", "James Rodr\u00edguez", "in AD 95 -- 110", "President since creation of the office in 1789", "2,500", "in the lower back", "forney Hull ( James Frain ), the surly librarian who looks after his alcoholic sister Mary Elizabeth ( Margaret Hoard )", "Ashoka", "the dermis", "Hodel", "October 27, 2017", "Howard Caine", "one of Jesus'disciples", "April 10, 2018", "the fourth C key from left on a standard 88 - key piano keyboard", "Aegisthus", "NFL coaches, general managers, and scouts", "no official release date has been given, though it is expected in either late 2018 or early 2019", "Terrell Suggs", "Latitude", "the courts", "September 29, 2017", "around 10 : 30am", "Angola", "Norway", "Manley", "the band released their fourth live album All My Friends We're Glorious : Death of a Bachelor Live", "Wyatt", "January", "In God we Trust", "2006", "Steve Martin & the Steep Canyon Rangers", "2027 Fairmount Avenue between Corinthian Avenue and North 22nd Street in the Fairmount section of the city", "competing", "At least 40", "Juan Martin Del Potro.", "the Caspian Sea", "Sweden", "photoelectric", "Namibia"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6501037157287157}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.8571428571428571, 0.0, 0.4, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-3121", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-4370", "mrqa_naturalquestions-validation-9009", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5164", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-327", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-7391", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-744", "mrqa_newsqa-validation-2843", "mrqa_triviaqa-validation-5834"], "SR": 0.59375, "CSR": 0.5348214285714286, "retrieved_ids": ["mrqa_squad-train-56535", "mrqa_squad-train-75642", "mrqa_squad-train-17906", "mrqa_squad-train-75373", "mrqa_squad-train-33301", "mrqa_squad-train-72546", "mrqa_squad-train-17350", "mrqa_squad-train-975", "mrqa_squad-train-29149", "mrqa_squad-train-84912", "mrqa_squad-train-68765", "mrqa_squad-train-28090", "mrqa_squad-train-71381", "mrqa_squad-train-63258", "mrqa_squad-train-16654", "mrqa_squad-train-10495", "mrqa_naturalquestions-validation-3902", "mrqa_hotpotqa-validation-3729", "mrqa_searchqa-validation-2462", "mrqa_newsqa-validation-1911", "mrqa_triviaqa-validation-5184", "mrqa_triviaqa-validation-7151", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7203", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-873", "mrqa_searchqa-validation-10318", "mrqa_searchqa-validation-1784", "mrqa_squad-validation-627", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-1598"], "EFR": 0.9615384615384616, "Overall": 0.6994282280219781}, {"timecode": 70, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5707", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4037", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-938", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1065", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-181", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1930", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-240", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2510", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2688", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3403", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-3962", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-14273", "mrqa_searchqa-validation-14346", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5339", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7285", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8710", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-10306", "mrqa_squad-validation-111", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2365", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-275", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-2942", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4001", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-4797", "mrqa_squad-validation-4908", "mrqa_squad-validation-5003", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-5470", "mrqa_squad-validation-5617", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6546", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-7918", "mrqa_squad-validation-7958", "mrqa_squad-validation-8149", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8575", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3790", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-495", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.865234375, "KG": 0.48203125, "before_eval_results": {"predictions": ["Chris Sarandon", "March 26, 1973", "Abanindranath Tagore CIE", "scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface ( particularly caveolae internalization ) as well as at the Gol", "Lagaan : Once Upon a Time in India", "Super Bowl XXXIX", "almost exclusively land based powers, able to summon large land armies that were very nearly invincibleable", "September 2017", "Kanawha River", "12.65 m", "the 1820s", "the customer's account", "his cousin D\u00e1in", "alternative rock", "volcanic and sedimentary rock sequences ( magnetostratigraphy )", "prison", "Supreme Court of Canada", "July 1, 1923", "an earthquake", "October 2008", "4 January 2011", "Yul Brynner", "mainly part of Assam and Meghalaya", "approximately 1,070 km ( 665 mi ) east - southeast of Cape Hatteras, North Carolina", "Kirsten Simone Vangsness", "Frankie Laine's `` I Believe", "between 1765 and 1783", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "the 2002 Tamil film Ramanaa", "RAF Coningsby in Lincolnshire", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha", "De statua", "more than 2,500 locations", "1919", "September 19, 1977", "Augustus Waters, an ex-basketball player and amputee", "Ferrari", "Tiger Woods", "2018", "Speaker of the House of Representatives", "the final scene of the fourth season", "Lord's", "a mid-size four - wheel drive luxury Volvo", "Ingrid Bergman", "Malayalam", "Hem Chandra Bose, Azizul Haque", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "The terrestrial biosphere", "Jack ( Billy Bob Thornton ) and Jill ( Amy Sedaris )", "Austria - Hungary", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "eyes", "Vietnam", "Jason Voorhees", "Canadian", "Robert Jenrick", "Srinagar", "all faiths", "the Dalai Lama's", "over 127 acres.", "Crawford", "the Blue Ridge Mountains", "Olivia", "electric currents and magnetic fields"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5878155219407617}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.9538461538461539, 0.7058823529411764, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.21739130434782608, 0.5454545454545454, 1.0, 0.6666666666666665, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.9142857142857143, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-4771", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-52", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-1586", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7605", "mrqa_triviaqa-validation-1122", "mrqa_triviaqa-validation-949", "mrqa_hotpotqa-validation-3566", "mrqa_hotpotqa-validation-2134", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-2630", "mrqa_searchqa-validation-9049", "mrqa_hotpotqa-validation-820"], "SR": 0.46875, "CSR": 0.5338908450704225, "EFR": 0.7941176470588235, "Overall": 0.6889610734258491}, {"timecode": 71, "before_eval_results": {"predictions": ["William Wyler", "Megyn Price", "National Football League ( NFL )", "the following day", "Conservative Party", "Judi Dench", "his servant M'ling, and the Sayer of the Law", "six degrees of freedom", "Spanish moss", "Matt Monro", "1990", "Friedman Billings Ramsey", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "drivers who meet more exclusive criteria", "Charles Carroll", "1959", "indigenous to many forested parts of the world", "Hermia", "in and around an unnamed village", "Bart Millard", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Super Bowl XIX", "2007", "Toto", "V\u1e5bksayurveda", "the 15th century", "Hasmukh Adhia", "16.5 quadrillion BTUs", "Benzodiazepines", "April 1, 2016", "its absolute temperature", "eTC )", "April 26, 2005", "Russia", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "1994", "2018", "Phosphorus pentoxide", "a cake", "1890", "a violation of nature and the resulting psychological effects on the mariner and on all those who hear him", "Ray Harroun", "Ethel Robinson", "Bonnie Aarons", "Fusajiro Yamauchi", "Manchuria", "Henry Purcell", "the base of the right ventricle", "Steve Russell", "August 21", "1799", "Italian, Spanish", "Zachary Taylor", "Wilde", "S6 Edge", "The New Yorker", "Citgo Petroleum Corporation", "school in South Africa", "Jenny Sanford,", "Rolling Stone.", "nuggets", "Mr. Smith Goes to Washington", "Fergie", "Forrest Gump"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6726662181020734}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.25, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.19999999999999998, 0.4, 0.3157894736842105, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-894", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4196", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-6049", "mrqa_triviaqa-validation-3298", "mrqa_triviaqa-validation-3623", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5766", "mrqa_newsqa-validation-3376", "mrqa_searchqa-validation-10641"], "SR": 0.546875, "CSR": 0.5340711805555556, "EFR": 0.9310344827586207, "Overall": 0.7163805076628352}, {"timecode": 72, "before_eval_results": {"predictions": ["pigs", "Michael Edwards", "Toby Keith", "General George Washington", "Andr\u00e9 Le N\u00f4tre", "Ed", "15 February 1998", "Diego Tinoco", "Bart Millard", "the BBC's loss of TV rights to ITV Sport in 1997", "Vasoepididymostomy", "Jonathan Harris", "Paul Lynde", "79", "President Lyndon Johnson", "16 seasons", "in 1999 the canal was taken over by the Panamanian government and is now managed and operated by the government - owned Panama Canal Authority", "John Brown", "nucleus", "Coroebus of Elis", "Carol Worthington", "the 17th episode in the third season", "Kansas City Chiefs", "Yuzuru Hanyu", "Owen Hunt", "Ceramic", "February 26, 2018", "Iran", "The alveolar process", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "not restricted to voting for one of the nominated candidates and may vote for any person, even for someone who is not a member of the House at all", "Lisa Stelly", "Holly", "Jetfire", "Rachel Kelly Tucker", "in 1881", "pneumonoultramicroscopicsilicovolcanoconiosis", "a forest", "Seton Hall Pirates", "13 episodes", "perhaps most common in Australia, but can occur at tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "R.D. Robb", "Japan", "Djokovic", "won gold in the half - pipe", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "before his 19th birthday", "Georgia Groome as Georgia Nic Nicholson : The main character, a 14 - year - old girl who falls in love with Robbie and tries to get him to be her boyfriend throughout the film", "Incudomalleolar joint", "London, United Kingdom", "the Attorney General", "a \"double rack of lamb\", with the ribs on both sides.", "Ross MacManus", "York", "Hamburger Sport-Verein e.V.", "2", "The Los Angeles Dance Theater", "100 meter", "President Sheikh Sharif Sheikh Ahmed", "Miami Beach, Florida,", "suntory", "wyatt", "a yoke", "video game"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6760697382401236}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.42857142857142855, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.7999999999999999, 0.0, 0.2352941176470588, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-250", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-9019", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-4194", "mrqa_triviaqa-validation-2333", "mrqa_hotpotqa-validation-1572", "mrqa_newsqa-validation-3181", "mrqa_searchqa-validation-13611", "mrqa_hotpotqa-validation-1074"], "SR": 0.59375, "CSR": 0.534888698630137, "retrieved_ids": ["mrqa_squad-train-6325", "mrqa_squad-train-69193", "mrqa_squad-train-33698", "mrqa_squad-train-79135", "mrqa_squad-train-79935", "mrqa_squad-train-24279", "mrqa_squad-train-63479", "mrqa_squad-train-23459", "mrqa_squad-train-9825", "mrqa_squad-train-24502", "mrqa_squad-train-45853", "mrqa_squad-train-78413", "mrqa_squad-train-54648", "mrqa_squad-train-3331", "mrqa_squad-train-42596", "mrqa_squad-train-26524", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-8441", "mrqa_newsqa-validation-3517", "mrqa_naturalquestions-validation-4097", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2198", "mrqa_searchqa-validation-5510", "mrqa_hotpotqa-validation-486", "mrqa_newsqa-validation-1445", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-1452", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-2946", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-1317", "mrqa_newsqa-validation-1967"], "EFR": 0.8846153846153846, "Overall": 0.7072601916491043}, {"timecode": 73, "before_eval_results": {"predictions": ["Robin", "by January 2018", "Patrick Swayze", "Martin Lawrence", "revenge and karma", "October 1986", "Disha Vakani", "usually restricted to the lower motor neurons, the efferent nerves that directly innervate muscles", "Johannes Gutenberg", "Shawn Wayans", "The United States of America ( USA ), commonly known as the United States ( U.S. ) or America ( / \u0259\u02c8m\u025br\u026ak\u0259 / )", "A regulatory site", "3", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "Woodrow Wilson", "Jeff East", "Terry Reid", "Brazil", "March 31 to April 8, 2018", "American Indian allies", "radius R of the turntable", "the Royal Air Force ( RAF ) defended the United Kingdom ( UK ) against large - scale attacks by Nazi Germany's air force, the Luftwaffe", "1945", "CeCe Drake", "April 12, 2017", "post translational modification", "1960", "naturalization law", "September 6, 2019", "Bulgaria", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Coldplay with special guest performers Beyonc\u00e9 and Bruno Mars", "save, rescue, savior", "1983", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "Werner Ruchti", "Brooklyn, New York", "British singer - songwriter Chris Rea", "Julie Adams", "pneumonoultramicroscopicsilicovolcanoconiosis", "2010", "General George Washington", "Mary Elizabeth ( Margaret Hoard )", "Michelangelo", "1,350", "Uruguay", "to ordain presbyters / bishops and to exercise general oversight", "All the world's a stage", "2002", "Anna Faris and Allison Janney", "Cress", "Montr\u00e9al", "Prince Edward, Duke of Kent", "Gerald Ford", "Bank of China Tower", "Tata Consultancy Services in Kochi", "Corendon Dutch Airlines", "Jenny Sanford,", "to alert patients of possible tendon ruptures and tendonitis.", "a particular health ailment or beauty concern.", "Herbert Hoover", "the White Nile", "a compound", "Pearl Jam"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7095595529878618}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5, 1.0, 0.7142857142857143, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-1910", "mrqa_triviaqa-validation-3448", "mrqa_triviaqa-validation-6593", "mrqa_triviaqa-validation-5000", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-26", "mrqa_hotpotqa-validation-1640", "mrqa_searchqa-validation-1415", "mrqa_searchqa-validation-15202"], "SR": 0.578125, "CSR": 0.535472972972973, "EFR": 0.9259259259259259, "Overall": 0.7156391547797798}, {"timecode": 74, "before_eval_results": {"predictions": ["the Legion of Honor", "a wheel", "(the nonvirtual, paper kind)", "berenice", "pharaoh", "Tony Dungy", "the Rolling Stones", "opera", "a chili pepper", "a cell", "president", "60", "the enigma", "a tornado", "a play", "lord Tennyson", "Laryngitis", "the bear", "terraces", "a voodoo sorcerer", "aquiline", "Hair", "a cozy", "Jalisco state", "Davenport", "Sammy Sosa", "Suzuki", "(0.05 g)", "othello", "Pindus National Park", "haematoma", "the four horsemen", "a rattlesnake", "General William Tecumseh Sherman", "Fess Parker", "a duvet", "Baltimore", "the Freshwater", "Japan", "fraternit", "the country of the world", "Wrigley", "Nepal", "USDA", "cat scratch fever", "freezing", "(Kim) Arbus", "the right to a fair trial", "the Snickers Cruncher", "the Rock and Roll Hall of Fame", "(Prince) Albert", "pigs", "between the Eastern Ghats and the Bay of Bengal", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "lord melbourne", "Sororicide", "saint aidan", "Sulla", "the Appenzell Alps", "Parlophone Records", "keyboardist and", "150", "a real person to talk to", "the contestant"], "metric_results": {"EM": 0.4375, "QA-F1": 0.485383064516129}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806]}}, "before_error_ids": ["mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-10139", "mrqa_searchqa-validation-12924", "mrqa_searchqa-validation-13908", "mrqa_searchqa-validation-11800", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1795", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-16428", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-13285", "mrqa_searchqa-validation-8968", "mrqa_searchqa-validation-14672", "mrqa_searchqa-validation-4272", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-8248", "mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-6289", "mrqa_searchqa-validation-1214", "mrqa_searchqa-validation-7585", "mrqa_searchqa-validation-10978", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-14159", "mrqa_searchqa-validation-976", "mrqa_searchqa-validation-5900", "mrqa_searchqa-validation-14189", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-1931", "mrqa_hotpotqa-validation-4525", "mrqa_newsqa-validation-1890", "mrqa_naturalquestions-validation-5636"], "SR": 0.4375, "CSR": 0.5341666666666667, "EFR": 1.0, "Overall": 0.7301927083333333}, {"timecode": 75, "before_eval_results": {"predictions": ["8 Mile", "(Prince) Stanley", "Louisiana", "a rabbit burrow", "Tombs of Kobol", "The Sound and the Fury", "a sandwich", "seven", "Cosmo Kramer", "Poetic Justice", "the guillitine", "Colossus of Rhodes", "(Hugh) Jackman", "silver", "Lebanon", "the eagle", "The CPC", "(Larry) King", "(King) Claudius", "Mussolini", "( Margot) Fonteyn", "( Alfred) Nobel", "lifejackets", "exterus", "General Mills", "Emmitt Smith", "a statue", "a black hole", "Uganda", "Committee on Agriculture", "Heisenberg", "Sin City:", "(David) Hyde Pierce", "HAYDN", "Old North Church", "spinal column", "Red Bull", "Jolly Roger", "Canada", "Alaska", "the Electric Company", "Vienna", "the City of Bridgeport, Connecticut", "Red", "a fig", "( Ellen) Wilson", "Esau", "the skull", "Agatha Christie", "( Ronald) Reagan", "Ford Motor Company", "1947", "American actress Moira Kelly", "Zoe Zebra", "Mount Kenya", "Christian Wulff", "Mata Hari", "Princess Aisha bint Hussein", "French", "Oxford", "Kaka", "133", "(Gunther) von Hagens", "Minnesota"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6336538461538461}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-9504", "mrqa_searchqa-validation-666", "mrqa_searchqa-validation-4053", "mrqa_searchqa-validation-14575", "mrqa_searchqa-validation-3276", "mrqa_searchqa-validation-2478", "mrqa_searchqa-validation-452", "mrqa_searchqa-validation-15327", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-4447", "mrqa_searchqa-validation-11404", "mrqa_searchqa-validation-899", "mrqa_searchqa-validation-2164", "mrqa_searchqa-validation-9179", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-10070", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-5450", "mrqa_searchqa-validation-14546", "mrqa_searchqa-validation-11498", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-7703", "mrqa_searchqa-validation-6857", "mrqa_naturalquestions-validation-6349", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-3364"], "SR": 0.5625, "CSR": 0.5345394736842105, "retrieved_ids": ["mrqa_squad-train-37546", "mrqa_squad-train-29122", "mrqa_squad-train-46884", "mrqa_squad-train-55264", "mrqa_squad-train-76815", "mrqa_squad-train-34360", "mrqa_squad-train-84192", "mrqa_squad-train-40862", "mrqa_squad-train-26227", "mrqa_squad-train-48188", "mrqa_squad-train-1706", "mrqa_squad-train-36191", "mrqa_squad-train-63500", "mrqa_squad-train-34", "mrqa_squad-train-37651", "mrqa_squad-train-66735", "mrqa_newsqa-validation-3015", "mrqa_searchqa-validation-10116", "mrqa_newsqa-validation-1077", "mrqa_triviaqa-validation-7343", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-3987", "mrqa_newsqa-validation-1699", "mrqa_triviaqa-validation-5913", "mrqa_newsqa-validation-609", "mrqa_naturalquestions-validation-3993", "mrqa_hotpotqa-validation-721", "mrqa_naturalquestions-validation-7356", "mrqa_searchqa-validation-2175", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-2677", "mrqa_searchqa-validation-9777"], "EFR": 1.0, "Overall": 0.7302672697368421}, {"timecode": 76, "before_eval_results": {"predictions": ["diabetes", "Wynton", "the Department of the Treasury", "Montserrat", "a cyclone", "the Starland Vocal Band", "the gallows", "ohm", "Roll of Thunder, Hear My Cry", "earthquakes", "the Potomac", "Oregon", "Mary Stuart", "Hulk Hogan", "air pressure", "Russia", "Adam Sandler", "Bob Costas", "David letterman", "Macbeth", "Erin Go Bragh", "Lake Victoria", "Thanksgiving", "a sack dress", "It\\'s a Small World,", "the Fore River Shipyard", "Capitol Hill", "a glider", "a heart", "Guyana", "jelly", "camels", "drought", "ex post facto", "Jonathan Winters", "Pink", "Rhode Island", "Newton", "the World", "Smith", "Roosevelt", "gold", "Joshua", "Jamestown", "a coal", "Seymour Cray", "Private Practice", "corticosteroids", "Georgetown", "cinnamon", "Beowulf", "Experimental neuropsychology", "pigs", "Nickelback", "Neptune", "Scotland", "yellow", "chalk quarry", "SBS", "Eternal Flame", "Tomas Olsson,", "71 percent of Americans consider China an economic threat to the United States,", "Appathurai", "GABA"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7744791666666666}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7433", "mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-5641", "mrqa_searchqa-validation-11726", "mrqa_searchqa-validation-4666", "mrqa_searchqa-validation-6634", "mrqa_searchqa-validation-7238", "mrqa_searchqa-validation-11245", "mrqa_searchqa-validation-14096", "mrqa_searchqa-validation-10473", "mrqa_searchqa-validation-4499", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-16881", "mrqa_searchqa-validation-15581", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-7095", "mrqa_triviaqa-validation-7732", "mrqa_hotpotqa-validation-512", "mrqa_naturalquestions-validation-4442"], "SR": 0.703125, "CSR": 0.5367288961038961, "EFR": 1.0, "Overall": 0.7307051542207792}, {"timecode": 77, "before_eval_results": {"predictions": ["Leif Ericson", "Inuit", "the Duchy of Savoy", "Billy the Kid", "Rudyard Kipling", "... Frasier Crane", "Tarzan", "Edward VI", "Leon Trotsky", "Belgium", "... Wendy Beckett", "1066", "ibuprofen", "vrijbuiter", "Carver", "the Bulldog Drummond", "...The Scarlet Letter", "the Beas Valley", "the Baltic Sea", "...nolo contendere", "gum", "Abel", "Louis XV", "Wayne Gretzky", "Anna Karenina", "Sacramento", "Cordillera Blanca", "jurors", "...Sigmund Freud", "Pantaloons", "Buddha", "Paul Newman", "...Harry S. Truman", "...Tumblers", "Rhode Island", "...The Simple Life", "Laos", "... Agent Orange", "the Philippines", "Kellogg's", "...The Backstreet Boys", "Cairo", "Latin", "Venus", "the Hawthorne", "the Congo River", "the French throne", "... Horatio Nelson,", "a caiman", "Ferrari", "iris", "John Adams", "1886", "Ali", "Chicago", "World War I", "Hedonismbot", "ESPN College Football Friday Primetime", "R&B vocal group", "Memphis Minnie's \"When the Levee Breaks\"", "little blue booties.", "Diego Maradona", "officers at a Texas  airport", "silver"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5738095238095238}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.8571428571428571, 0.20000000000000004, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5181", "mrqa_searchqa-validation-7100", "mrqa_searchqa-validation-918", "mrqa_searchqa-validation-13061", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-15855", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-13555", "mrqa_searchqa-validation-9720", "mrqa_searchqa-validation-1015", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-11373", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-11115", "mrqa_searchqa-validation-5367", "mrqa_searchqa-validation-7197", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-5637", "mrqa_triviaqa-validation-3370", "mrqa_triviaqa-validation-4449", "mrqa_hotpotqa-validation-3307", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5319", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-385"], "SR": 0.4375, "CSR": 0.5354567307692308, "EFR": 1.0, "Overall": 0.7304507211538461}, {"timecode": 78, "before_eval_results": {"predictions": ["Romulus", "March", "Christmas Eve", "The Firm", "Schwalbe", "circumnavigate", "Marilyn Monroe", "Colby", "a comet", "wings", "the Enigma", "surface-to-air missile", "the Inuit", "Phobos", "a dermatologist", "Kramer vs. Kramer", "The Tempest", "yellow", "Annie's Song", "tire", "Schwarzenegger", "Lafayette", "(Kim) Murdoch", "triathlon", "Swahili", "the National Hockey League", "trenlin", "tte", "tombs", "The Thousand and Second Tale of Scheherazade", "Scott McClellan", "Jeremiah", "Thomas Edison", "The Tony Awards", "Guadalajara", "Sydney", "flavor", "Dutchman", "Gideon v. Wainwright", "the Alamo", "oats", "Ronaldo", "tuition", "Rush", "being buried alive", "Swan", "Kansas Jayhawks", "Helsinki", "the kidney", "One Flew Over the Cuckoo's Nest", "the Nobel Prize in Literature", "non-ferrous", "Brooke Wexler", "Rosalind Bailey", "Standard Motor Company", "Portugal", "cooperative", "Double Agent", "Juan Manuel Mata Garc\u00eda", "Madeleine L'Engle", "British troops", "three", "$3 billion,", "Tom Ewell"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6565746753246753}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15817", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-4568", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-11817", "mrqa_searchqa-validation-11927", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-11559", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-4421", "mrqa_searchqa-validation-2707", "mrqa_searchqa-validation-3174", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-15178", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-8681", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-8766", "mrqa_searchqa-validation-5300", "mrqa_triviaqa-validation-5933", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-3010"], "SR": 0.578125, "CSR": 0.535996835443038, "retrieved_ids": ["mrqa_squad-train-62165", "mrqa_squad-train-78713", "mrqa_squad-train-31202", "mrqa_squad-train-46539", "mrqa_squad-train-5415", "mrqa_squad-train-14839", "mrqa_squad-train-31980", "mrqa_squad-train-68536", "mrqa_squad-train-66676", "mrqa_squad-train-47125", "mrqa_squad-train-10672", "mrqa_squad-train-20764", "mrqa_squad-train-20202", "mrqa_squad-train-69520", "mrqa_squad-train-33291", "mrqa_squad-train-79276", "mrqa_newsqa-validation-1068", "mrqa_naturalquestions-validation-7896", "mrqa_searchqa-validation-2029", "mrqa_newsqa-validation-2456", "mrqa_searchqa-validation-11800", "mrqa_naturalquestions-validation-2319", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-1636", "mrqa_naturalquestions-validation-10615", "mrqa_triviaqa-validation-2542", "mrqa_newsqa-validation-1271", "mrqa_naturalquestions-validation-3342", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2824", "mrqa_squad-validation-809", "mrqa_naturalquestions-validation-794"], "EFR": 1.0, "Overall": 0.7305587420886076}, {"timecode": 79, "before_eval_results": {"predictions": ["Wyandotte County", "sport", "Peter", "a puppy", "New Zealand", "fontanels", "Southern California", "Nero", "the Dalmatians", "Daniel Day Lewis", "cotton", "Bridget Fonda", "South Africa", "Punk", "the Mediterranean", "Catherine", "bacon", "the Adder", "a puzzle", "the River Thames", "(PIE) FLINGING", "Pitcairn", "Adam Sandler", "Mayo", "\" Shut up, just shut up\"", "arrested Development", "renaissance", "Indo-European", "Rodeo", "repent", "Denzel Washington", "Bonn", "nougat", "(Kim) Erdman", "rani", "Louis Comfort Tiffany", "Louise", "conk", "Hillary Clinton", "globalization", "Van Halen", "the Rhine", "salt", "Samsonite", "chili", "salaam", "Michael Faraday", "pearls", "Norse", "Niagara Falls", "the Bronx", "the National Football League ( NFL ) for the Atlanta Falcons, the San Francisco 49ers, the Dallas Cowboys, the Washington Redskins and the BaltimoreNFL", "Ethel Merman", "Forbes Burnham", "Denmark", "Angus Deayton", "Spain", "Russian Ark", "\"The Walking Dead\"", "237", "over two decades.", "does not", "14", "8th and 16th"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6300016534391534}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2962962962962963, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_searchqa-validation-6416", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-11741", "mrqa_searchqa-validation-3736", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-4188", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-6628", "mrqa_searchqa-validation-5786", "mrqa_searchqa-validation-5794", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6344", "mrqa_searchqa-validation-7343", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-15283", "mrqa_searchqa-validation-4080", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-13908", "mrqa_naturalquestions-validation-4544", "mrqa_naturalquestions-validation-8433", "mrqa_newsqa-validation-1430", "mrqa_hotpotqa-validation-3765"], "SR": 0.53125, "CSR": 0.5359375, "EFR": 0.9666666666666667, "Overall": 0.7238802083333333}, {"timecode": 80, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10383", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8043", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2229", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-264", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-346", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-615", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-77", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-861", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10105", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10303", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-1200", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13051", "mrqa_searchqa-validation-13295", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14189", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-1615", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4583", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5190", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7702", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8343", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9733", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-192", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-245", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6334", "mrqa_squad-validation-6393", "mrqa_squad-validation-641", "mrqa_squad-validation-6548", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7751", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-883", "mrqa_squad-validation-8869", "mrqa_squad-validation-9110", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2858", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.880859375, "KG": 0.50625, "before_eval_results": {"predictions": ["(George Washington)", "the National Hockey League (NHL)", "blue", "Georgia", "Major General William Devereaux", "scalpels", "the English Channel", "Shakespeare", "a phonological sign", "Thornton Wilder", "Baton Rouge", "a cupboard", "a frittata", "pardon", "Bartholomew", "myelogenous leukemia", "Target", "Regrets", "a possum", "\"Three\"", "Pamplona", "Easter Island", "Frans", "Madonna", "drought", "a staycation", "it is best not to take risks even when it seems boring or difficult", "Makkedah", "Yogi Bear", "Idaho", "(Margaret) Wood", "Highway lanes", "1215", "(Benjamin) Harrison", "a skyscraper", "(Billy the Kid)", "The Killing Fields", "Oliver Twist", "a landmark", "lamb", "bread", "Boston", "Martinique", "10-20 million", "the Grand Canal", "the Sons of Liberty", "a telescope", "Catholic", "a tuba", "football", "a square", "Nicole Gale Anderson", "`` Goodbye Toby ''", "1986", "Charles II", "16", "dragonflies", "acidic", "sixth studio album", "\"Twice in a Lifetime\"", "11-month-old Lisa Irwin was reported missing.", "Dean Martin, Katharine Hepburn and Spencer Tracy", "2006,", "attempted burgl stemming from a fatal encounter with police officer Daniel Enchautegui."], "metric_results": {"EM": 0.671875, "QA-F1": 0.7029246794871795}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-11868", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-7936", "mrqa_searchqa-validation-2356", "mrqa_searchqa-validation-9591", "mrqa_searchqa-validation-9229", "mrqa_searchqa-validation-16593", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-2069", "mrqa_searchqa-validation-9061", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-2804", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-224", "mrqa_triviaqa-validation-4590", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-2839"], "SR": 0.671875, "CSR": 0.5376157407407407, "EFR": 1.0, "Overall": 0.7361168981481481}, {"timecode": 81, "before_eval_results": {"predictions": ["phylum", "Warsaw", "Katrina & the Waves", "the French and Indian War", "Brady", "philosophy", "the American Red Cross", "harm", "Bonnie Raitt", "Titanic", "pickles", "Artemis", "dendrites", "Evian", "a geese", "The Mayor of Casterbridge", "the olfactory nerve", "a window", "Isaac Newton", "SpeedMatch", "Harriet Tubman", "the Colorado", "Dune", "an opera", "YouTube", "heresy", "comedy", "Charlie Watts", "a black widow spider", "a button", "Virginia", "abundant", "Albert Schweitzer", "the right hemisphere", "a dive bomber", "Toulouse-Lautrec", "Helen Hayes", "dada", "clamorous", "Wells", "\"Sex In Crazy Places\"", "Bill & Melinda Gates", "Hippopotamus", "Nietzsche", "a dog eat dog", "Alexander Hamilton", "American", "Niagara Falls", "a stern", "carrots", "the Flintstones", "Abanindranath Tagore", "at slightly different times when viewed from different points on Earth", "thoracic", "Carrefour", "Obama", "milk", "Todd Phillips", "Jeff Brannigan", "Bharat Ratna", "Joe Pantoliano", "national telephone", "Christian Film & Television Commission", "Quentin Tarantino"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6091314935064935}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.060606060606060615, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12749", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-10407", "mrqa_searchqa-validation-14139", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-1380", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-4889", "mrqa_searchqa-validation-6205", "mrqa_searchqa-validation-1775", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-9621", "mrqa_searchqa-validation-12904", "mrqa_searchqa-validation-4772", "mrqa_searchqa-validation-11719", "mrqa_searchqa-validation-2199", "mrqa_searchqa-validation-3980", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-8543", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-1250", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3173", "mrqa_triviaqa-validation-6193", "mrqa_hotpotqa-validation-3846", "mrqa_newsqa-validation-175", "mrqa_triviaqa-validation-5750"], "SR": 0.46875, "CSR": 0.5367759146341464, "retrieved_ids": ["mrqa_squad-train-33073", "mrqa_squad-train-35347", "mrqa_squad-train-26163", "mrqa_squad-train-36339", "mrqa_squad-train-71260", "mrqa_squad-train-22292", "mrqa_squad-train-23125", "mrqa_squad-train-22887", "mrqa_squad-train-44692", "mrqa_squad-train-26545", "mrqa_squad-train-82897", "mrqa_squad-train-64804", "mrqa_squad-train-38527", "mrqa_squad-train-14659", "mrqa_squad-train-49940", "mrqa_squad-train-66406", "mrqa_searchqa-validation-8619", "mrqa_searchqa-validation-2863", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3796", "mrqa_naturalquestions-validation-5554", "mrqa_searchqa-validation-16971", "mrqa_newsqa-validation-1963", "mrqa_triviaqa-validation-6684", "mrqa_searchqa-validation-3405", "mrqa_triviaqa-validation-2431", "mrqa_newsqa-validation-2812", "mrqa_hotpotqa-validation-1812", "mrqa_searchqa-validation-13939", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-4199"], "EFR": 0.9705882352941176, "Overall": 0.7300665799856528}, {"timecode": 82, "before_eval_results": {"predictions": ["Caesar", "The Big Easy", "the beaver", "Dorothy", "Survivor: Fiji", "The Wild Wild West", "Rudolf Nureyev", "Wilbur", "Maine", "Anne Hathaway", "Calvin Klein cologne", "Marvell", "Quiz Show", "the \"BCS Championship Game\"", "acetone", "Donald Trump", "Psycho", "Napoleon", "a lullaby", "a capuchins", "Napoleon", "the West of Africa", "the reticulated snake", "Munich", "a digestif", "a strabeculectomy", "Pope Benedict XVI", "Los Alamos Scientific Laboratory", "Somerset Maugham", "sapphire", "Three Coins in the Fountain", "\"ER\"", "the Goldenrod", "Luke", "the distal colon", "a neck warmer or scarf", "frequency", "Grease", "the salamander", "Alexander Solzhenitsyn", "Eyebrows", "le Roman de la Rose", "Guyana", "Charlie Bartlett", "Vanity Fair", "the Big Sky Conference", "the beavers", "Boston", "Michelle Pfeiffer", "a ruckus", "Sweden", "Ajay Tyagi", "April 28, 2008", "94 by 50", "Salix", "Dm", "daltsire", "Karl- Anthony Towns", "\"Rock You Like a Hurricane\"", "1988", "Hollywood", "processing data, requiring that all flight-plan information be processed", "$10 billion", "the front-seat passenger in the Mercedes that carried Diana, her boyfriend, Dodi Fayed, and their driver, Henri Paul."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6272343975468975}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_searchqa-validation-4041", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-6067", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-7336", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-1599", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-2271", "mrqa_searchqa-validation-4093", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-13719", "mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-1409", "mrqa_triviaqa-validation-2343", "mrqa_triviaqa-validation-1711", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1561", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2958"], "SR": 0.515625, "CSR": 0.5365210843373494, "EFR": 1.0, "Overall": 0.7358979668674699}, {"timecode": 83, "before_eval_results": {"predictions": ["the East Sea", "(Stitch)", "( Joe) Torre", "a kettledrum", "P.G. Wodehouse", "Santa Fe", "Christian", "cinnamon", "I Am the Very Model of a Modern Major-General", "Logic", "St. Patrick's Day", "beer", "Wall Street", "Nathaniel Hawthorne", "Trinity College", "Geneva", "Apollo", "troll", "(The Flying Dutchman)", "(Dan) Quayle", "Naomi", "Answer Who is", "Nothing without Providence", "a phaser", "Dylan Thomas", "Lincoln", "Crank Yankers", "the stratosphere", "Paul McCartney", "Juno", "distressing", "Mercury", "the Mad Hatter", "Kiribati", "Nepal", "Palladio", "names", "Indiana", "Hair", "cicadas", "Asbury Park", "( Shelley)", "the saguaro", "(Jimmy) Zappa", "hip hop", "Federico Fellini", "dampers", "Sirius", "onomatopoeia", "a loaf of bread", "Portugal", "Long Island", "lifetime", "Glynis Johns", "Porridge", "Thermopylae", "penitentiary", "\"$10,000 Kelly,\"", "\u00c6thelwald Moll", "Lord Cavendish", "60 euros", "Prince George's County Correctional Center,", "Kurdistan Freedom Falcons,", "1937"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7127604166666666}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-3409", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-12018", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-1991", "mrqa_searchqa-validation-401", "mrqa_searchqa-validation-2881", "mrqa_searchqa-validation-11315", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-8061", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16253", "mrqa_searchqa-validation-8399", "mrqa_searchqa-validation-15055", "mrqa_triviaqa-validation-1199", "mrqa_hotpotqa-validation-4204", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-1509"], "SR": 0.640625, "CSR": 0.5377604166666667, "EFR": 1.0, "Overall": 0.7361458333333334}, {"timecode": 84, "before_eval_results": {"predictions": ["the typing speed", "the crescent", "a trident", "Abercrombie & Fitch", "Jefferson", "Standard Oil", "Crustacean", "Laura Ingalls Wilder", "a carriage", "Monet", "tires", "Gerald R. Ford", "Louis Rukeyser", "Jupiter", "Clinton", "tongues", "Tin", "Stephen Hawking", "Kilimanjaro", "Munich", "London", "Nunavut", "Georgia Bulldogs", "Giacomo Puccini", "abbreviated", "Heroes", "cramps", "Kublai Khan", "Lafitte", "the Flushing River", "a relic", "cyclosporine", "the Northern Mockingbird", "a RESTRICTIVE TYPE OF THIS, CLAUSE", "comedy", "a owl", "the perimeter", "60 Minutes", "a terrarium", "Vulcan", "courage", "the narwhal", "Stephen Hawking", "a colony of seabirds", "Albert Camus", "Mexico", "Kleopatra", "Finding Nemo", "The Oresteia", "Scotland", "a star", "1924", "741 weeks", "January 17, 1899", "Gen. Douglas MacArthur", "Project Gutenberg", "Indonesia", "Latin American culture", "a farmers' co-op", "David Naughton, Jenny Agutter and Griffin Dunne", "\"Nothing But Love\"", "helping to plan the September 11, 2001, terror attacks,", "650", "$1.5 million."], "metric_results": {"EM": 0.6875, "QA-F1": 0.7680803571428572}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-511", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1633", "mrqa_searchqa-validation-15821", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-3331", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-6947", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-3199", "mrqa_searchqa-validation-9609", "mrqa_searchqa-validation-3503", "mrqa_naturalquestions-validation-4428", "mrqa_triviaqa-validation-3649", "mrqa_hotpotqa-validation-3921", "mrqa_newsqa-validation-3820"], "SR": 0.6875, "CSR": 0.5395220588235294, "retrieved_ids": ["mrqa_squad-train-18744", "mrqa_squad-train-50258", "mrqa_squad-train-43310", "mrqa_squad-train-51976", "mrqa_squad-train-58670", "mrqa_squad-train-68588", "mrqa_squad-train-3155", "mrqa_squad-train-52131", "mrqa_squad-train-85402", "mrqa_squad-train-45727", "mrqa_squad-train-12207", "mrqa_squad-train-764", "mrqa_squad-train-81752", "mrqa_squad-train-11347", "mrqa_squad-train-64072", "mrqa_squad-train-34058", "mrqa_searchqa-validation-10978", "mrqa_naturalquestions-validation-681", "mrqa_searchqa-validation-11817", "mrqa_hotpotqa-validation-4450", "mrqa_searchqa-validation-5457", "mrqa_naturalquestions-validation-9591", "mrqa_newsqa-validation-2199", "mrqa_squad-validation-6816", "mrqa_naturalquestions-validation-3770", "mrqa_newsqa-validation-3767", "mrqa_triviaqa-validation-7563", "mrqa_squad-validation-707", "mrqa_searchqa-validation-8272", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-562", "mrqa_searchqa-validation-9831"], "EFR": 1.0, "Overall": 0.736498161764706}, {"timecode": 85, "before_eval_results": {"predictions": ["archery", "Albright", "silver", "the Mummy", "the Washington Redskins", "asteroids", "Wilson", "The Prince & the Pauper", "Pushing Daisies", "July", "the Reaper", "Pearl Jam", "Candlemas", "apples", "Solomon", "Canada", "Lake County, Indiana", "Cleopatra", "a northern pike", "Krispy Kreme", "New York Luxury Real Estate", "Martin Luther", "rice", "Frasier", "Kansas City", "arteries", "\"Chinatown\"", "comedy", "Hamlet", "lime", "Antichrist", "Alkalinity", "Robert Duvall", "Joan of Arc", "abundance", "Crete", "Hitchcock", "Favre", "Their Eyes Were watching God", "Fiddler On The Roof", "Pitcairn Island", "hockey", "etching", "Mars", "bone", "David", "inflation", "a cookie jar", "Babe Ruth", "Steak sandwich", "Nicky Hilton", "he was unable to wrest", "2016", "Jessica Simpson", "William Schuman", "tree", "Robert Plant", "Oklahoma", "138,535 people", "Terence Winter", "her son has strong values.", "python", "Gustav", "\"A total of seven died on our property,\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.709415064102564}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.7692307692307693, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-6308", "mrqa_searchqa-validation-15912", "mrqa_searchqa-validation-765", "mrqa_searchqa-validation-5953", "mrqa_searchqa-validation-14943", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-4556", "mrqa_searchqa-validation-12891", "mrqa_searchqa-validation-14895", "mrqa_searchqa-validation-15209", "mrqa_searchqa-validation-9929", "mrqa_searchqa-validation-13581", "mrqa_searchqa-validation-8231", "mrqa_searchqa-validation-8377", "mrqa_searchqa-validation-6317", "mrqa_searchqa-validation-12173", "mrqa_naturalquestions-validation-9003", "mrqa_triviaqa-validation-533", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2753", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-2301"], "SR": 0.59375, "CSR": 0.5401526162790697, "EFR": 1.0, "Overall": 0.7366242732558139}, {"timecode": 86, "before_eval_results": {"predictions": ["a dishwasher", "Pulp Fiction", "Leo Tolstoy", "Louisiana", "The New Yorker", "president of Nicaragua", "Chastity", "Frank Sinatra", "Mendeleev", "Norman Mailer", "Blitzkrieg", "luminous intensity", "Tudor", "the Eurasian Economic Union", "Christina Ricci", "Jones", "The Rolling Stones", "Leigh Shahan", "Samuel A. Alito", "ships", "Civic", "Hermann Hesse", "Copernicus", "Jane Addams", "Paris", "a rail", "The Cat in the Hat", "Rich Girl", "Yogi Berra", "courage", "a jigger", "folate", "a constitution", "the eastern Mediterranean", "virtual reality", "a bass", "The Last Remake of Beau Geste", "hot air balloons", "Tarzan & Jane", "RBI", "(David) Berkowitz", "oblique", "a Space Odyssey", "Breed's Hill", "Sam Walton", "fritter", "the Spanish Republic", "Sweden", "Chicago", "The Matrix", "the Bolsheviks", "April 17, 1982", "the Garden of Gethsemane", "France", "James Cameron", "My Sweet Lord", "Japan", "Major Charles White Whittlesey", "Kingdom of Dalmatia", "Japan", "Monday.", "six", "Scotland", "Jacob Zuma,"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7557444852941175}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6099", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-14237", "mrqa_searchqa-validation-5240", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-1845", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-3534", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16576", "mrqa_searchqa-validation-7134", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-6355", "mrqa_hotpotqa-validation-4669"], "SR": 0.6875, "CSR": 0.5418462643678161, "EFR": 0.85, "Overall": 0.7069630028735633}, {"timecode": 87, "before_eval_results": {"predictions": ["Macbeth", "El burlador de Sevilla", "the Industrial Revolution", "oner", "the Nazi era", "Fargo", "The Language of Making Movies", "fibreboard", "the River Thames", "Napster", "the police series", "Coors Field", "Elizabeth I, the \"Virgin Queen,\"", "Wicked", "dementia", "the lightest interchangeable lens full-frame camera", "The lowest point", "the Golden Fleece", "the kingdom of God", "an interested party to a court, judge,... notice, advisement, alert, announcement, augury, caution, communication", "Macaulay Culkin", "the Tom Thumb locomotive", "John Edwards", "Hawaii", "John F. Kennedy", "Daniel Boone", "a taxi", "Hemoglobine", "Nancy Sinatra", "Swimmer's Ear", "the foxes", "tabby", "Amerigo Vespucci", "Wisconsin", "the Mediterranean states of the Persian Gulf", "Canada", "bipolar disorder", "a brownie", "the village clock", "Alexander Calder", "honey", "(Matthew) Broderick", "Christopher Columbus", "the Amazing Spider-Man", "Zyrtec", "a coyote", "Yahtzee", "Jerry Mathers", "The Midwestern United States", "an axiom", "electors", "in about 3.5 mya", "Tommy Shaw", "Mark Jackson", "All of the animal\u2019s blood", "albatrosses", "Meta", "Agent Carter", "Parthian Empire", "\"Kill Your Darlings\"", "The oceans are growing crowded,", "Iran", "Brett Cummins,", "Brown-Waite"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6387400793650794}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-5998", "mrqa_searchqa-validation-5774", "mrqa_searchqa-validation-535", "mrqa_searchqa-validation-5909", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-6484", "mrqa_searchqa-validation-873", "mrqa_searchqa-validation-1093", "mrqa_searchqa-validation-13560", "mrqa_searchqa-validation-14399", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-2933", "mrqa_searchqa-validation-12042", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-16734", "mrqa_searchqa-validation-11838", "mrqa_searchqa-validation-1792", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-10494", "mrqa_searchqa-validation-5640", "mrqa_searchqa-validation-5842", "mrqa_naturalquestions-validation-7027", "mrqa_triviaqa-validation-4384", "mrqa_triviaqa-validation-7725", "mrqa_newsqa-validation-4165"], "SR": 0.578125, "CSR": 0.5422585227272727, "retrieved_ids": ["mrqa_squad-train-80094", "mrqa_squad-train-22935", "mrqa_squad-train-6338", "mrqa_squad-train-63191", "mrqa_squad-train-82440", "mrqa_squad-train-76514", "mrqa_squad-train-37954", "mrqa_squad-train-23628", "mrqa_squad-train-8295", "mrqa_squad-train-40202", "mrqa_squad-train-19315", "mrqa_squad-train-2975", "mrqa_squad-train-78492", "mrqa_squad-train-41485", "mrqa_squad-train-52829", "mrqa_squad-train-5720", "mrqa_searchqa-validation-3554", "mrqa_searchqa-validation-12744", "mrqa_naturalquestions-validation-6333", "mrqa_triviaqa-validation-79", "mrqa_newsqa-validation-3370", "mrqa_squad-validation-627", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-222", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2684", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-8379", "mrqa_newsqa-validation-4144", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-11102"], "EFR": 1.0, "Overall": 0.7370454545454546}, {"timecode": 88, "before_eval_results": {"predictions": ["Cairo", "a booster seat", "B.I.G.", "Jesus", "John Paul II", "Nixon", "Ariel Sharon", "\"Rich Girl\"", "Macbeth", "James Strom Thurmond", "Windsor", "Armageddon", "yellow", "a gambler", "Sleepover", "Spain", "Scrabble", "the Caspian Sea", "Missouri", "Los Angeles Angels", "Cardiff", "the blacklist", "days", "go back into the water", "Graceland", "a telescope", "Nine to Five", "Dr. Hook & the Medicine Show", "the member who sits in the stern (except in bowloaders) facing the bow", "Transamerica", "China", "our nation", "the Delacorte", "Henry Clay", "a wire loop", "Petsmart", "On the Origin of Species", "Electric Avenue", "a glossary", "Jerusalem", "Vanna White", "Toyota", "a temenos", "Istanbul", "Fitzgerald", "Dixie", "Linkin Park", "Tycho Brahe", "Tudor", "the Knight", "an exorcism", "the following day", "1960s", "Taron Egerton", "a linesider", "Henry III", "The Undertones", "Groupe PSA", "Premier Division", "The SoLow Project", "stabbed Tate, who was 8\u00bd months pregnant,", "Herman Cain,", "a grizzly bear", "Harrison Ford"], "metric_results": {"EM": 0.5, "QA-F1": 0.5861979166666667}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.75, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10775", "mrqa_searchqa-validation-528", "mrqa_searchqa-validation-14245", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-8502", "mrqa_searchqa-validation-14886", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-8178", "mrqa_searchqa-validation-1656", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-8763", "mrqa_searchqa-validation-7301", "mrqa_searchqa-validation-8732", "mrqa_searchqa-validation-2831", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5542", "mrqa_searchqa-validation-13919", "mrqa_searchqa-validation-7826", "mrqa_searchqa-validation-10215", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-1225", "mrqa_searchqa-validation-5388", "mrqa_searchqa-validation-5520", "mrqa_searchqa-validation-3053", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-4664", "mrqa_naturalquestions-validation-844", "mrqa_triviaqa-validation-1404", "mrqa_triviaqa-validation-6545", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5468", "mrqa_newsqa-validation-3714"], "SR": 0.5, "CSR": 0.5417837078651686, "EFR": 1.0, "Overall": 0.7369504915730338}, {"timecode": 89, "before_eval_results": {"predictions": ["ermine", "Nemo", "easel", "a state of resting after exertion or strain", "Lewis and Clark", "Erica Kane", "Henry VIII", "Seattle", "England", "Denmark", "the saguaro", "Saigon", "Shintoism", "reshit", "Venus", "an iris", "(Kim) Parker", "Armistice", "toilet paper", "the Panama Canal", "Cesare Borgia", "pearl", "cognac", "Hangman", "Bleak House", "October", "Camptown", "(George Bernard) Shaw", "Linkin Park", "dogie", "wind", "lungs", "gravity", "\"Here, let me get out my wallet\"", "Robert I", "Marlon Brando", "Abraham Lincoln", "Lana Turner", "a bolt", "Othello", "Emiliano Zapata", "Bone Thugs-N-Harmony", "zebras", "Helio Castroneves", "Richard III", "Hugh Grant", "Godot", "voyeurism", "the Articles of Confederation", "Pavlov", "a hull", "Hot Wings", "Scandinavia", "James Madison", "Mission: Impossible \u2013 Rogue Nation", "Harriet Tubman", "hieroglyphic", "\" Finding Nemo\"", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe and Johnny Storm / Human Torch in \"Fantastic Four\" and", "Sam Raimi", "sniff out cell phones.", "forgery and flying without a valid license,", "Apple employees", "the Pir Panjal Range in Jammu and Kashmir"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7286022167487685}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3448275862068966, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-10034", "mrqa_searchqa-validation-10711", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-14958", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-3804", "mrqa_searchqa-validation-7463", "mrqa_searchqa-validation-9761", "mrqa_searchqa-validation-7480", "mrqa_searchqa-validation-15695", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-2383", "mrqa_searchqa-validation-1138", "mrqa_searchqa-validation-10008", "mrqa_naturalquestions-validation-8612", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-2737", "mrqa_hotpotqa-validation-881", "mrqa_newsqa-validation-2099"], "SR": 0.640625, "CSR": 0.5428819444444444, "EFR": 0.9565217391304348, "Overall": 0.7284744867149759}, {"timecode": 90, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5865", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3495", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7760", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1933", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2417", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3419", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-448", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-962", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11466", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-12947", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14395", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15869", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2832", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3682", "mrqa_searchqa-validation-3718", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4810", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-6074", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6490", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9020", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-938", "mrqa_searchqa-validation-9399", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9876", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-455", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-6393", "mrqa_squad-validation-7051", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.8671875, "KG": 0.51796875, "before_eval_results": {"predictions": ["Wisconsin", "the nose", "a stagecoach", "Henry Winkler", "faction & action", "Hasta la vista", "Indiana", "the guillotine", "bats", "Tunisia", "plexus", "a rattlesnake", "(Nicholas) Massie", "absinthe", "John F. Kennedy", "brakes", "Stonewall Jackson", "Captains Courageous", "Beyond the Sea", "\"AA\": Arabic for", "Catherine of Aragon", "flag", "Ravi Shankar", "Bangkok", "Spain", "archery", "slanting", "( Joe) Torre", "meatballs", "Kennedy Space Center", "the Rosetta Stone", "Pilate", "the United States", "Marco Polo", "the adder", "paddy", "Matt Leinart", "Alabama", "an ayahuasca", "Queen Anne Boleyn", "the banjo", "a major feature", "Lolita", "a coyote", "Graf Zeppelin", "Nirvana", "Frisbee", "Ceres", "Christopher Columbus", "prime", "Fi", "Telma Hopkins", "AD 95 -- 110", "pepsin", "Jorge Lorenzo", "1919", "Paris", "Point Place", "11", "enshrined at Dayton, Ohio, in the National Aviation Hall of Fame class of 2001", "Thursday", "78,000 parents of children ages 3 to 17.", "prisoners at the South Dakota State Penitentiary", "Anne Boleyn"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8187065972222223}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 0.875, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-4144", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-3808", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-15520", "mrqa_searchqa-validation-4692", "mrqa_searchqa-validation-7550", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-3063", "mrqa_naturalquestions-validation-2862", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3194"], "SR": 0.765625, "CSR": 0.5453296703296704, "retrieved_ids": ["mrqa_squad-train-51950", "mrqa_squad-train-13785", "mrqa_squad-train-2838", "mrqa_squad-train-44015", "mrqa_squad-train-74001", "mrqa_squad-train-68209", "mrqa_squad-train-54965", "mrqa_squad-train-53319", "mrqa_squad-train-15882", "mrqa_squad-train-23169", "mrqa_squad-train-10578", "mrqa_squad-train-7272", "mrqa_squad-train-80076", "mrqa_squad-train-56854", "mrqa_squad-train-83298", "mrqa_squad-train-84067", "mrqa_naturalquestions-validation-6022", "mrqa_searchqa-validation-6305", "mrqa_squad-validation-809", "mrqa_newsqa-validation-1148", "mrqa_searchqa-validation-2252", "mrqa_newsqa-validation-367", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3569", "mrqa_newsqa-validation-1157", "mrqa_naturalquestions-validation-10205", "mrqa_newsqa-validation-3247", "mrqa_searchqa-validation-8368", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-14868", "mrqa_naturalquestions-validation-9723", "mrqa_newsqa-validation-3861"], "EFR": 1.0, "Overall": 0.7372690590659341}, {"timecode": 91, "before_eval_results": {"predictions": ["Don Juan In Hell", "a Chile Relleno", "Oliver Twist", "Vampire Slayer", "Vistula", "Coriolanus", "Regency Energy Partners", "an aide-de-camp", "an oblique fracture", "Roman Polanski", "Court TV", "sharia", "Jake La Motta", "a journal", "Pan Am", "Athens", "Holiday Inn", "the Buffalo Bills", "Bret Harte", "Islam", "(Madeleine) Albright", "Turpan Pendi", "the Harlem Renaissance", "Calamity Jane", "John Lennon", "Ron Sandler", "Pudge", "daytime running lights", "Tarzan of the Apes", "Once", "Warren G. Harding", "Daniel & Philip", "Marilyn Monroe", "Icarus", "Flanders Field", "London", "Bonnie Raitt", "Man Friday", "Lord North", "Wrigley", "the euro", "a narwhal", "the wall", "a girlfriend", "Wyatt Earp", "Punjabi", "Macedonia", "Department of Agriculture", "heels", "Frottage", "a vertical angle", "1999", "pretends to be Rico's father for two - thousand dollars so he can get money to see Siena modeling in Peru", "2005", "Oskar Schindler", "Peterloo", "Estonia", "Jane Mayer", "1993 to 2001", "Reverend Lovejoy", "about 12 million", "Charlotte Gainsbourg", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "Audrey Roberts"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5965198863636363}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.06060606060606061, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1944", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-5822", "mrqa_searchqa-validation-9193", "mrqa_searchqa-validation-15667", "mrqa_searchqa-validation-2500", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-4052", "mrqa_searchqa-validation-736", "mrqa_searchqa-validation-359", "mrqa_searchqa-validation-11037", "mrqa_searchqa-validation-5401", "mrqa_searchqa-validation-16268", "mrqa_searchqa-validation-7524", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-427", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-4653", "mrqa_searchqa-validation-3730", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-12975", "mrqa_searchqa-validation-16351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10428", "mrqa_triviaqa-validation-6374", "mrqa_triviaqa-validation-6718", "mrqa_hotpotqa-validation-5098", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2748", "mrqa_triviaqa-validation-5670"], "SR": 0.515625, "CSR": 0.5450067934782609, "EFR": 1.0, "Overall": 0.7372044836956522}, {"timecode": 92, "before_eval_results": {"predictions": ["the Andes", "Fiddler on the Roof", "(Usama) Bin Laden", "Tennessee", "diamonds", "a lighthouse", "gypsum", "the Crimean War", "Sinclair Lewis", "Captains Courageous", "the base", "Central Park", "the nave", "The Tyger", "Chinese", "(Howard) Hughes", "Pablo Escobar", "a conifer", "run mate", "an asteroid", "first base", "cork", "Ichabod Crane", "the king", "\"Chinatown\"", "a butterfly", "Lolita", "the Rheingold", "Tango", "( Wesley) Clark", "Filet", "a saint", "Billie Jean King", "Bill & George Clinton", "Aristophanes", "Khrushchev", "Green Day", "Las Vegas", "The Museum of Modern Art", "canals", "Early Christian communities", "Lewis Carroll", "meters", "the ears", "Yale", "Brett Favre", "Tennessee", "Jean Harlow", "(Edouard) Manet", "Ramses the Great", "The Hairy Ape", "Jason Flemyng", "over 100 countries", "Service / Crown personnel serving in the UK or overseas in the British Armed Forces or with Her Majesty's Government", "Nicholas Garland", "Lincoln", "France", "2000", "Vinnie Jones, Scot Williams, and Vytautas \u0160apranauskas", "Humvee", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "Bright Automotive,", "Harry Nicolaides,", "1957"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6941964285714286}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.7619047619047621, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8403", "mrqa_searchqa-validation-11004", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-14692", "mrqa_searchqa-validation-12935", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-12517", "mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-931", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4322", "mrqa_searchqa-validation-10904", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-14833", "mrqa_searchqa-validation-1722", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-3881", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-3764", "mrqa_hotpotqa-validation-1040", "mrqa_hotpotqa-validation-2236"], "SR": 0.640625, "CSR": 0.5460349462365591, "EFR": 0.9130434782608695, "Overall": 0.7200188098994857}, {"timecode": 93, "before_eval_results": {"predictions": ["the First World War", "the Rhine & the Main", "Kingston", "Cheers", "Indiana", "Walt Kelly", "a dialyzer", "Paris", "the \"Gangbusters\"", "the Chinese pantheon", "Maine", "Gertrude Stein", "the promiscuous divorce", "a bathroom", "The Da Vinci Code", "cricket", "Death", "Mount Everest", "Rouen", "Varney Airlines", "Notre Dame", "the northern wars", "Jupiter", "loverly", "the scrum", "the Falkland Islands", "the 1968 film", "Iceland", "\"Le Petit Vingtime\"", "a checkerboard", "temperature", "Jonathan Swift", "Miracle on 34th Street", "turquoise", "Hamlet", "Mantle & Maris", "copper", "fuel", "the Mesozoic", "Eisenhower", "\"For What It's Worth\"", "the Fourteen Points", "Freddie Mercury", "Mount Aso", "Harry Potter and the Order of the Phoenix", "Geronimo", "( Wiley) Post", "the Misty Mountains", "a cantaloupe", "London", "(Carl) Sandburg", "the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "The Enchantress", "Eddie Murphy", "a herald", "kowloon", "the Treaty of Waitangi", "Jessica Phyllis Lange", "Heinkel He 178", "Kenan & Kel", "304,000", "in August 11, 12 and 13,", "around 8 p.m. local time Thursday", "backbreaking"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6247549019607843}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-15906", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-7293", "mrqa_searchqa-validation-10142", "mrqa_searchqa-validation-16766", "mrqa_searchqa-validation-15431", "mrqa_searchqa-validation-5539", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-15423", "mrqa_searchqa-validation-2724", "mrqa_searchqa-validation-11134", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-7173", "mrqa_searchqa-validation-13738", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-7166", "mrqa_triviaqa-validation-1062", "mrqa_triviaqa-validation-249", "mrqa_hotpotqa-validation-590", "mrqa_hotpotqa-validation-2223", "mrqa_hotpotqa-validation-4360", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4060"], "SR": 0.59375, "CSR": 0.5465425531914894, "retrieved_ids": ["mrqa_squad-train-27205", "mrqa_squad-train-38564", "mrqa_squad-train-68001", "mrqa_squad-train-47726", "mrqa_squad-train-19553", "mrqa_squad-train-31520", "mrqa_squad-train-76010", "mrqa_squad-train-45485", "mrqa_squad-train-13496", "mrqa_squad-train-83025", "mrqa_squad-train-20682", "mrqa_squad-train-52705", "mrqa_squad-train-63537", "mrqa_squad-train-67217", "mrqa_squad-train-7144", "mrqa_squad-train-50302", "mrqa_naturalquestions-validation-5942", "mrqa_searchqa-validation-4697", "mrqa_searchqa-validation-16913", "mrqa_searchqa-validation-14319", "mrqa_searchqa-validation-1784", "mrqa_squad-validation-6390", "mrqa_newsqa-validation-2196", "mrqa_searchqa-validation-9229", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-3075", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-6297", "mrqa_newsqa-validation-293", "mrqa_hotpotqa-validation-4316", "mrqa_triviaqa-validation-4194", "mrqa_searchqa-validation-9246"], "EFR": 1.0, "Overall": 0.7375116356382979}, {"timecode": 94, "before_eval_results": {"predictions": ["E.B. White", "Starfighter", "Ricardo Sanchez Robert Gates", "zoo", "Omega", "Nixon", "the Hudson River", "rodents", "Luxembourg", "Jimmy Doolittle", "a riot", "Lon Chaney", "New York", "the Coen clue", "Sicily", "the Boston Celtics", "sugar", "Enron", "the fulcrum", "Central African Republic", "Rudolf Hess", "fight", "the hippopotamus", "eye", "Bech", "Ronald Reagan, the Republican candidate, and former Vice President Walter Mondale", "Washington Irving", "the White Mountains", "the Nile", "Existentialism", "mezcal", "Scarface", "Mitch McConnell", "(Jerry) Mathers", "Nine to 5", "Housing and Urban Development", "extradition", "head", "the Nutty Professor", "Michael Collins", "The Sopranos", "The Sound and the Fury", "the mother- daughter dyad", "Brazil", "obsessive-compulsive", "Katie Holmes", "o oats", "arteries", "1773", "a joule", "Justice", "20 November 1989", "25 September 2007", "Andrew Moray and William Wallace", "Nafea Faa Ipoipo", "a window", "St. Augustine", "Newtonian mechanics", "PET", "SKUM", "12-hour-plus", "Donald Trump.", "second", "Mary Rose Foster"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6640625}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.8333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-6590", "mrqa_searchqa-validation-5272", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-6927", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-656", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-7614", "mrqa_searchqa-validation-11026", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-14194", "mrqa_searchqa-validation-11851", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-10970", "mrqa_searchqa-validation-10541", "mrqa_searchqa-validation-7196", "mrqa_searchqa-validation-13790", "mrqa_searchqa-validation-9869", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-11521", "mrqa_naturalquestions-validation-6972", "mrqa_naturalquestions-validation-6927", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-4784", "mrqa_hotpotqa-validation-391", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2638"], "SR": 0.53125, "CSR": 0.5463815789473685, "EFR": 0.9666666666666667, "Overall": 0.7308127741228071}, {"timecode": 95, "before_eval_results": {"predictions": ["Viktor Yatsenyuk", "paul melbourne", "the Communist Party", "The Goonies", "Velvet Revolver", "the Haunted Mansion", "the Continental Congress", "Robert Johnson", "Mahlemuts", "a loin", "fish", "parens", "Casablanca", "The Dutchess", "Detroit", "(George) Sand", "Northern Exposure", "Kilimanjaro", "balthazar", "a flip", "the Komodo", "Mordecai Richler", "The Simpsons", "The West Wing", "pears", "ravens", "quesadillas", "Pickren", "Pocahontas", "animal vectors", "John Hersey", "Patricia Arquette", "Ernie Banks", "Grotto", "(Prince) Abraham", "height", "Hades", "Henry Harrison", "Capone", "Maria Callas", "KELP", "the Medieval Times", "Ptolemy XIII", "Tennyson", "National Geographic", "Disney", "Jerusalem", "Edna Ferber", "the Edict of Nantes", "Achilles", "Omega", "at the end of an interrogative sentence : `` How old are you? ''", "six doctors from Seattle Grace Mercy West Hospital who are victims of an aviation accident fight to stay alive", "since 3, 1, and 4", "barbie humph\u00e1zy", "exponentiation", "cheshire", "1754", "nine", "Lowe's", "Snow,", "Carlos Moya", "Chester Arthur Stiles, 38,", "wasps"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5766369047619047}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7142857142857143, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6798", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-16114", "mrqa_searchqa-validation-7283", "mrqa_searchqa-validation-15023", "mrqa_searchqa-validation-2659", "mrqa_searchqa-validation-4356", "mrqa_searchqa-validation-11619", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-6973", "mrqa_searchqa-validation-15511", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-14446", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-12087", "mrqa_searchqa-validation-14382", "mrqa_searchqa-validation-5077", "mrqa_searchqa-validation-5931", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1656", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-7180", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-827"], "SR": 0.484375, "CSR": 0.5457356770833333, "EFR": 0.9393939393939394, "Overall": 0.7252290482954545}, {"timecode": 96, "before_eval_results": {"predictions": ["innovation", "a wheel", "assemble", "hot air balloons", "nature", "Nomar Garciparra", "John Glenn", "heron", "\"Gus\" Grissom", "The White Company", "New Balance", "\"S.F.\"", "Joan of Arc", "finale", "mollusca", "Camille Claudel", "the East", "caricaturist", "the Seven Years' War", "\"Pride and Prejudice\"", "Wizard of Oz", "madding", "tribes", "(Richard) Branson", "Argentina", "Woodrow Wilson", "the Osmonds", "sul tuo amore in franto", "the tribbles", "The Stranger", "Wyoming", "Tigger", "Geneva", "Frank Sinatra", "corned beef", "an Islamic leadership position", "backstroke", "the 7th century AD", "Sydney", "dermatology", "Solomon", "\"The Look Who's Talking\"", "Chirac", "20", "ATVs", "To Carrie and Irene Miner", "Guiana", "loco", "the Soviet Union", "the Romans", "dilithium", "Ticket to Ride", "1997", "2010", "1215", "\"Conchita\"", "Proclamation of Neutrality", "Mumbai", "Bob Gibson", "eight", "challenges a pregnancy", "Verzasca hydro-electric dam in Switzerland", "$150 billion", "the Rio Grande"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5980654761904762}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14411", "mrqa_searchqa-validation-7604", "mrqa_searchqa-validation-14762", "mrqa_searchqa-validation-6728", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-6065", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-10078", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-1824", "mrqa_searchqa-validation-6419", "mrqa_searchqa-validation-6998", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-7465", "mrqa_searchqa-validation-3467", "mrqa_searchqa-validation-6532", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7579", "mrqa_searchqa-validation-2149", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-1445", "mrqa_searchqa-validation-12162", "mrqa_naturalquestions-validation-9492", "mrqa_triviaqa-validation-1522", "mrqa_hotpotqa-validation-4572", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-4265", "mrqa_newsqa-validation-3859"], "SR": 0.53125, "CSR": 0.5455863402061856, "retrieved_ids": ["mrqa_squad-train-29886", "mrqa_squad-train-40798", "mrqa_squad-train-79570", "mrqa_squad-train-68971", "mrqa_squad-train-21659", "mrqa_squad-train-2613", "mrqa_squad-train-80904", "mrqa_squad-train-75691", "mrqa_squad-train-16514", "mrqa_squad-train-17737", "mrqa_squad-train-27525", "mrqa_squad-train-85837", "mrqa_squad-train-40791", "mrqa_squad-train-47093", "mrqa_squad-train-23645", "mrqa_squad-train-45889", "mrqa_searchqa-validation-8710", "mrqa_newsqa-validation-3687", "mrqa_searchqa-validation-11495", "mrqa_naturalquestions-validation-4309", "mrqa_searchqa-validation-13434", "mrqa_hotpotqa-validation-305", "mrqa_naturalquestions-validation-6022", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-2059", "mrqa_searchqa-validation-5955", "mrqa_naturalquestions-validation-554", "mrqa_searchqa-validation-2175", "mrqa_triviaqa-validation-7280", "mrqa_naturalquestions-validation-1818", "mrqa_searchqa-validation-1138"], "EFR": 0.9666666666666667, "Overall": 0.7306537263745705}, {"timecode": 97, "before_eval_results": {"predictions": ["Rear Window", "nomads", "Washington", "tribbles", "San Jose", "The Two Gentlemen of Verona", "a cobb", "the Hydra", "Gulliver's Travels", "the DEW Line", "Tordis Maurstad", "ice cream", "the Sikkim Autonomous Region", "sonic boom", "Fergie", "Sacramento", "emeralds", "Swiss Cheese", "Ernest Hemingway", "Blue Mountain Coffee", "Annika Sorenstam", "electrons", "Grenadine", "The Innocents Abroad", "Las Vegas", "Hawaii", "Helen Keller", "the tooth Fairy", "Henry Shrapnel", "Venezuela", "Aglauros", "Oklahoma City", "Brazil", "Bob Fosse", "the Dugong dugong", "\"Treading Water\"", "1880s", "the French & Indian War", "checkerboard", "Waterloo", "a waterbed", "a mulatta", "a deck of cards", "a propeller", "bonnet", "an acre", "(XVI) Calder", "a cruller", "Helium", "Tokyo", "cheese", "Charles Perrault", "Jourdan Miller", "c. 1000 AD", "Tony Blair", "neptococcus", "big Dipper", "Sofia the First", "Australia", "Ben Elton", "an annual road trip,", "Schalke", "April 22,", "Sugar Ray Robinson"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7008928571428572}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2611", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-906", "mrqa_searchqa-validation-231", "mrqa_searchqa-validation-10891", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-3549", "mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-15189", "mrqa_searchqa-validation-773", "mrqa_searchqa-validation-6665", "mrqa_searchqa-validation-16676", "mrqa_searchqa-validation-15997", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11177", "mrqa_triviaqa-validation-2390", "mrqa_hotpotqa-validation-3859", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3237"], "SR": 0.65625, "CSR": 0.5467155612244898, "EFR": 0.9090909090909091, "Overall": 0.7193644190630798}, {"timecode": 98, "before_eval_results": {"predictions": ["Jacob Marley", "\"Magnum, P.I.\"", "Ottoman Empire", "Helen of Sparta", "a whale", "New York City", "Himalayas", "Wayne's World", "Poland", "Kwanzaa", "a nuclear submarine", "Russell Crowe", "\"A Beautiful Mind\"", "a car", "tears", "roulette", "W. Somerset Maugham", "Christo", "Matisse", "the bottom", "\"All Quiet On The Western Front\"", "alternative rock band Red Hot Chili Peppers", "Sanskrit", "one", "Rock Hudson", "Spain", "Ford", "Edgar Allan Poe", "Surround", "Faraday", "breakfast", "Krispy Kreme", "Edo Civico", "Avery", "The Oregon Trail", "the Cumberland Gap", "yolk", "Department of Defense", "a dwelling", "a brown rat", "Cleveland", "\"Ligeia\"", "Belgium", "Charles de Gaulle", "\"The Civil War\"", "Destiny's Child", "Luxor Las Vegas", "Spain", "The Beatles", "coconut", "Florence", "Scarlett Johansson", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Madison, Wisconsin, United States", "his finger", "Macduff", "Lumphanan", "Carol Ann Duffy", "Ravenna", "travel diary", "keeping malls safe.", "Sgt. Jason Bendett of the 3rd Platoon, A Company, 2nd Light Reconnaissance Battalion,", "Bahrami", "make life a little easier"], "metric_results": {"EM": 0.421875, "QA-F1": 0.56850077006327}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false], "QA-F1": [0.6666666666666666, 0.6666666666666666, 1.0, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.42857142857142855, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-12595", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-14510", "mrqa_searchqa-validation-8581", "mrqa_searchqa-validation-11733", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-14079", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-12393", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-3568", "mrqa_searchqa-validation-3898", "mrqa_searchqa-validation-7249", "mrqa_searchqa-validation-13658", "mrqa_searchqa-validation-1978", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-2955", "mrqa_searchqa-validation-4971", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-13186", "mrqa_searchqa-validation-5066", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-4442", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-10014", "mrqa_naturalquestions-validation-6874", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-7585", "mrqa_triviaqa-validation-5084", "mrqa_hotpotqa-validation-1364", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-1146"], "SR": 0.421875, "CSR": 0.5454545454545454, "EFR": 1.0, "Overall": 0.7372940340909091}, {"timecode": 99, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1561", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-214", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-3765", "mrqa_hotpotqa-validation-3845", "mrqa_hotpotqa-validation-4580", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-92", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-10485", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-2234", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-3598", "mrqa_naturalquestions-validation-3631", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-3729", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-4147", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5466", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5989", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-673", "mrqa_naturalquestions-validation-6780", "mrqa_naturalquestions-validation-6898", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-8016", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8999", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9324", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9755", "mrqa_naturalquestions-validation-9850", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-9967", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-114", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1235", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-136", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-1565", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1600", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1849", "mrqa_newsqa-validation-1879", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2055", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2098", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-2186", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-222", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-2629", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2708", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2816", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-2854", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-2924", "mrqa_newsqa-validation-2930", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-297", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-317", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-327", "mrqa_newsqa-validation-3345", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-3490", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3672", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3860", "mrqa_newsqa-validation-3878", "mrqa_newsqa-validation-3898", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4075", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-546", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-694", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-825", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-878", "mrqa_newsqa-validation-971", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-10129", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-10262", "mrqa_searchqa-validation-10298", "mrqa_searchqa-validation-10505", "mrqa_searchqa-validation-10549", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10681", "mrqa_searchqa-validation-10777", "mrqa_searchqa-validation-10853", "mrqa_searchqa-validation-11001", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-11095", "mrqa_searchqa-validation-11183", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11513", "mrqa_searchqa-validation-11514", "mrqa_searchqa-validation-11557", "mrqa_searchqa-validation-12030", "mrqa_searchqa-validation-12075", "mrqa_searchqa-validation-12162", "mrqa_searchqa-validation-12248", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-126", "mrqa_searchqa-validation-12765", "mrqa_searchqa-validation-12913", "mrqa_searchqa-validation-1301", "mrqa_searchqa-validation-13100", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-13313", "mrqa_searchqa-validation-13326", "mrqa_searchqa-validation-13548", "mrqa_searchqa-validation-13573", "mrqa_searchqa-validation-13650", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-13738", "mrqa_searchqa-validation-13755", "mrqa_searchqa-validation-13918", "mrqa_searchqa-validation-13974", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14267", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-14464", "mrqa_searchqa-validation-14598", "mrqa_searchqa-validation-14631", "mrqa_searchqa-validation-14644", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14847", "mrqa_searchqa-validation-14855", "mrqa_searchqa-validation-14934", "mrqa_searchqa-validation-14987", "mrqa_searchqa-validation-15115", "mrqa_searchqa-validation-15123", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-1542", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15977", "mrqa_searchqa-validation-16131", "mrqa_searchqa-validation-16160", "mrqa_searchqa-validation-16262", "mrqa_searchqa-validation-16266", "mrqa_searchqa-validation-16305", "mrqa_searchqa-validation-1636", "mrqa_searchqa-validation-16422", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-16653", "mrqa_searchqa-validation-16749", "mrqa_searchqa-validation-16808", "mrqa_searchqa-validation-16831", "mrqa_searchqa-validation-16946", "mrqa_searchqa-validation-1793", "mrqa_searchqa-validation-1895", "mrqa_searchqa-validation-200", "mrqa_searchqa-validation-2035", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-2340", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-2449", "mrqa_searchqa-validation-2468", "mrqa_searchqa-validation-248", "mrqa_searchqa-validation-2532", "mrqa_searchqa-validation-2576", "mrqa_searchqa-validation-2725", "mrqa_searchqa-validation-2820", "mrqa_searchqa-validation-2950", "mrqa_searchqa-validation-3106", "mrqa_searchqa-validation-3121", "mrqa_searchqa-validation-3258", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-3399", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3591", "mrqa_searchqa-validation-3676", "mrqa_searchqa-validation-3774", "mrqa_searchqa-validation-3779", "mrqa_searchqa-validation-3867", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-3991", "mrqa_searchqa-validation-4163", "mrqa_searchqa-validation-4191", "mrqa_searchqa-validation-4197", "mrqa_searchqa-validation-4266", "mrqa_searchqa-validation-4295", "mrqa_searchqa-validation-4365", "mrqa_searchqa-validation-4369", "mrqa_searchqa-validation-4386", "mrqa_searchqa-validation-443", "mrqa_searchqa-validation-4553", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4763", "mrqa_searchqa-validation-5045", "mrqa_searchqa-validation-5724", "mrqa_searchqa-validation-5791", "mrqa_searchqa-validation-5955", "mrqa_searchqa-validation-5997", "mrqa_searchqa-validation-6041", "mrqa_searchqa-validation-611", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6341", "mrqa_searchqa-validation-638", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6394", "mrqa_searchqa-validation-6658", "mrqa_searchqa-validation-6727", "mrqa_searchqa-validation-6759", "mrqa_searchqa-validation-689", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7017", "mrqa_searchqa-validation-7028", "mrqa_searchqa-validation-7370", "mrqa_searchqa-validation-7405", "mrqa_searchqa-validation-7456", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7676", "mrqa_searchqa-validation-7708", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7790", "mrqa_searchqa-validation-7985", "mrqa_searchqa-validation-8055", "mrqa_searchqa-validation-8184", "mrqa_searchqa-validation-8190", "mrqa_searchqa-validation-8200", "mrqa_searchqa-validation-8225", "mrqa_searchqa-validation-8263", "mrqa_searchqa-validation-8272", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-8435", "mrqa_searchqa-validation-8478", "mrqa_searchqa-validation-8532", "mrqa_searchqa-validation-8623", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-9049", "mrqa_searchqa-validation-9087", "mrqa_searchqa-validation-9254", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9364", "mrqa_searchqa-validation-940", "mrqa_searchqa-validation-9425", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-952", "mrqa_searchqa-validation-9528", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9922", "mrqa_squad-validation-10011", "mrqa_squad-validation-10252", "mrqa_squad-validation-1290", "mrqa_squad-validation-1407", "mrqa_squad-validation-1441", "mrqa_squad-validation-1512", "mrqa_squad-validation-1583", "mrqa_squad-validation-1662", "mrqa_squad-validation-1955", "mrqa_squad-validation-2059", "mrqa_squad-validation-2748", "mrqa_squad-validation-2857", "mrqa_squad-validation-2893", "mrqa_squad-validation-2920", "mrqa_squad-validation-2932", "mrqa_squad-validation-3222", "mrqa_squad-validation-3493", "mrqa_squad-validation-3551", "mrqa_squad-validation-3663", "mrqa_squad-validation-4162", "mrqa_squad-validation-5029", "mrqa_squad-validation-5348", "mrqa_squad-validation-57", "mrqa_squad-validation-5730", "mrqa_squad-validation-5765", "mrqa_squad-validation-5956", "mrqa_squad-validation-5995", "mrqa_squad-validation-605", "mrqa_squad-validation-7330", "mrqa_squad-validation-7338", "mrqa_squad-validation-763", "mrqa_squad-validation-7836", "mrqa_squad-validation-8403", "mrqa_squad-validation-8495", "mrqa_squad-validation-8869", "mrqa_squad-validation-9178", "mrqa_squad-validation-9298", "mrqa_squad-validation-9362", "mrqa_squad-validation-9365", "mrqa_squad-validation-9373", "mrqa_squad-validation-9528", "mrqa_squad-validation-9687", "mrqa_squad-validation-973", "mrqa_squad-validation-9940", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-1237", "mrqa_triviaqa-validation-1315", "mrqa_triviaqa-validation-1358", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1931", "mrqa_triviaqa-validation-2071", "mrqa_triviaqa-validation-2101", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2202", "mrqa_triviaqa-validation-2251", "mrqa_triviaqa-validation-3090", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3626", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-4255", "mrqa_triviaqa-validation-4412", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-4501", "mrqa_triviaqa-validation-4952", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-538", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6427", "mrqa_triviaqa-validation-6637", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-7180", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7383", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-920"], "OKR": 0.880859375, "KG": 0.4953125, "before_eval_results": {"predictions": ["the Hundred Years' War", "vertebral", "Alfred Binet", "Venial", "a caveat", "\"There's no place like home\"", "cows", "Spanish", "Vanessa Hudgens", "Mighty Joe Young", "(The Two Towers)", "in Southeast Asia", "Rhiannon", "Scotland", "Beaver", "Kurdish", "Ann Richards", "half-staff", "France", "Langston Hughes", "Coke", "The Color Purple", "the THX surround sound system", "Macbeth", "El Greco", "General Motors", "sexy", "a shark", "Frankie Valli", "a Dagger", "a backpacking route", "pineapple", "Buffalo nickel", "pink", "Balaam", "standing in front of the bull and", "Jamestown", "Joy Division", "Fondue", "thriller", "Schwarzenegger", "(AT&T)", "Animal Crackers", "Oblivion", "Goethe", "an organ", "Texas Chainsaw Massacre", "Russia", "Students for a Democratic Society", "All the King's Men", "Charles Gounod", "by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "18", "July 10, 2017", "(James) Mason", "a slide trumpet", "Anne Frank", "YG Entertainment", "Nova Scotia", "Rochdale", "Matamoros, Mexico,", "Florida", "Capitol Hill.", "775 rooms"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7302083333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-14942", "mrqa_searchqa-validation-10474", "mrqa_searchqa-validation-3260", "mrqa_searchqa-validation-13979", "mrqa_searchqa-validation-14822", "mrqa_searchqa-validation-6184", "mrqa_searchqa-validation-8822", "mrqa_searchqa-validation-4302", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-6823", "mrqa_searchqa-validation-6740", "mrqa_searchqa-validation-14236", "mrqa_searchqa-validation-11396", "mrqa_searchqa-validation-1590", "mrqa_searchqa-validation-15094", "mrqa_searchqa-validation-9014", "mrqa_searchqa-validation-1302", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-8862", "mrqa_triviaqa-validation-2452", "mrqa_newsqa-validation-1996", "mrqa_naturalquestions-validation-9572"], "SR": 0.65625, "CSR": 0.5465625000000001, "retrieved_ids": ["mrqa_squad-train-24006", "mrqa_squad-train-55084", "mrqa_squad-train-54571", "mrqa_squad-train-13426", "mrqa_squad-train-39988", "mrqa_squad-train-50872", "mrqa_squad-train-33464", "mrqa_squad-train-46367", "mrqa_squad-train-76945", "mrqa_squad-train-45661", "mrqa_squad-train-37899", "mrqa_squad-train-58506", "mrqa_squad-train-66345", "mrqa_squad-train-58257", "mrqa_squad-train-67694", "mrqa_squad-train-3922", "mrqa_searchqa-validation-8582", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-1848", "mrqa_newsqa-validation-159", "mrqa_searchqa-validation-503", "mrqa_naturalquestions-validation-10428", "mrqa_searchqa-validation-12904", "mrqa_hotpotqa-validation-3846", "mrqa_searchqa-validation-16383", "mrqa_triviaqa-validation-7732", "mrqa_searchqa-validation-12947", "mrqa_newsqa-validation-3517", "mrqa_searchqa-validation-3398", "mrqa_newsqa-validation-3035", "mrqa_triviaqa-validation-1122", "mrqa_searchqa-validation-16710"], "EFR": 1.0, "Overall": 0.7400156250000001}]}