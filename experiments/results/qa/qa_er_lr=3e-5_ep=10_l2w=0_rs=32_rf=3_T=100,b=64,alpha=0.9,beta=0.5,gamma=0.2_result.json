{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.2.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5440, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["positive divisors", "quietist/non-political", "Jonathan Stewart", "surface condensers", "Anglo-Saxons", "one of the first peer-to-peer network architectures", "Tanzania", "structure", "ABC Cable News", "Turner and Vernon", "$2 million", "German-language publications", "-40%", "BBC 1", "the \"blurring of theological and confessional differences in the interests of unity.\"", "pamphlets on Islam", "mad dogs", "Mnemiopsis", "both Kenia and Kegnia", "electricity", "student tuition, endowments, scholarship/voucher funds", "Mongolian patrimonial feudalism and the traditional Chinese autocratic-bureaucratic system", "European Council", "826", "1999", "Latin", "semantical problems", "$2 million", "committee", "(trunnion", "South Pacific", "Spanish moss", "1850s", "Abercrombie was recalled and replaced by Jeffery Amherst", "saturating them unconsciously with electricity", "slightly more than normal sea-level O2 partial pressure", "Associating forces with vectors", "showmanship", "social networking support", "Children of Earth", "Soviet", "Brock Osweiler", "San Diego", "Economist", "liquid", "Jerricho Cotchery", "suggested it for use in the ARPANET", "disrupting their plasma membrane", "Genghis Khan", "Robert Boyle", "feigned retreat", "Rotterdam", "the problem of multiplying two integers", "he was illiterate in Czech", "the Monarch", "4.7 yards per carry", "Sports Programs, Inc.", "only pharmacists", "ideological", "behavioral and demographic data", "Kuviasungnerk/Kangeiko", "94", "October 16, 2012", "transportation, sewer, hazardous waste and water"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7783752554812338}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.11111111111111112, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-4676", "mrqa_squad-validation-10430", "mrqa_squad-validation-5326", "mrqa_squad-validation-2291", "mrqa_squad-validation-1530", "mrqa_squad-validation-7086", "mrqa_squad-validation-8412", "mrqa_squad-validation-2478", "mrqa_squad-validation-3590", "mrqa_squad-validation-1913", "mrqa_squad-validation-3771", "mrqa_squad-validation-6293", "mrqa_squad-validation-1766", "mrqa_squad-validation-1187", "mrqa_squad-validation-288"], "SR": 0.734375, "CSR": 0.734375, "EFR": 0.9411764705882353, "Overall": 0.8377757352941176}, {"timecode": 1, "before_eval_results": {"predictions": ["1929", "the lack of a Parliament of Scotland", "small islands", "US$10 a week raise", "A fundamental error", "Horace Walpole", "any object can be, essentially uniquely, decomposed into its prime components", "1968", "straight", "complexity classes", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "80%", "leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies", "Informal rule", "seven", "petroleum", "1671", "the highest penalty that can be inflicted upon me for what in law is a deliberate crime and what appears to me to be the highest duty of a citizen", "the headwaiter", "a comb jelly", "expositions", "1784", "terrorist organisation", "\"winds up\" the debate by speaking after all other participants.", "the Golden Gate Bridge", "Hulu", "National Galleries of Scotland", "Northern Rhodesia (today Zambia)", "Budapest", "Joanna Lumley", "Gateshead", "tentacles", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "East Smithfield burial site in England", "Jerome Schurf", "The arrival of satellite television", "we are neither making maximum effort nor achieving results necessary if this country is to reach a position of leadership.", "Isaac Newton", "dangerous enemies", "Robert Underwood Johnson", "the A13, Brenner Autobahn, en route to Italy", "kinetic friction", "X-rays", "Roger NFL", "Abu al-Rayhan al-Biruni", "British", "Spreading throughout the Mediterranean and Europe", "almost a month", "\"cellular\" and \"humoral\"", "traditional old boy network", "anti-Semitic policies", "the Scottish Government", "the Lisbon Treaty", "emerging market", "Bible", "24\u201310", "cellular respiration", "the lion, leopard, buffalo, rhinoceros, and elephant", "computer problems", "Her cameo was filmed on the set of the Sex and The City prequel, The Carrie Diaries ; the producers like to imagine that she was directing an episode", "From Russia", "Balvenie Castle", "Geological evidence shows that this 5,000-mile mountain chain may extend south into Antarctica", "Anne Noe"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6966562757783831}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913045, 1.0, 1.0, 0.0, 0.0, 0.10810810810810811, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1290322580645161, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9525", "mrqa_squad-validation-2704", "mrqa_squad-validation-3113", "mrqa_squad-validation-6759", "mrqa_squad-validation-739", "mrqa_squad-validation-9465", "mrqa_squad-validation-8342", "mrqa_squad-validation-6614", "mrqa_squad-validation-4902", "mrqa_squad-validation-10410", "mrqa_squad-validation-4332", "mrqa_squad-validation-85", "mrqa_squad-validation-9732", "mrqa_squad-validation-4856", "mrqa_squad-validation-2497", "mrqa_squad-validation-9488", "mrqa_squad-validation-3516", "mrqa_squad-validation-8278", "mrqa_newsqa-validation-911", "mrqa_naturalquestions-validation-1802", "mrqa_triviaqa-validation-1415", "mrqa_searchqa-validation-187", "mrqa_hotpotqa-validation-3155"], "SR": 0.640625, "CSR": 0.6875, "EFR": 0.9130434782608695, "Overall": 0.8002717391304348}, {"timecode": 2, "before_eval_results": {"predictions": ["V", "estimated 16,000 to 35,000", "second", "phagocytes", "Jochi", "Alsace", "West Lothian question", "Wiesner", "representatives elected to either house of parliament", "become more integral within the health care system", "trial division", "August 2004", "(sworn brother or blood brother)", "warships", "if they were non-discriminatory, \"justified by imperative requirements in the general interest\" and proportionately applied", "lower sixth", "2002", "Organizational", "the number of social services that people can access wherever they move", "cytokines", "an individual in the form of a postman or tax collector whose hand hits the wood", "civil disobedience", "eighteenth century", "glaucophyte", "jellyfish", "existing level of inequality", "well before Braddock's departure for North America", "the means of production by a class of owners", "Hughes Hotel", "Golden Gate Bridge", "Annual Status of Education Report", "Six", "the Pauli exclusion principle", "1 September 1939", "Mexico", "Battle of Dalan Balzhut", "the relative units of force and mass then are fixed", "Russell T Davies", "Innate", "1903", "photosynthesis", "private research university", "article 49", "wine", "Arabic numerals", "application of electricity", "bilaterians", "risen with increased income inequality", "life on Tyneside", "student-teacher relationships", "external combustion engines", "that each side is capable of performing the obligations set out", "the Russian defense ministry said Wednesday.Russia's Tupolev TU-160, pictured here in 2003, is a long-range strategic bomber.", "\"wipe out\" the United States if provoked", "located almost entirely in Wake County, it lies just north of the state capital, Raleigh", "Speaker of the House of Representatives shall, upon his resignation as Speaker and as Representative in Congress, act as President", "the last 32 FA Cup places club by club for the inter-war period  1919-20 to 1938-39.", "communion", "Robert Noyce", "Eliot Spitzer", "a toast to this heir whose support of local pubs got him dubbed \"Beer Drinker of the Year 2002\"", "this Native American rite of passage or of spiritual renewal often includes fasting", "The chain on the big chain ring, going for it", "the Augustan Age"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7638558201058201}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.962962962962963, 0.4166666666666667, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9697", "mrqa_squad-validation-6404", "mrqa_squad-validation-8909", "mrqa_squad-validation-3378", "mrqa_squad-validation-6970", "mrqa_squad-validation-7514", "mrqa_squad-validation-10428", "mrqa_squad-validation-7201", "mrqa_newsqa-validation-3489", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-3648", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-2265", "mrqa_hotpotqa-validation-1174", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-12652", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-4178"], "SR": 0.71875, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 3, "before_eval_results": {"predictions": ["New Orangery", "parish churches", "Michael Mullett", "Tibetan Buddhism", "North American", "phagosome", "Smalcald Articles", "begging his son to return home", "1788", "The Rankine cycle", "Several thousand", "Get Carter", "source of most of the chemical energy released", "the college", "bones", "NFL Commissioner Roger Goodell", "purple skin patches", "Apollo 17", "1562", "cilia", "by qualified majority", "Blum complexity axioms", "the Diffie\u2013Hellman key exchange", "America's Funniest Home Videos", "16", "seven-layer OSI-compliant networking protocol", "the Vosges Mountains", "0 \u00b0C (32 \u00b0F)", "May 1754", "infected corpses", "2002", "Australia's first public packet-switched data network", "even greater inequality", "Association of American Universities", "economic utility in society from resources devoted on high-end consumption", "cut in half", "uncertain", "1835", "720p high definition", "mainline Protestant Methodist denomination", "comb-bearing", "Tate Britain", "CBS", "Treaty of Rome 1957 and the Maastricht Treaty 1992", "M. Theo Kearney", "offering a higher wage the best of their labor", "Lenin", "Prince Houston", "the Mayor of the City of New York", "Vishal Bhardwaj", "Congress passed the Chinese Exclusion Act in 1882 which targeted a single ethnic group by specifically limiting further Chinese immigration", "routing protocols", "The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "Antoine Lavoisier", "CeeLo Green", "Ted Stillwell", "Zelaya", "Chinese nationals", "Evan Bayh", "wandins", "Anthony Powell and John Betjeman", "a type of large cushion", "rural California", "World leaders"], "metric_results": {"EM": 0.671875, "QA-F1": 0.783649781679673}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.4, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4347826086956522, 0.5, 0.8666666666666666, 1.0, 1.0, 0.0, 0.4, 0.5, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1146", "mrqa_squad-validation-3530", "mrqa_squad-validation-83", "mrqa_squad-validation-4074", "mrqa_squad-validation-4675", "mrqa_squad-validation-2914", "mrqa_squad-validation-4840", "mrqa_squad-validation-7502", "mrqa_squad-validation-7300", "mrqa_squad-validation-545", "mrqa_hotpotqa-validation-5344", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-1676", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-1834", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-7746"], "SR": 0.671875, "CSR": 0.69140625, "retrieved_ids": ["mrqa_squad-train-7354", "mrqa_squad-train-42259", "mrqa_squad-train-15983", "mrqa_squad-train-54423", "mrqa_squad-train-21764", "mrqa_squad-train-25460", "mrqa_squad-train-56667", "mrqa_squad-train-49426", "mrqa_squad-train-62350", "mrqa_squad-train-2617", "mrqa_squad-train-83890", "mrqa_squad-train-5561", "mrqa_squad-train-29186", "mrqa_squad-train-27053", "mrqa_squad-train-67702", "mrqa_squad-train-58172", "mrqa_naturalquestions-validation-1802", "mrqa_squad-validation-1530", "mrqa_squad-validation-6970", "mrqa_squad-validation-4902", "mrqa_squad-validation-9575", "mrqa_triviaqa-validation-6170", "mrqa_squad-validation-10428", "mrqa_squad-validation-288", "mrqa_squad-validation-1187", "mrqa_squad-validation-739", "mrqa_triviaqa-validation-1415", "mrqa_squad-validation-6404", "mrqa_squad-validation-9488", "mrqa_squad-validation-6293", "mrqa_hotpotqa-validation-3155", "mrqa_squad-validation-3516"], "EFR": 1.0, "Overall": 0.845703125}, {"timecode": 4, "before_eval_results": {"predictions": ["9\u201318", "Norman Foster", "Battle of Hastings", "9 October 2006", "a \"lifeboat\" in the event of a failure of the command ship", "Robert R. Gilruth", "hermaphroditism and early reproduction", "Moscone Center", "generally poor French results in most theaters of the Seven Years' War in 1758", "1994", "patients' prescriptions and patient safety issues", "December 12", "June 6, 1951", "three", "John Wesley", "legal requirements", "21 January 1788", "Gryphon", "between 1859 and 1865", "ESPN Deportes", "1784", "LeGrande", "St. Lawrence", "rule", "Golden Super Bowl", "TEU articles 4 and 5", "a Standard Model", "Westinghouse Electric", "Sayyid Abul Ala Maududi", "$5,000,000", "Mike Tolbert", "phagocytes", "glaucophyte", "a multiple access scheme", "Magnetophon tape recorder", "random access machines", "a trade magazine", "noisiest", "wars", "Ogr\u00f3d Saski", "Zygons", "Dorothy Skerrit", "Stratfor", "Apple's new iOS5 operating system", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients", "0300", "1997", "they believed that it violated their rights as Englishmen to `` No taxation without representation ''", "Rent", "James Intveld", "buzzards", "Lorelei", "Salvador Dal\u00ed", "the Combination Acts", "Ricky Marco", "Frank Sinatra", "Little Richard", "Gerry Adams", "Margie H. Answer #1", "German Shepherd", "a hat", "An Osiris", "Frank Sinatra", "President of the United States"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6762743639122315}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 1.0, 0.5490196078431372, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3806", "mrqa_squad-validation-10295", "mrqa_squad-validation-6877", "mrqa_squad-validation-5620", "mrqa_squad-validation-9810", "mrqa_squad-validation-8643", "mrqa_squad-validation-6692", "mrqa_squad-validation-718", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-3012", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-1403", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-265", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-6600"], "SR": 0.640625, "CSR": 0.68125, "EFR": 1.0, "Overall": 0.840625}, {"timecode": 5, "before_eval_results": {"predictions": ["atmospheric", "the constituting General Conference in Dallas, Texas", "central business district", "the third most abundant chemical element in the universe", "The mermaid", "Mick Mixon", "1992", "a course of study", "Mansfeld", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "peptidoglycan", "the soul does not sleep (anima non sic dormit), but wakes (sed vigilat) and experiences visions", "15", "in 1017", "84 hours", "Jordan Norwood", "Doritos", "four", "the American Revolutionary War", "the League of Nations", "the temperance movement", "the Albany Congress", "British", "Giovanni Branca", "Tiffany & Co.", "NASA discontinued the manned Block I program", "Andy Warhol", "2011", "five", "The ability to make probabilistic decisions", "Divine Right of Kings", "teaching", "research, exhibitions and other shows", "case law by the Court of Justice", "the United States Census Bureau", "The Deadly Assassin", "Seventy percent", "Taih\u014d Code (701) and re-stated in the Y\u014dr\u014d Code", "order", "a place for another non-European Union player in Frank Rijkaard's squad", "John McCain", "The National Telecommunications and Information Administration", "the prince, second in line to the throne, landed a Chinook helicopter", "1983", "Geraldine Margaret Agnew - Somerville", "Woodrow Wilson", "December 1886", "during a game in 1988 while playing for the University of Pittsburgh", "the Federal Reserve System", "the Daewoo Matiz", "the North Side", "polio", "The Spanish Armada", "Kinnairdy Castle", "the 2014\u201315 season", "Heathrow", "Wildhorn, Bricusse and Cuden", "County Executive", "the William Carter ad", "the Battle of Stalingrad", "a spear point", "The Lion", "The Weekly Comebacker", "tiger's milk"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6459140640061694}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 0.0, 0.0, 0.23076923076923078, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2675", "mrqa_squad-validation-3559", "mrqa_squad-validation-1877", "mrqa_squad-validation-2408", "mrqa_squad-validation-1061", "mrqa_squad-validation-7288", "mrqa_squad-validation-2961", "mrqa_squad-validation-3971", "mrqa_squad-validation-1824", "mrqa_squad-validation-4260", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1283", "mrqa_naturalquestions-validation-1975", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-4466", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3228", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-1203"], "SR": 0.5625, "CSR": 0.6614583333333333, "EFR": 0.9642857142857143, "Overall": 0.8128720238095237}, {"timecode": 6, "before_eval_results": {"predictions": ["Wahhabism or Salafism", "Del\u00fc\u00fcn Boldog", "forceful taking of property", "220 miles", "jiggle TV", "On the Councils and the Church", "a Western Union superintendent", "mid-Eocene", "\"do not disturb\" sign", "bounding", "Maling company", "water", "avionics, telecommunications, and computers.", "five", "The majority may be powerful but it is not necessarily right", "Robert Boyle", "new and enlarged bridges, a shuttle service and/or a tram.", "1997", "Presiding Officer", "Michelle Gomez.", "Laszlo Babai and Eugene Luks", "Wesleyan Holiness Consortium", "Aristotle", "1894", "average workers", "cholecalciferol", "1524\u201325", "religious", "Eight original series serials", "the historical era", "2011", "1665", "closure temperature", "light reactions", "Michael Krane", "the ireport form", "Les Bleus", "Movahedi", "a man's lifeless, naked body", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "10 logarithm", "Alan Menken", "Sanchez Navarro", "Vincent Price", "Article Two", "the first instalment in the long - running Harry Potter film series, and was written by Steve Kloves and produced by David Heyman", "Popeye", "celsius", "Donna Summer", "Vladimir Putin", "the Blue Riband", "Vietnam", "Atlantic Coast Conference", "Prince of Cambodia Norodom Sihanouk", "Don Hahn", "Constance M. Burge", "400", "heavy metal", "Gibbons v. Ogden", "Tokyo", "chicago", "wonderdrug.com", "Weehawken", "the Chesapeake Bay"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8152844551282051}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-9592", "mrqa_squad-validation-4631", "mrqa_squad-validation-6968", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-1643", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6477", "mrqa_hotpotqa-validation-5703", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-12747", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-748"], "SR": 0.765625, "CSR": 0.6763392857142857, "retrieved_ids": ["mrqa_squad-train-33498", "mrqa_squad-train-46467", "mrqa_squad-train-54200", "mrqa_squad-train-62871", "mrqa_squad-train-6534", "mrqa_squad-train-17347", "mrqa_squad-train-33631", "mrqa_squad-train-41927", "mrqa_squad-train-62082", "mrqa_squad-train-83389", "mrqa_squad-train-1070", "mrqa_squad-train-78836", "mrqa_squad-train-18535", "mrqa_squad-train-25145", "mrqa_squad-train-27643", "mrqa_squad-train-32803", "mrqa_newsqa-validation-911", "mrqa_squad-validation-7086", "mrqa_searchqa-validation-14936", "mrqa_squad-validation-4676", "mrqa_searchqa-validation-12103", "mrqa_squad-validation-10410", "mrqa_squad-validation-2914", "mrqa_squad-validation-2675", "mrqa_triviaqa-validation-2265", "mrqa_squad-validation-6293", "mrqa_squad-validation-8412", "mrqa_triviaqa-validation-7082", "mrqa_squad-validation-9488", "mrqa_squad-validation-7514", "mrqa_squad-validation-3113", "mrqa_squad-validation-3806"], "EFR": 0.9333333333333333, "Overall": 0.8048363095238096}, {"timecode": 7, "before_eval_results": {"predictions": ["two", "no French regular army troops were stationed in North America", "Ed Mangan", "German", "c1750", "bones", "central Europe", "since 1951", "immunoglobulins and T cell receptors", "average workers", "Brandon McManus", "Science", "33", "88", "monophyletic", "Bible", "smallest", "Sports Night", "superheaters", "Mercedes-Benz Superdome", "Wang Zhen", "dioxygen", "1953", "Capitol Hill, Washington, D.C.", "Parliament Square, High Street and George IV Bridge", "organic solvents", "Mnemiopsis", "topographic", "the Tesla coil", "National Galleries of Scotland", "The Pilgrim Street building", "Bright Automotive", "tax incentives", "wacko", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "more and more suspicious of the way their business books were being handled", "38", "the less tissue the X-rays have to go through", "only drivers who were Daytona Pole Award winners", "the Philippines and Guam", "Aristotle", "Peking", "Manuel `` Manny '' Heffley", "My Summer Story", "Mick Tully", "roast goose", "Backgammon", "season seven", "Colorado", "Inigo Jones", "Brian Doyle- Murray", "Flushed Away", "Cuban", "Sam Waterston", "Moon Shot  Moon Shot: The Inside Story of America's Race to the Moon", "Chief Strategy Officer", "Fernando Rey", "the Honshu seaport", "the Kaaba", "Charles Francis \"Charlie\" Harper", "Sri Lanka", "the Mediterranean Sea", "video icon", "The spiral tusk"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6729166666666667}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-174", "mrqa_squad-validation-2881", "mrqa_squad-validation-3657", "mrqa_squad-validation-5249", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-358", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-1689", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-720", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-4606", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15236", "mrqa_searchqa-validation-14908", "mrqa_searchqa-validation-6391"], "SR": 0.640625, "CSR": 0.671875, "EFR": 1.0, "Overall": 0.8359375}, {"timecode": 8, "before_eval_results": {"predictions": ["bars, caf\u00e9s and clubs.", "T\u00f6regene Khatun", "Michael Oppenheimer", "3.6", "Second Republic", "highly diversified", "Beijing", "a chain or screw stoking mechanism", "the type of reduction being used", "quickly", "use in chloroplast division", "historians", "stagnant", "Ali Shariati", "an algorithm", "Higher Real Gymnasium", "four", "events and festivals", "Apollo 1 backup crew", "Ikh Zasag", "1883", "independent schools", "Sophocles", "rare and desired", "One in five", "breaches of law", "electric lighting", "Christianized Yamasee", "six", "two", "revelry", "Dan Brown", "tennis", "$250,000", "Tim Clark, Matt Kuchar and Bubba Watson", "The ratio of the length s of the arc", "18th century", "Erica Rivera", "Davos", "breaking the single - season record", "As Chicano studies programs began to be implemented at universities", "a limited period of time", "Portugal", "SpongeBob", "photographer", "a christopher christopher kiefer Sutherland", "Thames Street", "christopher Columbus", "christopher Columbus", "Reginald Engelbach", "Robert \"Bobby\" Germaine, Sr.", "Nip/Tuck", "Manchester United", "John Faso", "The Rite of Spring", "a pioneer", "The Devil's Dictionary", "a Christmas carol.", "a fever.", "Titan", "a major plague pandemic", "9 to 5", "The Star-Spangled Banner", "Dennis Potter"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6293067002442003}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.3333333333333333, 0.0, 0.4, 0.4, 1.0, 0.5, 1.0, 0.2222222222222222, 1.0, 0.0, 0.16, 1.0, 0.16666666666666669, 0.0, 0.0, 0.30769230769230765, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8617", "mrqa_squad-validation-8649", "mrqa_squad-validation-6753", "mrqa_squad-validation-1290", "mrqa_squad-validation-10303", "mrqa_newsqa-validation-2636", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1585", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-688", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-4846", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-1178", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-9818", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-3588"], "SR": 0.546875, "CSR": 0.6579861111111112, "EFR": 0.9655172413793104, "Overall": 0.8117516762452108}, {"timecode": 9, "before_eval_results": {"predictions": ["Ed Lee", "non-Mongol physicians", "the Acasta gneiss of the Slave craton in northwestern Canada", "26", "cabin depressurization", "a restaurant situated at a Grade I-listed 16th century merchant's house at 28\u201330 Close", "Arabic numerals", "private", "nineteenth-century cartographic techniques", "democracy", "ten minutes", "the balance of parties across Parliament", "very rare", "areas cleared of forest", "a lute", "Chagatai", "Tesla Electric Company", "Cleveland, Phoenix, Detroit and Denver", "The Newcastle Beer Festival", "the western end of the second east-west shipping route", "land and housing", "Warsaw University of Technology building", "the plague was present somewhere in Europe in every year between 1346 and 1671", "three sources", "in herring barrels", "Donkey", "Judy Collins", "The Wild Bunch", "coffee beans", "The Flight to France", "a fracas", "Paul McCartney's", "Buffalo Bill Cody's Wild West Show", "Moton Field, the Tuskegee Army Air Field", "120 m ( 390 ft )", "pan control setting", "Tito Jackson, and two long - shot candidates, Robert Cappucci and Joseph Wiley", "Andrew Gold", "Brooke Wexler", "Titanic earned $657.4 million in North America and $1.528 billion in other countries, for a worldwide total of $2.187 billion", "Caracas", "Vienna", "Wawrinka", "Bear Grylls", "Harry Shearer", "1879", "Dian Fossey", "the E Street Band", "Cyclic Defrost", "Nathan Bedford Forrest", "Annales de chimie et de physique", "Hurricane Faith", "40 million", "Columbia Records", "last week", "September, Bianchi's death during childbirth", "the piracy incident was discussed as one of the \"tests\" of President Obama that Joe Biden warned about during the campaign.", "Noida, located in the outskirts of the capital New Delhi", "the Orbiting Carbon Observatory", "1960", "Daryeel Bulasho Guud", "James Jeffries", "Robert Duvall", "Joe Torre"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6778426434676434}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.19999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5234", "mrqa_squad-validation-5137", "mrqa_squad-validation-6059", "mrqa_squad-validation-9310", "mrqa_squad-validation-4054", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-1072", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-2082", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4698", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-535", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-7775"], "SR": 0.59375, "CSR": 0.6515625, "retrieved_ids": ["mrqa_squad-train-22851", "mrqa_squad-train-13529", "mrqa_squad-train-43437", "mrqa_squad-train-37500", "mrqa_squad-train-57860", "mrqa_squad-train-48775", "mrqa_squad-train-42625", "mrqa_squad-train-27742", "mrqa_squad-train-31689", "mrqa_squad-train-25074", "mrqa_squad-train-41724", "mrqa_squad-train-17993", "mrqa_squad-train-75145", "mrqa_squad-train-13233", "mrqa_squad-train-85959", "mrqa_squad-train-5868", "mrqa_squad-validation-10428", "mrqa_naturalquestions-validation-4983", "mrqa_newsqa-validation-4208", "mrqa_naturalquestions-validation-5355", "mrqa_searchqa-validation-4712", "mrqa_triviaqa-validation-7165", "mrqa_searchqa-validation-11848", "mrqa_naturalquestions-validation-283", "mrqa_squad-validation-6614", "mrqa_squad-validation-9810", "mrqa_newsqa-validation-3041", "mrqa_naturalquestions-validation-8203", "mrqa_searchqa-validation-11121", "mrqa_hotpotqa-validation-3060", "mrqa_squad-validation-4676", "mrqa_squad-validation-3771"], "EFR": 0.9615384615384616, "Overall": 0.8065504807692307}, {"timecode": 10, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-3259", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-64", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3722", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4108", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-6486", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7033", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9685", "mrqa_naturalquestions-validation-9866", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1551", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-213", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2928", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3041", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-460", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-91", "mrqa_searchqa-validation-1072", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-1203", "mrqa_searchqa-validation-12103", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12800", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-1435", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-4405", "mrqa_searchqa-validation-4680", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8999", "mrqa_squad-validation-10027", "mrqa_squad-validation-10044", "mrqa_squad-validation-10090", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10125", "mrqa_squad-validation-10136", "mrqa_squad-validation-10192", "mrqa_squad-validation-10211", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10295", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10309", "mrqa_squad-validation-10338", "mrqa_squad-validation-10346", "mrqa_squad-validation-10428", "mrqa_squad-validation-10430", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1123", "mrqa_squad-validation-1146", "mrqa_squad-validation-1187", "mrqa_squad-validation-1211", "mrqa_squad-validation-1218", "mrqa_squad-validation-1226", "mrqa_squad-validation-1253", "mrqa_squad-validation-1277", "mrqa_squad-validation-1290", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1367", "mrqa_squad-validation-1391", "mrqa_squad-validation-1411", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1530", "mrqa_squad-validation-1531", "mrqa_squad-validation-1539", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1664", "mrqa_squad-validation-1690", "mrqa_squad-validation-1695", "mrqa_squad-validation-1720", "mrqa_squad-validation-173", "mrqa_squad-validation-174", "mrqa_squad-validation-1766", "mrqa_squad-validation-1794", "mrqa_squad-validation-1824", "mrqa_squad-validation-1872", "mrqa_squad-validation-1877", "mrqa_squad-validation-1908", "mrqa_squad-validation-1913", "mrqa_squad-validation-1980", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2060", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2321", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2382", "mrqa_squad-validation-2402", "mrqa_squad-validation-2408", "mrqa_squad-validation-2439", "mrqa_squad-validation-2475", "mrqa_squad-validation-2478", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2675", "mrqa_squad-validation-2704", "mrqa_squad-validation-2724", "mrqa_squad-validation-2807", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-288", "mrqa_squad-validation-2881", "mrqa_squad-validation-2955", "mrqa_squad-validation-2961", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3255", "mrqa_squad-validation-3288", "mrqa_squad-validation-3355", "mrqa_squad-validation-3378", "mrqa_squad-validation-3388", "mrqa_squad-validation-3400", "mrqa_squad-validation-3447", "mrqa_squad-validation-3457", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3628", "mrqa_squad-validation-3657", "mrqa_squad-validation-3771", "mrqa_squad-validation-3799", "mrqa_squad-validation-38", "mrqa_squad-validation-3806", "mrqa_squad-validation-3813", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-3915", "mrqa_squad-validation-3916", "mrqa_squad-validation-3942", "mrqa_squad-validation-3971", "mrqa_squad-validation-3986", "mrqa_squad-validation-405", "mrqa_squad-validation-4054", "mrqa_squad-validation-4074", "mrqa_squad-validation-4080", "mrqa_squad-validation-409", "mrqa_squad-validation-4100", "mrqa_squad-validation-4127", "mrqa_squad-validation-4137", "mrqa_squad-validation-4149", "mrqa_squad-validation-4192", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-4320", "mrqa_squad-validation-4332", "mrqa_squad-validation-437", "mrqa_squad-validation-4425", "mrqa_squad-validation-4427", "mrqa_squad-validation-4439", "mrqa_squad-validation-4475", "mrqa_squad-validation-4488", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-453", "mrqa_squad-validation-4629", "mrqa_squad-validation-4642", "mrqa_squad-validation-4658", "mrqa_squad-validation-4675", "mrqa_squad-validation-4676", "mrqa_squad-validation-4701", "mrqa_squad-validation-4711", "mrqa_squad-validation-477", "mrqa_squad-validation-477", "mrqa_squad-validation-4795", "mrqa_squad-validation-4801", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4902", "mrqa_squad-validation-4930", "mrqa_squad-validation-5013", "mrqa_squad-validation-503", "mrqa_squad-validation-5063", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5208", "mrqa_squad-validation-5226", "mrqa_squad-validation-5234", "mrqa_squad-validation-5249", "mrqa_squad-validation-5260", "mrqa_squad-validation-5300", "mrqa_squad-validation-5320", "mrqa_squad-validation-5333", "mrqa_squad-validation-5360", "mrqa_squad-validation-545", "mrqa_squad-validation-551", "mrqa_squad-validation-5531", "mrqa_squad-validation-5535", "mrqa_squad-validation-5550", "mrqa_squad-validation-5597", "mrqa_squad-validation-5620", "mrqa_squad-validation-5631", "mrqa_squad-validation-5715", "mrqa_squad-validation-5721", "mrqa_squad-validation-5721", "mrqa_squad-validation-5736", "mrqa_squad-validation-5891", "mrqa_squad-validation-5908", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-5991", "mrqa_squad-validation-6059", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6156", "mrqa_squad-validation-6166", "mrqa_squad-validation-6191", "mrqa_squad-validation-6293", "mrqa_squad-validation-6326", "mrqa_squad-validation-6347", "mrqa_squad-validation-6389", "mrqa_squad-validation-6404", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6471", "mrqa_squad-validation-6473", "mrqa_squad-validation-6610", "mrqa_squad-validation-6614", "mrqa_squad-validation-6639", "mrqa_squad-validation-6644", "mrqa_squad-validation-6650", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6753", "mrqa_squad-validation-6759", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6889", "mrqa_squad-validation-6896", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6978", "mrqa_squad-validation-6988", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-7086", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7189", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7288", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7446", "mrqa_squad-validation-7466", "mrqa_squad-validation-7490", "mrqa_squad-validation-7502", "mrqa_squad-validation-7504", "mrqa_squad-validation-7508", "mrqa_squad-validation-7526", "mrqa_squad-validation-754", "mrqa_squad-validation-7563", "mrqa_squad-validation-7609", "mrqa_squad-validation-7653", "mrqa_squad-validation-7707", "mrqa_squad-validation-7718", "mrqa_squad-validation-7726", "mrqa_squad-validation-7727", "mrqa_squad-validation-7731", "mrqa_squad-validation-7744", "mrqa_squad-validation-7751", "mrqa_squad-validation-7767", "mrqa_squad-validation-778", "mrqa_squad-validation-7789", "mrqa_squad-validation-7813", "mrqa_squad-validation-7926", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7954", "mrqa_squad-validation-7997", "mrqa_squad-validation-8107", "mrqa_squad-validation-811", "mrqa_squad-validation-8154", "mrqa_squad-validation-8204", "mrqa_squad-validation-8212", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8269", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8350", "mrqa_squad-validation-8409", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8575", "mrqa_squad-validation-8576", "mrqa_squad-validation-8617", "mrqa_squad-validation-8643", "mrqa_squad-validation-8649", "mrqa_squad-validation-8658", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-9038", "mrqa_squad-validation-9103", "mrqa_squad-validation-916", "mrqa_squad-validation-9189", "mrqa_squad-validation-930", "mrqa_squad-validation-9310", "mrqa_squad-validation-932", "mrqa_squad-validation-934", "mrqa_squad-validation-9376", "mrqa_squad-validation-9378", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9476", "mrqa_squad-validation-9488", "mrqa_squad-validation-9498", "mrqa_squad-validation-9505", "mrqa_squad-validation-9525", "mrqa_squad-validation-9575", "mrqa_squad-validation-9590", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9717", "mrqa_squad-validation-9731", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9787", "mrqa_squad-validation-9810", "mrqa_squad-validation-9853", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_squad-validation-9920", "mrqa_squad-validation-9962", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-1883", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2106", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-370", "mrqa_triviaqa-validation-4178", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-512", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5984", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-6170", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-770", "mrqa_triviaqa-validation-7735", "mrqa_triviaqa-validation-7775"], "OKR": 0.921875, "KG": 0.44609375, "before_eval_results": {"predictions": ["Article 5", "0.3 to 0.6 \u00b0C", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "Meredith Vieira", "extremely difficult", "card catalogs", "5,000 years", "Utopia", "hydrogen and helium", "Central Pacific Railroad", "equality", "Jan Andrzej Menich", "private conferences", "The View and The Chew", "Fiorello La Guardia", "Goodell", "lower bounds", "over 90", "the same message routing methodology as developed by Baran", "as soon as they enter into force", "Sakya", "1688\u20131692", "French and Belgian delegates urged occupying the Ruhr as a way of forcing Germany to pay more, while the British delegate urged a lowering of the payments", "The United States is the only Western country currently applying the death penalty, one of 57 countries worldwide applying it, and was the first to develop lethal injection as a method of execution", "a major fall in stock prices", "Claims adjuster", "Dasharatha", "Grand Inquisition", "more than a million members", "disputes between two or more states", "soybean pods", "the shoulder", "Orson Welles", "Razor", "the plains bison", "bitter almond", "show business", "Gloucestershire", "Wilhelmus Simon Petrus Fortuijn", "Mr. Tumnus", "Augusta Ada King-Noel, Countess of Lovelace", "703", "Mauritian", "Lee Sun-mi", "music genres of electronic rock, electropop and R&B", "Hawaii", "Bill Gates", "56", "Jared Polis", "james Heim", "Seminole Tribe", "girls around 11 or 12.", "last week", "sculptures", "lite-mino", "Eris", "WFC", "Lumbini, Nepal", "the list of dos & don'ts", "the Moon", "the left side of the heart pumps oxygenated blood to the body", "(Anthony) trollope", "throng", "Silence of the Lambs"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6069568452380952}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.12500000000000003, 1.0, 0.0, 0.5, 0.6666666666666666, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.125, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6811", "mrqa_squad-validation-5505", "mrqa_squad-validation-3667", "mrqa_squad-validation-4588", "mrqa_naturalquestions-validation-4071", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-572", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-8092", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-2213", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-4133", "mrqa_newsqa-validation-3402", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3493", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-7853", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3937"], "SR": 0.515625, "CSR": 0.6392045454545454, "EFR": 1.0, "Overall": 0.7569034090909091}, {"timecode": 11, "before_eval_results": {"predictions": ["the expulsion of the Acadians", "The E. W. Scripps Company", "1974", "the Uighurs of the Kingdom of Qocho", "15", "Invocavit Sermons", "1899", "microorganisms", "Ealy", "Von Miller", "10", "23.9%", "cattle and citrus", "establish, equip, manage and maintain national and public libraries in the country", "In the 1060s, Robert Crispin led the Normans of Edessa against the Turks", "1862", "William the Lion", "external combustion engines", "Ten", "a proper legal basis", "an Islamic shrine located on the Temple Mount in the Old City of Jerusalem", "the utopian novels of H.G. Wells", "Albert Einstein", "drizzle, rain, sleet, snow, graupel and hail", "Richard Wright and non-lexical vocals by Clare Torry", "September 8, 2017", "Jehnna ( Olivia d'Abo )", "In the 1920s", "the newly independent nation, then known as the Republic of Spanish Haiti, was invaded by Haiti in 1822", "jack johnson", "james johnson", "a double dip recession", "Darby and Joan", "Bronx Mowgli", "a terrorist group (ETA) with politically motivated violence intended to instill fear in a population so the population will then influence policy and decision makers", "Jules Verne", "the United Kingdom and China", "Cheshire Phoenix is a professional basketball team based in Ellesmere Port, United Kingdom", "Gweilo", "Nick on Sunset theater in Hollywood", "various", "2006", "1943", "The Design Inference", "1861", "John Lennon and George Harrison", "Iran could develop a nuclear weapon and it is close to achieving that desire, does that mean it would use it against Israel", "One of Osama bin Laden's sons", "the inspector-general", "seven", "on an eight-day trip through Greece, the birthplace of the Olympics, before being transported to Canada", "Henrik Stenson", "a medicine that contained the banned substance cortisone", "Boston", "Walter Reed", "Like a Rock", "Indiana", "Canada", "Edgar Rice Burroughs", "a man whose brilliant and radically... In a variety of minor characters (Lois Cook, Ike the Genius, Gus Webb)", "a robot subs", "Russia", "James Lillywhite, Alfred Shaw and Arthur Shrewsbury", "Lesley Garrett"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6776690884687209}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.25, 0.4, 1.0, 0.8, 1.0, 0.11764705882352941, 0.4, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.16666666666666669, 0.2222222222222222, 0.0, 0.0, 0.13333333333333336, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2705", "mrqa_squad-validation-999", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6708", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-3588", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-5891", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-1458", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1654", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-13198", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2684"], "SR": 0.578125, "CSR": 0.6341145833333333, "EFR": 1.0, "Overall": 0.7558854166666666}, {"timecode": 12, "before_eval_results": {"predictions": ["The Knowledge School", "cloud storage service", "\"Into your hand I commit my spirit; you have redeemed me, O Lord, faithful God\"", "September 5, 1985", "62", "Jean Ribault", "James Bryant Conant", "journalism", "Western", "a green algal derived chloroplast", "electromagnetic", "in the kingdom", "Chicago Bears", "observer", "requiring his arrest", "giving her brother Polynices a proper burial", "26", "a larger challenge to the legal system that permits those decisions to be taken", "Iowa ( 36.6 % )", "Pepsi", "Blue with a harp of gold", "Curtis Armstrong", "Hem Chandra Bose", "due to Parker's pregnancy at the time of filming", "of Paris", "Afonso IV of Portugal", "Julie Gonzalo", "ostrich", "Jean Alexander", "Ellen Mary", "Frank Whittle", "Alfred Wainwright", "McDonnell Douglas", "3000m", "John Masefield", "1966", "The Prodigy", "\u00c6thelstan", "Sean Yseult", "the performance of Hofmannsthal's \"Jedermann\"", "Les Miles", "World War I", "County of York", "Charles Quinton Murphy", "Seminole and Miccosukee tribes", "Haiti.", "he to step down as majority leader.", "Wednesday.", "330- lbs", "and his administration didn't seriously consider an African-American woman for the job.", "seven", "the Bainbridge would be getting backup shortly.", "Araceli Valencia", "Sabina Guzzanti", "Margot Kidder", "free Expression", "(Bill) Hickok", "Mario Puzo", "of New Zealand", "bicarbonate", "bashing", "Salieri", "Rudyard Kipling", "Bangladesh"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5888606644580909}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.25, 1.0, 1.0, 0.823529411764706, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.923076923076923, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2686", "mrqa_squad-validation-2550", "mrqa_squad-validation-249", "mrqa_squad-validation-6750", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-1925", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-5120", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-59", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-633", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-14081"], "SR": 0.484375, "CSR": 0.6225961538461539, "retrieved_ids": ["mrqa_squad-train-58790", "mrqa_squad-train-18946", "mrqa_squad-train-59856", "mrqa_squad-train-77655", "mrqa_squad-train-82480", "mrqa_squad-train-79700", "mrqa_squad-train-24597", "mrqa_squad-train-30992", "mrqa_squad-train-82437", "mrqa_squad-train-81557", "mrqa_squad-train-71507", "mrqa_squad-train-33421", "mrqa_squad-train-29072", "mrqa_squad-train-50277", "mrqa_squad-train-54031", "mrqa_squad-train-82355", "mrqa_triviaqa-validation-4178", "mrqa_squad-validation-10430", "mrqa_newsqa-validation-3503", "mrqa_naturalquestions-validation-3157", "mrqa_squad-validation-1187", "mrqa_naturalquestions-validation-7223", "mrqa_newsqa-validation-358", "mrqa_squad-validation-9488", "mrqa_triviaqa-validation-1415", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-1425", "mrqa_naturalquestions-validation-9149", "mrqa_triviaqa-validation-4958", "mrqa_hotpotqa-validation-3876", "mrqa_newsqa-validation-3882", "mrqa_hotpotqa-validation-4133"], "EFR": 0.9696969696969697, "Overall": 0.7475211247086248}, {"timecode": 13, "before_eval_results": {"predictions": ["a lack of remorse, an attempt to avoid responsibility for her actions, and even a likelihood of repeating her illegal actions", "can produce both eggs and sperm at the same time", "Necessity-based", "Tesla coil", "2008", "spin", "Capability deprivation", "San Jose State", "1954", "Latin Rhenus", "Xbox One", "the bishop has read the appointments at the session of the Annual Conference", "1996", "Italian Plague of 1629\u20131631", "Northumbria University", "calcitriol", "2015", "Merrimen", "Andy Serkis", "Major Molineux", "Andrew Johnson", "2012", "1807", "1975", "Massachusetts", "from the Ute name for them", "16.5 feet", "Anser", "yellow", "second", "Nigel Short", "judoka", "Manchester", "leprosy", "Triumph", "Ouse and Foss", "a Christian church", "Government of Ireland", "Mathieu Kassovitz", "Red Rock West", "Slaughterhouse-Five", "three", "December 24, 1973", "1865", "nearly $162 billion in war funding", "almost 100", "his mother", "second", "Ozzy Osbourne", "Bob Dole", "composer of \"Phantom of the Opera\" and \"Cats\"", "strife in Somalia", "105-year history", "fibula", "ruffled pasta", "Newton", "Trinity", "Brigham Young", "China", "Catholic Archdiocese of Los Angeles", "South Korea", "Sideways", "Romance", "Hawaii Republican"], "metric_results": {"EM": 0.625, "QA-F1": 0.6882947781385281}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false], "QA-F1": [0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-6735", "mrqa_squad-validation-4645", "mrqa_squad-validation-9247", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-74", "mrqa_triviaqa-validation-6585", "mrqa_triviaqa-validation-1003", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-1811", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2453", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-6636", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-5808"], "SR": 0.625, "CSR": 0.6227678571428572, "EFR": 1.0, "Overall": 0.7536160714285715}, {"timecode": 14, "before_eval_results": {"predictions": ["as far back as the early Cambrian, about 515 million years ago", "tentilla", "a problem instance", "Knights Templar", "Algeria", "Edinburgh Pentlands", "828,000", "1206", "a thylakoid", "WLS", "about half", "Scotland", "Algeria", "Cadeby stone", "colonizing empires", "9 February 2018", "charbagh", "the 1960s", "22 November 1914", "Skat", "Herbert Hoover", "during Hanna's recovery masquerade celebration", "John Young", "David Joseph Madden", "lighter fluid", "Mr Loophole", "Homo sapiens.", "mudflats", "1881", "NASCAR", "Who We Are", "Roger Black", "Nova Scotia", "skebras", "Tampa", "Mineola", "Sofia the First", "Hong Kong", "was an American painter and writer who wrote the autobiography \"The Bite in the Apple\" about her relationship with Apple co-founder Steve Jobs.", "Hong Kong, New York City, London", "Chief of the Operations Staff", "Massapequa", "Kansas City crime family", "iPods", "death squad killings", "Woosuk Ken Choi", "Dominic Adiyiah", "Garth Brooks", "1,700 year old Roman mosaic", "nuclear", "five", "it has not intercepted any", "John Bunyan", "Steven Spielberg", "koan", "Diogenes", "radioactive leak", "Daniel Defoe", "National Gallery of Art", "Catherine", "Fairness Doctrine", "Michoacan state", "the abduction of minors.", "an Italian and six Africans"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6720305735930736}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 0.4, 0.0, 0.0, 1.0, 1.0, 0.36363636363636365, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4474", "mrqa_squad-validation-4957", "mrqa_squad-validation-9480", "mrqa_naturalquestions-validation-9222", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-4920", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-7154", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-3538", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-1675", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-12292", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-9954", "mrqa_newsqa-validation-2821"], "SR": 0.609375, "CSR": 0.621875, "EFR": 0.88, "Overall": 0.7294375000000001}, {"timecode": 15, "before_eval_results": {"predictions": ["Since the 1980s", "New York City", "a net", "energize electrons", "because \"prosecutors have reasoned (correctly) that if they arrest fully informed jury leafleters, the leaflets will have to be given to the leafleter's own jury as evidence.\"", "20th", "Lower Lorraine", "wave speeds", "three", "steady stream of oxygen gas", "a system of many biological structures and processes within an organism that protects against disease", "1606", "1951", "humy", "CIA", "humy Stout", "George Bernard Shaw", "Clarence Thomas", "Washington Irving", "denis", "Brooklyn", "David Lee Roth", "Abraham Lincoln", "Under normal conditions", "204,408 in 2013", "three more proton and is less metallic than its predecessor", "June 27, 2008", "a specific task", "in the middle of the Sermon on the Mount, and the short form in the Gospel of Luke", "the United States is the world's third - or fourth - largest country by total area and the third-most populous", "Staci Love", "asexually", "Donna", "sheer luck and following a strategy will not change the results", "Elizabeth Taylor", "Western Australia", "Sergeant-Major Bullimore", "Packers", "perfume", "1882", "brief history and the details of their incorporation in Canada", "the world's most prestigious universities", "Dick Turpin", "10th Cavalry Regiment", "Polo Grounds", "early 20th-century Europe.", "2,664", "Anna Clyne", "Caesars Entertainment Corporation", "13 May 2018", "Dhivehi Raa'jeyge Jumhooriyya", "Sam the Sham", "Lalit", "elephants, and only a handful of media members are able to visit each year, in an effort to make the animals' lives as natural as possible.", "At least 15", "the Ministry of Defense", "Kenneth Cole", "anaphylaxis", "10 to 15 percent", "had the surgery December 13 after lumpectomies failed to eradicate her breast cancer.", "the charity of kidnapping the children and concealing their identities.", "Isaac, 8, Hope, 7, Noah, 5, Phoebe Joy, 3, Lydia Beth, 2, Annie, also 2, and Pe peanut the dog.", "in the Angeles National Forest", "Jaime Andrade"], "metric_results": {"EM": 0.5, "QA-F1": 0.5618822150072149}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8636363636363636, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.375, 0.125, 0.5, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.13333333333333333, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4708", "mrqa_squad-validation-6832", "mrqa_squad-validation-3602", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-3026", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-1193", "mrqa_triviaqa-validation-2308", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-7215", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-4180", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-5502", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-1016", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-3621"], "SR": 0.5, "CSR": 0.6142578125, "retrieved_ids": ["mrqa_squad-train-75259", "mrqa_squad-train-14944", "mrqa_squad-train-78326", "mrqa_squad-train-10453", "mrqa_squad-train-52469", "mrqa_squad-train-5797", "mrqa_squad-train-84495", "mrqa_squad-train-63009", "mrqa_squad-train-86208", "mrqa_squad-train-2176", "mrqa_squad-train-68462", "mrqa_squad-train-38237", "mrqa_squad-train-73402", "mrqa_squad-train-81212", "mrqa_squad-train-77929", "mrqa_squad-train-38095", "mrqa_naturalquestions-validation-7659", "mrqa_searchqa-validation-1072", "mrqa_hotpotqa-validation-5365", "mrqa_squad-validation-6753", "mrqa_triviaqa-validation-1676", "mrqa_hotpotqa-validation-751", "mrqa_newsqa-validation-427", "mrqa_triviaqa-validation-6585", "mrqa_naturalquestions-validation-7657", "mrqa_squad-validation-5620", "mrqa_triviaqa-validation-2265", "mrqa_squad-validation-4902", "mrqa_triviaqa-validation-7775", "mrqa_searchqa-validation-7746", "mrqa_newsqa-validation-1159", "mrqa_searchqa-validation-11848"], "EFR": 1.0, "Overall": 0.7519140625}, {"timecode": 16, "before_eval_results": {"predictions": ["Rhin", "4", "chloroplasts", "neuronal dendrites", "evenly round the body", "Switzerland", "BSkyB", "British", "James E. Webb", "nitroaereus", "tidal delta", "DC traction motor", "Tokyo", "a bishop", "(Emma) Thompson", "eggshells", "Mort Sahl", "the Ford Motor Company", "silver", "William Howard Taft", "Delaware", "Da Vinci Code blue", "alveolar process", "a jealous ex-lover", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "The Pardoner's Tale", "Kanawha River", "a judge who lacks compassion", "brothers Henry, Jojo and Ringo Garza", "Mohammad Reza Pahlavi", "Pakistan", "the utopian novels of H.G. Wells", "saccharides", "The Bank of England", "Brussels", "Gary Oldman", "algae", "(John) Carpenter", "Midnight Cowboy", "james", "Germany", "Bratislava", "Rice University", "Mexico", "wargame", "March 23, 2017", "postmodern", "five", "1979", "Salzburg Festival", "three different covers", "Saint Louis County", "200", "cancerous tumor.", "5,600", "the California Highway Patrol", "a 57-year old male", "Citizens", "Empire of the Sun", "a Yemeni cleric and his personal assistant", "Jacob", "Jason Chaffetz", "Herbert Hoover", "New Croton Reservoir"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5633181871233341}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.35294117647058826, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.14285714285714288, 1.0, 0.6666666666666666, 1.0, 0.5454545454545454]}}, "before_error_ids": ["mrqa_squad-validation-10416", "mrqa_squad-validation-8792", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-10895", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-2196", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-3095", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-4859", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-3174", "mrqa_naturalquestions-validation-6035"], "SR": 0.515625, "CSR": 0.6084558823529411, "EFR": 1.0, "Overall": 0.7507536764705882}, {"timecode": 17, "before_eval_results": {"predictions": ["certifying, governing and enforcing the standards of practice for the teaching profession", "SAP Center in San Jose", "\"ash tree\" in Spanish, and an ash leaf is featured on the city's flag", "$5 million", "17", "mercuric oxide", "the Pacific", "Esel (\"Donkey\")", "a modern canalized section", "John Fox", "Sava Kosanovi\u0107", "the Twin Towers", "Alaska", "Spain and Portugal", "the 19th century", "klezmer", "Tiffany", "a locking pin", "Edith Wharton", "Norman Bates", "Indira Gandhi", "the intentional burning of almost any type of structure, building or forest", "the `` main line '' or `` first line '' is the main section of the parade, or the members of the actual club with the parading permit as well as the brass band", "the base of the right ventricle", "Indian Standard Time", "Yugoslav model of state organization, as well as a `` middle way '' between planned and liberal economy,", "Road / Track ( no `` and '' )", "in 1904, when the United States won all three medals, and in 1908 when the podium was occupied by three British teams", "their need to repent in time", "the intersection of Del Monte Blvd and Esplanade Street", "used as a pH indicator, a color marker, and a dye", "William Chatterton Dix", "france", "Austria", "The History Boys", "The Gambia", "the Kentucky Derby", "the Netherlands", "Robert Stroud", "William Lamb", "Malcolm Bradbury", "Pat Cash", "chalk quarry", "John II Casimir Vasa", "200,167", "American", "The Social Network", "2001", "926 East McLemore Avenue", "the increased risk of terrorist activity against Norwegian interests, including the oil platforms in the North Sea", "Dan Brandon Bilzerian", "YouTube", "jobs up and down the auto supply chain: from dealers to assembly workers and parts markers", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "the Russian word \"peregruzka\" printed on it.", "Nicole", "breast cancer.", "the Cowardly Lion", "high tide -- expected to reach about 4 meters (13 feet) high", "the auto industry has so many connected jobs in real estate, finance, manufacturing and other industries, that is hard to separate it out.", "more than 1.2 million people.", "Thabo Mbeki", "George I", "wake"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6335511406233835}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.06451612903225806, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5217391304347826, 0.9411764705882353, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.8, 1.0, 0.09523809523809522, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.2285714285714286, 0.888888888888889, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2022", "mrqa_squad-validation-457", "mrqa_squad-validation-4634", "mrqa_squad-validation-9190", "mrqa_searchqa-validation-8891", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-9094", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-6384", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-3931", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1380"], "SR": 0.515625, "CSR": 0.6032986111111112, "EFR": 0.9354838709677419, "Overall": 0.7368189964157706}, {"timecode": 18, "before_eval_results": {"predictions": ["if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "technological superiority", "tentilla", "Levi's Stadium", "a drug treatment for an individual", "Charlesfort", "2000", "five", "24 September 2007", "Infrastructure", "various newspaper reporters, including Sylvia F. Porter in a column for the May 4, 1951, edition of the New York Post", "Missouri River", "1960s", "a bow bridge with 16 arches shielded by ice guards", "more than 80 tank\u014dbon volumes", "on the Atlantic Ocean at the mouth of the Chesapeake Bay", "Gregor Mendel", "United Nations Peacekeeping Operations", "on a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Latitude", "20 locations all within the Pittsburgh metropolitan area", "Hyundai", "Sherlock Holmes", "the United States and Mexico", "congregation", "John Barbirolli", "Tahrir Square", "histamine", "100 years", "Paul C\u00e9zanne", "a cigarette", "Germany", "Philip Livingston", "Suzanne N.J. 'Susie' Chun Oakland is a Democratic member of the Hawaii Senate, representing the 13th District since 1996", "DreamWorks Animation", "Nick Harper", "London", "1926", "She received an Olivier Award for Best Actress in 2013 for her West End performance in \"The Audience\", in which she also portrayed Elizabeth II,", "Nebraska Cornhuskers women's basketball team", "Wu-Tang Clan", "This Picture winner \"Slumdog Millionaire\" (No. 4)", "two", "Caylee, who was 2", "Beijing", "11", "behavioral health-care provider.", "Amsterdam, in the Netherlands, to Ankara, Turkey", "October 19", "an open window that fits neatly around him", "display its 10-foot-tall, black, rat-shaped balloon at a rally held outside a fitness center.", "This syndicated TV show about show biz--a job I've held since 1982", "a cricket", "R2-D2", "Donald Trump", "dampers", "Sam Kinison", "The Vandellas", "Sisyphus", "Veep", "You have made my life complete", "Elizabeth Taylor", "fish", "Viking feet"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6575428911366412}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09999999999999999, 1.0, 1.0, 0.3636363636363636, 0.7499999999999999, 0.22222222222222224, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.923076923076923, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6282", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-5049", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3721", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3132", "mrqa_triviaqa-validation-712", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-3773", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-3858", "mrqa_newsqa-validation-3602", "mrqa_searchqa-validation-6221", "mrqa_searchqa-validation-13223", "mrqa_searchqa-validation-10860", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-7646", "mrqa_triviaqa-validation-3215"], "SR": 0.53125, "CSR": 0.5995065789473684, "retrieved_ids": ["mrqa_squad-train-67897", "mrqa_squad-train-82668", "mrqa_squad-train-32370", "mrqa_squad-train-70975", "mrqa_squad-train-61061", "mrqa_squad-train-84036", "mrqa_squad-train-38786", "mrqa_squad-train-77115", "mrqa_squad-train-56722", "mrqa_squad-train-37650", "mrqa_squad-train-7448", "mrqa_squad-train-3729", "mrqa_squad-train-70650", "mrqa_squad-train-508", "mrqa_squad-train-6416", "mrqa_squad-train-15916", "mrqa_naturalquestions-validation-6708", "mrqa_triviaqa-validation-6537", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-11561", "mrqa_newsqa-validation-3084", "mrqa_squad-validation-8649", "mrqa_hotpotqa-validation-4813", "mrqa_triviaqa-validation-6467", "mrqa_newsqa-validation-2353", "mrqa_naturalquestions-validation-572", "mrqa_searchqa-validation-3588", "mrqa_newsqa-validation-1210", "mrqa_squad-validation-6404", "mrqa_triviaqa-validation-3086", "mrqa_naturalquestions-validation-10161", "mrqa_triviaqa-validation-1403"], "EFR": 0.9666666666666667, "Overall": 0.742297149122807}, {"timecode": 19, "before_eval_results": {"predictions": ["Brandon McManus", "the Compromise of 1850", "1884", "Seven Stories", "1332", "Morgan", "Space", "rococo", "12.5", "Florence", "Canada goose", "the American Civil War", "the Siege of Orlans", "the part of a plant that is often brightly coloured and has a pleasant smell", "Star Trek", "the circulatory system", "Yitzhak Rabin", "The Letters of Lord Nelson to Lady Hamilton", "four", "Providence", "around the world", "landowner", "July 14, 2017", "at birth", "Chris Rea", "Anna Faris", "outside cultivated areas", "throughout the United States", "the most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "through the buttock and down the lower limb", "Ledger", "Mexico", "Funchal", "Geena Davis", "Peter MacTaggart", "massively multiplayer online games", "the Holy Roman Empire", "a board that has lines and pads that connect various points together", "Gaston Leroux", "Whitney Elizabeth Houston", "from 0 (or blank) to 6", "Scotland", "local South Australian and Australian produced content", "Big 12 Conference", "Animorphs", "MMA", "Vic Chesnutt", "William Douglas", "The Timekeeper", "2013", "poetry, theater, art, music, the media, and books", "on Shawnee Mission Parkway", "Native American tribes", "The drama of the action in-and-around the golf course has enraptured fans of the game through the generations and around the world.", "having an affair with a woman in Argentina.", "onto the college campus.", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "in the Yemeni port city of Aden", "the second missing person", "American icon's", "of a 15-year-old boy that has left dozens injured and scores of properties destroyed.", "use of torture and indefinite detention", "at least 25 dead", "Fareed Zakaria"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5729767628205128}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-15149", "mrqa_searchqa-validation-6804", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-1838", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3891"], "SR": 0.53125, "CSR": 0.59609375, "EFR": 0.9666666666666667, "Overall": 0.7416145833333333}, {"timecode": 20, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1136", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4685", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5891", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3837", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4410", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9852", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9896", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2353", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2488", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-3948", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-633", "mrqa_newsqa-validation-716", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-90", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-97", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-10926", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11543", "mrqa_searchqa-validation-12652", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-13987", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14795", "mrqa_searchqa-validation-14908", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15552", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2653", "mrqa_searchqa-validation-2996", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-3588", "mrqa_searchqa-validation-410", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-4551", "mrqa_searchqa-validation-5005", "mrqa_searchqa-validation-5028", "mrqa_searchqa-validation-5169", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7988", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8463", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-9748", "mrqa_searchqa-validation-9818", "mrqa_squad-validation-10087", "mrqa_squad-validation-10102", "mrqa_squad-validation-10103", "mrqa_squad-validation-10192", "mrqa_squad-validation-1021", "mrqa_squad-validation-10223", "mrqa_squad-validation-10293", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-1061", "mrqa_squad-validation-1176", "mrqa_squad-validation-1277", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-1349", "mrqa_squad-validation-1410", "mrqa_squad-validation-143", "mrqa_squad-validation-1530", "mrqa_squad-validation-1539", "mrqa_squad-validation-1577", "mrqa_squad-validation-1584", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1688", "mrqa_squad-validation-1766", "mrqa_squad-validation-1767", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2049", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2382", "mrqa_squad-validation-2408", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2533", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2819", "mrqa_squad-validation-2854", "mrqa_squad-validation-2955", "mrqa_squad-validation-2956", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3447", "mrqa_squad-validation-3516", "mrqa_squad-validation-3518", "mrqa_squad-validation-3530", "mrqa_squad-validation-3590", "mrqa_squad-validation-3601", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3628", "mrqa_squad-validation-3667", "mrqa_squad-validation-3806", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4054", "mrqa_squad-validation-4063", "mrqa_squad-validation-4074", "mrqa_squad-validation-409", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-4173", "mrqa_squad-validation-42", "mrqa_squad-validation-4260", "mrqa_squad-validation-4262", "mrqa_squad-validation-437", "mrqa_squad-validation-4439", "mrqa_squad-validation-453", "mrqa_squad-validation-457", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4642", "mrqa_squad-validation-4676", "mrqa_squad-validation-4772", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-49", "mrqa_squad-validation-4954", "mrqa_squad-validation-4957", "mrqa_squad-validation-509", "mrqa_squad-validation-5129", "mrqa_squad-validation-5137", "mrqa_squad-validation-5156", "mrqa_squad-validation-5197", "mrqa_squad-validation-5211", "mrqa_squad-validation-5229", "mrqa_squad-validation-526", "mrqa_squad-validation-5272", "mrqa_squad-validation-5477", "mrqa_squad-validation-5492", "mrqa_squad-validation-5505", "mrqa_squad-validation-551", "mrqa_squad-validation-5550", "mrqa_squad-validation-5592", "mrqa_squad-validation-5631", "mrqa_squad-validation-5721", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6060", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-6119", "mrqa_squad-validation-612", "mrqa_squad-validation-6231", "mrqa_squad-validation-6254", "mrqa_squad-validation-6282", "mrqa_squad-validation-6404", "mrqa_squad-validation-6471", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6564", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6695", "mrqa_squad-validation-6750", "mrqa_squad-validation-6753", "mrqa_squad-validation-677", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6877", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-6970", "mrqa_squad-validation-6990", "mrqa_squad-validation-7029", "mrqa_squad-validation-704", "mrqa_squad-validation-7086", "mrqa_squad-validation-7090", "mrqa_squad-validation-7133", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7233", "mrqa_squad-validation-7270", "mrqa_squad-validation-7273", "mrqa_squad-validation-7288", "mrqa_squad-validation-7322", "mrqa_squad-validation-739", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7514", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-794", "mrqa_squad-validation-7945", "mrqa_squad-validation-7958", "mrqa_squad-validation-7988", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-823", "mrqa_squad-validation-8278", "mrqa_squad-validation-83", "mrqa_squad-validation-8342", "mrqa_squad-validation-8352", "mrqa_squad-validation-839", "mrqa_squad-validation-8412", "mrqa_squad-validation-8443", "mrqa_squad-validation-85", "mrqa_squad-validation-8500", "mrqa_squad-validation-8600", "mrqa_squad-validation-8643", "mrqa_squad-validation-8695", "mrqa_squad-validation-8779", "mrqa_squad-validation-8871", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9365", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9592", "mrqa_squad-validation-9596", "mrqa_squad-validation-9628", "mrqa_squad-validation-9643", "mrqa_squad-validation-9675", "mrqa_squad-validation-9680", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-9762", "mrqa_squad-validation-9776", "mrqa_squad-validation-9859", "mrqa_squad-validation-9869", "mrqa_squad-validation-9920", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1838", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-2060", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3048", "mrqa_triviaqa-validation-3050", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3132", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-4634", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-549", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-6107", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-7154", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7337", "mrqa_triviaqa-validation-7443", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7472", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-839"], "OKR": 0.876953125, "KG": 0.4421875, "before_eval_results": {"predictions": ["2004", "extremely old", "a liquid in specially insulated tankers", "greater equality but not per capita income", "a green algal derived chloroplast", "3.55 inches", "Troika Design Group", "an Islamic rebellion against an allied Marxist regime in the Afghan Civil War", "a second lieutenant", "The Rolling Stones", "\"Everybody Lies\"", "toga pulla", "a blow", "Wayne & Brent Gretzky", "C.S. Lewis", "furlong", "a Baby Goat", "Coleridge's most famous poems", "Ringo Starr", "Dan's semi-autobiographical novel Inside Nate and Eric's literary counterparts", "parthenogenesis", "The Cornett family", "Zhu Yuanzhang", "Bob Dylan", "Sam Waterston", "a scythe", "the pressure is assumed to be 1 atm ( 101.325 kPa )", "Eddie Murphy", "the band's logo in gold lettering over black sleeve", "1700 Cascadia earthquake", "Al Shean", "para handy", "\"peasant,\"", "kendo", "tintagel", "300", "\"The Famous Toll House cookie recipe\"", "1944", "the Federal Reserve System", "Marlon Brando", "Neighbours", "Australia", "Christianity Today", "five", "3,384,569", "Steve Coogan", "Marcella", "25", "Nashville", "2006", "Chris Anderson", "Saddle Rock Elementary School", "I, the chief executive officer,", "almost 100", "Tibet's independence from China,", "opened considerably higher Tuesday and saw an unprecedented wave of buying amid the elections.", "videos of the chaos and horrified reactions after the July 7, 2005, London transit bombings", "Body Tap,", "Kim Il Sung's", "Saturday.", "The social and political vitality of the nation", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "The Real Housewives of Atlanta", "Renoir\u00b4s art historian, and editor of the Gazette des Beaux-Arts Charles Ephrussi,"], "metric_results": {"EM": 0.5, "QA-F1": 0.625147128597816}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.4, 0.2666666666666667, 1.0, 0.0, 1.0, 0.2222222222222222, 0.9565217391304348, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3689", "mrqa_squad-validation-9694", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-14127", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-5746", "mrqa_searchqa-validation-16866", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-4266", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-190", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-1038", "mrqa_newsqa-validation-250", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-3605", "mrqa_triviaqa-validation-1423"], "SR": 0.5, "CSR": 0.5915178571428572, "EFR": 0.96875, "Overall": 0.7239285714285715}, {"timecode": 21, "before_eval_results": {"predictions": ["non-violent", "$2 million", "specialised education and training", "Middle Miocene", "utterly debased", "Robert Maynard Hutchins", "five", "Mark Anthony \"Baz\" Luhrmann", "Christine MacIntyre", "Michele Bachmann", "French actress", "beer and soft drinks", "Sim Theme Park", "Everything Is wrong", "Congo River", "the fictional city of Quahog, Rhode Island", "\"Firestorm\", \"The Spectre\", and \"Martian Manhunter\"", "December 5, 1991", "the efferent nerves that directly innervate muscles", "renoir", "stable, non-radioactive rubidium", "Nancy Jean Cartwright", "seven", "seawater pearls", "Santa Fe, New Mexico", "May 2017", "$19.8 trillion or about 106 % of the previous 12 months of GDP", "the band's activities", "Dan Stevens", "Alexei Kosygin", "Saudi Arabia", "Ed Woodward", "Llyn Padarn", "Cyclops", "addiction and behavior change/issues", "Oklahoma", "julius", "Anna Mae Bullock", "Canada", "Madonna", "2050,", "Graeme Smith", "three", "the reaction of some gay rights activists", "The meter reader", "prostate cancer,", "Haiti", "A witness", "246", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "NATO's Membership Action Plan, or MAP,", "the Way of St. James", "Wu-Tang Clan", "Coral Reef", "uncontrolled", "Jean Valjean", "Botswana", "The Treasure of the Sierra Madre", "the daughter of Nokomis", "renoir", "The New York Tribune", "Olympia", "near Arenosa Creek and Matagorda Bay", "The Republic of Tecala"], "metric_results": {"EM": 0.5, "QA-F1": 0.5942918192918193}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 0.0, 0.4, 1.0, 1.0, 0.0, 0.6, 0.4, 0.3076923076923077, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-2910", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-4148", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2757", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-5062", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-15310", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-8781", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11945", "mrqa_searchqa-validation-14446"], "SR": 0.5, "CSR": 0.5873579545454546, "retrieved_ids": ["mrqa_squad-train-29839", "mrqa_squad-train-3076", "mrqa_squad-train-44432", "mrqa_squad-train-69270", "mrqa_squad-train-47304", "mrqa_squad-train-54440", "mrqa_squad-train-42583", "mrqa_squad-train-66973", "mrqa_squad-train-29052", "mrqa_squad-train-12241", "mrqa_squad-train-60105", "mrqa_squad-train-11645", "mrqa_squad-train-5467", "mrqa_squad-train-76106", "mrqa_squad-train-46508", "mrqa_squad-train-11450", "mrqa_hotpotqa-validation-246", "mrqa_triviaqa-validation-4843", "mrqa_triviaqa-validation-7549", "mrqa_squad-validation-83", "mrqa_hotpotqa-validation-5698", "mrqa_squad-validation-6811", "mrqa_squad-validation-4676", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-3174", "mrqa_squad-validation-5505", "mrqa_searchqa-validation-15149", "mrqa_squad-validation-6735", "mrqa_hotpotqa-validation-1906", "mrqa_newsqa-validation-3501", "mrqa_squad-validation-6059", "mrqa_hotpotqa-validation-5891"], "EFR": 1.0, "Overall": 0.7293465909090909}, {"timecode": 22, "before_eval_results": {"predictions": ["three", "Federica Mogherini", "18 of 26 songs", "luxury items", "the Scottish Parliament", "Maling company", "Johnny Cash and Jennings", "DI Humphrey Goodman", "ITV", "31 July 1975", "Edmonton, Alberta", "National Subscription Television", "Johnson Press", "1877", "Tom Jones", "The Killer", "Kew Gardens", "Sullenberger III", "Walter Brennan", "2015 World Series", "Freddie Highmore", "13th centuries", "the Second Continental Congress", "newly formed vesicles", "the 14th most common surname in Wales and 21st most common in England", "an expression of unknown origin", "the semilunar pulmonary valve", "30 October 1918", "360", "Addis Ababa", "Piglet", "Persia", "yah nyah", "moyi", "alberich", "david moyes", "Jeffrey Archer", "yachts", "Montezuma", "Secretary of State", "job training for all service members leaving the military", "Gaddafi's", "former Pakistani Prime Minister Benazir Bhutto", "an Italian and six Africans", "in southwestern Mexico,", "Another high tide", "Camorra has been blamed for about 60 killings this year in Naples and its surrounding county.", "0-0 draw", "the release of the four men", "November 1", "Sodra nongovernmental organization,", "Chen Lu", "cheese", "detective fiction", "Henry Hudson", "Pitney Postal Machine Company", "the ear", "John F. Kennedy", "British rock band", "Kilkenny cats", "Aleph", "shiatsu", "drivers who qualified for the 2017 Playoffs", "BC Jean and Toby Gad"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6267124368686868}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.16666666666666669, 0.4444444444444444, 0.6666666666666666, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2386", "mrqa_squad-validation-7427", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-4934", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-4556", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-12153"], "SR": 0.5625, "CSR": 0.5862771739130435, "EFR": 0.9285714285714286, "Overall": 0.7148447204968944}, {"timecode": 23, "before_eval_results": {"predictions": ["antigens", "the courts of member states and the Court of Justice of the European Union", "sold two years later to satisfy a debt", "pastors", "Abercynon", "\u00dcberseering BV v Nordic Construction GmbH", "9", "Graham Payn", "Democratic", "Charles L. Clifford", "Ludwig van Beethoven", "Jacking", "Nippon Professional Baseball", "Viscount Cranborne", "Kew", "Kim Bauer", "Citizens for a Sound Economy", "Gian Carlo Menotti", "in the 1820s", "The genome", "an address bar", "country", "in 1976", "Lex Luger and Rick Rude", "the biblical name of a Canaanite god associated with child sacrifice", "Presley Smith", "foreign investors", "Utah, Arizona, Wyoming, and Oroville, California", "Aishwarya Narkar", "Salford", "George H. W. Bush", "Spain", "Margaret Thatcher", "Ascot", "Tennis", "Brussels", "christopher", "united states", "Carousel", "Apollodorus", "used", "power lines downed by Saturday's winds,", "Karen Floyd", "Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "his record breaking victory as he claimed his sixth world title at a different weight by beating Cotto on Saturday night.", "for security reasons and not because of their faith.", "on the family's blog", "$81,8709", "Ryder Russell", "A Colorado prosecutor", "al-Maqdessi's group \"outlaws\" and said they have been \"terrorizing the country and attacking civilians.\"", "Sweden", "the King's Men", "Beloved", "Canada", "George Orwell", "the Lincoln cent", "Queen Margrethe", "ruseds", "the metric system", "Northwestern University", "Angelina Jolie", "Sergei Rachmaninoff", "the sympathetic nervous system,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6005199510634294}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.14285714285714288, 0.08695652173913045, 0.0, 0.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1406", "mrqa_squad-validation-3218", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-1886", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-8484", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7265", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-1428", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1634", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2736", "mrqa_searchqa-validation-4111", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-12618", "mrqa_searchqa-validation-5110", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-2833"], "SR": 0.515625, "CSR": 0.5833333333333333, "EFR": 1.0, "Overall": 0.7285416666666666}, {"timecode": 24, "before_eval_results": {"predictions": ["38", "the inertia of the ball associated with its constant velocity in the direction of the vehicle's motion", "Edmonton, Canada", "the contrary idea that Islam is, or can be, apolitical is an error", "conservative Muslims", "Lord Gort", "north", "Ford Island", "Europe", "Amber Heard", "Marlon St\u00f6ckinger", "the Rose Garden", "House of Hohenstaufen", "Austria", "2011", "\"Kids React\"", "The Vaudevillains", "The Portuguese", "Kelly Osbourne", "the left ring finger", "1804", "Steve Mazzaro", "Lady Gaga", "James Hutton", "1901", "Games", "a god of the Ammonites", "1959", "Rose-Marie", "Desdemona", "The 20 Highest-Grossing Movies of All Time", "Australia", "will create a fees bonanza for the company's many advisers", "Adolphe Adam", "Brooklyn", "Paris", "phoebus", "Today", "Lady Gaga", "the will now move out, given our orders, and we'll carry them out.", "Europe", "an upper respiratory infection,", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "the Muslim Eid-ul-Adha", "10 to 15 percent", "Kenya", "Mexico", "and they believe that he left on a flight to Haiti on Friday.", "the European Commission", "Amanda Knox's", "jack Sprat", "Hollywood Canteen", "the Arabian Peninsula", "the period between fertilization and birth", "Aberdeen", "the T.H.X. System", "The Office", "Mount Everest", "a junkyard dog", "Napoleon", "phoebus", "Maine", "Ben Watson", "Hokkaido"], "metric_results": {"EM": 0.5, "QA-F1": 0.5496741310160427}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.09090909090909093, 1.0, 0.16666666666666669, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5882352941176471, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-10328", "mrqa_squad-validation-9519", "mrqa_hotpotqa-validation-2128", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-3609", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-1903", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-1602", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-6214", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3213", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-856", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-7147", "mrqa_searchqa-validation-12372"], "SR": 0.5, "CSR": 0.5800000000000001, "retrieved_ids": ["mrqa_squad-train-9414", "mrqa_squad-train-49053", "mrqa_squad-train-27119", "mrqa_squad-train-72437", "mrqa_squad-train-31121", "mrqa_squad-train-73280", "mrqa_squad-train-20549", "mrqa_squad-train-59631", "mrqa_squad-train-86343", "mrqa_squad-train-82805", "mrqa_squad-train-16441", "mrqa_squad-train-38787", "mrqa_squad-train-50968", "mrqa_squad-train-55980", "mrqa_squad-train-53117", "mrqa_squad-train-34563", "mrqa_triviaqa-validation-7215", "mrqa_squad-validation-7502", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3737", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-6658", "mrqa_naturalquestions-validation-3362", "mrqa_squad-validation-457", "mrqa_squad-validation-4840", "mrqa_newsqa-validation-1634", "mrqa_triviaqa-validation-549", "mrqa_newsqa-validation-1283", "mrqa_squad-validation-3590", "mrqa_naturalquestions-validation-6564", "mrqa_searchqa-validation-12618"], "EFR": 1.0, "Overall": 0.727875}, {"timecode": 25, "before_eval_results": {"predictions": ["Oirads", "via electron microscopy", "The packets are routed individually, sometimes resulting in different paths and out-of-order delivery", "the time complexity of a problem", "We are beggars,", "a fictional South American country", "JackScanlon", "Naval Base", "Western Australia", "non-ferrous", "Veterans Committee", "1978", "Pakistan", "Cleveland Indians", "94 by 50 feet", "Joe Spano", "the Latin word autumnus", "Steve Miller Band", "a bacteria", "Slumdog Millionaire", "flowers", "John Poulson", "one", "food", "the Penguin", "Israel", "Southwest Airlines", "james hogg", "Macau", "the International Conference on LGBT Human Rights", "Reserve Division", "Terry Malloy", "Weare", "Rigel VII", "M Rookie Blaylock", "1988", "1959", "40 million", "Leonard Cohen", "President Bush", "Empire of the Sun", "Afghan lawmakers", "\"totaled,\"", "some of the best stunt ever pulled off", "kite boards", "five", "the two bodies out of the plant,", "9-week-old", "Australian officials", "Barack Obama sent a message that fight against terror will respect America's values.", "(Jack) London", "Philadelphia", "NYPD", "BLIMPISH", "clouds", "a vacuum", "Jane Eyre", "the Democrats", "Catholicism", "The Big Easy", "Gorbachev", "four months ago,", "Polo", "\"Rin Tin Tin: The Life and the Legend\""], "metric_results": {"EM": 0.5625, "QA-F1": 0.618944597069597}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.3076923076923077, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4677", "mrqa_squad-validation-1784", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7827", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-7353", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-4941", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-2151", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-2627", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-12326", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-1003"], "SR": 0.5625, "CSR": 0.5793269230769231, "EFR": 0.9642857142857143, "Overall": 0.7205975274725275}, {"timecode": 26, "before_eval_results": {"predictions": ["Disneyland", "Scandinavia", "back to an Earth ocean landing", "as soon as they enter into force, unless stated otherwise, and are generally concluded for an unlimited period", "Charlene Holt", "the highest number of votes, and also greater than 50 % of the votes", "the case of disputes between two or more states", "NIRA", "10 million travellers", "she was `` sick of keeping all these feelings inside and not speaking up for myself ''", "Atlanta", "late 1920s", "every 23 hours, 56 minutes, and 4 seconds with respect to the stars", "novella", "Yuzuru Hanyu", "Luke 6 : 67 -- 71", "Washington, D.C.", "\"Wild Thing\"", "Socrates", "170", "President Nixon", "Diamondbacks", "surf", "king of Cyprus", "Verdi", "Passepartout", "Wikipedia", "leopons", "1884", "nursery rhyme", "Sully", "Thomas Christopher Ince", "quantum mechanics", "26,000", "The entity", "John de Mol Jr.", "KBS2", "the Waters of Death", "james Fell", "Adelaide", "Islamic militants", "Dan Brown's", "the armed robbery and kidnapping of another victim,", "killed in an attempted car-jacking as he dropped his children off at a relative's house,", "citizenship to a man because he allegedly forced his wife to wear a full Islamic veil, the French immigration minister said in a statement Wednesday.", "the triangular bone within the pelvis", "\"illegitimate.\"", "Friday,", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Isabella", "the Kurdish militant group in Turkey", "the civil affairs division of the U.S. military,", "leeches", "The Rolling Stones", "Cheddar", "the human breast", "the Cretaceous Period", "King Arthur", "commission", "coral", "One Small Step", "butter", "a swan pan", "elixir of life"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6721661865503777}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 0.9444444444444444, 1.0, 0.9166666666666666, 0.9411764705882353, 1.0, 0.0, 0.1818181818181818, 1.0, 0.5714285714285715, 0.13333333333333333, 1.0, 1.0, 0.75, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0909090909090909, 0.25, 1.0, 1.0, 0.1212121212121212, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-3879", "mrqa_squad-validation-4176", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-3770", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-1115", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-1858", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-7662", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-2153", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-300", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1508", "mrqa_newsqa-validation-1143", "mrqa_searchqa-validation-11394", "mrqa_searchqa-validation-8544", "mrqa_searchqa-validation-12936", "mrqa_searchqa-validation-7895", "mrqa_searchqa-validation-14095"], "SR": 0.515625, "CSR": 0.5769675925925926, "EFR": 0.9354838709677419, "Overall": 0.714365292712067}, {"timecode": 27, "before_eval_results": {"predictions": ["(Rhenus) fluvius Galliae", "13th", "probabilistic", "Donald Davies", "tickets to Italy", "said the measure -- dubbed the \"card check\" bill by its opponents,", "\"intimidating the population and election officials as well.\"", "the tape was given to authorities in September 2007 by a man who said he had found it in the desert five months before.", "allergies in general -- both food and inhalant -- are on the rise, but no one is sure why.", "the Nazi war crimes suspect who had been ordered deported to Germany,", "150", "jean", "made some power moves which forever changed Hollywood.", "New York,", "more than 100", "police", "Dirk Benedict", "flawed democracy", "1986", "response", "July 2, 1776", "parthenogenesis", "in pilgrimages to Jerusalem", "Daniel A. Dailey", "Gustav Bauer", "over 74", "Nashville, Tennessee", "Colman", "Sicily", "lukagjini", "stanis\u0142aw", "e pluribus unum", "Eddie Cochran", "the Salem witch trials", "Ghana", "El Hiero", "Nero", "Dos Equis", "fish", "Telstar", "Vaisakhi List", "Sierra Leone", "Tranquebar", "Citizens for a Sound Economy", "Tie Domi", "the Rolls of Ol\u00e9ron", "bank of China Building", "Aligarh Muslim University", "dice", "activist", "Laurel, Mississippi", "Interstate 22", "Dr. Seuss", "a cardinal", "Paris", "ear infections", "brder Grimm", "Sally Field", "platinum", "apples", "William Shakespeare", "lala lala", "Herbert Hoover", "Bulgaria"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6440790979853479}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.08333333333333334, 0.0, 0.0, 0.0, 0.5333333333333333, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.46153846153846156, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9274", "mrqa_newsqa-validation-824", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-829", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-828", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-6730", "mrqa_triviaqa-validation-5892", "mrqa_triviaqa-validation-6854", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-495", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-3973", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-2096", "mrqa_searchqa-validation-14465", "mrqa_searchqa-validation-14772", "mrqa_searchqa-validation-11551", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-1119"], "SR": 0.5625, "CSR": 0.5764508928571428, "retrieved_ids": ["mrqa_squad-train-1433", "mrqa_squad-train-57069", "mrqa_squad-train-80130", "mrqa_squad-train-71138", "mrqa_squad-train-1525", "mrqa_squad-train-32659", "mrqa_squad-train-40968", "mrqa_squad-train-42033", "mrqa_squad-train-34049", "mrqa_squad-train-70243", "mrqa_squad-train-6756", "mrqa_squad-train-46670", "mrqa_squad-train-2390", "mrqa_squad-train-39215", "mrqa_squad-train-13143", "mrqa_squad-train-40466", "mrqa_newsqa-validation-633", "mrqa_hotpotqa-validation-5135", "mrqa_triviaqa-validation-1403", "mrqa_searchqa-validation-9818", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-8205", "mrqa_naturalquestions-validation-10501", "mrqa_newsqa-validation-3640", "mrqa_searchqa-validation-14936", "mrqa_newsqa-validation-1085", "mrqa_searchqa-validation-2136", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-661", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-300", "mrqa_triviaqa-validation-6170"], "EFR": 1.0, "Overall": 0.7271651785714286}, {"timecode": 28, "before_eval_results": {"predictions": ["ARPANET", "cyclic photophosphorylation", "without destroying historical legitimacy", "The Da Vinci Code", "Gustav's top winds", "President Obama", "Walk -- Don't Run", "her dancing in short pants.", "2006", "to put a lid on the marking of Ashura", "Satsuma, Florida,", "her son has strong values.", "17 Again", "3-2", "Africa", "Atlantic Ocean", "Super Bowl LII", "The neck", "New Delhi", "July 25, 2017", "December 12, 2017", "St. Louis Blues", "increased productivity, trade, and secular economic trends", "c. 1000 AD", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "off the southernmost tip of the South American mainland", "1904", "Noah", "alobos", "Rugby School", "Dutch", "David Frost", "Athens", "Mustique", "Macbeth", "the sound of the human voice", "a written record", "Turkey", "judoka", "puppy", "Hee Haw", "2012", "Coyote Ugly", "Rowan Blanchard", "Brady Haran", "Squam Lake", "Believe", "Donald Carl \"Don\" Swayze", "East Is East", "Washington, D.C.", "Burning Man", "the Hebrew Joseph", "Harriet Tubman", "Chuck Berry", "Virginia", "60 Minutes", "the Huang He", "Spain", "Dorothy", "fibula", "Machiavelli", "Hammer", "open fracture", "the uvula"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6447096306471306}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 0.923076923076923, 0.28571428571428575, 0.7692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.923076923076923, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2308", "mrqa_newsqa-validation-1419", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-2513", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8450", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-7011", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-4148", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-14304", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-6598", "mrqa_searchqa-validation-862", "mrqa_searchqa-validation-15667"], "SR": 0.546875, "CSR": 0.5754310344827587, "EFR": 1.0, "Overall": 0.7269612068965519}, {"timecode": 29, "before_eval_results": {"predictions": ["rediscovery of \"Christ and His salvation\"", "$2.50 per AC horsepower", "recant his writings", "pregnancy.", "15", "Dr. Death in Germany", "Tim Clark, Matt Kuchar and Bubba Watson", "15 years ago", "new, specific procedures and repair recommendations, said Gerry Bonanni,", "said the most important attacks on the church don't come from the outside,", "Body Tap,", "how health care can affect families.", "2-1", "Jose Miguel Vivanco,", "India", "Roman Reigns", "Nationalists", "April 10, 2018", "September 2017", "September 1972", "Sergeant Himmelstoss", "Burj Khalifa", "1932", "smacking a fly on her mirror", "a solitary figure who is not understood by others", "T.S. Eliot", "Ludacris", "America's Cup", "Midsomer Murders", "the Andesite Line", "nitrogen", "colorblindness", "david hemery", "the first news periodical", "Director General of the Security Service", "Scottish League", "Ghana", "Jan Van Eyck", "achromatopsia", "Wake Island", "emotion poetry", "1884", "Tampa Bay Lightning", "Madonna", "Bonkile", "Dan Rowan", "National Socialists", "Nikita Khrushchev", "Speedway World Championship", "47,818", "1903", "a compound", "a compound", "Whitney Houston", "Prada", "the Lionheart", "Grapefruit", "G.I. Jane", "Lady Sings the Blues", "the zebra mussel", "a saltire", "the Blessed Virgin", "the heptathlon", "Asiana Town building"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5485720009157509}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.25, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-1036", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-1049", "mrqa_naturalquestions-validation-3119", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-824", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-112", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-4034", "mrqa_hotpotqa-validation-1851", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-3974", "mrqa_searchqa-validation-14390", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-14664"], "SR": 0.484375, "CSR": 0.5723958333333333, "EFR": 1.0, "Overall": 0.7263541666666666}, {"timecode": 30, "UKR": 0.787109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1683", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-2586", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2705", "mrqa_hotpotqa-validation-2821", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2945", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3082", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3378", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-3516", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3776", "mrqa_hotpotqa-validation-3806", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3866", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-4092", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4171", "mrqa_hotpotqa-validation-4282", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-4595", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4698", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5377", "mrqa_hotpotqa-validation-5480", "mrqa_hotpotqa-validation-5502", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5752", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2479", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2600", "mrqa_naturalquestions-validation-2884", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-2960", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5000", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-8862", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9897", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1098", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1217", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1889", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2103", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2127", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2640", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-3229", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3400", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3719", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4174", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-48", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-535", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-980", "mrqa_newsqa-validation-99", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10364", "mrqa_searchqa-validation-10930", "mrqa_searchqa-validation-11121", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-1146", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12936", "mrqa_searchqa-validation-13117", "mrqa_searchqa-validation-13198", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13719", "mrqa_searchqa-validation-13920", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-15018", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-187", "mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-2192", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3441", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-4282", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5201", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6388", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-694", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7375", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7692", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-8524", "mrqa_searchqa-validation-8672", "mrqa_searchqa-validation-9797", "mrqa_squad-validation-10103", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10299", "mrqa_squad-validation-10303", "mrqa_squad-validation-10307", "mrqa_squad-validation-10319", "mrqa_squad-validation-10428", "mrqa_squad-validation-10436", "mrqa_squad-validation-10438", "mrqa_squad-validation-10449", "mrqa_squad-validation-1126", "mrqa_squad-validation-1299", "mrqa_squad-validation-132", "mrqa_squad-validation-143", "mrqa_squad-validation-1539", "mrqa_squad-validation-1611", "mrqa_squad-validation-1644", "mrqa_squad-validation-1645", "mrqa_squad-validation-1657", "mrqa_squad-validation-1688", "mrqa_squad-validation-1695", "mrqa_squad-validation-1787", "mrqa_squad-validation-1850", "mrqa_squad-validation-1877", "mrqa_squad-validation-1980", "mrqa_squad-validation-2382", "mrqa_squad-validation-2478", "mrqa_squad-validation-2489", "mrqa_squad-validation-2497", "mrqa_squad-validation-2636", "mrqa_squad-validation-2658", "mrqa_squad-validation-2704", "mrqa_squad-validation-2709", "mrqa_squad-validation-2810", "mrqa_squad-validation-2854", "mrqa_squad-validation-288", "mrqa_squad-validation-2949", "mrqa_squad-validation-2955", "mrqa_squad-validation-299", "mrqa_squad-validation-3061", "mrqa_squad-validation-3113", "mrqa_squad-validation-3141", "mrqa_squad-validation-3147", "mrqa_squad-validation-3168", "mrqa_squad-validation-3197", "mrqa_squad-validation-3300", "mrqa_squad-validation-3302", "mrqa_squad-validation-3516", "mrqa_squad-validation-3559", "mrqa_squad-validation-3566", "mrqa_squad-validation-3590", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3812", "mrqa_squad-validation-3829", "mrqa_squad-validation-3898", "mrqa_squad-validation-3909", "mrqa_squad-validation-4063", "mrqa_squad-validation-4097", "mrqa_squad-validation-4121", "mrqa_squad-validation-4137", "mrqa_squad-validation-4142", "mrqa_squad-validation-415", "mrqa_squad-validation-42", "mrqa_squad-validation-4262", "mrqa_squad-validation-4439", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4631", "mrqa_squad-validation-4676", "mrqa_squad-validation-4801", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-4856", "mrqa_squad-validation-4951", "mrqa_squad-validation-509", "mrqa_squad-validation-5156", "mrqa_squad-validation-5190", "mrqa_squad-validation-5229", "mrqa_squad-validation-5272", "mrqa_squad-validation-5505", "mrqa_squad-validation-5758", "mrqa_squad-validation-5951", "mrqa_squad-validation-5958", "mrqa_squad-validation-5975", "mrqa_squad-validation-6071", "mrqa_squad-validation-6106", "mrqa_squad-validation-612", "mrqa_squad-validation-6221", "mrqa_squad-validation-6254", "mrqa_squad-validation-6404", "mrqa_squad-validation-6472", "mrqa_squad-validation-6505", "mrqa_squad-validation-6674", "mrqa_squad-validation-6681", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6916", "mrqa_squad-validation-6938", "mrqa_squad-validation-694", "mrqa_squad-validation-6968", "mrqa_squad-validation-704", "mrqa_squad-validation-7133", "mrqa_squad-validation-718", "mrqa_squad-validation-7233", "mrqa_squad-validation-7273", "mrqa_squad-validation-7322", "mrqa_squad-validation-742", "mrqa_squad-validation-7427", "mrqa_squad-validation-7490", "mrqa_squad-validation-7718", "mrqa_squad-validation-7723", "mrqa_squad-validation-7726", "mrqa_squad-validation-7731", "mrqa_squad-validation-7767", "mrqa_squad-validation-7783", "mrqa_squad-validation-7789", "mrqa_squad-validation-7886", "mrqa_squad-validation-799", "mrqa_squad-validation-8012", "mrqa_squad-validation-8107", "mrqa_squad-validation-8154", "mrqa_squad-validation-8202", "mrqa_squad-validation-8212", "mrqa_squad-validation-8278", "mrqa_squad-validation-8352", "mrqa_squad-validation-85", "mrqa_squad-validation-8600", "mrqa_squad-validation-8695", "mrqa_squad-validation-8792", "mrqa_squad-validation-8909", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9103", "mrqa_squad-validation-9189", "mrqa_squad-validation-9190", "mrqa_squad-validation-932", "mrqa_squad-validation-9405", "mrqa_squad-validation-9438", "mrqa_squad-validation-9465", "mrqa_squad-validation-9488", "mrqa_squad-validation-9519", "mrqa_squad-validation-9525", "mrqa_squad-validation-9534", "mrqa_squad-validation-9629", "mrqa_squad-validation-9639", "mrqa_squad-validation-9675", "mrqa_squad-validation-9698", "mrqa_squad-validation-9700", "mrqa_squad-validation-9701", "mrqa_squad-validation-9717", "mrqa_squad-validation-9732", "mrqa_squad-validation-975", "mrqa_squad-validation-9762", "mrqa_squad-validation-9859", "mrqa_squad-validation-9898", "mrqa_triviaqa-validation-1403", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-1489", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1627", "mrqa_triviaqa-validation-1675", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-2082", "mrqa_triviaqa-validation-219", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-2396", "mrqa_triviaqa-validation-2424", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2757", "mrqa_triviaqa-validation-2885", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-3163", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3341", "mrqa_triviaqa-validation-3352", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3932", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-4047", "mrqa_triviaqa-validation-4077", "mrqa_triviaqa-validation-4113", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4275", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-434", "mrqa_triviaqa-validation-4457", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4846", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5785", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6151", "mrqa_triviaqa-validation-6257", "mrqa_triviaqa-validation-6270", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-637", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6562", "mrqa_triviaqa-validation-6728", "mrqa_triviaqa-validation-6827", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7536", "mrqa_triviaqa-validation-758", "mrqa_triviaqa-validation-960"], "OKR": 0.880859375, "KG": 0.496875, "before_eval_results": {"predictions": ["Sierra Sky Park", "second", "Recognized Student Organizations", "fractured pelvis and sacrum", "Saturday.", "lower house of parliament,", "the same drama that pulls in the crowds", "Daniel Nestor", "40", "at my undergrad alma mater, Wake Forest", "him to step down as majority leader.", "Christopher Savoie", "Missouri", "Mitt Romney", "nine-wicket", "Robin", "1990", "a piece of foam insulation broke off from the Space Shuttle external tank", "embryo", "Bill Pullman", "a small portion of West Virginia", "the fovea centralis", "Julie Stichbury", "Dorothy Gale", "George Strait", "11 January 1923", "2015", "hair", "bituminous", "Craggy Island", "Jaime", "Sodor", "crippen", "a crossbred dog", "Cadbury", "Wharton", "chlorine", "COBRA", "Big Brother", "Whoopi Goldberg", "Bryan Kocis", "\"D Daredevil\"", "Lundbeck", "Shakespeare's reputation", "311", "1998", "Belarus", "\"The Braes o' Bowhether\"", "Marvel", "beer", "Google Office", "stiletto", "Dune", "Jane Eyre", "Ron Paul", "Wall Street Journal", "the JFK assassination", "Bora Bora", "\"Joliet\"", "the Bolshoi Ballet", "Debussy", "Florida State", "Armenia", "Don Garlits"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5160962301587302}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7857142857142858, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-4113", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-3053", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-4091", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2688", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-195", "mrqa_triviaqa-validation-1340", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-6254", "mrqa_triviaqa-validation-1465", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-6834", "mrqa_triviaqa-validation-4267", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-4718", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2966", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-2718", "mrqa_hotpotqa-validation-4956", "mrqa_hotpotqa-validation-5514", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-314", "mrqa_searchqa-validation-15451"], "SR": 0.453125, "CSR": 0.5685483870967742, "retrieved_ids": ["mrqa_squad-train-72335", "mrqa_squad-train-85333", "mrqa_squad-train-43142", "mrqa_squad-train-36973", "mrqa_squad-train-82366", "mrqa_squad-train-31121", "mrqa_squad-train-64434", "mrqa_squad-train-68986", "mrqa_squad-train-42759", "mrqa_squad-train-55605", "mrqa_squad-train-7932", "mrqa_squad-train-85108", "mrqa_squad-train-78423", "mrqa_squad-train-34768", "mrqa_squad-train-20182", "mrqa_squad-train-63077", "mrqa_triviaqa-validation-4920", "mrqa_naturalquestions-validation-4711", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-6070", "mrqa_hotpotqa-validation-1016", "mrqa_squad-validation-4708", "mrqa_searchqa-validation-15667", "mrqa_newsqa-validation-595", "mrqa_naturalquestions-validation-8203", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-2964", "mrqa_squad-validation-6968", "mrqa_hotpotqa-validation-4148", "mrqa_squad-validation-2686", "mrqa_triviaqa-validation-2307", "mrqa_newsqa-validation-3130"], "EFR": 1.0, "Overall": 0.7466784274193549}, {"timecode": 31, "before_eval_results": {"predictions": ["article 30", "Seine", "a house party in Crandon, Wisconsin", "Jaime Andrade", "South Africa", "Anjuna beach in Goa", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "all 246", "consumer confidence", "missing Florida toddler Caylee Anthony,", "Siri", "Lance Cpl. Maria Lauterbach", "little blue booties.", "Communist Party of Nepal", "Acts 1 : 13", "Nazi Germany and Fascist Italy", "George II", "Andrew Lloyd Webber", "five", "gastrocnemius", "accomplish the objectives of the organization", "Narendra Modi", "1960", "Marie Fredriksson", "John Frank Stevens", "Jesus Christ", "gebrselassie", "9", "Radio City Music Hall", "geyser", "the Battle of Camlann", "hawkeye", "France", "cgs", "piano", "florida", "180", "john F. Kennedy", "Bill Walton", "sphagnum", "Homer Hickam, Jr.", "1981", "250cc world championship", "Kinnairdy Castle", "1999 Odisha", "EBSCO Information Services", "Sylvia Pankhurst", "Hillary Clinton", "Roseann O'Donnell", "Iranian government\u2019s propaganda channel", "Bessie Coleman", "They were marooned", "Hawks", "California", "coral", "Green Lantern", "Hawaii", "The Greatest Show on Earth", "James V, King of Scotland", "Blue state", "save the best for last", "Henry James", "Matt Monro", "birth certificate from a local authority is commonly provided to the federal government to obtain a U.S. passport"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6368303571428571}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.28571428571428575, 1.0, 0.5, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.8, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2981", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-960", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-6084", "mrqa_triviaqa-validation-2300", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-5362", "mrqa_triviaqa-validation-821", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-147", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4995", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-4660", "mrqa_searchqa-validation-12421", "mrqa_searchqa-validation-6423", "mrqa_naturalquestions-validation-6998"], "SR": 0.53125, "CSR": 0.5673828125, "EFR": 1.0, "Overall": 0.7464453125}, {"timecode": 32, "before_eval_results": {"predictions": ["banded iron", "multiplication", "five starting pitchers", "AMX - 30", "Captain Jones", "the orthophosphate ion ( PO )", "1988", "the Seton Hall Pirates men's basketball team", "Manhattan Project", "former Daytona 500 pole winners who competed full - time in 2017", "October 29, 2015", "Norman Pritchard", "Teddy Randazzo", "Kim Basinger", "Charles Manson", "The Life of a Great Sinner", "sebills", "Hindu", "The Green Mile", "Paris", "Labrador", "phobia", "pickled peppers", "Joan-Christophe Novelli", "Liszt Strauss Wagner Dvorak", "Pakistan", "Kentucky Music Hall of Fame", "3730 km", "Antiochia", "a theme of global peace", "Neotropical realm", "Moselle", "Prince Sung-won", "actor", "Drowning Pool", "\"Odorama\"", "\"Beauty and the Beast\"", "creeks", "Amsterdam", "speaking out about a cause someone feels passionate about.", "the river will crest Saturday about 20 feet above flood stage.", "71 percent of Americans consider China an economic threat to the United States,", "Columbia, Illinois,", "At least 33", "42 prostitutes", "Sri Lanka's war zone.", "near Garacad, Somalia,", "June 17 and 18,", "Afghanistan's restive provinces", "When you're going into a restaurant environment, you're putting your child's safety and livelihood into other hands,", "a single HIV-1", "John Hersey", "King of Solomon", "the Lord of the Rings", "Thailand", "a statue", "a solecism", "Rome", "Dragons", "coral", "lyrebirds", "Joseph Bonaparte", "Bangkok", "MGM Resorts International"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5227328431372549}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5882352941176471, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10213", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-1582", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-1120", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-3300", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3600", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-3059", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3736", "mrqa_searchqa-validation-12171", "mrqa_searchqa-validation-5533", "mrqa_searchqa-validation-3946", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13044", "mrqa_searchqa-validation-16682", "mrqa_searchqa-validation-10131"], "SR": 0.46875, "CSR": 0.5643939393939394, "EFR": 0.9705882352941176, "Overall": 0.7399651849376114}, {"timecode": 33, "before_eval_results": {"predictions": ["the foot of the mast of a moving ship", "Business Connect", "the Arctic Ocean in the north", "ase", "Part 2 was released in July 2011", "Daniel A. Dailey", "Jeff Bezos", "Sharecropping", "a single epididymal tubule ( luminal diameter. 15 -. 25 mm ) to the lumen of the vas deferens", "Nodar Kumaritashvili", "on the inner edge of the constellation Arm,", "Archie Marries Betty", "inverted - drop - shaped icon", "Greek \u1f61\u03c3\u03b1\u03bd\u03bd\u03ac, h\u014dsann\u00e1", "\u201cMy dear, I don\u2019t give a damn\u201d", "australia", "Jackie Kennedy", "a rapid drop in your blood sugar", "Bangladesh", "winton", "a fluid", "blancmengier", "ivan evans", "eagles", "the Book of Esther", "Julia Child's", "at the State House in Augusta", "Pan Am Railways", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "NCAA Division II football season", "David S. Goyer", "journal", "main east-west road connecting the inner northern suburbs of Adelaide", "Sleeping Beauty", "a secularist and nationalist", "Martin Joseph O'Malley", "Whitesnake", "Atlas ICBM", "Tuesday in Los Angeles.", "Brian Smith.", "the Russian air force,", "nearly $2 billion", "three", "Genocide Prevention Task Force", "NATO", "more than 30", "And we are aware that people are trying to convince other legislatures to go down this path, I think it's clear that both as a policy matter and the tough economic times as catalysts for introducing legislation.", "Muslim festival of Eid al-Adha", "$81,880", "college student who wanted to go cheer on their Cinderella in person", "Hercules", "Connecticut", "elementary", "Nixon's next nominee, Judge Harrold Carswell of the Fifth Circuit,", "FDR", "a cob", "Paul Vernon Hornung", "a fisheye lens", "The Blues Brothers", "Anne Rice's", "a circle", "dishwasher", "And, it is estimated to be carefully planned and managed so that the number of \"over flights\" -- that is legs where the aircraft is empty -- are kept to an absolute minimum.", "Andhra Police Chief Waseem Ahmad."], "metric_results": {"EM": 0.390625, "QA-F1": 0.5127987718980366}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.923076923076923, 1.0, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.47619047619047616, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 0.888888888888889, 1.0, 0.0909090909090909, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10442", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-5951", "mrqa_triviaqa-validation-1766", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-2750", "mrqa_triviaqa-validation-597", "mrqa_triviaqa-validation-3383", "mrqa_triviaqa-validation-6480", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-6731", "mrqa_triviaqa-validation-7634", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-1350", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2445", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3051", "mrqa_searchqa-validation-14045", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-6124", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-1095"], "SR": 0.390625, "CSR": 0.5592830882352942, "retrieved_ids": ["mrqa_squad-train-86392", "mrqa_squad-train-20976", "mrqa_squad-train-39541", "mrqa_squad-train-59275", "mrqa_squad-train-45054", "mrqa_squad-train-27594", "mrqa_squad-train-7110", "mrqa_squad-train-14005", "mrqa_squad-train-4420", "mrqa_squad-train-81394", "mrqa_squad-train-38443", "mrqa_squad-train-81654", "mrqa_squad-train-45345", "mrqa_squad-train-31897", "mrqa_squad-train-63265", "mrqa_squad-train-52058", "mrqa_searchqa-validation-7115", "mrqa_newsqa-validation-3882", "mrqa_searchqa-validation-1435", "mrqa_hotpotqa-validation-5480", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6121", "mrqa_triviaqa-validation-824", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-4644", "mrqa_newsqa-validation-3942", "mrqa_searchqa-validation-8451", "mrqa_searchqa-validation-1065", "mrqa_squad-validation-1913", "mrqa_naturalquestions-validation-1003", "mrqa_squad-validation-3806", "mrqa_hotpotqa-validation-5752"], "EFR": 1.0, "Overall": 0.7448253676470589}, {"timecode": 34, "before_eval_results": {"predictions": ["he did not want disloyal men in his army", "a \"world classic of epoch-making oratory.\"", "1960", "Ernest Hemingway", "Master Christopher Jones", "Pradyumna", "the most recent Super Bowl champions", "One day, while listening to what seems to be a crossed telephone connection, she hears two men planning a woman's murder", "Hirschman", "United States customary units are a system of measurements commonly used in the United States", "Zeus", "after initially peaking at number 41 in the UK, it re-entered the charts after the group performed the track at Nelson Mandela's 70th Birthday concert", "an anembryonic gestation", "The Divergent Series : Ascendant was never made, due to Allegiant's poor showing at the box office", "Wizard of Oz", "the Andes", "us", "Barack Obama", "julia", "Super Bowl Sunday", "Elizabeth II", "Republican", "baseball cards", "Cuban cigars", "Byron Welles", "Microsoft", "1955", "The ones Who Walk Away from Omelas", "Province of New York", "historic buildings, arts, and published works", "Jim Jones", "Amberley Village", "Colonial colleges", "USA Network thriller drama", "Eisenhower Executive Office Building", "Oklahoma Sooners", "Mike Greenwell", "Kurt Vonnegut Jr.", "Bryant Purvis,", "the Gulf of Aden", "researchers", "fighting for his life in a Buenos Aires hospital after being shot in the head", "affray", "Keep American Beautiful campaign", "Monday night", "CNN's \"Piers Morgan Tonight\"", "The federal government has set aside nearly $2 billion in stimulus funds to clean up Washington State's decommissioned Hanford nuclear site,", "Louvre", "terror groups", "emergency aid", "Homo sapiens", "cuddle", "Red Sox", "a beehive", "Walter Cronkite", "Twenty", "Jon Heder", "Green olives", "Lincoln", "Hermann Hesse", "a ready-to-use cotton swab", "Rick Springfield", "David Bowie", "The Miracles"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6487957935190205}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.25, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 0.0, 0.0, 0.375, 0.0, 0.0, 1.0, 0.4, 0.3225806451612903, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-2333", "mrqa_triviaqa-validation-1230", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-603", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-4700", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3369", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-1952", "mrqa_searchqa-validation-12158", "mrqa_triviaqa-validation-6414"], "SR": 0.578125, "CSR": 0.5598214285714286, "EFR": 1.0, "Overall": 0.7449330357142857}, {"timecode": 35, "before_eval_results": {"predictions": ["Doctorin' the Tardis", "religious", "punk rock band", "Teddy Riley", "Biola University", "1916 Easter Rising", "A.S. Roma", "Arena of Khazan", "Dungeness crab", "Man Booker Prize", "England", "Texas's 27th congressional district", "1969", "Mel Blanc", "Jerry Ekandjo", "Samaria", "warmth", "with the American Revolutionary War", "Gertrude Niesen", "Spain", "Peking", "Union", "the nucleus", "over 1,100 years ago", "Patrick Swayze", "Michael Phelps", "i second that with the implementation of Tesla's World System a \"Radio City\" would arise in the area,", "humidity", "us", "i second that emotion", "Tennessee", "i second that emotion", "i second that emotion", "Pegida", "hastings", "frottage", "United States", "Peter Kay.", "Felipe Massa.", "supply vessel Damon Bankston", "his health", "the shelling of the compound", "three years", "President Paul Biya,", "twice", "Iran's parliament speaker", "a nuclear weapon", "2008,", "natural gas", "Mehsud", "Henry Hudson", "geometrical", "i second that emotion", "Viktor Yanokuvich", "diamonds", "i second that emotion", "anthrax", "us", "Michael Clayton", "Katrina & the Waves", "William Jennings Bryan", "i second that emotion", "Herman Cain,", "Hakeemullah Mehsud"], "metric_results": {"EM": 0.46875, "QA-F1": 0.546812996031746}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-766", "mrqa_hotpotqa-validation-4399", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-10182", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-493", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-5556", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-44", "mrqa_searchqa-validation-16347", "mrqa_searchqa-validation-16364", "mrqa_searchqa-validation-9486", "mrqa_searchqa-validation-6421", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-3697", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-8884"], "SR": 0.46875, "CSR": 0.5572916666666667, "EFR": 1.0, "Overall": 0.7444270833333334}, {"timecode": 36, "before_eval_results": {"predictions": ["5", "Pat McCormick", "1984 Summer Olympics in Los Angeles.", "heads of federal executive departments who form the Cabinet of the United States", "reservoirs", "Marty Stuart", "A simple majority vote", "the court from its members for a three - year term.", "bacteria", "ncis los angeles", "Representative", "Mexican Seismic Alert System", "the root cells actively take part in the process,", "March 16, 2018", "My Favorite Martian", "rodents", "Indonesia", "Heston Blumenthal", "colorblind", "Jeffrey Archer", "Shirley Bassey", "Cahaba", "Mercury", "Timor Sea", "crippen", "Mendip", "Tallaght, South Dublin", "Tropical Storm Ann", "Christina Ricci", "ten", "Derry", "Koch Industries", "YouTube.", "1978", "Arthur Miller", "over 1 million", "Allies of World War I, or Entente Powers", "Near the geographical center of the state", "African National Congress Deputy President Kgalema Motlanthe,", "Thursday night,", "The Red Cross, UNHCR and UNICEF", "the \"People of Palestine\"", "the Arab world to use the Internet for fun and not interfere with government and serious issues,", "Sudanese nor orphans,", "authorizing killings and kidnappings by paramilitary death squads.", "forgery and flying without a valid license,", "Intensifying", "more than 200.", "\"I always kind of admired him, oddly.\"", "MDC offshoot", "Dick Cheney's", "Norah Jones", "king David", "Casablanca", "Sympathy for the Devil", "macaroon", "Montego Bay", "Risky Business", "Alien", "New York Harbor", "nikola parker", "Russia", "Vichy", "So You Think You Can Dance"], "metric_results": {"EM": 0.515625, "QA-F1": 0.597867796931012}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.4444444444444444, 0.0, 0.08695652173913045, 1.0, 0.8, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 0.16666666666666666, 0.4444444444444445, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2105263157894737, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-1682", "mrqa_triviaqa-validation-5798", "mrqa_triviaqa-validation-888", "mrqa_triviaqa-validation-1857", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-5686", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-5740", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-3392", "mrqa_searchqa-validation-11138", "mrqa_searchqa-validation-10163", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-2114"], "SR": 0.515625, "CSR": 0.5561655405405406, "retrieved_ids": ["mrqa_squad-train-63434", "mrqa_squad-train-74472", "mrqa_squad-train-28349", "mrqa_squad-train-28289", "mrqa_squad-train-66720", "mrqa_squad-train-51107", "mrqa_squad-train-34749", "mrqa_squad-train-62707", "mrqa_squad-train-33679", "mrqa_squad-train-38442", "mrqa_squad-train-59329", "mrqa_squad-train-52042", "mrqa_squad-train-79793", "mrqa_squad-train-57425", "mrqa_squad-train-1327", "mrqa_squad-train-37515", "mrqa_triviaqa-validation-1838", "mrqa_hotpotqa-validation-4148", "mrqa_searchqa-validation-12292", "mrqa_hotpotqa-validation-2989", "mrqa_newsqa-validation-2005", "mrqa_searchqa-validation-11496", "mrqa_newsqa-validation-2981", "mrqa_squad-validation-6811", "mrqa_newsqa-validation-1941", "mrqa_hotpotqa-validation-4845", "mrqa_hotpotqa-validation-2592", "mrqa_squad-validation-8917", "mrqa_triviaqa-validation-5178", "mrqa_triviaqa-validation-7154", "mrqa_squad-validation-3113", "mrqa_squad-validation-4054"], "EFR": 0.967741935483871, "Overall": 0.7377502452048823}, {"timecode": 37, "before_eval_results": {"predictions": ["Wisdom, Compassion, Justice and Integrity", "Alex Ryan", "16,801", "1939", "Montreal Canadiens", "Dalveer Bhandari", "Payaya Indians", "Gibraltar", "216", "5,534", "Mirabilis", "ideology", "Indian Ocean", "Tessa Virtue and Scott Moir", "Pontiac Silverdome", "batsman", "2005", "Telstar", "Nikkei", "jack Nicholson", "Munich", "tenerife", "Papua New Guinea", "bridge", "King Idris", "July 14th 1789", "The Rural Electrification Act of 1936", "Thomas Allen \"Tom\" Coburn", "Adolfo Rodr\u00edguez Sa\u00e1", "20 July 1981", "Peterhouse, Cambridge", "Visigoths", "34", "\"Kill Your Darlings\"", "Christopher Nolan", "David May", "April 8, 1943", "Leucippus", "two", "Nick Adenhart", "\"We're not going to forget you in Washington, D.C.\"", "forged credit cards and identity theft", "Diprivan, is administered intravenously in operating rooms as a powerful anesthetic and sedative.", "Barack Obama", "\"I'm just getting started.\"", "Omar bin Laden,", "five Texas A&M University crew mates", "Sharon Bialek", "Henry", "Antichrist.", "inch", "dioxins and hexachlorobenzene", "the Chief", "Tom Cruise", "Philippines", "shrimp", "Bugsy Siegel", "Oliver", "archery quiver", "Wolfgang Johannes Puck", "Chile", "Hell Week", "Alien", "sebastia parker"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6612379807692308}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2, 0.5, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-9163", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-6486", "mrqa_triviaqa-validation-633", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3997", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4027", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-6565", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-4180", "mrqa_triviaqa-validation-2605"], "SR": 0.546875, "CSR": 0.555921052631579, "EFR": 1.0, "Overall": 0.7441529605263157}, {"timecode": 38, "before_eval_results": {"predictions": ["194", "Sunset Publishing Corporation", "25 August 1949", "Thrushcross Grange", "September 10, 1993", "Harold Edward Holt", "the number of men killed and the manner of the attacks", "Ray Romano", "Bundesliga club Bayern Munich", "Overijssel, Netherlands", "Budget Rent a Car", "Atlanta Athletic Club", "1986", "Lord Baden-Powell", "Richie Cunningham", "Amanda Bynes", "March 31, 2018", "March 1995", "September 21, 2017", "January 17, 1899", "Super Bowl LII,", "Heather Stebbins", "Arkansas", "1,228 km / h ( 763 mph )", "Andaman and Nicobar Islands -- Port Blair", "Britain and France", "Lyon, France", "nikozy kozy", "kyu", "jugs", "the Ordovices", "anesthetic agents", "90%", "South Africa", "Epiphany", "Emmy Awards", "Eleanor Roosevelt", "The Real Miracle of Charlotte's Web", "a key witness -- Dennis Davern,", "12 million", "misdemeanor assault charges", "improve the environment", "three full-length animated films.", "Thursday", "Turkey can possibly make the greatest contribution by helping the United States frame the challenges", "ketamine.", "Al Nisr Al Saudi", "French trimaran l'Hydroptere", "Wednesday", "Iran's atomic energy chief", "Xerox", "volcanic eruptions", "a small metal cap", "Rhizo", "Popular Science magazine", "Xerox", "Mount Rushmore", "Michelle Pfeiffer", "HOV", "Louis Brandeis", "May", "Henry Morgan", "the International Campaign to Abolish Nuclear Weapons ( ICAN )", "the President"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5659798534798535}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.3076923076923077, 0.0, 0.0, 0.07692307692307691, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-3308", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-924", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-1218", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-154", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-3214", "mrqa_triviaqa-validation-2581", "mrqa_triviaqa-validation-1480", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-6180", "mrqa_triviaqa-validation-2270", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-6002", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-1451", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-2663", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-11590", "mrqa_searchqa-validation-1349", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-9322", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-9745", "mrqa_searchqa-validation-2980"], "SR": 0.4375, "CSR": 0.5528846153846154, "EFR": 1.0, "Overall": 0.7435456730769231}, {"timecode": 39, "before_eval_results": {"predictions": ["Stephen Greenblatt", "Billy J. Kramer", "2.1 million", "British", "Happy Death Day", "ITV", "G\u00e9rard Depardieu", "Harvard", "ehrlichman", "an Anglo-Saxon saint", "Animorphs", "Caesars Entertainment Corporation", "Romeo", "Autopia", "The balance sheet", "al - Mamlakah al - \u02bbArab\u012byah", "Saint Alphonsa", "Sophia Akuffo", "John Travolta", "New England", "Daya", "Antigonon leptopus", "the referee", "1959", "Nick Kroll", "Morgan Freeman", "Andy Warhol", "rugby", "Orion", "eucalyptus", "Canada", "Henry Addington", "Aunt Harriet", "Madagascar", "tempera", "Holly Johnson", "Rossano Brazzi & Mitzi Gaynor", "danzibouti", "Chinese President Hu Jintao.", "school, their books burned,", "a \"momentous discovery\"", "involvement during World War II in killings at a Nazi German death camp in Poland.", "Arsene Wenger", "Gov. Mark Sanford", "bronze medal in the women's figure skating final,", "seven", "$40 billion", "political consul at Zimbabwe's embassy in Washington.", "Islamabad.", "\"Oprah is an angel, she is God-sent,\"", "diamond", "The Jetsons", "1917", "a trumpet", "Riga", "Thelonious Monk", "Queen Mary II", "minority stockholders", "an intermediate form between", "Gangbusters", "to be admired by the woman he love", "coffee", "HMS Amethyst", "Spearchucker"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5971660539215686}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-4321", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-4302", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-3228", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-5839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-6396", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-1134", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-7000", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-11357", "mrqa_searchqa-validation-15555", "mrqa_triviaqa-validation-7264"], "SR": 0.53125, "CSR": 0.55234375, "retrieved_ids": ["mrqa_squad-train-28160", "mrqa_squad-train-49798", "mrqa_squad-train-81642", "mrqa_squad-train-76448", "mrqa_squad-train-34655", "mrqa_squad-train-50136", "mrqa_squad-train-61801", "mrqa_squad-train-45154", "mrqa_squad-train-3541", "mrqa_squad-train-82966", "mrqa_squad-train-17279", "mrqa_squad-train-8914", "mrqa_squad-train-35167", "mrqa_squad-train-32841", "mrqa_squad-train-7796", "mrqa_squad-train-59808", "mrqa_searchqa-validation-5110", "mrqa_triviaqa-validation-6670", "mrqa_triviaqa-validation-712", "mrqa_newsqa-validation-76", "mrqa_hotpotqa-validation-246", "mrqa_searchqa-validation-10860", "mrqa_triviaqa-validation-2605", "mrqa_newsqa-validation-1857", "mrqa_naturalquestions-validation-6121", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-2729", "mrqa_squad-validation-5249", "mrqa_naturalquestions-validation-2733", "mrqa_newsqa-validation-3051", "mrqa_newsqa-validation-2209", "mrqa_hotpotqa-validation-2268"], "EFR": 0.9666666666666667, "Overall": 0.7367708333333334}, {"timecode": 40, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1037", "mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1377", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1901", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3166", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3828", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-4675", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5497", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5800", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10561", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7202", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7657", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8177", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9870", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1455", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2010", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2218", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2498", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3221", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3762", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-4008", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4162", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12195", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12663", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12857", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-15057", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15788", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-1604", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16047", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2534", "mrqa_searchqa-validation-2866", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3797", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4026", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-5165", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-65", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6637", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7589", "mrqa_searchqa-validation-7643", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8475", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9143", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9486", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-10416", "mrqa_squad-validation-1049", "mrqa_squad-validation-1176", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2361", "mrqa_squad-validation-2402", "mrqa_squad-validation-2489", "mrqa_squad-validation-2840", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3300", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-4127", "mrqa_squad-validation-415", "mrqa_squad-validation-4320", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5129", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5597", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6282", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6610", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-677", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-7154", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7300", "mrqa_squad-validation-742", "mrqa_squad-validation-7480", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7788", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-7982", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8213", "mrqa_squad-validation-8269", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8744", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9498", "mrqa_squad-validation-9590", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9697", "mrqa_squad-validation-9717", "mrqa_squad-validation-972", "mrqa_squad-validation-9732", "mrqa_squad-validation-9776", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1158", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1852", "mrqa_triviaqa-validation-1857", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1923", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3818", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4703", "mrqa_triviaqa-validation-4844", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6396", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6910", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839"], "OKR": 0.845703125, "KG": 0.48515625, "before_eval_results": {"predictions": ["a few hundred feet", "In fashionable neighborhoods of Tokyo customers are lining up for vitamin injections that promise to improve health and beauty.", "30", "upper respiratory infection,", "two Metro transit trains that crashed the day before, killing nine,", "Manmohan Singh's Congress party,", "Jaipur", "in the military, yet they fought on opposing sides.", "The father of Haleigh coughing, a Florida girl who disappeared in February,", "in the 20 years since the Berlin Wall has fallen there has been a renaissance of the game in the region.", "more than 15,000", "People Against Switching Sides", "seven", "glass shards", "1991", "the head of the Imperial Family and the traditional head of state of Japan", "a response to the sensation of food within the esophagus itself", "the hydrological cycle or the hydrologic cycle", "Mike Czerwien", "18", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "Victory gardens", "in Middlesex County, Province of Massachusetts Bay", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "defense against rain rather than sun", "18", "two 1.6 koz (45.4 g) 100 per cent beef patties, special sauce, lettuce, cheese, pickles, onions", "The First Publically Subscribed Passenger Railroad", "South Africa", "a purebred Siamese cat", "Caucasus", "lorne Greene", "Georgia", "My Favorite Martian,", "Portugal", "Eurovision 2014", "Ford, Kia and Volkswagen,", "Telstar", "Reverend Timothy \"Tim\" Lovejoy", "Faby Apache", "Girls' Generation", "Mikhail Fokine", "6'5\" and 190 pounds", "Martha Coolidge", "6 January 1915", "Centennial Olympic Stadium", "David Anthony O'Leary", "Andalusia, Spain", "Roswell", "Prince Antoni Radziwi\u0142\u0142", "e-oo", "America", "Madagascar", "Hip Hop", "a constellation", "champagne", "Como", "Sputnik", "Joseph Smith", "DASB", "Hangman", "Crying of Lot 49", "Kiev", "Best Buy"], "metric_results": {"EM": 0.46875, "QA-F1": 0.567406341830684}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.6428571428571429, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.36363636363636365, 0.0, 0.15384615384615385, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.5263157894736842, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5384615384615384, 0.7999999999999999, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-1459", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1319", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-6810", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-4437", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-1492", "mrqa_searchqa-validation-9226", "mrqa_searchqa-validation-14412", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-15002", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-9355", "mrqa_searchqa-validation-44"], "SR": 0.46875, "CSR": 0.5503048780487805, "EFR": 0.9705882352941176, "Overall": 0.7195692476685797}, {"timecode": 41, "before_eval_results": {"predictions": ["Industry and manufacturing", "possible victims of physical and sexual abuse.", "Brian David Mitchell,", "$250,000 for Rivers' charity: God's Love We Deliver.", "Ferraris, a Lamborghini and an Acura NSX", "Larry Ellison,", "Nkepile M abuse", "Seminole", "18", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "the National Archives in Washington.", "100% of its byproducts", "\"utterly baseless.\"", "Asashoryu", "the town of Acolman, just north of Mexico City", "27 January -- 16 April 1898", "must receive the highest number of votes, and also greater than 50 % of the votes", "Isaiah Amir Mustafa", "political ideology", "different philosophers and statesmen", "crowned the dome of the U.S. Capitol building in Washington, D.C.", "October 12, 1979", "issues of the American Civil War", "the breast or lower chest of beef or veal", "the Beldam / Other Mother", "in capillaries, alveoli, glomeruli, outer layer of skin and other tissues where rapid diffusion is required", "six", "i second that emotion", "nutshell", "Martin Luther King,", "Rio de Janeiro", "piano", "Pyrrhic War", "Nigel Short", "dans", "dansun", "The Feel Good Drag", "brash", "Scott Mosier", "Leslie Knope", "1535", "the Prescription Drug User Fee Act", "the University of Oxford", "Bruce Grobbelaar", "Rockhill Furnace, Pennsylvania", "Omaha Nighthawks", "Farmingville", "Juventus", "'valley of the hazels'", "June 19, 2017", "Mickey Spillane", "Thurman Munson", "Exodus", "The 82nd Annual Academy Awards", "Monica Lewinsky", "the Pacific", "Amish", "coffee", "the foot", "The New York", "Fettuccine", "Joe Jackson", "Benjam\u00edn Arellano F\u00e9lix", "pubs, bars and restaurants"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5744290243271222}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.3333333333333333, 0.7499999999999999, 0.6666666666666666, 0.0, 0.6086956521739131, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3469", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-2604", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-9273", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2133", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-5985", "mrqa_triviaqa-validation-1613", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1963", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-1495", "mrqa_searchqa-validation-16897", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-10356", "mrqa_searchqa-validation-16878", "mrqa_hotpotqa-validation-655"], "SR": 0.515625, "CSR": 0.5494791666666667, "EFR": 1.0, "Overall": 0.7252864583333334}, {"timecode": 42, "before_eval_results": {"predictions": ["The Dornbirner Ach", "cricket fighting", "XVideos", "William Harold \"Bill\" Ponsford", "Samuel Joel \" Zero\" Mostel", "various bigfoot-like sightings, giant snakes and \"thunderbirds.\"", "The Grandmaster", "Miss Universe 2010", "Charles Hastings Judd", "1952", "Squam Lake", "MGM Resorts International", "Golden Gate National Recreation Area", "William Corcoran Eustis", "sedimentary", "ideology", "an idiom for the most direct path between two points", "126 by Wilt Chamberlain from October 19, 1961 -- January 19, 1963", "March 5, 2014", "The genome", "Kit Harington", "April 26, 2005", "since 3, 1, and 4", "birth", "aiding the war effort", "Vincenzo Peruggia", "Netherlands", "gold", "landmasses", "puma", "neoclassic", "british miescher", "Sergei Rachmaninoff", "british island", "an \"on\"", "Galileo", "Rocky Horror Show", "Abbey Theatre", "16", "Ralph Lauren,", "Sen. Evan Bayh of Indiana and Virginia Gov. Tim Kaine,", "2,700-acre", "Jeanne Tripplehorn,", "served in the military,", "Karen Floyd", "ties", "bard", "a 100-day killing", "5 1/2-year-old", "Arsene Wenger", "Boston", "Odysseus", "Nine to Five", "a polar bear", "French and Indian War", "Henry Hudson", "Double Jeopardy", "Calais", "Lois Lane", "Falafel", "Easter Island", "alsacien le schnockeloch", "Haiti", "India is the world's second most populous country after the People's Republic of China"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6727926587301587}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.16666666666666669, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444444]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-3426", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-169", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3398", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-5163", "mrqa_triviaqa-validation-2777", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-436", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-5233", "mrqa_searchqa-validation-14554", "mrqa_naturalquestions-validation-8420"], "SR": 0.578125, "CSR": 0.5501453488372092, "retrieved_ids": ["mrqa_squad-train-53059", "mrqa_squad-train-60312", "mrqa_squad-train-30953", "mrqa_squad-train-67889", "mrqa_squad-train-17574", "mrqa_squad-train-48560", "mrqa_squad-train-59182", "mrqa_squad-train-64826", "mrqa_squad-train-1334", "mrqa_squad-train-583", "mrqa_squad-train-37216", "mrqa_squad-train-23825", "mrqa_squad-train-26831", "mrqa_squad-train-78880", "mrqa_squad-train-20359", "mrqa_squad-train-9892", "mrqa_searchqa-validation-14446", "mrqa_hotpotqa-validation-3826", "mrqa_triviaqa-validation-7353", "mrqa_searchqa-validation-15923", "mrqa_naturalquestions-validation-75", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2177", "mrqa_naturalquestions-validation-2250", "mrqa_hotpotqa-validation-3609", "mrqa_searchqa-validation-196", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-2329", "mrqa_newsqa-validation-2618", "mrqa_triviaqa-validation-2843", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-2219"], "EFR": 0.9259259259259259, "Overall": 0.710604879952627}, {"timecode": 43, "before_eval_results": {"predictions": ["November 2006", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "Firth of Forth", "Brigadier General Raden Panji Nugroho Notosusanto", "trio", "Lonely", "Anne Perry", "\"SOS\"", "872 to 930", "Cleveland, Ohio", "Smoothie King Center", "Indooroopilly Shopping Centre", "1960s to the 1990s", "Eyes Wide Shut", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "1939", "the body - centered cubic ( BCC ) lattice", "a donor molecule to an acceptor molecule", "Clarence Williams", "Homer Banks, Carl Hampton and Raymond Jackson", "2014", "20 year - old Kyla Coleman", "the Hongwu Emperor of the Ming Dynasty", "Sebastian Vettel", "the Holy See", "John Goodman", "Verona", "I Wanna Be Like You", "Sardinian", "the Herald of Free Enterprise", "Poland", "Magic Circle", "Anglo Saxon", "Leicester", "a 12", "John le Carr\u00e9", "misfits", "France", "the Los Alamitos Joint Forces Training Base", "Latin American and Caribbean nations", "foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "warning -- the FDA's strongest -- to alert patients of possible tendon ruptures and tendonitis.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "the bedrooms of their two-floor home in the St. Louis suburb of Columbia, Illinois,", "12", "Nafees A. Syed,", "2002.", "The U.S. Food and Drug Administration", "iTunes,", "at a depth of about 1,300 meters in the Mediterranean Sea.", "deluge", "a 1.8", "Roget", "diesel", "the 'Great Game'", "nosy", "South African", "DreamWorks", "Pitney Bowes", "microwave", "Agatha Christie", "Elvis Presley", "electrons", "The Hudson River"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5742572096698452}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.4210526315789474, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.6923076923076924, 0.8571428571428571, 0.7499999999999999, 0.5882352941176471, 1.0, 0.0, 0.2857142857142857, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-2853", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-2852", "mrqa_naturalquestions-validation-6324", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-3048", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-1419", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-6811", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-202", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-620", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2617", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-7587", "mrqa_searchqa-validation-11850", "mrqa_searchqa-validation-14325", "mrqa_searchqa-validation-16638", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7107"], "SR": 0.453125, "CSR": 0.5479403409090908, "EFR": 1.0, "Overall": 0.7249786931818182}, {"timecode": 44, "before_eval_results": {"predictions": ["the Warsaw Uprising Museum and in the Katy\u0144 Museum", "Scotiabank Saddledome", "once", "Scandinavian design", "1896", "Juventus of Italy", "1968", "FX", "SKUM", "1998", "Jack White", "Boyd Gaming", "Double Agent", "ThonMaker", "Acid rain", "Around 1200", "toys or doorbell installations", "China", "1992 to 2013", "Morgan Freeman", "Lewis Carroll", "14 December", "foreign investors", "Sir Rowland Hill", "Bonnie Aarons", "Dan Stevens", "Titan", "rugby school", "Vienna", "pangrams", "Gower Peninsula", "Atlantic Ocean", "Adventure", "montgomery", "Today newspaper", "Home Alone 2: Lost in New York", "drag club", "pink", "as he tried to throw a petrol bomb", "Idriss Deby hopes the journalists and the flight crew", "used-luxury cars", "July 4.", "Ford is in a different position. We're not seeking emergency taxpayer assistance.", "Spaniard Carlos Moya", "400 years ago", "in tunnels for the German government.", "caster Semenya", "July 1999,", "Chris Robinson", "Democratic National Convention", "A Few Good Men", "Brown University", "Fidelio", "porcelain", "Macaulay Culkin", "Neil Diamond", "Northern Exposure", "American romantic comedy sports film", "HUD", "Jacqueline Kennedy", "Marie Antoinette", "William Conrad", "Hobart", "reckless arson"], "metric_results": {"EM": 0.515625, "QA-F1": 0.633377544858523}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false], "QA-F1": [0.6, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.08695652173913042, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.5714285714285714, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-708", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-5472", "mrqa_hotpotqa-validation-839", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-5897", "mrqa_naturalquestions-validation-4326", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-1762", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-123", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-1361", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-423", "mrqa_searchqa-validation-12912", "mrqa_searchqa-validation-10831", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-8737", "mrqa_searchqa-validation-7791", "mrqa_triviaqa-validation-7370"], "SR": 0.515625, "CSR": 0.5472222222222223, "EFR": 0.967741935483871, "Overall": 0.7183834565412187}, {"timecode": 45, "before_eval_results": {"predictions": ["Germany and Austria", "Djibouti and Yemen", "the pancake house", "Patrick Henry", "fog", "Matlock", "Belgium", "can any one help me", "Peter Ackroyd", "chlorophyll", "even numbers", "obtaining and proper handling of human blood", "buch rugs", "Isolde", "the one - mile - wide ( 1.6 km ) strait connecting San Francisco Bay and the Pacific Ocean", "June 11, 2002", "early 20th century", "Eurasian Plate", "Michael Crawford", "the heart", "mathematical model", "111 straight wins", "a theory", "President Lyndon Johnson", "electric potential generated", "Michael Moriarty", "Barbara Niven", "The Supremes", "1974", "Love the Way You Lie", "archery bow", "Stephen Lee", "1949", "Anne Erin \"Annie\" Clark", "1899", "Gal Gadot", "John Lennon", "2009", "Grayback forest-firefighters", "a number of celebrities and ministers, ranging from Yolanda Adams to Bishop T.D. Jakes to Kirk Franklin.", "Darrel Mohler", "natural gas", "Friday,", "protective shoes", "Ernesto Bertarelli", "Ryder Russell", "France, Russia, India, South Korea, China, South Africa, Brazil, and Poland.", "issued his first military orders as leader of North Korea", "Brazil", "eight", "iconoclasm", "Tiger", "Hallmark Cards", "the chancellor", "food combining", "General Andrew Jackson", "pumice", "Kilobytes", "eucalyptus", "Maria Montessori", "pot roast", "Berlin", "Brooke Hogan", "Over the Rainbow"], "metric_results": {"EM": 0.5, "QA-F1": 0.579635741607249}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.375, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-943", "mrqa_triviaqa-validation-4798", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-7489", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3552", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-6243", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-5363", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-4674", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-1507", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-807", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-961", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-4069", "mrqa_searchqa-validation-13958", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-10665", "mrqa_searchqa-validation-572"], "SR": 0.5, "CSR": 0.5461956521739131, "retrieved_ids": ["mrqa_squad-train-80573", "mrqa_squad-train-3690", "mrqa_squad-train-79680", "mrqa_squad-train-74943", "mrqa_squad-train-1425", "mrqa_squad-train-50753", "mrqa_squad-train-82701", "mrqa_squad-train-71533", "mrqa_squad-train-71350", "mrqa_squad-train-15408", "mrqa_squad-train-14662", "mrqa_squad-train-22343", "mrqa_squad-train-7205", "mrqa_squad-train-84611", "mrqa_squad-train-77647", "mrqa_squad-train-19529", "mrqa_searchqa-validation-12800", "mrqa_triviaqa-validation-370", "mrqa_hotpotqa-validation-3973", "mrqa_searchqa-validation-7791", "mrqa_newsqa-validation-829", "mrqa_squad-validation-9480", "mrqa_naturalquestions-validation-1049", "mrqa_naturalquestions-validation-6897", "mrqa_hotpotqa-validation-2715", "mrqa_naturalquestions-validation-1682", "mrqa_newsqa-validation-1962", "mrqa_naturalquestions-validation-5036", "mrqa_newsqa-validation-1675", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-3722", "mrqa_triviaqa-validation-7230"], "EFR": 0.96875, "Overall": 0.7183797554347826}, {"timecode": 46, "before_eval_results": {"predictions": ["beautiful voice", "squid", "Sir Anthony Eden", "driving Miss Daisy", "Jackie Kennedy", "Tanzania", "Don Quixote", "Buddhism", "the U.S. Naval Academy", "Mercury", "Menninger", "a groom", "PEZ", "Hollaback", "1986", "$2.187 billion", "Donna", "10 June 1940", "Americans who served in the armed forces", "23 November 1996", "Aaron Harrison", "Bob Dylan", "to meet a sudden need for glucose", "the Sui", "Gustav Bauer", "1913", "biathlon", "embellish", "Mohandas Karamchand Gandhi", "Big Ben", "the solar system", "Tomb Raiders", "a tasmanian tiger", "the Dormouse", "Toll House cookie recipe", "trade union", "Tunisia", "tosca", "Tainted Love", "\"18 months\"", "67,038", "Everton", "Tom Kitt", "Ellie Kemper", "11,791", "pronghorn", "KULR", "Big 12 Conference", "June 1975", "My Beautiful Dark Twisted Fantasy", "for death squad killings carried out during his rule in the 1990s.", "Switzerland", "North Korea", "second-degree aggravated battery.", "debris", "Pakistan's", "that her most important work is her charity, the Happy Hearts Fund.", "The Sopranos", "Paktika province", "16", "on the Ohio River near Warsaw, Kentucky,", "Adam Lambert and Kris Allen", "HMS Thunderbolt", "the marquis de Montcalm"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7216987781954887}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-801", "mrqa_searchqa-validation-10673", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-15080", "mrqa_searchqa-validation-6632", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-7910", "mrqa_triviaqa-validation-4300", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-6480", "mrqa_hotpotqa-validation-1103", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-3992", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-2573", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4334"], "SR": 0.609375, "CSR": 0.5475398936170213, "EFR": 1.0, "Overall": 0.7248986037234043}, {"timecode": 47, "before_eval_results": {"predictions": ["Thomas Merton", "Morocco", "Walter Reed", "Okinawa", "Lake Victoria", "oil", "Walter Reed", "the gallbladder", "Out of Africa", "Dublin", "Marseille", "a light year", "archery", "awarded to the team that lost the pre-game coin toss", "The Massachusetts Compromise", "a nearby islet on the coast, named Puerto Rico ( Rich Port )", "Garfield Sobers", "the Old English pyrige ( pear tree )", "Burj Khalifa", "Thespis", "1920", "54 Mbit / s, plus error correction code", "Jack Barry", "American daytime drama", "Wisconsin", "united states", "Maine is the only state that has only one bordering, neighboring state", "mongoose", "a piece of material", "Fred Gwynne", "romanticism", "Vietnam", "Greece", "John McCarthy", "tobacco", "playing cards", "Leonard Nimoy", "Rana Daggubati", "Philadelphia", "Bart Conner", "200", "a North American carnivorous rodent of the family Cricetidae", "1986", "public", "The Dragon School in Oxford", "Aamir Khan", "Arrowhead Stadium", "Standard Oil", "Matt Groening", "nine years.", "voluntary manslaughter", "10,000 refugees,", "Tsvangirai", "animal products.", "different women coping with breast cancer in five vignettes.", "338 million", "ultra-high-strength steel and boron", "Malawi.", "Uzbekistan.", "a muddy barley field", "Symbionese Liberation Army", "a snowman", "Dick Van Dyke", "Pope Benedict XVI"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7066490800865801}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11833", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-121", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-6603", "mrqa_naturalquestions-validation-3246", "mrqa_triviaqa-validation-3680", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-7591", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-827", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-442", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3097", "mrqa_triviaqa-validation-7750"], "SR": 0.640625, "CSR": 0.5494791666666667, "EFR": 1.0, "Overall": 0.7252864583333334}, {"timecode": 48, "before_eval_results": {"predictions": ["bears", "(John) Tyler", "roundabout", "gravity", "Ka-bala", "sausages", "Larry King", "the pipa", "the Nez Perce", "\"9 To 5\"", "Thor", "Mount Hood", "\"Smallville\"", "Isley Brothers", "Freddie Highmore", "March 16, 2018", "Nepal", "ulcerative colitis", "Ferm\u00edn Francisco", "about 15 metres ( 49 feet ) per year", "1938", "Humphrey Bogart", "Egypt", "23 %", "1930s", "france", "onion", "france", "france Drake", "a karst cave", "france", "Jean-Paul Gaultier", "france", "Sheffield United", "\"Monster\"", "Dawn French", "murder/mystery", "Matt Groening", "Bulgarian", "\"Val\" Bure", "Get Him to the Greek", "balloons Street, Manchester", "New York Islanders", "\" training Day\"", "his exploration and settlement of what is now Kentucky,", "Burny Mattinson", "the Treaty of Trianon", "Tucum\u00e1n", "August 19, 2013", "bought themselves a massive sports car collection or something similar,\"", "\"Nu au Plateau de Sculpteur,\"", "(l-r)", "her niece", "an \"aesthetic environment\"", "new DNA evidence", "\"I didn't know who was coming. If the name 'Rihanna' had been mentioned, well, no disrespect, but it wouldn't have meant anything.\"", "allegations that a dorm parent mistreated students at the school.", "1616.", "the results are based on a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Fullerton, California,", "death", "sent an e-mail to reporters Wednesday", "france", "The patient, who prefers to be anonymous, is finally able to breathe through her nose, smell, eat solid foods and drink out of a cup,"], "metric_results": {"EM": 0.359375, "QA-F1": 0.45441468253968254}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9690", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-6975", "mrqa_naturalquestions-validation-10549", "mrqa_naturalquestions-validation-5932", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5483", "mrqa_triviaqa-validation-7656", "mrqa_triviaqa-validation-2540", "mrqa_triviaqa-validation-6405", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-1449", "mrqa_triviaqa-validation-4694", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-4862", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-3197", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1092"], "SR": 0.359375, "CSR": 0.5455994897959184, "retrieved_ids": ["mrqa_squad-train-62438", "mrqa_squad-train-36913", "mrqa_squad-train-14778", "mrqa_squad-train-74513", "mrqa_squad-train-20696", "mrqa_squad-train-71259", "mrqa_squad-train-60879", "mrqa_squad-train-73409", "mrqa_squad-train-19943", "mrqa_squad-train-8603", "mrqa_squad-train-49963", "mrqa_squad-train-18930", "mrqa_squad-train-48579", "mrqa_squad-train-21258", "mrqa_squad-train-13896", "mrqa_squad-train-18725", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-4092", "mrqa_squad-validation-2675", "mrqa_squad-validation-4675", "mrqa_searchqa-validation-15667", "mrqa_triviaqa-validation-1855", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-13223", "mrqa_newsqa-validation-38", "mrqa_naturalquestions-validation-9163", "mrqa_newsqa-validation-4113", "mrqa_triviaqa-validation-2757", "mrqa_squad-validation-3667", "mrqa_hotpotqa-validation-3931", "mrqa_naturalquestions-validation-8346", "mrqa_newsqa-validation-442"], "EFR": 1.0, "Overall": 0.7245105229591837}, {"timecode": 49, "before_eval_results": {"predictions": ["Some Like It Hot", "Margaret", "(bronchodilator)", "Legoland", "William Henry", "Abraham Lincoln", "Floyd Mayweather Jr.", "Isadora Duncan", "Beth Israel Deaconess Medical Center", "Venice", "Scarlet fever", "fog", "The British Broadcasting Corporation (BBC)", "January 1, 2016", "Charlotte of Mecklenburg - Strelitz", "Andrew Lloyd Webber", "business applications", "Great Plains and U.S. Interior Highlands region", "Rashida Jones", "Representatives", "fascia surrounding skeletal muscle", "Harold Godwinson", "divided into several successor polities", "Robin Cousins, Jason Gardiner, Barber and Ashley Roberts", "Buddhism", "Howard Hoagland", "ambassador to the United Nations", "1960", "about 800 miles (1,280 km)", "macbeth", "Cold Comfort Farm", "Prussian 2nd Army", "jackstones", "a horizontal desire", "Morgan Choir", "leaf", "Don Quixote", "the Mayor of the City of New York", "more than 110", "The Killer", "Louth", "Lawton Mainor Chiles", "right-hand", "XVideos", "1997", "Mauthausen-Gusen", "French", "Inverness", "Joshua Rowley", "Russia's", "producing rock music with a country influence.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Buenos Aires.", "Heshmat Tehran Attarzadeh", "hostile war zones,", "Lifeway Christian Stores", "two years,", "enterprise in history", "the \"face of the peace initiative has been attacked,\"", "tanker", "Nouri al-Maliki", "GM Motors", "Bagel set", "Street Art & Graffiti"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6436507936507936}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.7142857142857143, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-3004", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-11090", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7009", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10448", "mrqa_naturalquestions-validation-1786", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-2875", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7262", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-5267", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-2475", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-1599", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3671", "mrqa_triviaqa-validation-2662", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-3671"], "SR": 0.484375, "CSR": 0.544375, "EFR": 0.9696969696969697, "Overall": 0.718205018939394}, {"timecode": 50, "UKR": 0.7421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3773", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-957", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-10727", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4488", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-4592", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1120", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1808", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2075", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-3801", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-4102", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11459", "mrqa_searchqa-validation-11470", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-1320", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14148", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15219", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16588", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16866", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-2525", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3315", "mrqa_searchqa-validation-3389", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3849", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-409", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5285", "mrqa_searchqa-validation-5352", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-5705", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6423", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7115", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7646", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1850", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-3218", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3378", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3530", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4439", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5197", "mrqa_squad-validation-5260", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5326", "mrqa_squad-validation-5360", "mrqa_squad-validation-5480", "mrqa_squad-validation-551", "mrqa_squad-validation-5631", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-7565", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-811", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1369", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1476", "mrqa_triviaqa-validation-1567", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4381", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4694", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-584", "mrqa_triviaqa-validation-5848", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7117", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7647", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-890", "mrqa_triviaqa-validation-943"], "OKR": 0.845703125, "KG": 0.50546875, "before_eval_results": {"predictions": ["the skull & crossbones", "d'autrefois", "the Swamp Fox", "jelly", "Zsa Zsa Gabor", "Eurydice", "the eye", "foot", "Osteosarcoma", "Papua New Guinea", "William Jennings Bryan", "apocrypha", "Australian", "Network - Protocol driver", "Hon July Moyo", "1773", "the Vice President of the United States", "Sebastian Lund ( Rob Kerkovich )", "Burbank, California", "the bank's own funds", "lumbar enlargement", "Missouri River", "Sultans", "a moral tale", "pour point of a liquid", "shotguns", "China", "silver", "Moon River", "Bronx Mowgli", "driving", "Jeremy Bates", "senior", "pyramids", "wigan", "Zagreb", "a wedge-shaped microcar", "Michael Seater", "\"Sleeping Beauty\"", "ten years of probation", "\"Empire Falls\"", "Imagine", "Paul Avery", "Louis Silvie \"Louie\" Zamperini", "baeocystin", "Venice", "Ghanaian", "Kirk Humphreys", "the Mayor of the City of New York", "enjoyed", "managing his time.", "Bobby Jindal", "Dr. Jennifer Arnold and husband Bill Klein,", "gasoline", "The station", "56", "Lieberman", "Roy Foster", "Three", "Don Draper", "April 28", "Ethel `` Edy '' Proctor", "Jean - Jacques Rousseau's Confessions", "1979"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6222531288156288}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444444, 1.0, 1.0, 0.08000000000000002, 0.6666666666666666, 0.3076923076923077, 0.0, 0.5, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-10065", "mrqa_searchqa-validation-8164", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-12114", "mrqa_searchqa-validation-2548", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-1282", "mrqa_naturalquestions-validation-6012", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-5940", "mrqa_triviaqa-validation-689", "mrqa_triviaqa-validation-5385", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-443", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-974", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-2262", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-879", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-834"], "SR": 0.484375, "CSR": 0.5431985294117647, "EFR": 0.9696969696969697, "Overall": 0.7212509748217468}, {"timecode": 51, "before_eval_results": {"predictions": ["cherries", "Belgium", "Mark Antony", "the Netherlands", "god of love, Light, Beauty", "lento", "crayfish", "time", "Jupiter's", "Shropshire", "red", "the Earth's shadow", "Jean Foucault", "1986", "Shreya Bhushan Pethewala", "Database - Protocol driver ( Pure Java driver )", "hydrogen", "April 6, 1917", "2002", "Johnny Logan", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1955", "an instant messaging client", "France", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Space Oddity", "Maria Ouspenskaya", "red", "Stereophonics", "Greyfriars Bobby, Skye terrier", "matricide", "Claude Monet's", "curling", "Melpomene", "vitamin K1", "Wat Tyler", "vegetation", "USS \"Essex\" (CV/CVA/CVS-9)", "February 20, 1978", "Kirkcudbright", "a working-class young man who spends his weekends dancing and drinking at a local Brooklyn discoth\u00e8que", "Port of Boston", "Lucas Grabeel", "first used the polite \"Hands Up!\"", "Scarface", "evangelical Christian periodical", "Tallahassee City Commission in February 2003", "The Rebirth", "press conference", "Chinese tourists", "Henry Ford", "returning combat veterans", "non-European Union player in Frank Rijkaard's squad.", "for the girl", "top designers, such as Stella McCartney,", "in the west African nation later this year.", "Lunsmann in July in the Philippines was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "in the southern port city of Karachi,", "fractured pelvis and sacrum -- the triangular bone within the pelvis.", "in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "Ali Bongo,", "port", "Mel Brooks", "feet"], "metric_results": {"EM": 0.5, "QA-F1": 0.646446608946609}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 1.0, 0.8, 0.22222222222222224, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 0.22222222222222224, 1.0, 0.0, 0.0, 0.9090909090909091, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-4975", "mrqa_searchqa-validation-4573", "mrqa_searchqa-validation-9522", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-10265", "mrqa_triviaqa-validation-4491", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-6621", "mrqa_triviaqa-validation-1694", "mrqa_triviaqa-validation-1088", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-3001", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-3076", "mrqa_newsqa-validation-3035", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-3925", "mrqa_triviaqa-validation-6748"], "SR": 0.5, "CSR": 0.5423677884615384, "retrieved_ids": ["mrqa_squad-train-55003", "mrqa_squad-train-22889", "mrqa_squad-train-65483", "mrqa_squad-train-60789", "mrqa_squad-train-72230", "mrqa_squad-train-64046", "mrqa_squad-train-64788", "mrqa_squad-train-3789", "mrqa_squad-train-72593", "mrqa_squad-train-3302", "mrqa_squad-train-43091", "mrqa_squad-train-61131", "mrqa_squad-train-65791", "mrqa_squad-train-65044", "mrqa_squad-train-35357", "mrqa_squad-train-70274", "mrqa_naturalquestions-validation-1282", "mrqa_hotpotqa-validation-3059", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-14772", "mrqa_squad-validation-3689", "mrqa_hotpotqa-validation-2153", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-9226", "mrqa_searchqa-validation-44", "mrqa_squad-validation-8643", "mrqa_squad-validation-2497", "mrqa_squad-validation-6753", "mrqa_searchqa-validation-15002", "mrqa_naturalquestions-validation-4326", "mrqa_searchqa-validation-2089"], "EFR": 0.9375, "Overall": 0.7146454326923076}, {"timecode": 52, "before_eval_results": {"predictions": ["President George Washington", "a head", "William Howard Taft", "Sly & the Family Stone", "Saudi Arabia", "Absalom", "Odysseus", "Xinjiang", "Wales", "the Galapagos", "drum", "Big Brown", "Patrick Henry", "the Naturalization Act of 1790", "the forex market", "6 January 793", "Camping World Stadium in Orlando, Florida", "Tandi, in Lahaul", "Dadra and Nagar Haveli", "1820s", "Vasoepididymostomy", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "self - closing flood barrier", "nucleus", "Gillen Simone Vangsness", "The Rocky Horror Picture Show", "great hothouse", "Virginia", "China", "War and Peace", "sports agent", "Wisconsin", "points based scoring system", "cosmology", "Mary Seacole", "benedict moseley", "per annum exclusive", "Vilnius Old Town", "Samuel Burl \"Sam\" Kinison", "Wabanaki Confederacy", "DI Humphrey Goodman", "Mika H\u00e4kkinen", "Liga MX", "Kevin Peter Hall", "a \"homeward bounder\" a sailor coming home from a round trip", "Martha Wainwright", "Chicago", "Blue Origin", "Canada's first train robbery", "'Xanadu.'\"", "taught a song about freedom of speech.", "stood 6 feet 6 inches,", "their emergency plans", "Akshay Kumar", "Ketchum, Idaho.", "the return of a fallen U.S. service member", "a 13-week extension of unemployment benefits", "South Africa", "July 8", "Vernon Forrest,", "Larry King Live", "an embroidered cloth", "Harry Potter", "Jeremy Bates"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6306795634920636}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12283", "mrqa_searchqa-validation-105", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-15615", "mrqa_searchqa-validation-1640", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-5170", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-3005", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-5236", "mrqa_hotpotqa-validation-981", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-283", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-3652", "mrqa_newsqa-validation-4128", "mrqa_triviaqa-validation-412"], "SR": 0.5625, "CSR": 0.5427476415094339, "EFR": 1.0, "Overall": 0.7272214033018868}, {"timecode": 53, "before_eval_results": {"predictions": ["the Salt Lake City", "the llama", "El Cid", "Reykjavk", "Rudy Giuliani", "Ramen", "Wilhelm II", "House", "Arkansas", "the phi phenomenon", "bowl", "the nomadic", "Wichita", "Ernest Rutherford", "81.617", "CBS Television City", "Donald Fauntleroy Duck", "1786", "24", "shatrughan", "unknown origin", "January 2017", "1989", "Peter Andrew Beardsley MBE", "the 1980s", "1951", "brettshire", "david Copperfield", "as You Like It", "bees", "Canada", "Argentina", "heart", "World War I", "brekboere", "harry redknapp", "earthquake", "roller coaster", "Hawaii", "1826", "14 December 1990", "Miriam Margolyes", "Edward of Caernarfon", "Charles Eug\u00e8ne Jules Marie Nungesser", "media", "Valley Falls", "Lucas Stephen Grabeel", "1966", "Vin Diesel", "Orlando police.", "alert patients of possible tendon ruptures and tendonitis.", "seven or eight", "Amir Zaki", "Iran's nuclear program.", "bipartisan", "Mashhad", "Lars von Trier.", "Gary Branca and John McClain,", "Father Cutie", "Longo-Ciprelli", "tells stories of different women coping with breast cancer in five vignettes.", "Clio", "Spanish", "the Andes Mountains of Chile and Argentina"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6583276098901099}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-3290", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-9555", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-1770", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-3532", "mrqa_hotpotqa-validation-1667", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-4129", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-3483", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-3984", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-3547"], "SR": 0.578125, "CSR": 0.5434027777777778, "EFR": 0.9259259259259259, "Overall": 0.7125376157407407}, {"timecode": 54, "before_eval_results": {"predictions": ["Spain", "the Montague", "The Pirates of Penzance", "Joseph Goebbels", "city of atlanta", "the Blue", "Donald Trump", "Vipers", "Darren Star", "Catalina", "Alaska", "Cleveland", "Marilyn Monroe", "Nicole Gale Anderson", "James Watson and Francis Crick", "The Live - Stock Dealers '", "when the forward reaction proceeds at the same rate as the reverse reaction", "over 74", "Sasha Banks", "(Bill ) Brady", "free floating", "up to 100,000", "the left coronary artery", "thylakoid membranes", "1800", "bobby Fischer", "creme anglaise", "tonsure", "1990", "copper", "1979", "robinson crusoe", "HMS Conqueror", "morphine", "vinegar", "wist", "four red", "Virginia", "1941", "Larry Eustachy", "Oregon State Beavers", "Mexico", "shock", "The Books", "Cecily Strong", "1933", "Steve John Carell", "MediaCityUK", "first baseman", "Three French journalists,", "Gulf of Aden", "top designers,", "Authorities in Fayetteville, North Carolina,", "military trial system", "six innings,", "first", "Former Mobile County Circuit Judge Herman Thomas", "poems", "the British capital's other two airports, Stansted and Gatwick,", "\"illeg illegitimate.\"", "Arthur E. Morgan III", "Utena", "$10\u201320 million", "September 25, 2017"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5845734126984126}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.8, 0.3333333333333333, 1.0, 0.6666666666666666, 0.4, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.888888888888889, 0.05714285714285714, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4626", "mrqa_searchqa-validation-1877", "mrqa_searchqa-validation-15700", "mrqa_searchqa-validation-13612", "mrqa_searchqa-validation-874", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-10279", "mrqa_triviaqa-validation-6660", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-2107", "mrqa_triviaqa-validation-678", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-891", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-1279", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-2554", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-867", "mrqa_hotpotqa-validation-1196"], "SR": 0.46875, "CSR": 0.5420454545454545, "retrieved_ids": ["mrqa_squad-train-34302", "mrqa_squad-train-16435", "mrqa_squad-train-30565", "mrqa_squad-train-46147", "mrqa_squad-train-79389", "mrqa_squad-train-83212", "mrqa_squad-train-65207", "mrqa_squad-train-73742", "mrqa_squad-train-58403", "mrqa_squad-train-50099", "mrqa_squad-train-38144", "mrqa_squad-train-50805", "mrqa_squad-train-6457", "mrqa_squad-train-79582", "mrqa_squad-train-55407", "mrqa_squad-train-47733", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-1284", "mrqa_searchqa-validation-6565", "mrqa_searchqa-validation-8891", "mrqa_newsqa-validation-3769", "mrqa_triviaqa-validation-6585", "mrqa_newsqa-validation-1098", "mrqa_triviaqa-validation-4679", "mrqa_triviaqa-validation-6728", "mrqa_searchqa-validation-3585", "mrqa_squad-validation-2704", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-901", "mrqa_squad-validation-6735", "mrqa_naturalquestions-validation-2250", "mrqa_newsqa-validation-3732"], "EFR": 0.9705882352941176, "Overall": 0.7211986129679144}, {"timecode": 55, "before_eval_results": {"predictions": ["Dylan Massett", "Ohio", "a phrase with several meanings, some of which are polar opposites", "c. 3000 BC", "Donna Mills", "for the red - bed country of its watershed", "SIP ( Session Initiation Protocol )", "Thirty years after the Galactic Civil War", "grades 1 ( threshold 85 %, a distinction ), 2 ( 70 -- 84 % ) & 4 ( 40 -- 54 % )", "Carol Ann Susi", "De Waynene Warren", "`` whistleblowers ''", "Yugoslavia", "musical term Glossary", "Emily Davison", "Ellice Islands", "a nerve cell cluster", "Tripoli", "in the fortified grounds of an old mission known as the Alamo fortress", "Oklahoma", "Treaty of Lisbon", "peregrines", "Wadsworth", "\"Stars on 45 Medley\"", "Big Dipper", "Dwarka", "Daniel Sturridge", "Texas Longhorn", "Omega SA", "\"The Gang\"", "Delilah Rene", "Italian", "WB Television Network", "Reese Witherspoon", "British Labour Party", "Christopher Tin", "former Chelsea and Middlesbrough striker Jimmy Floyd Hasselbaink", "President Barack Obama,", "bipartisan rhetoric", "a woman", "a weight-loss show", "Mikkel Kessler", "$250,000 for Rivers' charity: God's Love We Deliver.", "Kindle Fire", "Agent Mark Steinberg", "800,000", "The 19-year-old woman", "102", "\"Now that we know Muhammad is an Ennis man, we will be back,\"", "zinc", "birds", "a vessel", "the 2014 holiday season", "West Point", "Joe DiMaggio", "the welfare of the people", "Quasars", "Japan", "the United States presidential", "resuscitation", "Jumbo", "Spock", "Debbie Abrahams", "Apollo 11"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5612621753246753}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1976", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-993", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-7521", "mrqa_hotpotqa-validation-1240", "mrqa_hotpotqa-validation-4865", "mrqa_hotpotqa-validation-1435", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-4544", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3759", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2803", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-14981", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-9641", "mrqa_searchqa-validation-2882", "mrqa_searchqa-validation-16358", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-125"], "SR": 0.46875, "CSR": 0.5407366071428572, "EFR": 0.9705882352941176, "Overall": 0.720936843487395}, {"timecode": 56, "before_eval_results": {"predictions": ["Michael Phelps", "scurvy", "isosceles", "belgian port", "1801", "clefts", "perfume", "Stage 1", "with a definite, visible tail joint", "woolchen Franklin", "(John) Bercow", "Petrus Stuyvesant", "Ian Botham", "Ed Sheeran", "Jackie Robinson", "erosion", "Kelly Osbourne", "British pop band T'Pau", "January 12, 2017", "more than 80", "Thomas Edison", "Pop", "Andrew Garfield", "the Titanic never sank", "the ulnar nerve", "Kareena Kapoor", "Juan Francisco Antonio Hilari\u00f3n Zea D\u00edaz", "ARY Films", "1967", "a particular nation", "Lowestoft", "Antonio Salieri", "McKinsey & Company", "British Labour Party", "Camille Saint-Sa\u00ebns", "Spiro Agnew", "In a Better World", "by a team of eight surgeons at the Cleveland Clinic.", "second time since the 1990s", "blind Majid Movahedi,", "weight-loss", "the home,", "attempted murder,", "three different videos", "1995", "for death squad killings carried out during his rule in the 1990s.", "the Saudi Arabian man who was arrested for bragging about his sex life on television", "make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "Paul Schlesselman", "Worcestershire", "Isaac Newton", "titanium", "Robert Browning", "St. Augustine", "staff", "pizza al taglio", "Patriotism", "quiet", "the Dalmatian", "pitch", "Grover Cleveland.", "Jasenovac concentration camp", "Kohlberg K Travis Roberts", "John Robert Cocker"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5971345464355333}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.8, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2105263157894737, 0.14814814814814817, 0.22222222222222218, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-3403", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-7556", "mrqa_triviaqa-validation-405", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-1879", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-9741", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-5490", "mrqa_hotpotqa-validation-5110", "mrqa_hotpotqa-validation-3512", "mrqa_newsqa-validation-1679", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-1146", "mrqa_searchqa-validation-1319", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-14110", "mrqa_searchqa-validation-13085", "mrqa_searchqa-validation-4014", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4926"], "SR": 0.546875, "CSR": 0.5408442982456141, "EFR": 1.0, "Overall": 0.7268407346491228}, {"timecode": 57, "before_eval_results": {"predictions": ["scored a hat-trick", "a long-range missile", "bipartisan", "10", "The show went on without the self-proclaimed \"King of the South,\"", "individual pieces.", "Himalayan kingdom", "Stratfor,", "tells stories of different women coping with breast cancer in five vignettes.", "Ayelet Zurer", "the Obama chief of staff", "the North Korea's announcement", "the Dalai Lama's current \"middle way approach,\"", "early 2017", "piety", "chain elongation", "the coffee shop Monk's", "Richard Carpenter", "Kate Flannery", "October 2012", "Matthew Gregory Wise", "2003", "Noah Schnapp", "about 13,000 astronomical units ( 0.21 ly )", "May 26, 2017", "1973", "Toy Story 2", "gymnastics", "Charlie Sheen", "Italy", "tony elton", "blue", "Prophet Joseph Smith", "sprite", "Ohio", "e.g. tauri", "Madagascar", "British", "Adult Swim", "Chester", "a large green dinosaur", "Sean", "45th Infantry Division", "Knoxville", "Lionsgate", "his superhero roles as the Marvel Comics characters Steve Rogers / Captain America in the Marvel Cinematic Universe", "A123 Systems, LLC", "Groom Lake Valley portion of the Tonopah Basin", "Jenji Kohan", "(Samuel) Dickens", "November 22, 1963", "(Big) Brown", "a torpedo", "Parkinson's", "the California Missions", "potential energy", "(Samuel) Clemens", "the Coward of the County", "hockey", "Minnesota", "the McLean House", "(Dwight) Hearst", "Dublin", "Moses"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5622395833333332}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-177", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-1660", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-8832", "mrqa_naturalquestions-validation-7262", "mrqa_triviaqa-validation-3695", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-845", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-3137", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-5649", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-2749", "mrqa_searchqa-validation-11956", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-13713", "mrqa_searchqa-validation-9484", "mrqa_searchqa-validation-495", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-13336", "mrqa_searchqa-validation-7078", "mrqa_searchqa-validation-1462"], "SR": 0.4375, "CSR": 0.5390625, "retrieved_ids": ["mrqa_squad-train-14072", "mrqa_squad-train-7570", "mrqa_squad-train-27814", "mrqa_squad-train-74989", "mrqa_squad-train-49158", "mrqa_squad-train-18016", "mrqa_squad-train-11882", "mrqa_squad-train-10434", "mrqa_squad-train-49410", "mrqa_squad-train-27692", "mrqa_squad-train-51617", "mrqa_squad-train-30299", "mrqa_squad-train-57637", "mrqa_squad-train-2349", "mrqa_squad-train-85085", "mrqa_squad-train-78425", "mrqa_newsqa-validation-300", "mrqa_searchqa-validation-15331", "mrqa_searchqa-validation-8891", "mrqa_hotpotqa-validation-1127", "mrqa_triviaqa-validation-2300", "mrqa_triviaqa-validation-821", "mrqa_hotpotqa-validation-2475", "mrqa_naturalquestions-validation-3214", "mrqa_triviaqa-validation-7010", "mrqa_triviaqa-validation-502", "mrqa_newsqa-validation-2262", "mrqa_naturalquestions-validation-9149", "mrqa_searchqa-validation-16840", "mrqa_triviaqa-validation-6467", "mrqa_squad-validation-4176", "mrqa_triviaqa-validation-3642"], "EFR": 0.9722222222222222, "Overall": 0.7209288194444444}, {"timecode": 58, "before_eval_results": {"predictions": ["December 2, 1973", "Dialogues des Carm\u00e9lites", "Dr. Alberto Taquini", "England", "Mary Harron", "Bank of China Tower", "Borgo San Donnino", "the world's deadliest mid-air collision", "electric currents and magnetic fields", "Benjamin Andrew \"Ben\" Stokes", "Robert Frost's former home in Franconia, New Hampshire, United States", "2006", "Tianhe Stadium", "Mickey Rourke", "Gayla Peevey", "2001", "part - Samoyed terrier", "Otis Timson", "Richard Masur", "most organisms combine to form a zygote with n pairs of chromosomes, i.e. 2n chromosomes in total", "the church sexton Robert Newman and Captain John Pulling", "commemorating fealty and filial piety", "February 9, 2018", "West African traditions", "pop ballad", "genus", "James Dean", "bodhisattva path", "Russia", "NOW Magazine", "Canada", "Sugarloaf Mountain", "Battle of Hastings", "sebastian king", "patgfisher", "Hasidic Jews", "british McKinley", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "German Chancellor Angela Merkel", "\"Quiet Nights,\"", "wildfires", "15,000", "four", "London and Buenos Aires", "Workers' Party.", "President Obama and Britain's Prince Charles", "Dolgorsuren Dagvadorj,", "$1.45 billion", "Dr. Cade", "Columbia River", "metal alloys", "comet", "Dollywood-FM", "Terry Bradshaw", "Scotland", "al-Jaz'ir", "Kunta Kinte", "Venetian", "a cappella", "Orchids", "the FBI", "21-year-old", "$12.9 million", "their activism and their tour."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5530448717948718}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, true, false, false], "QA-F1": [0.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.6666666666666666, 0.26666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-1763", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-733", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-4925", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-2612", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-7492", "mrqa_newsqa-validation-3364", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2497", "mrqa_searchqa-validation-6023", "mrqa_searchqa-validation-7395", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-6939", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-9072", "mrqa_searchqa-validation-2458", "mrqa_searchqa-validation-13194", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1348"], "SR": 0.453125, "CSR": 0.5376059322033898, "EFR": 0.9714285714285714, "Overall": 0.7204787757263922}, {"timecode": 59, "before_eval_results": {"predictions": ["February 9, 1994", "Bank of China Building", "johnson", "Battelle Energy Alliance", "John of Gaunt", "Croatian", "1946", "over 20,950", "voice-work", "Mwabvi river", "\"Rich Girl\"", "California State University, Dominguez Hills", "Nickelodeon Animation Studio", "H.G. Wells", "President Gerald Ford", "September 1947", "160km / hour", "March 16, 2018", "Lalo Schifrin", "The Cornett family", "Baaghi", "the Old Testament", "October 21, 2016", "Third Five - year Plan", "Andaman and Nicobar Islands -- Port Blair", "6", "speedway", "furniture", "linda of marie piave", "mrs. robin", "midsomer Murders", "Norman Mailer", "british skipper Dee Caffari", "(Harry) Truman", "in groundwater, swamp water, and lakes", "mars", "bamboozled", "Sri Lanka", "romantic e-mails", "parents", "Nigeria, Africa's largest producer.", "Fullerton, California,", "never perform, the credit card company still has your money and can give it right back to you.", "the man was dead,", "back at work.", "World-renowned security expert Gavin de Becker", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.", "a review of state government practices completed in 100 days.", "Sunday.", "Crete", "Tartarus", "climbing", "Alaska", "Wings", "birds", "Luzon", "stigma", "M&M's", "quicksand", "tanks", "\"9 To 5\"", "Eleven", "Elin Nordegren,", "Shanghai"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6205468322386425}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2222222222222222, 0.4, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.06896551724137931, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-4905", "mrqa_hotpotqa-validation-3020", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-10687", "mrqa_triviaqa-validation-3282", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-1828", "mrqa_triviaqa-validation-2374", "mrqa_triviaqa-validation-2709", "mrqa_newsqa-validation-2896", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1178", "mrqa_newsqa-validation-2401", "mrqa_searchqa-validation-209", "mrqa_searchqa-validation-10663", "mrqa_searchqa-validation-2826"], "SR": 0.515625, "CSR": 0.5372395833333333, "EFR": 0.9354838709677419, "Overall": 0.713216565860215}, {"timecode": 60, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1174", "mrqa_hotpotqa-validation-1196", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1489", "mrqa_hotpotqa-validation-1497", "mrqa_hotpotqa-validation-1563", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1595", "mrqa_hotpotqa-validation-1701", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1773", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-1903", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-1951", "mrqa_hotpotqa-validation-2096", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2475", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-260", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-264", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-2906", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3197", "mrqa_hotpotqa-validation-3326", "mrqa_hotpotqa-validation-3405", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3771", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3791", "mrqa_hotpotqa-validation-3855", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-4147", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-480", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5170", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5464", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5702", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-575", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5827", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-588", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-664", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1802", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2209", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3008", "mrqa_naturalquestions-validation-3043", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-3109", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-360", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-3738", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4559", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5562", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6417", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6624", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6733", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7705", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-7968", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8289", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8911", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-9013", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9939", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1239", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-153", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1969", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-2421", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2587", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3102", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-3495", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-3769", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-3806", "mrqa_newsqa-validation-3984", "mrqa_newsqa-validation-40", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-643", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-731", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-99", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10183", "mrqa_searchqa-validation-10439", "mrqa_searchqa-validation-10939", "mrqa_searchqa-validation-11190", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12326", "mrqa_searchqa-validation-12438", "mrqa_searchqa-validation-12694", "mrqa_searchqa-validation-12955", "mrqa_searchqa-validation-13259", "mrqa_searchqa-validation-13311", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-14095", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14560", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-14834", "mrqa_searchqa-validation-1506", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-15437", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15640", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16137", "mrqa_searchqa-validation-16321", "mrqa_searchqa-validation-16598", "mrqa_searchqa-validation-16848", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-171", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-2114", "mrqa_searchqa-validation-2485", "mrqa_searchqa-validation-3007", "mrqa_searchqa-validation-3076", "mrqa_searchqa-validation-3525", "mrqa_searchqa-validation-355", "mrqa_searchqa-validation-3585", "mrqa_searchqa-validation-3785", "mrqa_searchqa-validation-3890", "mrqa_searchqa-validation-4046", "mrqa_searchqa-validation-411", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-498", "mrqa_searchqa-validation-5058", "mrqa_searchqa-validation-5442", "mrqa_searchqa-validation-5470", "mrqa_searchqa-validation-5676", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6465", "mrqa_searchqa-validation-6561", "mrqa_searchqa-validation-6651", "mrqa_searchqa-validation-6783", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7019", "mrqa_searchqa-validation-7160", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7745", "mrqa_searchqa-validation-7746", "mrqa_searchqa-validation-7845", "mrqa_searchqa-validation-7943", "mrqa_searchqa-validation-8079", "mrqa_searchqa-validation-8484", "mrqa_searchqa-validation-8528", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-8999", "mrqa_searchqa-validation-9118", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-9272", "mrqa_searchqa-validation-9518", "mrqa_squad-validation-10106", "mrqa_squad-validation-10223", "mrqa_squad-validation-10410", "mrqa_squad-validation-1049", "mrqa_squad-validation-1226", "mrqa_squad-validation-1299", "mrqa_squad-validation-1391", "mrqa_squad-validation-1530", "mrqa_squad-validation-1568", "mrqa_squad-validation-1690", "mrqa_squad-validation-173", "mrqa_squad-validation-1766", "mrqa_squad-validation-1784", "mrqa_squad-validation-1872", "mrqa_squad-validation-2171", "mrqa_squad-validation-2324", "mrqa_squad-validation-2402", "mrqa_squad-validation-2914", "mrqa_squad-validation-2956", "mrqa_squad-validation-3197", "mrqa_squad-validation-324", "mrqa_squad-validation-3410", "mrqa_squad-validation-353", "mrqa_squad-validation-3602", "mrqa_squad-validation-3628", "mrqa_squad-validation-3799", "mrqa_squad-validation-3938", "mrqa_squad-validation-4003", "mrqa_squad-validation-415", "mrqa_squad-validation-4484", "mrqa_squad-validation-4486", "mrqa_squad-validation-4527", "mrqa_squad-validation-453", "mrqa_squad-validation-4764", "mrqa_squad-validation-4801", "mrqa_squad-validation-4954", "mrqa_squad-validation-5272", "mrqa_squad-validation-5320", "mrqa_squad-validation-5360", "mrqa_squad-validation-551", "mrqa_squad-validation-5941", "mrqa_squad-validation-5951", "mrqa_squad-validation-6119", "mrqa_squad-validation-6480", "mrqa_squad-validation-6505", "mrqa_squad-validation-6639", "mrqa_squad-validation-6750", "mrqa_squad-validation-6795", "mrqa_squad-validation-6810", "mrqa_squad-validation-6812", "mrqa_squad-validation-6896", "mrqa_squad-validation-6978", "mrqa_squad-validation-7090", "mrqa_squad-validation-718", "mrqa_squad-validation-7201", "mrqa_squad-validation-7293", "mrqa_squad-validation-7490", "mrqa_squad-validation-770", "mrqa_squad-validation-7723", "mrqa_squad-validation-7789", "mrqa_squad-validation-7945", "mrqa_squad-validation-8012", "mrqa_squad-validation-8498", "mrqa_squad-validation-8500", "mrqa_squad-validation-8658", "mrqa_squad-validation-8917", "mrqa_squad-validation-8975", "mrqa_squad-validation-9038", "mrqa_squad-validation-912", "mrqa_squad-validation-916", "mrqa_squad-validation-9240", "mrqa_squad-validation-934", "mrqa_squad-validation-9476", "mrqa_squad-validation-9480", "mrqa_squad-validation-9643", "mrqa_squad-validation-9694", "mrqa_squad-validation-9810", "mrqa_squad-validation-9818", "mrqa_squad-validation-9859", "mrqa_squad-validation-9900", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1104", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-1405", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1694", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1879", "mrqa_triviaqa-validation-190", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-2540", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2999", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3175", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3223", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3279", "mrqa_triviaqa-validation-3573", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-3653", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-3795", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-392", "mrqa_triviaqa-validation-3936", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4165", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5338", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5615", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-5766", "mrqa_triviaqa-validation-5858", "mrqa_triviaqa-validation-596", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6029", "mrqa_triviaqa-validation-6059", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6085", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6114", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-6388", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-6424", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6808", "mrqa_triviaqa-validation-6811", "mrqa_triviaqa-validation-6814", "mrqa_triviaqa-validation-682", "mrqa_triviaqa-validation-6956", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7264", "mrqa_triviaqa-validation-7438", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7514", "mrqa_triviaqa-validation-7537", "mrqa_triviaqa-validation-7549", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-917", "mrqa_triviaqa-validation-943"], "OKR": 0.798828125, "KG": 0.4671875, "before_eval_results": {"predictions": ["British", "Timo Hildebrand", "Samuel Beckett", "constant support from propaganda campaigns", "The Defenders", "Philadelphia", "Buddha's delight", "11 June 1959", "Anishinaabeg", "Northwest Indiana", "55%", "850 saloon", "1942", "bicameral Congress", "Horace Lawson Hunley", "Human fertilization", "3D computer graphics", "the March of Dimes for Franklin D. Roosevelt", "Germany", "- ase", "Andreas Vesalius", "Mary Elizabeth Patterson", "the fictional town of West Egg", "William Jennings Bryan", "Minneapolis", "five", "arthur", "Sherlock Holmes", "Djibouti and Yemen", "Pallenberg", "hay fever", "arthur", "arthur", "s truman", "Styal", "Sicily", "Gorky", "Brian Mabry", "The opposition group, also known as the \"red shirts,\"", "Somali forces and Islamic insurgents.", "eight", "Nairobi, Kenya,", "Sweden,", "one", "Stratfor", "183", "Sub-Saharan Africa", "jazz", "Clarkson", "cavities", "Paterno", "a Bacon", "the Shrew", "Mall", "Heijo", "Universal Studios Hollywood", "conspicuous", "Fairfax", "the Clark bar", "diameter", "the mammoth", "Ma Khin Khin Leh,", "a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies.", "Woods"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6223710317460317}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.5, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-126", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-3617", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-525", "mrqa_hotpotqa-validation-871", "mrqa_hotpotqa-validation-1869", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-2799", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-5674", "mrqa_triviaqa-validation-5273", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-4829", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-2879", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-3178", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2200", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-924", "mrqa_searchqa-validation-1024", "mrqa_searchqa-validation-10411", "mrqa_searchqa-validation-10482", "mrqa_searchqa-validation-4850", "mrqa_newsqa-validation-4168"], "SR": 0.53125, "CSR": 0.537141393442623, "retrieved_ids": ["mrqa_squad-train-61294", "mrqa_squad-train-59202", "mrqa_squad-train-65570", "mrqa_squad-train-80093", "mrqa_squad-train-38989", "mrqa_squad-train-14177", "mrqa_squad-train-30444", "mrqa_squad-train-81810", "mrqa_squad-train-15901", "mrqa_squad-train-5171", "mrqa_squad-train-64608", "mrqa_squad-train-37095", "mrqa_squad-train-17059", "mrqa_squad-train-39057", "mrqa_squad-train-3135", "mrqa_squad-train-44333", "mrqa_naturalquestions-validation-5835", "mrqa_searchqa-validation-7978", "mrqa_squad-validation-4332", "mrqa_newsqa-validation-3051", "mrqa_naturalquestions-validation-7968", "mrqa_newsqa-validation-2019", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-3710", "mrqa_triviaqa-validation-7011", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-3059", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-10448", "mrqa_hotpotqa-validation-3512", "mrqa_searchqa-validation-5201", "mrqa_triviaqa-validation-6170"], "EFR": 1.0, "Overall": 0.7118032786885247}, {"timecode": 61, "before_eval_results": {"predictions": ["\"King of Cool\"", "Rhode Island", "Lake Placid, New York", "the Dominican Republic", "on the River North Esk in Midlothian, Scotland", "Bolshoi Theatre", "Franconia, New Hampshire", "Tahir \"Tie\" Domi", "Charice", "Ian Fleming", "17 December 1998", "30", "James William McCutcheon", "Nodar Kumaritashvili", "Aaron Lewis ( / ste\u026and / STAYND )", "the United States, its NATO allies and others )", "Havana Harbor", "`` Far Away '' by Jos\u00e9 Gonz\u00e1lez", "Help!", "British and French Canadian fur traders", "Georgia", "1997", "Michigan and surrounding states and provinces", "Mary Elizabeth Patterson", "Hank J. Deutschendorf II", "Catherine of Aragon", "Upstairs Downstairs", "Ruth Ellis", "Batman", "Turing", "Manifest Destiny", "Rio de Janeiro", "Gary Oldman", "federal district of Washington, D.C.", "britannica", "Rats", "apem The Borough (1810)", "Chinese President Hu Jintao", "ties", "the vicious brutality which accompanied the murders of his father and brother.\"", "telling CNN his comments had been taken out of context.", "opposition party members.", "Manmohan Singh's Congress party,", "southern port city of Karachi,", "could be secretly working on a nuclear weapon", "Ashley \"A.J.\" Jewell", "Seoul,", "South Africa", "10 percent", "Leah", "(John) Deere", "Sarai", "3y + 8", "Victor Hugo", "a pram", "(Jojo)", "Herman Wouk", "Bubba's Book Club", "Azeroth", "Jupiter", "an Extender", "between 1765 and 1783", "Tigris and Euphrates rivers", "Agra Cantonment - H. Nizamuddin Gatimaan Express"], "metric_results": {"EM": 0.5, "QA-F1": 0.6203896496016061}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.33333333333333337, 0.4, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 0.1739130434782609, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.1818181818181818]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-5875", "mrqa_hotpotqa-validation-3151", "mrqa_hotpotqa-validation-1083", "mrqa_naturalquestions-validation-1089", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-2870", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-5968", "mrqa_triviaqa-validation-2427", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-6870", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3944", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2375", "mrqa_searchqa-validation-1990", "mrqa_searchqa-validation-15104", "mrqa_searchqa-validation-6699", "mrqa_searchqa-validation-6028", "mrqa_searchqa-validation-4816", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-3667", "mrqa_searchqa-validation-14344", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-3416"], "SR": 0.5, "CSR": 0.5365423387096775, "EFR": 0.96875, "Overall": 0.7054334677419355}, {"timecode": 62, "before_eval_results": {"predictions": ["Ukraine", "Ethel Merman", "Poems : Series 1", "Julie Deborah Kavner", "Iraq", "1937", "French Canadian", "1997", "2014 Winter Olympics", "Guantanamo Bay Naval Base", "Thomas Mundy Peterson", "Walter Egan", "Alex Drake", "Sicily", "john Walsh", "100", "Ford Motor Company", "america", "someone named Godot", "Sony Interactive Entertainment", "hansa", "Gibraltar", "Mercury", "Isolde", "kenny Everett", "1936", "Tommy Cannon", "Charles Reed Bishop", "D\u00e2mbovi\u021ba", "400", "supernatural psychological horror", "Martin Truex Jr.", "848", "blue Ridge Parkway", "Blender (magazine)", "\"My Boss, My Hero\"", "Grave Digger", "an \"unnamed international terror group\"", "voluntary manslaughter", "AS", "Boys And Girls alone", "Mitt Romney", "CNN", "Iowa's critical presidential caucuses", "Yemen.", "dozens", "North Korea", "Democrats and Republicans", "Government Accountability Office", "John Glenn", "Bering Strait", "\"Ken doll\" Celso Santebanes", "\"House of Sand and Fog\"", "molasses", "Knocked Up", "upright", "Rhine", "turkey wing", "foot", "grain", "xenon", "Kentucky", "1892", "Nana Patekar"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7118229166666667}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.24000000000000002, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-7409", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-2900", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-7633", "mrqa_triviaqa-validation-4203", "mrqa_triviaqa-validation-7555", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6243", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-5174", "mrqa_newsqa-validation-781", "mrqa_newsqa-validation-3151", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-9465", "mrqa_hotpotqa-validation-4624"], "SR": 0.640625, "CSR": 0.5381944444444444, "EFR": 0.9130434782608695, "Overall": 0.6946225845410627}, {"timecode": 63, "before_eval_results": {"predictions": ["Saint Michael, Barbados", "Danish", "rickyard", "Richard Arthur", "Blue Grass Airport", "France", "1872", "Philip Livingston", "General Sir John Monash", "Margarine Unie", "July 8, 2014", "Dragons: Riders of Berk", "tony Alexander-Arnold", "Humpty Dumpty and Kitty Softpaws", "the grossly inadequate representation of Scheduled Castes, Scheduled Tribes and Other Backward Castes in employment and education due to historic, societal and cultural reasons", "erosion", "a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "the gender of the reigning monarch", "1923", "parashah ( or parshah / p\u0251\u02d0r\u0283\u0259 / or parsha )", "Clarence L. Tinker", "Daren Maxwell Kagasoff", "the federal government, the territories under its authority, and the provincial governments", "16", "Beorn", "shark", "columbia", "9", "Tax Day", "German state of North Rhine-Westphalia", "dungeon master", "katherine parr", "Patrick Troughton", "isosceles", "petticoat", "vienna", "umbrella", ".We were petitioned and have been looking into it for the past two years,\"", "heart", "NASCAR.", "the Defense of Marriage Act", "the immorality of these deviant young men", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "the Magneto to my Wolverine, the Saruman to my Frodo, the Dr. Octopus to my Spiderman.", "Between 1,000 and 2,000", "normal maritime traffic", "sculptures", "250,000", "the secret talks between the two leaders about shaping Brazilian foreign policy filtered down to Brazilian military officers by a \"Cabinet leak.\"", "the Five Orders of Architecture", "poppy", "(Shannon) Lucid", "Paganini", "The Untouchables", "Hinduism", "Maryland", "Dances With Balanchine", "buffa", "Ezra Pound", "Ponies", "Homicide: Life on the Street", "Baku", "About Eve", "fats"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5727095514779338}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.19999999999999998, 1.0, 0.8181818181818181, 0.8, 1.0, 0.0, 0.4615384615384615, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.8571428571428571, 0.0, 0.23076923076923078, 0.0, 0.0, 0.8, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1676", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-5242", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-4149", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-6092", "mrqa_naturalquestions-validation-1640", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-3819", "mrqa_triviaqa-validation-6810", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1273", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-12690", "mrqa_searchqa-validation-963", "mrqa_searchqa-validation-14562", "mrqa_searchqa-validation-8778", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-5762", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-5176"], "SR": 0.453125, "CSR": 0.536865234375, "retrieved_ids": ["mrqa_squad-train-63811", "mrqa_squad-train-57674", "mrqa_squad-train-20317", "mrqa_squad-train-59891", "mrqa_squad-train-10349", "mrqa_squad-train-33364", "mrqa_squad-train-66271", "mrqa_squad-train-56685", "mrqa_squad-train-26191", "mrqa_squad-train-68834", "mrqa_squad-train-34974", "mrqa_squad-train-53423", "mrqa_squad-train-76135", "mrqa_squad-train-58575", "mrqa_squad-train-83627", "mrqa_squad-train-34517", "mrqa_naturalquestions-validation-3648", "mrqa_newsqa-validation-2419", "mrqa_triviaqa-validation-3733", "mrqa_newsqa-validation-3364", "mrqa_triviaqa-validation-3128", "mrqa_squad-validation-6750", "mrqa_squad-validation-9592", "mrqa_searchqa-validation-8451", "mrqa_triviaqa-validation-1567", "mrqa_naturalquestions-validation-6708", "mrqa_newsqa-validation-1277", "mrqa_squad-validation-3879", "mrqa_newsqa-validation-2844", "mrqa_squad-validation-7300", "mrqa_searchqa-validation-5930", "mrqa_naturalquestions-validation-4711"], "EFR": 0.9714285714285714, "Overall": 0.7060337611607144}, {"timecode": 64, "before_eval_results": {"predictions": ["Scottish", "anne boleyn", "germany", "Wellington", "Batan", "tosca", "Denver", "Eva Marie", "apple", "Caractacus Potts", "Brazil", "raw hides", "County Cork", "Cheryl Campbell", "a crust of potatoes", "colonists of the Thirteen Colonies who rebelled against British control", "Duck", "Wednesday, September 21, 2016", "a mashed potato crust", "Matthew Gregory Wise", "Magnavox Odyssey", "only produced for export and is not sold in Germany", "Taron Egerton", "Mankombu Sambasivan Swaminathan", "Western Australia", "Oahu", "Kathleen O'Brien", "John John Florence", "Jean-Marie Pfaff", "John Lennon/Plastic Ono Band", "Konstant\u012bns Raudive", "1694", "Ben R. Guttery", "Manchester", "1999", "Warrington, Florida", "Christopher Lloyd Smalling", "About $10 billion", "28 years", "heavy brush", "Defense of Marriage", "19", "up three", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "45 minutes, five days a week", "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "The minister later apologized,", "Caylee Anthony's", "10 to 15 percent", "zero - Search-ID.com", "a claymore", "an elk", "Solidarity", "Christopher Columbus", "shalom", "Finnish", "a machinist", "Sartre", "a diamond", "Peter Shaffer", "a magnet", "Palatine Hill", "a cow", "a bassoon"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6657399891774891}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5454545454545454, 0.5, 0.8571428571428571, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-1055", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7504", "mrqa_triviaqa-validation-7570", "mrqa_naturalquestions-validation-10616", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-8298", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-4883", "mrqa_newsqa-validation-344", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-3561", "mrqa_searchqa-validation-4383", "mrqa_searchqa-validation-780", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-16119", "mrqa_searchqa-validation-13520", "mrqa_searchqa-validation-13757"], "SR": 0.515625, "CSR": 0.5365384615384615, "EFR": 1.0, "Overall": 0.7116826923076923}, {"timecode": 65, "before_eval_results": {"predictions": ["over 300,000", "DeWayne Warren", "Warren Hastings", "Colman", "Dougie MacLean", "Saint Alphonsa, F.C.C, ( born Anna Muttathupadathu ; 19 August 1910 -- 28 July 1946 )", "after winning in 1948, 1949, and 1960", "Carroll O'Connor", "the coasts of Australia, New Zealand, Tahiti, Hawaii, Senegal, Ghana, Nigeria and South Africa", "A simple majority", "Jenny Slate", "Pinar del R\u00edo Province ( now in Artemisa Province )", "It was held in England and Wales from 1 June to 18 June 2017", "cotswold", "Thelma Long", "butane pie tins", "Davis wing", "elytra", "Tara Lipinski", "cyclops", "John Constable", "russia", "Baroness Karren Brady", "dolphins", "scotland hanks", "filibuster and scathing rhetoric", "Stephen Ireland", "The United States presidential election of 2016", "ten", "8/7c", "d\u00edsabl\u00f3t", "The Indianapolis Times", "Sippin' on Some Syrup", "film playback singer, director, writer and producer", "27 January 1974", "1848", "Patton Oswalt", "Michael Krane,", "Jaime Andrade", "Larry Ellison", "teenage", "allergen-free", "breast cancer", "10,000", "3rd Platoon, A Company, 2nd Light armored Reconnaissance Battalion", "drug cartels", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "16th grand Slam title.", "Katherine", "Henry David Thoreau", "Woody Allen", "Afghanistan", "Panda", "Catherine the Great", "bass viol", "Jawaharlal Nehru", "isthmus", "Crunching the Numbers", "the Rights of Man", "Crone", "frequency", "Toni Morrison", "Type A personality", "V Vampire"], "metric_results": {"EM": 0.421875, "QA-F1": 0.49735863095238086}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.761904761904762, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-8944", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-4955", "mrqa_triviaqa-validation-6965", "mrqa_triviaqa-validation-5910", "mrqa_triviaqa-validation-4287", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-1666", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-5170", "mrqa_hotpotqa-validation-5564", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-367", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-2525", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-1154", "mrqa_searchqa-validation-11354", "mrqa_searchqa-validation-15729", "mrqa_searchqa-validation-14525", "mrqa_searchqa-validation-10278", "mrqa_searchqa-validation-5874", "mrqa_searchqa-validation-3848"], "SR": 0.421875, "CSR": 0.5348011363636364, "EFR": 0.972972972972973, "Overall": 0.7059298218673219}, {"timecode": 66, "before_eval_results": {"predictions": ["after being absent for a time, they were reintroduced to grocery stores under the Popsicle brand name", "1956", "in a counter clockwise direction around the Sun", "2018", "Olivia Olson", "1987", "humid subtropical climate, with hot summers and mild winters", "agriculture", "13 February", "eleven", "Janelle", "the commemoration of Jesus'birth", "Kyla Pratt", "Nile River", "anteater", "Emeril Lagasse", "five", "solitaire", "son of his father", "drake", "cuckoo", "the land between two rivers", "geoffrey smith smith holliday", "montparnesse", "france", "United States Marine Corps fighter ace during World War II", "Rhode Island School of Design", "Ector County", "Major League Soccer", "Gatwick Airport", "Antoinette \"Toni\" Childs\u2013G Garrett", "J. K. Rowling", "a personalized certificate", "Colin Vaines", "11 November 1821", "The Crowned Prince of the Philadelphia Mob/M Mafia", "Championnat National 3", "in the cellar with their mother, never seeing daylight.", "Tuesday afternoon.", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "Marie-Therese Walter", "Michael Jackson", "the reality he has seen is \"terrifying.\"", "Salt Lake City, Utah,", "his past and his future", "27-year-old's", "38", "Ameneh Bahrami's attacker", "Spc. Megan Lynn Touma,", "the ecliptic", "$800", "Genesis", "the proverbial forest", "True Grit", "Atlas", "Carthage", "( Otto) von Bismarck", "Brazil", "the Bad Girls", "Ralph Lauren", "Colorado", "was crying when she was talking about her daughters.", "old Hickory Steakhouse", "for flooding from Hurricane Irene that pummeled the East Coast last August and for damages from Tropical Storm Lee in Schoharie, Tioga, Broome, Greene, and Orange counties."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5402867965367966}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727272, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-5787", "mrqa_naturalquestions-validation-7227", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-1805", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-5010", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-4346", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-2289", "mrqa_hotpotqa-validation-1982", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-2820", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-242", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-906", "mrqa_newsqa-validation-1642", "mrqa_searchqa-validation-15537", "mrqa_searchqa-validation-14101", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-536", "mrqa_searchqa-validation-8381", "mrqa_searchqa-validation-4370", "mrqa_searchqa-validation-7242", "mrqa_searchqa-validation-9653", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-3369"], "SR": 0.4375, "CSR": 0.5333488805970149, "retrieved_ids": ["mrqa_squad-train-24240", "mrqa_squad-train-82403", "mrqa_squad-train-65159", "mrqa_squad-train-82302", "mrqa_squad-train-84406", "mrqa_squad-train-58773", "mrqa_squad-train-68795", "mrqa_squad-train-24820", "mrqa_squad-train-25405", "mrqa_squad-train-15106", "mrqa_squad-train-2667", "mrqa_squad-train-78967", "mrqa_squad-train-47709", "mrqa_squad-train-39640", "mrqa_squad-train-75476", "mrqa_squad-train-21364", "mrqa_hotpotqa-validation-5829", "mrqa_hotpotqa-validation-5052", "mrqa_triviaqa-validation-4466", "mrqa_searchqa-validation-11956", "mrqa_newsqa-validation-1545", "mrqa_naturalquestions-validation-4983", "mrqa_newsqa-validation-1159", "mrqa_hotpotqa-validation-147", "mrqa_newsqa-validation-840", "mrqa_triviaqa-validation-6810", "mrqa_naturalquestions-validation-975", "mrqa_hotpotqa-validation-4612", "mrqa_newsqa-validation-3343", "mrqa_naturalquestions-validation-3246", "mrqa_searchqa-validation-16682", "mrqa_hotpotqa-validation-2787"], "EFR": 1.0, "Overall": 0.711044776119403}, {"timecode": 67, "before_eval_results": {"predictions": ["low coercivity", "branch roots", "Iden Versio, leader of an Imperial Special Forces group known as Inferno Squad", "Glen W. Dickson", "James Corden", "often linked to high - ranking ( though not necessarily royalty ) in China", "four", "Hellenismos", "1878", "Randy Newman", "2007", "Pangaea", "John Dalton", "Peter Butterworth", "Orion", "James Callaghan", "germany", "Brazil", "Tomorrow Never Dies", "Bolivarian Republic of Venezuela", "magnetic force", "Madagascar", "radiotelephony Spelling Alphabet", "Bhutan", "cats", "motor ships", "Arlo Looking Cloud", "Francis Egerton", "Max Kellerman", "Rothschild", "10 June 1921", "Scotiabank Saddledome", "\"Slaughterhouse-Five\"", "Indian", "Plato", "1942", "National Lottery", "two remaining crew members", "a deceased organ donor,", "The Wall Street Journal Europe", "Alwin Landry's", "2,800", "Sgt. Barbara Jones", "Jund Ansar Allah", "\"Body Works\"", "Afghanistan,", "the body of the aircraft", "Chuck Bass", "between 1917 and 1924", "the Diamonds", "Julio", "chicken pot pie", "\"Sweet Home Alabama", "the black market", "the National Hockey League", "Tara Reid", "Y", "Mariska Hargitay", "Germany", "Roosevelt", "Moses", "Franz Kafka", "humus", "Dante's Inferio"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6870535714285714}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false], "QA-F1": [0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-7387", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-7177", "mrqa_triviaqa-validation-3393", "mrqa_triviaqa-validation-4237", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5604", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2732", "mrqa_newsqa-validation-3099", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-3197", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-13487", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-16220"], "SR": 0.640625, "CSR": 0.5349264705882353, "EFR": 1.0, "Overall": 0.7113602941176471}, {"timecode": 68, "before_eval_results": {"predictions": ["Fran\u00e7ois Hollande", "jonnie hemerstein", "japan", "UPS", "Kevin Painter", "east", "twenty-one", "guinea", "pantomime", "cubed", "lamb", "rosary", "s tracey paddison", "in the stems and roots of many plants, specifically in dicots such as buttercups and oak trees, and gymnosperms such as pine trees", "Coton in the Elms", "the government - owned Panama Canal Authority", "Sir Donald Bradman", "Killer Within", "The Vamps", "Profit maximization", "Ishaani Ishaan Sinha", "Mockingjay -- Part 1 ( 2014 )", "Vasoepididymostomy", "John Goodman", "Eddie Murphy", "Morocco", "Golden State Warriors", "Kurt Vonnegut", "political party", "Comodoro Arturo Merino Ben\u00edtez International Airport", "Mike Mills", "1620", "250 million", "Terry the Tomboy", "Umina Beach", "Taeko Ikeda", "Chevy Motor Car Company", "bartering", "General Motors'", "The Kirchners", "undergoing a double mastectomy and reconstructive surgery,", "Mugabe", "Val d'Isere, France", "free laundry service.", "could develop a", "33-year-old", "pulled the trigger", "federal ocean planning.", "a one-shot victory in the Bob Hope Classic", "daisy bacall", "Katharine McPhee", "the Eiffel tower", "a brick", "(Casey) Stengel", "Hodgkin\\'s lymphoma", "Marcia Clark", "cement", "Madden", "foolish", "hubris", "Colombia", "Tuesday", "1967", "\"Canadien(ne)s fran\u00e7ais(es)\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.604595924908425}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4159", "mrqa_triviaqa-validation-2488", "mrqa_triviaqa-validation-2237", "mrqa_triviaqa-validation-1044", "mrqa_triviaqa-validation-1332", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-2441", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-986", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-1427", "mrqa_hotpotqa-validation-3156", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-571", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1593", "mrqa_newsqa-validation-656", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-2858", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-14255", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-13586", "mrqa_searchqa-validation-7532", "mrqa_searchqa-validation-7179", "mrqa_hotpotqa-validation-179"], "SR": 0.515625, "CSR": 0.5346467391304348, "EFR": 0.967741935483871, "Overall": 0.7048527349228612}, {"timecode": 69, "before_eval_results": {"predictions": ["the Black Russian", "the Sarajevo Haggadah", "Honolulu", "(Thomas) Paine", "the red heron", "Universal Studios", "coal", "a (spinning) top", "spelunking", "apples", "Finding Nemo", "a catalog", "the Mariachi", "in 1837", "in 2000s", "Australia", "Brooklyn, New York", "Achal Kumar Jyoti", "dorsally on the forearm", "Darren McGavin", "A rotation", "Speaker of the House of Representatives", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "energy loss", "October 29, 2015", "Ellesmere Port", "\"Sugar Baby Love\"", "stephennie wahlberg", "Wisconsin", "stephen fry", "Richard Strauss", "islands in the south east", "36", "1971", "French", "buddhists", "king Edward III", "Four Weddings and a Funeral", "Los Angeles", "Karl Johan Schuster", "elderships", "The Goddess of 1967", "The Division of Cook", "its eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "1894", "Walt Disney Productions", "KlingStubbins", "Bambi: Eine Lebensgeschichte aus dem Walde", "Australian", "Arsene Wenger", "two women", "two years,", "a cause of death,", "cars", "a series of wildfires", "Jeddah, Saudi Arabia,", "$50", "root out terrorists within its borders.", "protective shoes", "J.G. Ballard,", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "Jerry Lee Lewis", "April 10", "Terry Reid"], "metric_results": {"EM": 0.625, "QA-F1": 0.6876619816586922}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-12214", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-3440", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-5706", "mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-5958", "mrqa_triviaqa-validation-4594", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-4257", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-444", "mrqa_naturalquestions-validation-10037", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-522"], "SR": 0.625, "CSR": 0.5359375, "retrieved_ids": ["mrqa_squad-train-80906", "mrqa_squad-train-22320", "mrqa_squad-train-70707", "mrqa_squad-train-3655", "mrqa_squad-train-26225", "mrqa_squad-train-75845", "mrqa_squad-train-47220", "mrqa_squad-train-30983", "mrqa_squad-train-14174", "mrqa_squad-train-60863", "mrqa_squad-train-2549", "mrqa_squad-train-42354", "mrqa_squad-train-84102", "mrqa_squad-train-20153", "mrqa_squad-train-51145", "mrqa_squad-train-59834", "mrqa_newsqa-validation-3733", "mrqa_hotpotqa-validation-4510", "mrqa_squad-validation-5620", "mrqa_naturalquestions-validation-7211", "mrqa_triviaqa-validation-1066", "mrqa_searchqa-validation-15236", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-15806", "mrqa_naturalquestions-validation-9895", "mrqa_squad-validation-4677", "mrqa_searchqa-validation-3069", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-1508", "mrqa_naturalquestions-validation-1039", "mrqa_searchqa-validation-4712", "mrqa_naturalquestions-validation-4609"], "EFR": 1.0, "Overall": 0.7115625000000001}, {"timecode": 70, "UKR": 0.77734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1562", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2116", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3826", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5222", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5740", "mrqa_hotpotqa-validation-5742", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-733", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-6382", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7268", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-800", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-10179", "mrqa_searchqa-validation-10216", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11663", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15232", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7236", "mrqa_searchqa-validation-7369", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7672", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10102", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1794", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2133", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2819", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4834", "mrqa_squad-validation-4840", "mrqa_squad-validation-5197", "mrqa_squad-validation-5410", "mrqa_squad-validation-551", "mrqa_squad-validation-5592", "mrqa_squad-validation-5721", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6471", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6692", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-6988", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7751", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8042", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8575", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8767", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9732", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2986", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3044", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4448", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5114", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5691", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6053", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-613", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.849609375, "KG": 0.5015625, "before_eval_results": {"predictions": ["Genesis", "Quebec", "gravity", "Roger Federer", "Roald Dahl", "Legally Blonde", "Voyager 1", "a giant Peach", "the Naval Academy", "the Renaissance", "Halle Berry", "a prism schism", "the Society of Jesus", "a premalignant flat ( or sessile ) lesion", "John Joseph Patrick Ryan", "Jason Lee", "February 14, 2015", "the homicidal thoughts of a troubled youth", "Hodel", "31 December 1947", "supervillains who pose catastrophic challenges to the world", "Alamodome and city of San Antonio", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher", "Gupta Empire", "caused by chlorine and bromine from manmade organohalogens", "stephen VI", "liriope", "\"Upside Down\"", "wED/RLS", "Hindenburg", "Sicily", "dinar", "Henley-on-Thames", "6", "way Susan", "hand gun", "pickwick", "Sun Belt Conference", "47", "1 December 1948", "Manchester, England", "Ronnie Schell", "organist", "Loch Moidart", "Motorised quadricycle", "the field of science", "PlayStation 4", "small family car", "Elton John", "Carol Browner", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "Jimi Hendrix and Janis Joplin,", "one", "22", "August 19, 2007.", "Russia", "southwestern Mexico,", "Nigeria,", "an animal tranquilizer,", "Barnes & Noble", "American soldiers held as slaves by Nazi Germany", "illegal immigrants", "75", "Angola"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6496932435254804}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.21052631578947367, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8708", "mrqa_searchqa-validation-14312", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-8777", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-654", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-2782", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2342", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-2558", "mrqa_hotpotqa-validation-4322", "mrqa_hotpotqa-validation-4573", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1959", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-3610"], "SR": 0.53125, "CSR": 0.5358714788732395, "EFR": 0.9666666666666667, "Overall": 0.7262107541079812}, {"timecode": 71, "before_eval_results": {"predictions": ["133", "him to step down as majority leader.", "\"If a retailer has not gotten involved with fight over this bill, he \"should be shot,\"", "July", "off the coast of Dubai", "45 minutes, five days a week.", "Monica Majumdar", "This will be the second", "against ethnic Somalis by rebels and Ethiopian troops are rampant.", "Asashoryu", "Mohamed Alanssi,", "a national telephone survey", "Australian Open,", "Charles Dickens's novel Oliver Twist", "snoods ( or gangions )", "eight", "2009", "The Winds of Winter", "September of that year", "Etienne de Mestre", "the American League ( AL ) champion Cleveland Indians", "North Atlantic Ocean", "September 1959", "a young girl", "May 2016", "stained glass", "axe", "whist", "chilies", "eros", "2240", "Naomi Watts", "Patrick McGoohan", "the Passage of the Red Sea", "gretzable", "Arctic Monkeys", "Peter Sellers", "Montagues and Capulets", "its variety of shops ranging from upscale boutiques to national and international chain store outlets", "Argentine", "United States Auto Club", "Australian", "44", "LA Galaxy", "World Championship Wrestling", "Apatosaurus", "the Emancipation Proclamation", "blood", "June 1975", "Florence Nightingale", "deuterium", "Oregon", "asteroids", "Artesian", "The Silence of the Lambs", "Earth shoes", "Dizzy", "Louise", "Virginia", "the Marathon", "Cairo", "H CO", "Naomi", "the North Korean won"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6546672077922078}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.8, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 0.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3192", "mrqa_newsqa-validation-2809", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2466", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6987", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-1732", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-1001", "mrqa_searchqa-validation-11421", "mrqa_searchqa-validation-15955", "mrqa_searchqa-validation-16961", "mrqa_searchqa-validation-4873", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-9260"], "SR": 0.5625, "CSR": 0.5362413194444444, "EFR": 0.9285714285714286, "Overall": 0.7186656746031745}, {"timecode": 72, "before_eval_results": {"predictions": ["Max Martin and Shellback", "the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "1976", "head of the Cabinet of Bluhme I", "Logar Province", "The Kennedy Center", "Kansas", "Marshal of France", "Wiltshire", "Austrian Landwehr", "Glendale", "Captain", "Adam Karpel", "Barnabas", "Karen Gillan", "775 rooms", "the geologist James Hutton", "Cheap trick", "Experimental neuropsychology", "card verification data ( CVD )", "Katharine Hepburn -- Ethel Thayer", "2 %", "zinc silicate primer and vinyl topcoats", "Watson and Crick", "historical fiction", "Kyle Lafferty", "spiral", "apple", "red ball", "zanzibar", "cork City", "garet court", "henna", "Bugsy Malone", "will Smith", "robinson crusoe", "1919", "(Rambosk)", "misdemeanor assault charges", "Brazil", "can play an important role in Afghanistan as a reliable NATO ally.", "\"He is obviously very relieved and grateful that the pardon was granted,\"", "one Iraqi soldier,", "12-1", "Juan Martin Del Potro.", "AS", "Camorra", "1969", "London", "the Blackberry", "a skunk", "\"Ricochet\"", "John Lennon", "Homer", "an owl", "the Panama Canal", "the San Simeon Creek Campground", "tulle", "the saber-tooth cat", "Wall Street", "the Kensington Palace", "the previous week", "the British", "the placing of repentance ashes on the foreheads of participants"], "metric_results": {"EM": 0.390625, "QA-F1": 0.507714420995671}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 0.18181818181818182, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.6666666666666666, 0.09523809523809523, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8333333333333333, 0.3, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-2544", "mrqa_hotpotqa-validation-4569", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-1365", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-2902", "mrqa_naturalquestions-validation-2583", "mrqa_naturalquestions-validation-9572", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-875", "mrqa_naturalquestions-validation-1617", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-7134", "mrqa_triviaqa-validation-4235", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-6859", "mrqa_triviaqa-validation-3238", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-4812", "mrqa_triviaqa-validation-1939", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-703", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-3666", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-11097", "mrqa_searchqa-validation-1927", "mrqa_searchqa-validation-8587", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-259"], "SR": 0.390625, "CSR": 0.5342465753424658, "retrieved_ids": ["mrqa_squad-train-15324", "mrqa_squad-train-19823", "mrqa_squad-train-56004", "mrqa_squad-train-15558", "mrqa_squad-train-7545", "mrqa_squad-train-7412", "mrqa_squad-train-39447", "mrqa_squad-train-41147", "mrqa_squad-train-79212", "mrqa_squad-train-7756", "mrqa_squad-train-73019", "mrqa_squad-train-54361", "mrqa_squad-train-18282", "mrqa_squad-train-44770", "mrqa_squad-train-26713", "mrqa_squad-train-80793", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-1119", "mrqa_searchqa-validation-6939", "mrqa_triviaqa-validation-6044", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-2143", "mrqa_triviaqa-validation-1480", "mrqa_naturalquestions-validation-9530", "mrqa_hotpotqa-validation-5180", "mrqa_naturalquestions-validation-4981", "mrqa_newsqa-validation-2907", "mrqa_triviaqa-validation-7082", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-436", "mrqa_triviaqa-validation-3137", "mrqa_searchqa-validation-5787"], "EFR": 0.9743589743589743, "Overall": 0.727424234940288}, {"timecode": 73, "before_eval_results": {"predictions": ["The Virgin Queen, Gloriana or Good Queen Bess", "C. W. Grafton", "4,972", "Hugh Hefner", "July 8, 2014", "Julia Kathleen McKenzie", "sarod", "Asif Kapadia", "\"Two Is Better Than One\"", "26\u201330 August 1914", "Jack St. Clair Kilby", "the Saddledome", "Balvenie Castle", "the Bulgarian commander - in - chief", "pigs", "The ulnar collateral ligament of elbow joint", "an outlaw motorcycle club", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Carlos Alan Autry Jr.", "Nick Kroll", "New York University", "relieves the driving motor from the load of holding the elevator cab", "Ravi River", "James Madison", "at the hour of death or in the presence of the dying", "26 miles and 385 yards", "david bowie", "Robert De Niro", "de Havilland moth", "Kiri Te Kanawa", "Hokkaido", "The Shard", "darth viii", "japan", "stasis", "Jaguar Land Rover", "3DS", "Silvio Berlusconi.", "Mumbai", "the United States", "a level of autonomy that will allow them to protect and preserve their culture, religion and national identity.", "Isabella", "Monday and Tuesday", "15-year-old's", "two", "Lana Clarkson", "customers are lining up for vitamin injections that promise", "Donald Trump.", "Mugabe's opponents", "Dolphins", "Zombies", "Oprah Winfrey", "St. Nicholas' Day", "India", "Three Is a Magic Number", "cable cars", "the Framers", "The Usual Suspects", "the Process Art", "World War I", "the Nominative Case", "(Ken) Russell", "Sadat", "the Puget Sound Express"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5756076388888889}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.25, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3357", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-3902", "mrqa_hotpotqa-validation-5399", "mrqa_naturalquestions-validation-2819", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-2730", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-1421", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-5215", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-2245", "mrqa_newsqa-validation-625", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-1587", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14049", "mrqa_searchqa-validation-8984", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-11000", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-7619", "mrqa_searchqa-validation-11444"], "SR": 0.484375, "CSR": 0.5335726351351351, "EFR": 1.0, "Overall": 0.732417652027027}, {"timecode": 74, "before_eval_results": {"predictions": ["yellow fever", "6,012,331", "Nickelodeon", "848", "Alabama", "Cushman", "Mark Neveldine and Brian Taylor", "Harold Edward Holt", "Michael Crawford", "Fort Valley, Georgia", "Netflix", "Edward R. Murrow", "Chiltern", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Titus to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority '' ( Titus 2 : 15 )", "Bill Broussard in JFK ( 1991 ), Hal Tucker in Cliffhanger ( 1993 ), Jared Svenning in Mallrats ( 1995 )", "Patris", "Roman Reigns", "May 2010", "supported modern programming practices and enabled business applications to be developed with Flash", "South Asia", "Rajendra Prasad", "Ariana Clarice Richards", "Yahya Khan", "Colon Street", "h Hannibal", "united states", "dadaism", "peterloo massacre", "petain", "robinsons", "gretzky", "The IT Crowd", "Jim Peters", "sewing machines", "small faces", "Mike Tyson", "New York City Mayor Michael Bloomberg", "Ross Perot.", "depression", "the Employee Free Choice act", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues.", "suppress the memories and to live as normal a life as possible;", "southern port city of Karachi,", "Britain's Prime Minister Gordon Brown, France's President Nicolas Sarkozy", "designer", "Chandni Chowk", "16", "Steven Gerrard", "Steller's sea cows", "Buffalo", "Peter Sellers", "Herod", "American Samoa", "Chuck Berry", "Students for a Democratic Society", "petits fours", "Smallville", "Mike Nichols", "a volcano", "15", "ophthalmologist", "wagen", "benevento"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6433779761904762}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.4, 1.0, 0.06666666666666667, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-2104", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-4089", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-8702", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-6718", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-3882", "mrqa_newsqa-validation-2212", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-1877", "mrqa_searchqa-validation-773", "mrqa_searchqa-validation-12306", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-6581"], "SR": 0.59375, "CSR": 0.534375, "EFR": 1.0, "Overall": 0.732578125}, {"timecode": 75, "before_eval_results": {"predictions": ["Bolognese", "a watch", "a Raspberry", "14", "veal sweetbreads", "a jump ball", "Gerard Mercator", "not", "a prey drive", "natural selection", "The Communist Party", "Heinrich Heine", "University of Exeter", "the New York Yankees", "C\u03bc and C\u03b4", "he has been back this whole time searching for whatever force brought him back and hunting with his mother's side of the family, the Campbells, led by their grandfather Samuel", "orogenic belt", "butane", "James Hutton", "The Pittsburgh Steelers", "Ming dynasty", "20 March 2011", "Daniel A. Dailey", "The Maginot Line", "eurozone", "Cato", "henchman", "carthaginian", "Andr\u00e9s Iniesta", "Joy Division", "Tony Manero", "japan", "max Immelmann", "hawkeye", "9 (b) 10 (c) 11 (d) 12", "Dublin", "Barack Obama", "Major League Soccer", "$7.3 billion", "Sir Michael Kemp Tippett", "King R\u00e6dwald of East Anglia", "Art Bell", "Valhalla Highlands Historic District", "late 19th and early 20th centuries", "37", "Central Avenue", "Reese Witherspoon", "2015 Baylor Bears football team", "Alistair Grant", "light snow or flurries", "Saturday", "his native Philippines", "renew registration until the manufacturer's fix has been made.", "1975", "97-year-old", "raping and killing a 14-year-old Iraqi girl.", "Workers'", "two", "glamour and hedonism", "246", "Eleven people", "wooden top", "henley regatta", "Otto von Bismarck"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5889818948412698}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.1142857142857143, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.375, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5771", "mrqa_searchqa-validation-2976", "mrqa_searchqa-validation-4441", "mrqa_searchqa-validation-11658", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-665", "mrqa_searchqa-validation-10897", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-250", "mrqa_naturalquestions-validation-1162", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-3604", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-497", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-2269", "mrqa_hotpotqa-validation-2538", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-2357", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-334", "mrqa_triviaqa-validation-5331"], "SR": 0.515625, "CSR": 0.5341282894736843, "retrieved_ids": ["mrqa_squad-train-76235", "mrqa_squad-train-14671", "mrqa_squad-train-16549", "mrqa_squad-train-6937", "mrqa_squad-train-48395", "mrqa_squad-train-26152", "mrqa_squad-train-13381", "mrqa_squad-train-23790", "mrqa_squad-train-38067", "mrqa_squad-train-24603", "mrqa_squad-train-11834", "mrqa_squad-train-33272", "mrqa_squad-train-14581", "mrqa_squad-train-24734", "mrqa_squad-train-53545", "mrqa_squad-train-26959", "mrqa_newsqa-validation-4203", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-3231", "mrqa_hotpotqa-validation-4995", "mrqa_searchqa-validation-1640", "mrqa_triviaqa-validation-3282", "mrqa_hotpotqa-validation-3059", "mrqa_triviaqa-validation-5362", "mrqa_hotpotqa-validation-5180", "mrqa_squad-validation-6282", "mrqa_newsqa-validation-2609", "mrqa_newsqa-validation-1731", "mrqa_searchqa-validation-7619", "mrqa_triviaqa-validation-4525", "mrqa_searchqa-validation-8708"], "EFR": 0.967741935483871, "Overall": 0.726077169991511}, {"timecode": 76, "before_eval_results": {"predictions": ["John Rockwell", "Tempo", "Kensan-Devan Wildlife Sanctuary", "Francis Nethersole", "Bill Walton", "La vendedora de rosas", "Finding Nemo", "Wu-Tang Clan", "Debbie Reynolds", "Mark \"Chopper\" Read", "north-east Lithuania", "Aly Raisman", "Harry Booth", "to manage the characteristics of the beer's head", "In Time", "in the mid - to late 1920s", "Cee - Lo", "Vicente Fox", "Nebuchadnezzar", "Renhe Sports Management Ltd", "often linked to high - ranking ( though not necessarily royalty ) in China", "rubidium - 85", "in 1967, Celtic became the first British team to win the competition", "forested parts of the world", "The Mandate of Heaven", "My Fair Lady", "Basketball", "areolar connective tissue", "gagapedia", "peacock", "Antarctica", "farchamp", "at Oxford Circus", "Laura Solon", "j Jim Jones", "pini di Roma", "celine Dion", "almost 100", "one", "An", "Peshawar", "millionaire's surtax,", "Muslims", "No 4,", "Matthew Fisher,", "Laura Ling and Euna Lee", "snow,", "Basel", "he knew the owner of the home,", "kryptonite", "The Sixth Sense", "Mao", "Confeitaria Colombo", "the Pierian Spring", "fracture", "Andy Warhol", "Rocky", "a \"rigid\" constitution", "the Vietnam War", "The Thorn Birds", "frostbite", "he hosted a short - lived talk show in WCW called A Flair for the Gold", "to examine trends in global carbon monoxide and inhalol pollution", "Pir Panjal Range"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6148476797325482}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7692307692307692, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.7272727272727273, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.8, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.10526315789473685, 0.6]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-1196", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-9092", "mrqa_naturalquestions-validation-1705", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-7377", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5068", "mrqa_triviaqa-validation-3615", "mrqa_triviaqa-validation-5589", "mrqa_triviaqa-validation-1368", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2839", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4264", "mrqa_searchqa-validation-12071", "mrqa_searchqa-validation-14003", "mrqa_naturalquestions-validation-9003", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-1848"], "SR": 0.515625, "CSR": 0.533887987012987, "EFR": 0.9354838709677419, "Overall": 0.7195774965961458}, {"timecode": 77, "before_eval_results": {"predictions": ["Morgan", "Mahalia Jackson", "WD-40", "a Bunsen burner", "shortened", "Cuba", "vanilla", "Gatsby", "Mozzarella", "the Black Sea", "prime time", "New Mexico", "the Black sheep", "Michael Rosen", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "an adopted daughter of Thanos", "Sir Hugh Beaver", "king Gautamiputra Satakarni", "Francisco Pizarro", "September 27, 2017", "Real Madrid", "Paradise, Nevada", "the official flag of Hungary", "1857", "Richard Masur", "banjo", "Philippines", "wrong throat", "Volcanism", "south africa", "Moaning Myrtle", "billowing skirt", "the Holy Roman Empire", "forage", "Pierre Laval", "phone", "jim smith", "1935", "Premier League club Manchester City", "Joseph Cheshire Cotten", "Washington metropolitan area", "Christopher Tin", "cruiserweight", "the reigning monarch of the United Kingdom", "The Ethics of Ambiguity", "Kaep", "Luke Bryan", "Francisco Rafael Arellano F\u00e9lix", "Dolly Records", "allegedly involved in forged credit cards and identity theft led authorities to a $13 million global crime ring,", "Italy Trembles.", "Michael Schumacher", "South Africa's", "former Alabama judge is standing trial on charges he checked male inmates out of jail and forced them to engage in sexual activity such as paddling in exchange for leniency.", "\"in the interest of justice.\"", "reached an agreement late Thursday.", "Bill Stanton", "Natalie Cole", "11,", "\"the strawberry,\"", "public opinion.", "San Jose, California", "Audrey II", "the Iraq War"], "metric_results": {"EM": 0.515625, "QA-F1": 0.604513770221007}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7878787878787877, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.1142857142857143, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1816", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-9638", "mrqa_searchqa-validation-5936", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-346", "mrqa_triviaqa-validation-2316", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-1851", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-5438", "mrqa_hotpotqa-validation-1623", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-1320", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-4445", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-627", "mrqa_newsqa-validation-3597", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-3833", "mrqa_naturalquestions-validation-878", "mrqa_naturalquestions-validation-5583"], "SR": 0.515625, "CSR": 0.5336538461538461, "EFR": 0.9032258064516129, "Overall": 0.7130790555210917}, {"timecode": 78, "before_eval_results": {"predictions": ["Copenhagen", "BraveStarr", "The 7 Habits of Highly Effective Families", "Picric acid", "Quentin Coldwater", "1940s and 1950s", "Brickyard", "Hong Kong", "Rabies", "M. Night Shyamalan", "You Can Be a Star", "1996 NBA Slam Dunk Contest", "Currer Bell", "the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere )", "the libretto", "Chuck Noland", "King Willem - Alexander", "an exultation of spirit", "the senior-most judge of the supreme court", "Mohammad Reza Pahlavi", "Sesel Zvidzai", "Bonnie Aarons", "the right side of the heart", "1984", "home on Chesapeake Bay, south of Annapolis in Maryland", "bear", "united republic of Tanzania", "blue", "henry kramer", "Turin", "taka", "germanotta", "Kenya", "silks", "bear", "Colossus of Rhodes", "albania", "Old Trafford", "The station", "President Robert Mugabe's", "Malawi.", "Les Bleus", "Canada.", "closed on 366 for eight wickets on the opening day.", "removal of his diamond-studded teeth.", "Kurt Cobain's", "Afghanistan,", "\"Body Works\"", "businessman", "thirteen", "English", "the \"Axis of Evil\"", "Spanish", "Peter Shaffer", "twin-lens reflex", "Madagascar", "the Count of Monte Cristo", "Pablo Picasso", "General David H. Petraeus", "Dreamgirls", "the Lincoln Memorial", "powerful anesthetic and sedative.", "the release of the four men", "Michael Schumacher"], "metric_results": {"EM": 0.671875, "QA-F1": 0.735546875}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.8333333333333334, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8000000000000002, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-1032", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4029", "mrqa_naturalquestions-validation-6851", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-1218", "mrqa_searchqa-validation-674", "mrqa_searchqa-validation-3510", "mrqa_searchqa-validation-9620", "mrqa_searchqa-validation-12666"], "SR": 0.671875, "CSR": 0.5354034810126582, "retrieved_ids": ["mrqa_squad-train-81920", "mrqa_squad-train-52441", "mrqa_squad-train-11406", "mrqa_squad-train-28845", "mrqa_squad-train-56581", "mrqa_squad-train-67551", "mrqa_squad-train-44154", "mrqa_squad-train-16128", "mrqa_squad-train-80280", "mrqa_squad-train-75719", "mrqa_squad-train-63246", "mrqa_squad-train-69470", "mrqa_squad-train-13133", "mrqa_squad-train-81067", "mrqa_squad-train-41864", "mrqa_squad-train-51619", "mrqa_naturalquestions-validation-4071", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-2353", "mrqa_naturalquestions-validation-5034", "mrqa_triviaqa-validation-7264", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3082", "mrqa_triviaqa-validation-6112", "mrqa_hotpotqa-validation-3971", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-476", "mrqa_triviaqa-validation-2396", "mrqa_newsqa-validation-2620", "mrqa_searchqa-validation-9465", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-13057"], "EFR": 1.0, "Overall": 0.7327838212025316}, {"timecode": 79, "before_eval_results": {"predictions": ["Theodore Robert Cowell", "2017", "David Naughton", "June 1975", "September 26, 2010", "The Prodigy", "Gal\u00e1pagos Islands", "119", "son of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "James Weldon Johnson", "Manhattan Project", "Green Chair", "17", "2013", "1961", "Andy Serkis", "January 1, 1976", "Alex Ryan", "Massachusetts", "neuropsychology", "10 May 1940", "a foul - tempered monarch whom Carroll himself describes as `` a blind fury '', and who is quick to give death sentences at the slightest offense", "in the Washington metropolitan area", "2017", "U.S. service members who have died without their remains being identified", "frank burns", "six-pocket", "david mitchell", "Big Ears", "Hawaii", "Velazquez", "the Forum", "10", "magic", "giannina arangi-Lombardi", "24", "marquess of Burlington", "The Rosie Show", "Trevor Rees-Jones,", "a remote part of northwestern Montana", "15-year-old's", "a pregnancy", "reached an agreement late Thursday to form a government of national reconciliation.", "a remedy to unemployment among veterans.", "a former captor.", "30", "open heart surgery,", "President Bush", "Chinese", "the Capitol", "Gobblers's Knob", "Madonna", "the Prius", "Anja Prson", "the Cannoli", "Los Angeles", "William Henry Harrison", "Charlie Sheen", "Dante", "Jeannette Rankin", "Roosevelt", "Trinity", "peppers", "Harvard"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6507097630718954}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-1148", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-2588", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-2448", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-1573", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-5695", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-2546", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-6746", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-9856", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-15319", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-510"], "SR": 0.578125, "CSR": 0.5359375, "EFR": 0.9629629629629629, "Overall": 0.7254832175925926}, {"timecode": 80, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2645", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3896", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5100", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5201", "mrqa_hotpotqa-validation-5225", "mrqa_hotpotqa-validation-5253", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4200", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-7846", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1834", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3043", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3190", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10138", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11668", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11892", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12153", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-12874", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14914", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15294", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1814", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4537", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-553", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6391", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-674", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7645", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-3590", "mrqa_squad-validation-3628", "mrqa_squad-validation-4127", "mrqa_squad-validation-4192", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4698", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-6916", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7565", "mrqa_squad-validation-7707", "mrqa_squad-validation-7813", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9103", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1472", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-333", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4488", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5330", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7102", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-820", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-898"], "OKR": 0.849609375, "KG": 0.4890625, "before_eval_results": {"predictions": ["in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "in a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "the \u01c0Xam people", "India", "$19.8 trillion", "Tigris and Euphrates rivers", "December 1972", "under the age of 18", "Scar's henchmen", "Theodore Roosevelt, Robert M. La Follette, Sr., and Charles Evans Hughes on the Republican side, and William Jennings Bryan, Woodrow Wilson and Al Smith on the Democratic side", "Howard Caine", "Buddhist", "Roman Reigns", "Jack Daniels whiskey", "Joan Rivers", "fish", "motorway", "the South Saskatchewan River", "plac\u0113b\u014d", "three", "germaniner", "Hispaniola", "flowers", "balustrade", "tartan", "University of Georgia", "the Qin dynasty", "CBS", "South America", "Milwaukee Bucks", "Prince Ioann Konstantinovich", "Selinsgrove", "The Tempest", "Los Angeles", "Pacific Place", "Franconia, New Hampshire", "compact car (North America), or small family car", "\"We are a nation of Christians and Muslims, Jews and Hindus, and nonbelievers.\"", "Sri Lanka's", "a motor scooter", "United Arab Emirates", "Derek Mears", "President Obama's surge plan", "Hamas", "Sea World in San Antonio,", "four military officers", "Six members of Zoe's Ark", "More than 15,000", "\"Quiet Nights,\"", "Col. Eli Lilly", "Austen", "the zebra", "the Charleston", "the wishbone", "the asteroid", "The Prince and the Pauper", "taxicab", "Toby Keith", "the Systeme International d'Unites", "Oahu", "Thor Heyerdahl", "Lesser Island", "Calcium carbonate", "nuts are related to the cashew and naturally feature a green tint. These nuts have been cultivated and consumed for thousands of years."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6361088564213564}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false], "QA-F1": [0.0, 0.9523809523809523, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0909090909090909]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-5808", "mrqa_triviaqa-validation-1408", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-4205", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-5714", "mrqa_hotpotqa-validation-3993", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-916", "mrqa_searchqa-validation-9215", "mrqa_searchqa-validation-1158", "mrqa_searchqa-validation-16239", "mrqa_searchqa-validation-5623", "mrqa_searchqa-validation-12933", "mrqa_searchqa-validation-143", "mrqa_triviaqa-validation-598", "mrqa_triviaqa-validation-1488"], "SR": 0.515625, "CSR": 0.5356867283950617, "EFR": 0.967741935483871, "Overall": 0.7176388577757866}, {"timecode": 81, "before_eval_results": {"predictions": ["noddy", "apprentice", "egremont", "titan", "noddy", "fungi", "australia", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "Pontiac Silverdome", "hogmanay", "acetone", "virgil", "The Battle of the Three Emperors", "a very long forward pass in American football, made in desperation, with only a small chance of success and time running out on the clock", "when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "Bob Gaudio and his future wife Judy Parker", "ideology", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "the sudden appearance of a worldwide storm", "Rent's script", "1773", "the Detroit Tigers", "Meri", "a role in synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "before 1986", "Telugu", "Paris", "Tampa Bay Lightning", "Melville", "11", "Argentine cuisine", "Comodoro Arturo Merino Ben\u00edtez International Airport", "Ready Player One", "\"Sonic\" Smith", "\"City of Ghosts\"", "near ovulation", "331", "two-state solution", "12 hours", "allegations that a dorm parent mistreated students at the school.", "any abuse that occurred in his diocese.", "bring precision marches, the somber tones of taps and the nerve-rattling three-gun salutes.", "Revolutionary Armed Forces of Colombia,", "two", "southern Bangladesh,", "a violent government crackdown seeped out.\"", "Las Vegas.\"", "education", "Rod Blagojevich,", "aimeter", "Versace", "\"the Doctor\"", "Wings", "Ivica Zubac", "salmon", "a gate", "Canada", "monkeys", "the Monsoon", "Marie Antoinette", "Google", "Montreal", "left - sided heart failure", "the brain and spinal cord"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5702120217715085}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08108108108108107, 0.967741935483871, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.9375, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.16666666666666666, 0.17391304347826086, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-2592", "mrqa_triviaqa-validation-3691", "mrqa_triviaqa-validation-4808", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-1630", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-10576", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-9487", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4305", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-4598", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-3629", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-12769", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-8219", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-7342"], "SR": 0.484375, "CSR": 0.5350609756097561, "retrieved_ids": ["mrqa_squad-train-57532", "mrqa_squad-train-15788", "mrqa_squad-train-33596", "mrqa_squad-train-57006", "mrqa_squad-train-82329", "mrqa_squad-train-73958", "mrqa_squad-train-17514", "mrqa_squad-train-53150", "mrqa_squad-train-51154", "mrqa_squad-train-80574", "mrqa_squad-train-25339", "mrqa_squad-train-82610", "mrqa_squad-train-69071", "mrqa_squad-train-23155", "mrqa_squad-train-41929", "mrqa_squad-train-71624", "mrqa_newsqa-validation-1432", "mrqa_triviaqa-validation-7549", "mrqa_hotpotqa-validation-3228", "mrqa_newsqa-validation-3833", "mrqa_naturalquestions-validation-4829", "mrqa_newsqa-validation-2278", "mrqa_naturalquestions-validation-1277", "mrqa_squad-validation-2705", "mrqa_triviaqa-validation-117", "mrqa_hotpotqa-validation-2919", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-2538", "mrqa_searchqa-validation-8778", "mrqa_squad-validation-249", "mrqa_hotpotqa-validation-871", "mrqa_triviaqa-validation-549"], "EFR": 0.9696969696969697, "Overall": 0.7179047140613453}, {"timecode": 82, "before_eval_results": {"predictions": ["a heart", "Space Cadet", "Excalibur", "Judy Garland", "Miranda", "Absinthe", "time", "Guatemala", "Ban Ki-moon", "a missile", "Babe Ruth", "Drew", "the Sons of Liberty", "1834", "Ra\u00fal Eduardo Esparza", "Hellenic Polytheism", "Leonardo da Vinci", "five", "92 %", "wool", "Panic! at the Disco", "John Smith", "Andaman and Nicobar Islands", "The Bellamy Brothers", "4.37 light - years", "Martian", "arkansas", "colette", "Monopoly", "a castle", "Margaret Beckett", "six", "goose Green", "horseradish", "Thor", "tepuis", "paul Krugman", "21st Century Fox", "the Wanda Metropolitano", "67,575", "USS Essex", "Towards the Sun", "Japan", "7 February 14786", "1981 World Rowing Championships", "business magnate, investor, and philanthropist", "2 March 1972", "Angel Parrish", "Buck Owens", "five victims by helicopter,", "Eintracht Frankfurt", "authorizing killings and kidnappings by paramilitary death squads.", "84-year-old", "its new restaurant next to the home of Mona Lisa", "not", "Larry Ellison,", "Ameneh Bahrami", "France's famous Louvre", "Carol Browner", "CNN's", "10 years", "Union Gap", "1768", "antelope"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7412727591036414}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5009", "mrqa_searchqa-validation-10740", "mrqa_searchqa-validation-16888", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-5602", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-5511", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-3580", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-3700", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-83", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-3264"], "SR": 0.671875, "CSR": 0.5367093373493976, "EFR": 1.0, "Overall": 0.7242949924698795}, {"timecode": 83, "before_eval_results": {"predictions": ["Woods", "Mugabe,", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Lana Clarkson", "Wednesday.", "Moscow,", "two weeks after Black History Month", "Cain's", "90", "more than 1.2 million people.", "Madeleine K. Albright", "a government-run health facility that provides her with free drug treatment.", "Iran,", "$2.187 billion", "to symbolize his guilt in killing the bird", "gastrocnemius", "August 15, 1971", "humid subtropical climate, with hot summers and mild winters", "never made", "the center of the Northern Hemisphere", "1977", "the President", "on a sound stage in front of a live audience in Burbank, California", "from a crown cutting of the fruit, possibly flowering in five to ten months and fruiting in the following six months", "HTTP / 1.1", "the Treaty of Waitangi", "Basketball", "lactic acid", "Moldova", "Rosetta Stone", "Chang'an Avenue", "Ghana", "Robert Maxwell Farrago", "archery", "Byker Grove", "malted barley", "stained glass", "energy trading company", "English", "2006", "her translation of and commentary on Isaac Newton's book \"Principia\"", "Portsea", "two", "Tonde Burin", "Salford, Lancashire", "15,000 people", "Christy Walton", "25 points", "Adrian Lyne", "Ted Turner", "GILBERT & SULLIVAN", "tartar", "Cheddar", "Alexander Pope", "the Travel Book: A Journey Through Every Country in the World", "Herod", "graft-versus-host disease", "Joe Montana", "Jewelers", "Ursine", "Max Factor", "March 15, 1945", "The Parlement de Bretagne", "three"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6679845328282827}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4083", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-9275", "mrqa_triviaqa-validation-2675", "mrqa_triviaqa-validation-6967", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-3389", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-1742", "mrqa_searchqa-validation-5315", "mrqa_searchqa-validation-15829", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-10421"], "SR": 0.578125, "CSR": 0.5372023809523809, "EFR": 1.0, "Overall": 0.7243936011904762}, {"timecode": 84, "before_eval_results": {"predictions": ["360", "Paul Monti", "Geothermal gradient", "hyperinflation", "Procol Harum", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "October 1976", "to prevent further offense", "American Indian allies", "Star Wars", "ECB", "the right to vote", "longest rotation period ( 243 days )", "turkey", "peter Townsend", "20", "head", "aragonite", "toot uncommons", "cable", "groucho", "south africa", "helium", "Superman", "rowing", "Kal Ho Naa Ho", "Volvo 850", "Clara Petacci", "three", "Critics' Choice Television Award", "Brishta", "\"the Gentle Don\"", "the upper Missouri River", "magnus", "Mark Dayton", "35", "public", "independently verify the authenticity of the voice on the tape.", "Mother's", "Gov. Jan Brewer.", "Dube, 43, was killed", "\"We're just buttoning up a lot of our clay levees and putting a few more sandbags in place, and we hope to be protected up to 40 feet.\"", "spiral into economic disaster.", "Nairobi, Kenya,", "its part to improve the environment by taking on greenhouse gas emissions.", "Derek Mears:", "the captain of a nearby ship", "84-year-old", "Virginia, West Virginia, the Carolinas, Tennessee, Kentucky and Arkansas.", "Three Mile Island", "ragweed", "Charles I", "Daniel Boone", "Atlanta", "Annie", "the Weekly World News", "Tablecloth", "neurons", "fibre optics", "John Ford", "Latin, English", "Liverpool", "the sun", "Exodus"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5615165832151718}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.8387096774193548, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5333333333333333, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.15384615384615388, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-7694", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-1217", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-4977", "mrqa_triviaqa-validation-7520", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-3648", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-1822", "mrqa_hotpotqa-validation-1454", "mrqa_hotpotqa-validation-406", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-1273", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-3841", "mrqa_searchqa-validation-14853", "mrqa_searchqa-validation-12766", "mrqa_searchqa-validation-9351", "mrqa_searchqa-validation-7505", "mrqa_searchqa-validation-13197"], "SR": 0.484375, "CSR": 0.5365808823529412, "retrieved_ids": ["mrqa_squad-train-62619", "mrqa_squad-train-82866", "mrqa_squad-train-44704", "mrqa_squad-train-21648", "mrqa_squad-train-41933", "mrqa_squad-train-30257", "mrqa_squad-train-63462", "mrqa_squad-train-9661", "mrqa_squad-train-11038", "mrqa_squad-train-17353", "mrqa_squad-train-35857", "mrqa_squad-train-58732", "mrqa_squad-train-7822", "mrqa_squad-train-58900", "mrqa_squad-train-26645", "mrqa_squad-train-13504", "mrqa_hotpotqa-validation-4932", "mrqa_newsqa-validation-960", "mrqa_triviaqa-validation-1676", "mrqa_naturalquestions-validation-7910", "mrqa_triviaqa-validation-2843", "mrqa_naturalquestions-validation-4592", "mrqa_newsqa-validation-1432", "mrqa_squad-validation-4676", "mrqa_searchqa-validation-15537", "mrqa_triviaqa-validation-1939", "mrqa_searchqa-validation-3004", "mrqa_hotpotqa-validation-1240", "mrqa_naturalquestions-validation-9620", "mrqa_searchqa-validation-16047", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-943"], "EFR": 1.0, "Overall": 0.7242693014705883}, {"timecode": 85, "before_eval_results": {"predictions": ["red", "the fifth century", "Mason Alan Dinehart", "red, white, and blue", "O'Meara", "12.65 m ( 41.50 ft )", "two", "phaseout schedules were delayed for less developed ('Article 5 ( 1 )') countries", "2013", "Raya Yarbrough", "Sir Henry Cole", "the late 1980s", "IX", "Dick Advocaat", "joseph", "sister", "stieg Larsson", "Cuthbert", "mulberry", "Tina Turner", "ad nausea", "cereal", "Operation Overlord", "cumbria", "bowie", "Tom Shadyac", "pornographicstar", "Abel Makkonen Tesfaye", "Walldorf", "the Beatles", "Alfred Preis", "the Czech Kingdom", "third", "Elliot Fletcher", "$1 million", "Marika Nicolette Green", "Javed Miandad", "Sunday,", "\"Nude, Green Leaves and Bust\"", "Cameroon,", "Florida's Everglades.", "Olivia Newton-John", "\"Empire of the Sun,\"", "9 p.m. ET", "1,500 Marines will be part of the initial wave of President Obama's surge plan", "Kris Allen,", "\"Angels & Demons,\"", "to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally.", "Bahrain", "Forrest Gump", "Achilles", "Bright Lights, Big City", "Timex", "landfills", "Harvard", "James Bond", "butterflies", "Sybil", "A Canticle for Leibowitz", "John Harvard", "Julia Roberts", "queens", "Johnny Mathis", "a central council"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6213875534188034}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.0, 1.0, 0.0, 0.3, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.25, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-9185", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-1198", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-1658", "mrqa_triviaqa-validation-6304", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-5193", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-5658", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-2450", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-3911", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-178", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-1932", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-4298", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-3227", "mrqa_triviaqa-validation-3052"], "SR": 0.53125, "CSR": 0.5365188953488372, "EFR": 1.0, "Overall": 0.7242569040697675}, {"timecode": 86, "before_eval_results": {"predictions": ["741 weeks", "Speaker of the House of Representatives", "Exodus 20 : 1 -- 17", "Massachusetts", "ancient Athens", "Kevin Spacey", "6 March 1983", "innermost in the eye", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Hellenic Polytheistic Reconstructionism", "Tim Duncan", "September 8, 2017", "1989 album Sleeping with the Past", "shrek", "1875", "odergaard", "peter brown,", "beehive", "wheel arrangement", "Philippines", "adventure", "state of india", "crossword puzzle clue", "birthday", "travel sickness", "Kim Sung-su", "Lauren Lane", "Witch Twister", "Amway", "Joseph I", "Reinhard Heydrich", "\"What's My Line?", "Woodsy owl", "Andr\u00e9 3000", "Big Bad Wolf", "Mandarin Airlines Cargo", "American pharmaceutical company", "autonomy.", "around 10:30 p.m. October 3,", "as soon as 2050,", "British Prime Minister Gordon Brown's", "Unseeded Frenchwoman Aravane Rezai", "humans", "businessman", "opposition supporters in Libreville, Gabon.", "to digital.", "President Sheikh Sharif Sheikh Ahmed", "22", "\"The two leaders also discussed economic issues, issuing a joint statement after their meeting saying in part that \"open trade and investment are essential for competitiveness and sustainable growth in North America and globally.\"", "Robert Anthony \" Tony\" Snow", "Frank Sinatra", "Auschwitz-Birkenau", "garlic", "radius", "Anna", "\"Drogon is Coming\"", "a molar", "Earhart", "Halloween", "Al Czervik", "a hat", "Times Square", "Immanuel Kant", "Australia"], "metric_results": {"EM": 0.5, "QA-F1": 0.616827876984127}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.8, 0.5, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-2467", "mrqa_triviaqa-validation-4741", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-4787", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-3158", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-4497", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-4506", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-12", "mrqa_searchqa-validation-12870", "mrqa_searchqa-validation-16008", "mrqa_searchqa-validation-5992", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-12332"], "SR": 0.5, "CSR": 0.5360991379310345, "EFR": 0.96875, "Overall": 0.717922952586207}, {"timecode": 87, "before_eval_results": {"predictions": ["prince Igor", "conchita wurst", "Jessica Simpson", "The port of Terneuzen", "a bull", "isleone", "London", "is known for playing sold out tours around the world, including monumental performances at the 2010 Super Bowl and the closing ceremonies of the 2012 London Olympics.", "Thailand", "sports agent", "Friday", "antelope", "role-playing games", "non-voters", "the Gospel of Matthew", "Antigonon leptopus", "Service / Crown personnel serving in the UK or overseas in the British Armed Forces or with Her Majesty's Government", "Rick Marshall", "a section of the Torah ( Five Books of Moses ) used in Jewish liturgy during a single week", "2014", "Haliaeetus", "Kepner", "usually in May", "The Uralic languages", "$66.5 million", "12 countries", "Ice Princess", "1943", "jonathan no\u00eblle levesque", "The Dayton Memorial Hall", "The Division of Cook", "24 hours a day", "its air-cushioned sole", "January 4, 1821", "alcoholic drinks", "twenty-three episodes", "TD Garden", "generally from an older generation", "Diego Milito's", "a resident of la colonia Partido Romero in Ciudad Juarez,", "Summer", "a skilled hacker", "Manmohan Singh's", "more than 4,000", "development of two courses on the Black Sea coast in Bulgaria.", "Christopher Columbus", "sovereignty over them.", "flipped and landed on its right side,", "183 people,", "the American Civil War", "Texas A&M", "the Crossword", "Charlotte", "Dover", "a tuning", "Hammurabi", "Ohio State", "\"the Hoosier State\"", "the Rhine", "veterans", "San Francisco", "The Powerpuff Girls", "Love", "5"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6153087797619048}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7619047619047621, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6, 0.375, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-4493", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3242", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-3881", "mrqa_naturalquestions-validation-7464", "mrqa_hotpotqa-validation-5868", "mrqa_hotpotqa-validation-5327", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4457", "mrqa_hotpotqa-validation-2377", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-3536", "mrqa_newsqa-validation-780", "mrqa_searchqa-validation-6934", "mrqa_searchqa-validation-12503", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-14831", "mrqa_searchqa-validation-9575", "mrqa_searchqa-validation-10773", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-1073"], "SR": 0.515625, "CSR": 0.5358664772727273, "retrieved_ids": ["mrqa_squad-train-63731", "mrqa_squad-train-50146", "mrqa_squad-train-38005", "mrqa_squad-train-67309", "mrqa_squad-train-7501", "mrqa_squad-train-29720", "mrqa_squad-train-46796", "mrqa_squad-train-79765", "mrqa_squad-train-8096", "mrqa_squad-train-84126", "mrqa_squad-train-75019", "mrqa_squad-train-75848", "mrqa_squad-train-66337", "mrqa_squad-train-73117", "mrqa_squad-train-55468", "mrqa_squad-train-61506", "mrqa_newsqa-validation-4086", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-5649", "mrqa_naturalquestions-validation-10279", "mrqa_hotpotqa-validation-4698", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-10230", "mrqa_naturalquestions-validation-10161", "mrqa_searchqa-validation-9620", "mrqa_hotpotqa-validation-797", "mrqa_searchqa-validation-1386", "mrqa_triviaqa-validation-376", "mrqa_hotpotqa-validation-2017", "mrqa_searchqa-validation-7375", "mrqa_triviaqa-validation-6660", "mrqa_newsqa-validation-3364"], "EFR": 0.967741935483871, "Overall": 0.7176748075513197}, {"timecode": 88, "before_eval_results": {"predictions": ["almost entirely in Wake County, it lies just north of the state capital, Raleigh", "`` house edge '', a statistical advantage for the casino that is built into the game", "John Adams", "1945", "2002 Mitsubishi Lancer OZ Rally - Provided for Brian, in 2 Fast 2 Furious", "Kansas", "Texas, Oklahoma, and the surrounding Great Plains", "September 9, 2010", "peninsulas", "awarded to the team that lost the pre-game coin toss", "San Antonio, Texas", "Peter Andrew Beardsley MBE", "Steve Russell", "japan", "city of atherton", "Jan van Eyck", "the musoleum of giza", "trade union", "cataio", "from russia with love", "blackfriars railway bridge", "Christian Dior", "My Fair Lady", "migration, disabilities, rights of women and children, sexual orientation, and the rights of various minorities", "ynys m\u00f4n", "Pamela Chopra", "December 19, 1998", "Black Swan", "Wes Unseld", "a minor basilica", "Hampton University", "United States", "Kramer's caddy Stan", "Interstate 95", "\"Nebo Zovyot\"", "1993", "Arabella Churchill", "It's a place where elephants can roam freely, largely feed and shelter themselves and interact with others, often after years living alone in captivity.", "almost 9 million", "US Airways Flight 1549", "Kenneth Cole", "Hussein's Revolutionary Command Council.", "those missing", "a motor scooter", "average of 25 percent", "The United Nations", "through a facility in Salt Lake City, Utah,", "Nineteen", "hardship for terminally ill patients and their caregivers,", "Out-of-print book", "repent", "Vespa", "Johnson", "Dmitri Mendeleev", "in the windup", "the Red Sea", "the Dauphin", "Ulysses S. Grant", "Super Bowl XI", "a shawl", "a cat`s tongue", "build temporary dikes or to bolster existing ones.", "Haiti.", "police dogs in Duesseldorf, Germany"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5806282693001443}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5555555555555556, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.125, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.7272727272727273, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-7069", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-1999", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-5477", "mrqa_triviaqa-validation-5417", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-5614", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3219", "mrqa_newsqa-validation-1086", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-908", "mrqa_searchqa-validation-1481", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-5185", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-4967", "mrqa_searchqa-validation-16107", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-414"], "SR": 0.46875, "CSR": 0.5351123595505618, "EFR": 0.9705882352941176, "Overall": 0.7180932439689359}, {"timecode": 89, "before_eval_results": {"predictions": ["Ricardo Valles de la Rosa,", "his mother, Katherine Jackson, his three children and undisclosed charities.", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "The Swiss art heist follows the recent theft in Switzerland of two paintings", "Kgalema Motlanthe,", "it -- you know -- black is beautiful,\"", "a complicated man underneath a confident exterior,", "the 1999 British Open at Carnoustie", "executive director of the Americas Division of Human Rights Watch,", "refusal or inability to \"turn it off\"", "12", "surgical anesthetic propofol", "English", "to help batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship", "The Jamestown settlement in the Colony of Virginia", "April 8, 2016", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "the centre of Munich", "U + 002A * Asterisk", "Kirstjen Nielsen", "British R&B girl group Eternal", "Cyndi Grecco", "such famous figures as Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "about 3.5 mya", "Antony", "salsa", "h Harriet harman", "civil rights leader", "Nowhere Boy", "Cascade Range", "corvidae", "The Lone Ranger", "Buddhist", "2013", "steel", "sharpening stone", "2011", "Nick Cassavetes", "Sun Records founder Sam Phillips", "National Basketball Development League", "Station Casinos", "2,627", "life insurance", "Kareena Kapoor Khan", "Hopeless Records", "1971", "Justin Bieber, Monica, Britney Spears, Usher, Keri Hilson", "the 2012 Summer Olympics", "Anzio", "L.T.", "cvicus", "blue ribbon", "Hillary Rodham Clinton", "Louisiana", "occipital", "the divine right of Kings", "Uranus", "cauliflower", "a kettledrum", "Reform Judaism", "a insect", "The Office", "Elmore Leonard"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5573660714285714}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.16666666666666669, 0.04761904761904762, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-3899", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-199", "mrqa_naturalquestions-validation-7228", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-6968", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-154", "mrqa_triviaqa-validation-6501", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-7008", "mrqa_triviaqa-validation-3067", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-3787", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-4669", "mrqa_searchqa-validation-11910", "mrqa_searchqa-validation-14695", "mrqa_searchqa-validation-5328", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-8375", "mrqa_searchqa-validation-926"], "SR": 0.453125, "CSR": 0.5342013888888889, "EFR": 0.9714285714285714, "Overall": 0.718079117063492}, {"timecode": 90, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-1275", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3757", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-297", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6811", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9434", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1231", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2550", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-891", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-1065", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12473", "mrqa_searchqa-validation-12539", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-1462", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16161", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-3229", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5112", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7225", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-10309", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-1980", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-477", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3509", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4727", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5302", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-608", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6634", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.83984375, "KG": 0.49140625, "before_eval_results": {"predictions": ["David Joseph Madden", "Garbi\u00f1e Muguruza", "fresh nuclear fuel", "more than 1,000", "9.7", "Andy Serkis", "Andrew Garfield", "by the hip, and under the left shoulder", "Ali Daei", "the star", "February 2017", "Nala", "Germany", "a dove", "Jimmy Boyd", "copper", "Coke", "Joan Rivers", "ball break", "coelacanth", "Mel Blanc", "german Kinnock", "Daft As A Brush", "burt Kwouk", "steveland hardaway", "Edmonton, Alberta", "Chelsea", "Indiana", "Christina Ricci", "Treaty of Gandamak", "Newell Highway", "Sports Illustrated", "Lieutenant Colonel Iceal Hambleton", "1730", "1903", "Oregon Ducks football", "Nye County", "Thabo Mbeki,", "37th", "$81,4705", "40", "Sabina Guzzanti,", "It has never been, and never will be,", "400 farmers", "U.S. Court of Appeals for the District of Columbia.", "heavy flannel or wool", "Chester Stiles,", "a nuclear weapon", "Malcolm X", "It's My Party", "the Haunted Mansion", "Apollo 8", "the White Mountains of California", "South Carolina", "the Grand Canyon", "the Book of Judges", "Judas", "Edgar Rice Burroughs", "the Seine", "Toy Story", "(Samuel) Morse", "a fraction", "a relic", "Kansas State"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6517609126984127}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8653", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-422", "mrqa_triviaqa-validation-293", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-2973", "mrqa_triviaqa-validation-5299", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3657", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-15093", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-1398", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-2316", "mrqa_searchqa-validation-7599", "mrqa_searchqa-validation-13576"], "SR": 0.5625, "CSR": 0.5345123626373627, "retrieved_ids": ["mrqa_squad-train-22038", "mrqa_squad-train-56728", "mrqa_squad-train-83408", "mrqa_squad-train-7463", "mrqa_squad-train-57830", "mrqa_squad-train-60887", "mrqa_squad-train-66258", "mrqa_squad-train-67157", "mrqa_squad-train-47724", "mrqa_squad-train-81689", "mrqa_squad-train-46369", "mrqa_squad-train-54492", "mrqa_squad-train-38363", "mrqa_squad-train-22508", "mrqa_squad-train-35415", "mrqa_squad-train-23428", "mrqa_naturalquestions-validation-2333", "mrqa_naturalquestions-validation-6851", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-1474", "mrqa_searchqa-validation-1877", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-10070", "mrqa_hotpotqa-validation-2627", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-4568", "mrqa_hotpotqa-validation-4122", "mrqa_triviaqa-validation-1630", "mrqa_naturalquestions-validation-10364", "mrqa_searchqa-validation-8737", "mrqa_triviaqa-validation-5170", "mrqa_searchqa-validation-13487"], "EFR": 1.0, "Overall": 0.7208087225274725}, {"timecode": 91, "before_eval_results": {"predictions": ["1612", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods )", "Wyatt and Dylan Walters", "the kidneys", "Coton", "2015", "2007", "12.65 m", "French Canadian", "counter clockwise", "332", "Narendra Modi", "Vice President", "chancery", "step right up - Circus in America", "tony blair", "melodic", "Judges 16", "Islam", "Steve Biko", "Warren Commission", "Coldplay", "sow", "Donald Trump", "daniel peggotty", "King of Hanover", "Central Park", "over 281", "Craig William Macneill", "evangelical Christian periodical", "Prussia", "Dutch", "Bangkok, Thailand", "Springfield, Massachusetts", "1983", "Sam Kinison", "\"The Tonight Show\"", "two hunters -- one of whom witnessed the attack --", "Tetris,", "Mary Phagan,", "Brett Cummins,", "$40 and a bread.", "Al-Aqsa", "Aung San Suu Kyi", "Former U.S. soldier Steven Green", "September,", "Michelle Obama", "OneLegacy,", "\"Oprah is an angel, she is God-sent,\"", "ice hockey", "a Birch-tree", "a pomegranate", "PachelbeJ", "Graceland", "Castle Rock", "North Korea & South Korea", "Wrigley", "Daytona", "the Damned (Turkish sultan)", "Inuit", "Dorian Gray", "Panama Canal", "iron", "J.R. Tolkien"], "metric_results": {"EM": 0.453125, "QA-F1": 0.575657242063492}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false], "QA-F1": [0.0, 0.19999999999999998, 1.0, 0.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-5537", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-5512", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-6795", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-1149", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-618", "mrqa_newsqa-validation-3679", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-3803", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-11577", "mrqa_searchqa-validation-5807", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-14018", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-12632"], "SR": 0.453125, "CSR": 0.5336277173913043, "EFR": 0.9428571428571428, "Overall": 0.7092032220496894}, {"timecode": 92, "before_eval_results": {"predictions": ["mitosis", "Dave Kelly", "2011", "Lana Del Rey", "George Strait", "126", "Benzodiazepines", "a recipient of the Medal of Honor, the Victoria Cross, and several other foreign nations'highest service awards", "due to Parker's pregnancy at the time of filming", "Malvolio", "James Corden", "November 17, 1800", "`` Psychomachia, '' an epic poem written in the fifth century", "Salman Rushdie", "robben island", "petero verde", "john j. peterhing", "flowers", "Bjorn Borg", "Romania", "Baton Rouge", "Prince Andrew", "john johnson johnson", "December", "harrods", "Polka", "Ford Falcon", "Vernon Charles Kay", "third baseman and shortstop", "1926", "Irish Chekhov", "Adrian Peter McLaren", "London", "bioelectromagnetics", "Dunlop Tyres", "Pakistan", "Hampton University", "1994", "leftist rebels who often would not let him talk,", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "Serie A", "a crocodile", "a $13 million global crime ring,", "Bush administration", "the coalition", "Ignazio La Russa", "581 points", "near the George Washington Bridge,", "CNN's", "john johnson", "Wisconsin", "John Madden", "50 First Dates", "the Left Bank", "Nikita Khrushchev", "Newport", "South Carolina", "Harold Ramis", "1914", "Cairo", "E", "Tainy Sledstviya", "Stalybridge Celtic", "written for \"The New York Times\" and \"Popular Mechanics\", and is a regular contributor to various CNBC shows such as \"On the Money\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.692195298573975}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.08, 0.823529411764706, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-7507", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-5975", "mrqa_triviaqa-validation-5698", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-277", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-64", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-1729", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-9478", "mrqa_searchqa-validation-10237", "mrqa_searchqa-validation-2094", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-5211"], "SR": 0.59375, "CSR": 0.5342741935483871, "EFR": 0.9230769230769231, "Overall": 0.705376473325062}, {"timecode": 93, "before_eval_results": {"predictions": ["romantic comedy", "Richard Strauss", "Elbow River", "King R\u00e6dwald of East Anglia", "gull-wing", "12", "Dutch", "40 Days and 40 Nights", "Las Vegas", "a mid-ninth-century Viking chieftain", "benjamin ciaramello", "green and yellow", "Skegness", "season seven", "Speaker of the House of Representatives", "A 30 - something man", "from the Anglo - Norman French waleis", "molecular clouds in interstellar space", "chairman ( more usually called the `` chair '' or `` chairperson '' )", "Harlem River", "Simon Callow", "Mamata Banerjee", "May 5, 1904", "layered systems of sovereignty", "portal tomb", "printed circuit", "Bulletin", "cricket", "doe", "left book club", "giant", "baryon number", "south Ossetia", "Ghana", "Tony Blair", "master", "Camellia", "peanuts, nuts, shellfish and fish", "NASCAR.", "Many have complained that his wins are too routine, and purists grouse that he does not poses the quality of \"hinkaku,\"", "Marines we talked to in this coastal, scrub pine-covered North Carolina base", "two Israeli soldiers,", "Mugabe's opponents", "retired Navy F-14", "The Sopranos", "dead", "William Randolph Hearst.", "Four", "education", "A Beautiful Mind", "Reader's Digest", "John Wesley", "metal", "40-Year-Old Virgin", "William Faulkner", "Chocolate", "Clinton", "Canada", "The Sound of Silence", "Nastia Liukin", "General Lafayette", "one", "Melbourne", "\"Let it Roll:"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6639910130718953}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.5, 0.5555555555555556, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3252", "mrqa_hotpotqa-validation-3362", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-5627", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-3559", "mrqa_triviaqa-validation-3116", "mrqa_triviaqa-validation-2398", "mrqa_triviaqa-validation-6635", "mrqa_triviaqa-validation-302", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3072", "mrqa_searchqa-validation-11865", "mrqa_searchqa-validation-1864", "mrqa_searchqa-validation-9649", "mrqa_searchqa-validation-1090", "mrqa_searchqa-validation-11066", "mrqa_searchqa-validation-15122"], "SR": 0.546875, "CSR": 0.5344082446808511, "retrieved_ids": ["mrqa_squad-train-60361", "mrqa_squad-train-33706", "mrqa_squad-train-71000", "mrqa_squad-train-63704", "mrqa_squad-train-30780", "mrqa_squad-train-23706", "mrqa_squad-train-36132", "mrqa_squad-train-40981", "mrqa_squad-train-46115", "mrqa_squad-train-86569", "mrqa_squad-train-9491", "mrqa_squad-train-86480", "mrqa_squad-train-67074", "mrqa_squad-train-11644", "mrqa_squad-train-35859", "mrqa_squad-train-4525", "mrqa_searchqa-validation-7242", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-924", "mrqa_newsqa-validation-2027", "mrqa_naturalquestions-validation-9620", "mrqa_newsqa-validation-1278", "mrqa_naturalquestions-validation-1449", "mrqa_triviaqa-validation-2605", "mrqa_hotpotqa-validation-3826", "mrqa_newsqa-validation-4073", "mrqa_hotpotqa-validation-2274", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1427", "mrqa_newsqa-validation-1146", "mrqa_triviaqa-validation-4568", "mrqa_naturalquestions-validation-337"], "EFR": 0.9655172413793104, "Overall": 0.7138913472120323}, {"timecode": 94, "before_eval_results": {"predictions": ["rash", "1865", "Chrysler K platform", "secondary school study", "three times", "Celtic", "Prince of Cambodia Norodom Sihanouk", "Kevin Smith", "Kansas", "My Cat from Hell", "February 12, 2014", "novelist and poet", "Bergen County", "$2.187 billion", "Evermoist", "India", "Super Bowl LII", "Ludacris", "1939", "Barbara Windsor", "Universal Pictures and Focus Features", "normally show IIII for four o'clock", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Chris Martin", "21 February", "Malta", "roosevelt", "Leonardo da Vinci", "severn", "strychnine", "Hercule Poirot", "Colombia", "Austria", "hans lippershey", "Il Divo", "Omid Djalili", "the cow", "the announcement was greeted with general astonishment in Seoul,", "Casa de Campo International Airport", "a depth of about 1,300 meters in the Mediterranean Sea.", "Michelle Rounds", "1950s,", "the southern city of Naples", "Hong Kong and Shenzhen,", "Employee Free Choice act", "north-south highway", "a suicide bomber", "10-person", "six-month amnesty period,", "the orbiter", "Morse code", "My Fair Lady", "the Kingdom of the Crystal Skull", "the ulnar nerve", "John Updike", "the Indian Ocean", "lm", "(John) Boehner", "The Maltese Falcon", "William Blake", "Troy", "a Pringles can", "Medical malpractice", "a hat"], "metric_results": {"EM": 0.609375, "QA-F1": 0.693036666152475}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7692307692307693, 0.9411764705882353, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7272727272727273, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-3763", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1864", "mrqa_hotpotqa-validation-1269", "mrqa_triviaqa-validation-7582", "mrqa_triviaqa-validation-3094", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-273", "mrqa_searchqa-validation-9025", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-3826", "mrqa_searchqa-validation-12343", "mrqa_searchqa-validation-9376", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-11460", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-14791"], "SR": 0.609375, "CSR": 0.5351973684210527, "EFR": 1.0, "Overall": 0.7209457236842105}, {"timecode": 95, "before_eval_results": {"predictions": ["6,396", "23 July 1989", "princess", "James Mitchum", "Hampton's hump and Hampton's line", "right-hand", "Mika H\u00e4kkinen", "Todd McFarlane", "Republic of Ireland", "1942", "Germanic", "Marc Bolan", "Realty Bites", "amphetamines", "Louis XV", "Steve Goodman", "Evermoist", "Thomas Jefferson's", "Miami Heat", "1920", "Dirk Benedict", "1983", "December 19, 2016", "The vascular cambium", "Nicolette Larson", "Venezuela", "Dick Smith", "Madrid", "Lithium", "Wednesday's child", "Rajasthan", "sweater", "mustard", "james Gang", "Dick Fosbury", "Esau", "peter Nichols", "The Kirchners", "Friday,", "vegan bake", "Muslim revolutionary named Malcolm X", "are not for sale,", "10", "the Pacific Ocean territory of Guam", "a \"prostitute\"", "Colombia.", "Alan Graham", "Dancing With the Stars", "Kyra and Violet,", "Python", "pi", "Rio de Janeiro", "Chuck Yeager", "the tsuba", "lipos", "Central Park", "Mary", "whales", "the Battle of Fort Donelson", "Bech", "Aaron Burr", "Dougie MacLean", "victims of rape", "2010"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6629836309523809}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-988", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2374", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-8859", "mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-5632", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-6146", "mrqa_newsqa-validation-3125", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3380", "mrqa_searchqa-validation-2508", "mrqa_searchqa-validation-1010", "mrqa_searchqa-validation-1405", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-15280", "mrqa_searchqa-validation-11026", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-1856"], "SR": 0.609375, "CSR": 0.5359700520833333, "EFR": 0.96, "Overall": 0.7131002604166665}, {"timecode": 96, "before_eval_results": {"predictions": ["Shropshire Union Canal", "Big Fucking German", "American", "1982", "Harlem", "Jeff Meldrum", "private", "gorillas", "Henry Lau", "most awarded female act of all-time", "private liberal arts college", "Marktown", "North Kesteven, Lincolnshire", "the long - hair variety", "An elevator with a counterbalance", "a castle", "Laura Jane Haddock", "David Gahan", "Ernest Rutherford", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "the Baltic Fleet of 41 sail under convoy of the HMS Serapis and HM hired armed vessel Countess of Scarborough near Flamborough Head", "chromosome 21 attached to another chromosome", "mashed potato", "1986", "Nicole DuPort", "driving miss daisy", "dogs", "Margaret Thatcher", "Leicester", "Tito Jackson", "Wash", "pussia", "rugby", "bone", "France", "paul sartre", "branson", "rolled over", "finance", "people", "strangled his wife in his sleep while dreaming that she was an intruder", "problems with the way Britain implements European Union employment directives.", "Wednesday.", "action in-and-around the golf course", "Roberto Micheletti,", "kuranyi's", "40 militants and six Pakistan soldiers", "engineering and construction", "July 18, 1994,", "A Tale of Murder", "the Proletariat", "the waggle dance", "Israel", "the yottabyte", "One Flew Over the Cuckoo's Nest", "Danny Elfman", "arsenic", "Tchaikovsky", "salmon", "pitch", "Senegal", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Muqtada al-Sadr,", "two"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6371558779761906}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.5, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.25, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6875000000000001, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1891", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-5188", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-4307", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-234", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-3555", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-259", "mrqa_triviaqa-validation-2141", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-1967", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3219", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-7020", "mrqa_searchqa-validation-9706", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-14560", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-2036"], "SR": 0.546875, "CSR": 0.5360824742268041, "retrieved_ids": ["mrqa_squad-train-43581", "mrqa_squad-train-40392", "mrqa_squad-train-56719", "mrqa_squad-train-18425", "mrqa_squad-train-79636", "mrqa_squad-train-45112", "mrqa_squad-train-81928", "mrqa_squad-train-12853", "mrqa_squad-train-80571", "mrqa_squad-train-71133", "mrqa_squad-train-65224", "mrqa_squad-train-19340", "mrqa_squad-train-3886", "mrqa_squad-train-2509", "mrqa_squad-train-63620", "mrqa_squad-train-46533", "mrqa_naturalquestions-validation-5152", "mrqa_searchqa-validation-14003", "mrqa_hotpotqa-validation-1038", "mrqa_triviaqa-validation-2879", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-4987", "mrqa_hotpotqa-validation-171", "mrqa_triviaqa-validation-3580", "mrqa_newsqa-validation-2251", "mrqa_naturalquestions-validation-5363", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-593", "mrqa_triviaqa-validation-6253", "mrqa_triviaqa-validation-896", "mrqa_searchqa-validation-16844", "mrqa_naturalquestions-validation-3246"], "EFR": 1.0, "Overall": 0.7211227448453608}, {"timecode": 97, "before_eval_results": {"predictions": ["11 to 12 year old", "bronze medal in the women's figure skating final,", "an independent homeland", "Ryder Russell,", "three-time road race world champion,", "Pakistan's", "137", "23 million square meters (248 million square feet)", "Frank,", "Azzam the American,", "Barbara Dainton-West,", "Louela Binlac", "Dennis Davern,", "The Sun", "American author Joseph Heller", "in all cases affecting ambassadors, other public ministers and consuls, and those in which a state shall be party.", "an edible tuber", "a combination of Shakespearean actresses and car salespeople", "IBM", "Kida", "heart", "by fermenting dietary fiber into short - chain fatty acids ( SCFAs )", "over a 20 - year period", "absorbed the superhuman powers and the psyche of Carol Danvers, the original Ms. Marvel", "Institute of Chartered Accountants of India ( ICAI )", "yttrium", "aircraft", "Cole Porter", "calf", "Dry Ice", "Clint Eastwood", "yichang City", "Arthur Ashe", "False Claim Of Game", "comedian", "Jane Seymour", "poland", "Rhode Island", "Chelsea Vanessa Peretti (born February 20, 1978)", "Donna Paige Helmintoller", "London's", "Dukes of Westminster", "Bentley Twins", "2.1 million", "Bridgetown", "\"Baa, Baa, Black sheep\"", "4370 ft", "ABC1 and ABC2", "Pontins", "multiplication", "the IAFIS", "acid", "a black bear", "Alcoholics Anonymous", "Venus", "French", "(Jerry) Maguire", "The Wall Street Journal", "Mowgli", "Vicomte de Valvert", "India", "(Fredic) Chopin", "Kentucky", "Blackbeard"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5542966582029082}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6060606060606061, 0.8571428571428571, 0.9166666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-3220", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-748", "mrqa_newsqa-validation-3852", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-1433", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-2309", "mrqa_triviaqa-validation-6380", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5466", "mrqa_triviaqa-validation-7534", "mrqa_triviaqa-validation-4750", "mrqa_triviaqa-validation-1086", "mrqa_hotpotqa-validation-513", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-4637", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2961", "mrqa_searchqa-validation-2295", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-6936", "mrqa_searchqa-validation-16710", "mrqa_searchqa-validation-15968", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-11515", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-10743"], "SR": 0.453125, "CSR": 0.5352359693877551, "EFR": 1.0, "Overall": 0.720953443877551}, {"timecode": 98, "before_eval_results": {"predictions": ["big island", "Los Angeles", "john heston", "yorkshire terriers", "Francois Mitterrand", "phrenology", "London", "shanghai", "John Travolta", "reckless arson", "IKEA", "Colossus of Rhodes", "a Goddess of Revenge", "a video game series developed by Telltale Games, loosely based on the comic", "euro", "Charles Woodson", "Georges Auguste Escoffier", "Arthur `` The President '' Flanders -- An inmate on death row, convicted of killing his father in an insurance - fraud scheme", "in either Tagalog or English", "altitude", "to prevent any contaminants in the sink from flowing into the potable water system by siphonage and is the least expensive form of backflow prevention", "Watson", "Sohrai", "July 1, 1923", "eight", "40 Acres and a Mule Filmworks", "Lake Wallace", "the Magic Band", "Nevada", "Brookhaven", "atomic bomb", "the Americas and the entire South American temperate zone", "the Rose Garden", "Marvel Comics", "the Marx Brothers film", "Umar S. Israilov (c. 1982 \u2013 January 13, 2009) was a former bodyguard of Chechen President Ramzan Kadyrov", "number 1", "$40 and a loaf of bread.", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance for parts of the Midwest", "a lock break", "Ity Bitsy Teeny Weeny Yellow Polka Dot Bikini.", "UNICEF", "Ali Larijani", "Thamer Bin Saeed Ahmed al-Shanfari.", "Saturday,", "coalition troops", "Daniel Wozniak,", "hardship for terminally ill patients and their caregivers,", "St Petersburg and Moscow,", "Rudy Giuliani", "a coyote", "Punch", "the Spark Ranger", "a bulbous flowering plant", "The Little Prince", "Garfield", "Repent, for the Kingdom of Heaven is at hand", "the drum", "Horatio Nelson", "Moscow", "tabula rasa", "March 2016", "159", "New York City"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6575243447159566}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 0.8571428571428571, 0.5, 0.875, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3168", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-892", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-1446", "mrqa_naturalquestions-validation-225", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4237", "mrqa_hotpotqa-validation-2282", "mrqa_hotpotqa-validation-4662", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-47", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-4118", "mrqa_searchqa-validation-15462", "mrqa_searchqa-validation-11428", "mrqa_searchqa-validation-2609", "mrqa_naturalquestions-validation-3558"], "SR": 0.5625, "CSR": 0.5355113636363636, "EFR": 0.9642857142857143, "Overall": 0.7138656655844156}, {"timecode": 99, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-1259", "mrqa_hotpotqa-validation-128", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1659", "mrqa_hotpotqa-validation-1800", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-210", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-2320", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-2485", "mrqa_hotpotqa-validation-258", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-2695", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3067", "mrqa_hotpotqa-validation-3116", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-3455", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-3585", "mrqa_hotpotqa-validation-3598", "mrqa_hotpotqa-validation-3612", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-3974", "mrqa_hotpotqa-validation-3990", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4031", "mrqa_hotpotqa-validation-4121", "mrqa_hotpotqa-validation-4461", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4688", "mrqa_hotpotqa-validation-4759", "mrqa_hotpotqa-validation-4786", "mrqa_hotpotqa-validation-4868", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-5133", "mrqa_hotpotqa-validation-5211", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5289", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-5387", "mrqa_hotpotqa-validation-5405", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-5567", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5697", "mrqa_hotpotqa-validation-5758", "mrqa_hotpotqa-validation-5808", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-794", "mrqa_hotpotqa-validation-817", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10446", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1409", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-4053", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4249", "mrqa_naturalquestions-validation-4303", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-44", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-477", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4895", "mrqa_naturalquestions-validation-4904", "mrqa_naturalquestions-validation-4915", "mrqa_naturalquestions-validation-493", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5288", "mrqa_naturalquestions-validation-5539", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-5998", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6164", "mrqa_naturalquestions-validation-6298", "mrqa_naturalquestions-validation-6329", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-779", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-851", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-8754", "mrqa_naturalquestions-validation-9028", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-934", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-9581", "mrqa_naturalquestions-validation-9613", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9799", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9816", "mrqa_naturalquestions-validation-9835", "mrqa_naturalquestions-validation-994", "mrqa_newsqa-validation-1051", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1179", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-160", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2091", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-2209", "mrqa_newsqa-validation-2226", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-2600", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-2765", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3080", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3442", "mrqa_newsqa-validation-350", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3779", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3853", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-595", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-999", "mrqa_searchqa-validation-10117", "mrqa_searchqa-validation-10170", "mrqa_searchqa-validation-1025", "mrqa_searchqa-validation-10290", "mrqa_searchqa-validation-10311", "mrqa_searchqa-validation-10357", "mrqa_searchqa-validation-10483", "mrqa_searchqa-validation-11082", "mrqa_searchqa-validation-11096", "mrqa_searchqa-validation-1128", "mrqa_searchqa-validation-11389", "mrqa_searchqa-validation-11545", "mrqa_searchqa-validation-11631", "mrqa_searchqa-validation-11703", "mrqa_searchqa-validation-11816", "mrqa_searchqa-validation-11848", "mrqa_searchqa-validation-11865", "mrqa_searchqa-validation-12136", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12332", "mrqa_searchqa-validation-12351", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-12833", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13161", "mrqa_searchqa-validation-13192", "mrqa_searchqa-validation-13342", "mrqa_searchqa-validation-13429", "mrqa_searchqa-validation-13545", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13615", "mrqa_searchqa-validation-13792", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-13970", "mrqa_searchqa-validation-1428", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14577", "mrqa_searchqa-validation-14843", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-155", "mrqa_searchqa-validation-15683", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16044", "mrqa_searchqa-validation-1623", "mrqa_searchqa-validation-16277", "mrqa_searchqa-validation-16358", "mrqa_searchqa-validation-16380", "mrqa_searchqa-validation-16844", "mrqa_searchqa-validation-16958", "mrqa_searchqa-validation-1820", "mrqa_searchqa-validation-1907", "mrqa_searchqa-validation-1968", "mrqa_searchqa-validation-201", "mrqa_searchqa-validation-2334", "mrqa_searchqa-validation-3021", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-3937", "mrqa_searchqa-validation-3963", "mrqa_searchqa-validation-4005", "mrqa_searchqa-validation-4177", "mrqa_searchqa-validation-4342", "mrqa_searchqa-validation-4352", "mrqa_searchqa-validation-4893", "mrqa_searchqa-validation-5123", "mrqa_searchqa-validation-5144", "mrqa_searchqa-validation-5157", "mrqa_searchqa-validation-539", "mrqa_searchqa-validation-5454", "mrqa_searchqa-validation-5472", "mrqa_searchqa-validation-571", "mrqa_searchqa-validation-5930", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6369", "mrqa_searchqa-validation-6449", "mrqa_searchqa-validation-6513", "mrqa_searchqa-validation-6702", "mrqa_searchqa-validation-6766", "mrqa_searchqa-validation-6990", "mrqa_searchqa-validation-7029", "mrqa_searchqa-validation-7037", "mrqa_searchqa-validation-7149", "mrqa_searchqa-validation-7235", "mrqa_searchqa-validation-7552", "mrqa_searchqa-validation-7847", "mrqa_searchqa-validation-788", "mrqa_searchqa-validation-7929", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8884", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9492", "mrqa_searchqa-validation-9515", "mrqa_searchqa-validation-9595", "mrqa_searchqa-validation-9653", "mrqa_searchqa-validation-9659", "mrqa_squad-validation-10076", "mrqa_squad-validation-10295", "mrqa_squad-validation-10307", "mrqa_squad-validation-1176", "mrqa_squad-validation-1657", "mrqa_squad-validation-1908", "mrqa_squad-validation-2114", "mrqa_squad-validation-2171", "mrqa_squad-validation-2240", "mrqa_squad-validation-2675", "mrqa_squad-validation-2849", "mrqa_squad-validation-3147", "mrqa_squad-validation-324", "mrqa_squad-validation-3289", "mrqa_squad-validation-3355", "mrqa_squad-validation-4127", "mrqa_squad-validation-4429", "mrqa_squad-validation-4623", "mrqa_squad-validation-4764", "mrqa_squad-validation-4840", "mrqa_squad-validation-5410", "mrqa_squad-validation-5592", "mrqa_squad-validation-5951", "mrqa_squad-validation-6106", "mrqa_squad-validation-6166", "mrqa_squad-validation-6409", "mrqa_squad-validation-6564", "mrqa_squad-validation-6681", "mrqa_squad-validation-6812", "mrqa_squad-validation-7270", "mrqa_squad-validation-7300", "mrqa_squad-validation-7480", "mrqa_squad-validation-7707", "mrqa_squad-validation-7917", "mrqa_squad-validation-794", "mrqa_squad-validation-8213", "mrqa_squad-validation-8258", "mrqa_squad-validation-8592", "mrqa_squad-validation-8679", "mrqa_squad-validation-8917", "mrqa_squad-validation-9575", "mrqa_squad-validation-9643", "mrqa_squad-validation-9697", "mrqa_squad-validation-9787", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1191", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-1362", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-1574", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1803", "mrqa_triviaqa-validation-1820", "mrqa_triviaqa-validation-1972", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2306", "mrqa_triviaqa-validation-2498", "mrqa_triviaqa-validation-2506", "mrqa_triviaqa-validation-2544", "mrqa_triviaqa-validation-2657", "mrqa_triviaqa-validation-2762", "mrqa_triviaqa-validation-2792", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3149", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-3281", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3358", "mrqa_triviaqa-validation-3427", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-3745", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3798", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-381", "mrqa_triviaqa-validation-3841", "mrqa_triviaqa-validation-3959", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-425", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-4305", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4437", "mrqa_triviaqa-validation-4455", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-4863", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-5301", "mrqa_triviaqa-validation-5310", "mrqa_triviaqa-validation-5375", "mrqa_triviaqa-validation-5417", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5535", "mrqa_triviaqa-validation-5537", "mrqa_triviaqa-validation-558", "mrqa_triviaqa-validation-5657", "mrqa_triviaqa-validation-5705", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5977", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6082", "mrqa_triviaqa-validation-6146", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6268", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-6325", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-6377", "mrqa_triviaqa-validation-641", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6632", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-6697", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-723", "mrqa_triviaqa-validation-7232", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-7310", "mrqa_triviaqa-validation-7417", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-7492", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-7588", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-884"], "OKR": 0.83984375, "KG": 0.50078125, "before_eval_results": {"predictions": ["dharma", "Dublin", "Pilgrim's Progress", "seaweed", "christopher wren", "Pakistan", "shanghai", "a rich widow", "south pacific", "Massachusetts", "black", "Melbourne", "opera Libretti", "International Orange", "warm and is considered to be the most comfortable climatic conditions of the year", "Claudia Grace Wells", "Jules Shear", "a compiler", "Meeting Sweet at The Bronze", "Herod", "Jacques Cousteau", "Florida", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "1922", "two", "musical theatre", "Guangzhou", "lower Manhattan", "GE Appliances", "The Keeping Hours", "The Life of Charlotte Bront\u00eb", "\"Catch Me If You Can\"", "the 2007 Formula One season", "43rd President of the United States", "Girls' Generation", "The Andy Williams Christmas Album", "183", "Venus Williams", "Lifeway's 100-plus stores nationwide", "additional information", "Marie-Therese Walter.", "alcohol", "attacked L.K. Chaudhary, the chief executive of an Italian car parts manufacturing company.", "56,", "A Colorado prosecutor", "11th year in a row.", "Michael Krane,", "Thursday", "the asylum", "horses", "The Bravados", "Cessna", "China", "South Africa", "Alien", "Goose Gossage", "The Rolling Thunder", "Sephora", "Fletcher Christian", "Latter-day Saints", "Doctor Dolittle", "Over the Rainbow", "tea"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7312304197994988}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.7368421052631579, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1069", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5690", "mrqa_triviaqa-validation-640", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-1911", "mrqa_hotpotqa-validation-5748", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-350", "mrqa_newsqa-validation-3562", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-3858", "mrqa_searchqa-validation-2348", "mrqa_searchqa-validation-2773", "mrqa_searchqa-validation-9449", "mrqa_searchqa-validation-11184", "mrqa_searchqa-validation-15555"], "SR": 0.640625, "CSR": 0.5365625, "retrieved_ids": ["mrqa_squad-train-58349", "mrqa_squad-train-39960", "mrqa_squad-train-23312", "mrqa_squad-train-44239", "mrqa_squad-train-73828", "mrqa_squad-train-45185", "mrqa_squad-train-63183", "mrqa_squad-train-85241", "mrqa_squad-train-31709", "mrqa_squad-train-39417", "mrqa_squad-train-64537", "mrqa_squad-train-21547", "mrqa_squad-train-18551", "mrqa_squad-train-60704", "mrqa_squad-train-54742", "mrqa_squad-train-25179", "mrqa_newsqa-validation-3242", "mrqa_searchqa-validation-1927", "mrqa_naturalquestions-validation-3189", "mrqa_triviaqa-validation-5163", "mrqa_searchqa-validation-6632", "mrqa_naturalquestions-validation-5825", "mrqa_searchqa-validation-1864", "mrqa_triviaqa-validation-5120", "mrqa_searchqa-validation-2548", "mrqa_squad-validation-3378", "mrqa_naturalquestions-validation-2183", "mrqa_hotpotqa-validation-277", "mrqa_triviaqa-validation-3604", "mrqa_hotpotqa-validation-2398", "mrqa_triviaqa-validation-4225", "mrqa_searchqa-validation-9954"], "EFR": 0.9565217391304348, "Overall": 0.719476222826087}]}