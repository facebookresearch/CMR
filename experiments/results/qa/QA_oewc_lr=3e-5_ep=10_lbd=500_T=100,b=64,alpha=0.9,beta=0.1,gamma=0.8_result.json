{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_oewc_lr=3e-5_ep=10_lbd=500_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', ewc_gamma=1.0, ewc_lambda=500.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_oewc_lr=3e-5_ep=10_lbd=500_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4050, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 0.875, "Overall": 0.8125}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7842046957671958}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.734375, "CSR": 0.7421875, "EFR": 1.0, "Overall": 0.87109375}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Combined Statistical Area", "European Union law", "monophyletic", "\"Provisional Registration\"", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "The General Board of Church and Society, and the United Methodist Women", "successfully preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "The increasing use of technology", "10 years", "Batu", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "breaches of law in protest against international organizations and foreign governments.", "Anglo-Saxon", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "Iberia", "1913", "patient compliance", "20th century", "ambiguity", "\"Bells\" was introduced by Bob Hope in the 1951 movie The Lemon Drop Kid", "a beautiful bride headdress, lace + diamond + white, each with a fine.", "Abraham Lincoln", "\"the Archer\"", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The 10 dog breeds with the best sense of smell - Dogtime  Basset Hound.", "Dardanelles Bosporus... Strait passed Up Young Troy Trojan Empire Hellespont Aegean Sea... Argonauts sailed into the Straits of Dardanus to enter on-up-into the Black Sea", "March, and two others pleaded guilty in 2013 on similar charges.", "City on the south side of the most congested U.S.-Mexico crossing; half the northbound cars wait 90 minutes", "by a then little known sculptor called Edvard Erichsen.", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7295758928571429}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-2717", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.6875, "CSR": 0.7239583333333333, "EFR": 1.0, "Overall": 0.8619791666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "oxyacetylene", "9.6%", "Commander", "macrophages and lymphocytes", "kill", "Duncan", "Sharia", "the Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "\"The Book of Roger\"", "the object's mass", "Africa", "Pierre Bayle", "a strain that caused the Black Death is ancestral to most modern strains of the disease", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Radiohead", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25", "a national transgender figure", "672", "the Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "the company's products are roadworthy", "Himalayan", "murder"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8412878787878788}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-4019", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5372", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.78125, "CSR": 0.73828125, "EFR": 0.9285714285714286, "Overall": 0.8334263392857143}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "between 25-minute episodes", "their captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "the March Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "a global", "force model", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Independence Day: Resurgence", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "the ATP is synthesized there, in position to be used in the dark reactions", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm by Sebastian Junger", "\" Terry and June Medford\"", "architecture", "a bolt", "Common moles", "a complex number raised to the zero", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "a second", "a \"nucleons\"", "James Hoban", "elia Earhart", "1963", "a large cricket bat shaped piece willow ready to be shaped into a bat proper", "The main event was the first - ever 30 - woman Royal Rumble match for a women's championship match at WrestleMania 34", "The United States of America", "Tuesday's iPhone 4S news", "Charles M. Schulz"], "metric_results": {"EM": 0.625, "QA-F1": 0.7067708333333333}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-7708", "mrqa_squad-validation-3752", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.625, "CSR": 0.715625, "EFR": 1.0, "Overall": 0.8578125}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments that override the chlorophylls' green colors", "his last statement", "Pleistocene", "he published his findings first", "Nurses", "time and space", "1951", "Wales", "black earth", "Nederrijn", "opposite end from the mouth", "Refined Hindu and Buddhist sculptures", "the mid-sixties", "Kuz nets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger equation", "90\u00b0 out of phase", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "near the center of the chloroplast", "cotton spinning", "2010", "psilocybin", "\"To SquarePants\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Ed O'Neill", "Kristine Leahy", "1999 Odisha", "Fat Albert", "\" Frontline\"", "d\u00edsir", "Shinola LLC", "modern genetics", "an astronaut", "suspend all aid operations", "british", "independence"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6437094155844156}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-8312", "mrqa_squad-validation-9176", "mrqa_squad-validation-7463", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.546875, "CSR": 0.6875, "EFR": 0.9655172413793104, "Overall": 0.8265086206896552}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "an immunological memory", "Calvin cycle", "his grandfather", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "for voters to line up behind their favoured candidates instead of a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "ABC Circle A", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "velocity", "Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "for 738 days, successfully preventing it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "forte", "1", "Brazil", "McKinney", "Spock", "Solomon", "Blackstar", "geology", "Saturn", "krokos", "Richmond in North Yorkshire", "Passenger Pigeon", "Richard Wagner", "a \"pair of horse-blinders\"", "Debbie Rowe", "Japan", "1973", "Life of Brian", "London", "Jane Thompson", "Southaven", "East Java", "\"Gold Digger\"", "Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6045478418640183}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.3333333333333333, 0.8571428571428571, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.8235294117647058, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-8167", "mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-5859", "mrqa_squad-validation-10287", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_squad-validation-7013", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-426", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.515625, "CSR": 0.6629464285714286, "EFR": 0.967741935483871, "Overall": 0.8153441820276498}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time magazine", "\"Rhine-kilometers\" (Rheinkilometer)", "ten", "150", "North American Aviation", "register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "weakness in school discipline", "Fort Caroline", "Distributed Adaptive Message Block Switching", "at elevated partial pressures", "His lab was torn down in 1904", "interacting and working directly with students", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "he signalled his reinvention as a conservative force", "March Battle of Fort Bull", "swimming-plates", "eleven", "it would undermine the law", "1332", "pharmacists are regulated separately from physicians", "in the south", "Geordie", "special training to ensure that ignition sources are minimized", "US$10 a week", "living a life of decaying rural leisure, with income from the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "surrey", "he built a shed", "paris", "a police car", "dead man's curve", "paris", "the Chetniks", "paris", "paris", "Rookwood Necropolis", "the devil rarely wears Prada", "Edward R. Murrow", "surrey", "paris", "deborah", "paris", "paris", "trophy hunting", "Christopher Marlowe", "Tom Krazit", "all right angles are congruent", "Domenico Colombo", "jostled against Esau", "jonathan Frusciante", "paris", "paris", "eight episode series", "rich master of Thornfield", "World War II", "Hussein's Revolutionary Command Council", "cursory search", "cowardly lion", "March 22"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5489583333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.4, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9109", "mrqa_squad-validation-6324", "mrqa_squad-validation-1404", "mrqa_squad-validation-2434", "mrqa_squad-validation-6773", "mrqa_squad-validation-6408", "mrqa_squad-validation-7961", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_naturalquestions-validation-1372", "mrqa_triviaqa-validation-2045", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.453125, "CSR": 0.63671875, "EFR": 0.9714285714285714, "Overall": 0.8040736607142858}, {"timecode": 8, "before_eval_results": {"predictions": ["a flour mill Boulton & Watt were building", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th century", "counterflow", "pattern recognition receptors", "deforestation", "Glucocorticoids", "The Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Cardiff", "November 1979", "linear", "governmental entities", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "Ethiopia", "Florida State University", "MC Hammer", "Mao Zedong", "arroz con leche", "Hawaii", "Kiwanis International", "the log cabin", "jolly japes", "jane", "the Chateau de Vendeuvre", "the Z", "Rome", "the dizygotic type of twins", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "the Supreme Court", "the Princess Diaries", "the Konabar", "Massachusetts", "larynx", "John Galt", "Arbor Day", "cinnamomum", "the right angle", "Kentucky", "Henry Clay", "the 1870s", "a jane", "1995", "Harry Nicolaides", "Mineola, New York", "\"500 Greatest Songs Since You Were Born\"", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.59375, "QA-F1": 0.662797619047619}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-4357", "mrqa_squad-validation-434", "mrqa_squad-validation-7959", "mrqa_squad-validation-8747", "mrqa_squad-validation-6753", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.59375, "CSR": 0.6319444444444444, "EFR": 1.0, "Overall": 0.8159722222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Dutch law said only people established in the Netherlands could give legal advice", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Fort Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore", "William S. Paley", "anaerobic bacteria", "a form of starch called floridean, which collects into granules outside the rhodoplast, in the cytoplasm of the red alga", "eicosanoids and cytokines", "live", "50-yard line", "captured the mermaid", "1/6", "a DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "divinity of Jesus", "EastEnders", "Wolf Heintz", "highest", "a few drops", "1882", "Mel Jones", "Greenland", "Alastair Cook", "Coton in the Elms", "the frequency f, wavelength \u03bb, or photon energy E", "Mushnik", "the last Ice Age", "Bonnie Plunkett", "2026", "Georgia", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "1984", "4 September 1936", "the forces of Andrew Moray and William Wallace", "the heart", "Pangaea or Pangea", "Have I Told You Lately", "the sinoatrial node", "the fourth quarter of the preceding year", "the vicinity of Fort Riley, Kansas", "to prevent further offense by convincing the offender that their conduct was wrong", "Bob Dylan", "1977", "a judge", "Lynda Carter", "100,000 writes", "a substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Dolph Lundgren", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "American Samoa", "around 3.5 percent of global greenhouse emissions", "God Bless Captain Vere", "a case moves Through the Court System"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6735243336347515}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.06666666666666667, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8372093023255813, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-7459", "mrqa_squad-validation-2451", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-12968"], "SR": 0.578125, "CSR": 0.6265625, "EFR": 0.9259259259259259, "Overall": 0.776244212962963}, {"timecode": 10, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.857421875, "KG": 0.4515625, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter Tesla", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "Yosemite Freeway/Eisenhower Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "\"War of Currents\"", "\"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood", "U.S. economy", "Roger Goodell", "events and festivals", "9 venues", "Adelaide", "once", "Around 200,000 passengers", "\"Kitty Hawk\"", "Nidal Hasan", "the Atlantic Coast Conference", "\"Boston Herald\" Rumor Clinic to combat fascist misinformation.", "Sean", "Consigliere of the Outfit", "Pierce County", "Harry F. Sinclair", "Rick \"the brewer\" Tufts", "December 1974", "2012", "1999", "2004", "Best Sound", "Nelson Rockefeller", "Fort Snelling, Minnesota", "Amy Winehouse", "State House in Augusta", "1970", "1978", "2005", "My Cat from Hell", "Mark Sinclair", "Colonel", "1999", "17", "La Liga", "Buffalo Soldiers", "Tomorrow May Never Come", "Key West", "gastrocnemius", "John Roberts", "repechage", "Carl Johan", "two", "Madonna", "Freddie Mercury", "the Sousa Band"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7404647435897436}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.3333333333333333, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-1491", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-9578", "mrqa_squad-validation-7398", "mrqa_squad-validation-680", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-928", "mrqa_naturalquestions-validation-7608", "mrqa_searchqa-validation-4509"], "SR": 0.640625, "CSR": 0.6278409090909092, "EFR": 1.0, "Overall": 0.7334588068181819}, {"timecode": 11, "before_eval_results": {"predictions": ["pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Bishopsgate", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "huge", "Orange Democratic Movement (ODM)", "Charlesfort", "rapidly evolve and adapt", "Battle of the Restigouche", "Boston", "force of gravity", "head writer and executive producer", "The elephant Man", "a psychologist", "every year", "Sir Arthur", "A Blood Light", "Hong Kong", "ambidevous", "The Batman", "a horse", "Irrawaddy River", "Ed White", "River Hull", "Tet", "The Life of Samuel Johnson", "Copenhagen", "Troy", "a non-governmental organisation focused on human rights with over 3 million members and supporters", "John Gorman", "bison", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "The Enigma code", "The change", "Djokovic", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "The Union Gap", "floating ribs", "The G8 summit is an annual meeting between leaders from eight of the most powerful countries in the world", "golf", "The current Secretary of Homeland Security", "Bee Gees", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "largest and perhaps most sophisticated ring of its kind in U.S. history", "a quark", "krypton", "major celebrity with praise"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6576307960291035}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.11320754716981131, 1.0, 0.0, 0.3529411764705882]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-9255", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.5625, "CSR": 0.6223958333333333, "EFR": 0.8214285714285714, "Overall": 0.6966555059523809}, {"timecode": 12, "before_eval_results": {"predictions": ["convulsions", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Keraites", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "James G. Kiernan", "Omega SA", "September 14, 1877", "third kit or alternate uniform", "Yasir Hussain", "Malayalam movies", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Emilia-Romagna Region", "Buckingham Palace", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "The 2000 PGA Championship", "Revolver", "Jack Nicklaus", "atransformation of the Greek \u03bc\u03b5\u03c4\u03ac\u03bd\u03bf\u03b9\u03b1", "Mussolini", "jenny conan doyle", "off the coast of Dubai", "1918", "butter", "Boston", "rainfall lasting many thousands of years"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7632215007215006}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790", "mrqa_triviaqa-validation-2147"], "SR": 0.71875, "CSR": 0.6298076923076923, "EFR": 1.0, "Overall": 0.7338521634615385}, {"timecode": 13, "before_eval_results": {"predictions": ["riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "\"Guilt implies wrong-doing.", "huge, stiffened cilia that act as teeth", "$20.4 billion", "twelve residential Houses", "Anglo-Saxons", "The Christmas Invasion", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Roman law meaning 'empty land'", "james boswell", "chipmunk", "james boswell", "Melbourne", "Albania", "trout", "The Pilgrim Fathers", "ali boswell", "lacrimal fluid", "Best", "ali", "The Great British Bake Off", "ali boswell", "The Fenn Street School", "ali boswell", "ali boswell", "The Nobel Prize in Literature", "Pakistan", "The Observer", "United States", "Big Fat Gypsy fitness DVD", "ali boswell", "The Andes", "Thor", "The conan doyle", "ali boswell", "ali boswell", "SW19", "Lancashire", "Pacific Ocean", "racing", "Rustle My Davies", "climate", "Charlie Brown", "jane boswell", "avocado", "Black Sea", "glucose", "The history of the Philadelphia Eagles begins in 1933", "The episode typically ends as a cliffhanger showing the first few moments of Sam's next leap", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000", "volcano", "giant slalom", "Serie B league", "Saoirse Ronan"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5248263888888889}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-564", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1330", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-2335", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315", "mrqa_hotpotqa-validation-1687"], "SR": 0.4375, "CSR": 0.6160714285714286, "EFR": 0.9444444444444444, "Overall": 0.7199937996031747}, {"timecode": 14, "before_eval_results": {"predictions": ["Ferncliff Cemetery", "The Ruhr", "Hulu", "best, worst and average case complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "Magnetophon tape recorder", "evaded being drafted into the Austro-Hungarian Army", "Rotterdam", "If there were fewer people", "Charles Dickens", "force", "best teachers", "imperfect", "laysan albatross", "woodlands", "go and safe the best for last", "The National Gallery of Art", "Portland", "ponds", "Solferino", "fiery woman", "a number whose 5th power contains every digit at least once", "turkeys", "henry james grissom", "lionhead", "William Shakespeare", "henry hudson", "1", "smiths", "henry hudson", "with reason, to the achievements of the company's CEO, Rose Marie Bravo", "Prince of Wales", "cocoa butter", "Violent Femmes", "woodlands", "fiery", "laser", "James Fenimore Cooper", "Veep", "fiery", "henry mother stole the ghi", "lead villain", "henry", "Copenhagen", "henry james ritchie", "Jose de San", "Madrid", "fiery", "fiery", "Rocky Mountain", "henry roshi", "fertilization", "world's second most populous country after the People's Republic of China", "Renault", "wood", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "honda", "henry hudson"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4695560515873015}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-1700", "mrqa_squad-validation-1234", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7170"], "SR": 0.40625, "CSR": 0.6020833333333333, "EFR": 0.9736842105263158, "Overall": 0.7230441337719299}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway", "Osama bin Laden", "September 1944", "paramagnetic", "criminal investigations", "complexity classes", "April 1, 1963", "Jamukha", "consultant", "711,988", "12951 / 52 Mumbai Rajdhani Express", "Speaker of the House of Representatives", "Hugo Weaving", "the passing of the year", "The Dursley family", "Brobee", "the nerves and ganglia", "Aman Gandotra", "Daya Jethalal Gada", "Kevin Sumlin", "tree species", "the beginning of the American colonies", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Emma Watson", "Guant\u00e1namo Bay", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Tatsumi", "December 15, 2017", "the Sunni Muslim family", "Magnavox Odyssey", "the Internet", "Christianity", "India", "the nucleus", "The Occupation of the Ruhr", "Moscazzano", "the stems and roots of certain vascular plants", "the beer is only produced for export and is not sold in Germany", "the most recent Super Bowl champions", "in the reverse direction", "San Francisco", "Hal Derwin", "a more clearly defined episcopate", "2007", "0.3 mm", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "(Diego) Maradona", "Akshay Kumar", "Harriet M. Welsch", "(Robert) Zemeckis", "(temperature)", "a bouquet"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5893100822788324}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.19999999999999998, 0.5454545454545454, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.30769230769230765, 0.5, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6716", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-11316", "mrqa_searchqa-validation-8385"], "SR": 0.484375, "CSR": 0.5947265625, "EFR": 0.9696969696969697, "Overall": 0.720775331439394}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex, however, given the uncertainties of fossilization, the localization of fossil types due to lateral changes in habitat (facies change in sedimentary strata), and that not all fossils may be found globally at the same time", "the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague); that transference via fleas in goods was likely to be of marginal significance", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies", "Montmorency", "Nut & Honey Crunch", "Elton John", "beer", "Simon Moores", "a double dip recession", "Corfu", "midrib", "Kinshasa", "8 minutes", "Federal Reserve System", "four red stars", "Polyphemus", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "White spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "Shooting Star", "West Point", "ostrich", "Moby Dick", "William Golding", "the 5th fret on the 6th String", "The Runaways", "Clijsters", "Les Dennis", "the A38", "Nicola Walker", "Virgin", "1949", "Port Talbot", "rain", "\"The best is yet to come.\"", "Nicola Adams", "Sax Rohmer", "data stored on computers or in an organised paper filing system", "May 2010", "Bruce R. Cook", "the third Viscount", "the group must recommend a national policy on the subject that's designed to protect ocean ecology, address climate change and promote sustainable ocean economies", "blew himself up", "N/S", "Aerosmith", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5543856577718806}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false], "QA-F1": [0.6666666666666666, 0.3, 0.0, 1.0, 1.0, 0.05882352941176471, 0.23809523809523808, 0.3157894736842105, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-5177", "mrqa_squad-validation-5001", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-7689", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-1360", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-15652", "mrqa_hotpotqa-validation-4298"], "SR": 0.46875, "CSR": 0.5873161764705883, "EFR": 1.0, "Overall": 0.7253538602941176}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "by cutting the French fortress at Louisbourg off from land-based reinforcements", "journalist", "Seventy percent", "modern hatred of the Jews, cloaking it with the authority of the Reformer", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "the Sierra Freeway", "Mickey Mouse", "Rugby School", "Spain", "may", "Google", "dance", "kolkata brothels", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "sugar", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial Corp.", "red light", "Conan O'Brien", "norway", "(GRMC)", "Nixon", "Other Rooms", "hair", "the Black Forest", "Robert Stempel", "joan", "sepoy", "last", "the 2000s", "submarines", "joan la Pucelie", "pea soup", "Trinidad and Tobago", "Vladimir Nabokov", "sugar", "Tinker Bell", "of", "a laser beam", "Phi Beta Phi Society", "Joel", "the angel of the Lord", "vaud, Switzerland", "Prince Philip", "Athenion", "5.3 million", "pilot", "Lance Cpl. Maria Lauterbach", "Pandora", "Tears for Fears", "dunder Mifflin Paper Company"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5379245448179272}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.11764705882352941, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-10273", "mrqa_squad-validation-4260", "mrqa_squad-validation-2609", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-9551", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-6435"], "SR": 0.4375, "CSR": 0.5789930555555556, "EFR": 1.0, "Overall": 0.7236892361111111}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "near the surface", "Alfred Stevens", "state intervention through taxation", "algebraic", "eight", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "schoolteacher", "Dunlop India Ltd.", "David Anthony O'Leary", "a family member", "Tamil Nadu", "Attorney General and as Lord Chancellor of England", "North Dakota", "fennec", "Norwood, Massachusetts", "1993", "the 10-metre platform event", "liquidambarisella", "(Guaith Caer)", "Flashback", "Kentucky", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas, Anne Bancroft, James Fox, Derek Jacobi, and Sean Penn", "Clark Gable", "evangelical Christian", "paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Shakespeare", "1967", "Guthred", "Centers for Medicare & Medicaid Services (CMS)", "South Australia", "1912", "1912", "the Teatro Carlo Felice", "How to Train Your Dragon", "pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "Monk's", "Sir Ernest Rutherford", "a leg break", "d Dot Cotton (June Brown)", "France", "at least $20 million to $30 million", "(Ulysses S. Grant)", "Virgil Tibbs", "the eventual closure of Guant Bay prison and CIA \"black site\" prisons", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5632710375816994}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8333333333333334, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.5, 0.0, 0.5882352941176471, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-9888", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-2844", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.453125, "CSR": 0.5723684210526316, "EFR": 0.9714285714285714, "Overall": 0.7166500234962406}, {"timecode": 19, "before_eval_results": {"predictions": ["40,000 plant species", "swimming-plates", "MHC I", "Executive Vice President of Football Operations and General Manager", "10", "France", "Time magazine", "Stan Lebar", "Warszawa", "Troggs", "schizophrenia", "Cressida", "Tom Osborne", "Moses", "Pekinese Dog", "Golda Meir", "Fiddler on the Roof", "Monopoly", "In 1963 she said, \"I feel as though I'm suddenly on stage for a part I never rehearsed\"", "Stanislaw Leszczyski", "mask", "Alien", "Tower of London", "reptiles", "Madonna", "onion", "Walter Alston", "Bhutto", "Coca-Cola", "Ja", "Chaillot", "Ibrahim Hannibal", "butter", "grow a beard", "Yev Kassem", "Pyrrhic victory", "Guatemala", "bonds", "the Rue Morgue", "huevos rancheros", "August Strindberg", "Sacher Torte", "South Africa", "freefall", "lovebird", "Leonardo DiCaprio", "flavours", "Daisy Miller", "a calculator", "Give Me Liberty or Give Me Death", "Frank Sinatra", "Sonnets", "South Africa", "ship of allied services", "the Infamy Speech of US President Franklin D. Roosevelt", "Costa Del Sol", "Kidderminster", "Annales de chimie et deikini", "gull-wing doors", "New Jersey Economic Development Authority", "sexual assault", "1994", "state senators", "38"], "metric_results": {"EM": 0.453125, "QA-F1": 0.49609374999999994}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6633", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.453125, "CSR": 0.56640625, "EFR": 1.0, "Overall": 0.721171875}, {"timecode": 20, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.865234375, "KG": 0.45703125, "before_eval_results": {"predictions": ["dendritic cells, keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "The John W. Weeks Bridge", "9th", "the clinical pharmacy movement initially began inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "the Mascarene Plateau", "Pam Anderson Lee", "Golda Meir", "xerophyte", "anions", "Uranus", "George III", "Mike Danger", "Iolani Palace", "Gandalf", "Mungo Park", "squash", "(Bill) Pertwee", "Magnetite", "Sam Mendes", "the R\u00edo de Guadalupe Hidalgo", "Emeril Lagasse", "\u201cShine,\u201d", "Marx", "an ornamental figure or illustration", "four hours", "norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Zephyrus", "Frobisher Bay", "Dumbo", "William Makepeace Thackeray", "in Botany Bay", "Peterborough United", "FC Porto", "albedo", "11", "Washington State", "red", "a neutron star", "Groucho Marx", "Virginia Elliott", "Prince Eddy", "Algerian state", "(Dos Sicilias)", "Barry White", "Gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Robert E. Peary", "Simon & Garfunkel", "(Robert) Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5838541666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3635", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-922", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.515625, "CSR": 0.5639880952380952, "EFR": 0.967741935483871, "Overall": 0.7204085061443932}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "foreclosure", "enterprise application development", "February 6, 2005", "ARPANET", "159", "an Easter egg", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter", "King", "Robert Hooke", "rocks and minerals", "October 2, 2017", "to avoid the inconveniences of a pure barter system", "four", "Laodicean Church", "Lykan", "in the pachytene stage of prophase I of meiosis", "Baltimore", "Lagaan", "Oscar", "a toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Dan Stevens", "Canterbury Tales", "May 19, 2008", "Albert Einstein", "2018", "1992", "restored to life", "Master Christopher Jones", "solve its problem of lack of food self - sufficiency", "Bud '' Bergstein", "Tavares", "in the bloodstream or surrounding tissue", "spain", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "mexican", "31", "the disputed 1824 presidential election", "12", "a form of business network", "for control purposes", "twelve", "Paige O'Hara", "ghee", "\"The Crow\"", "Corinna and seven-time Formula One World Champion Michael Schumacher", "micronutrient-rich", "spain", "top designers", "mexican", "gold", "blackfield Cathedral", "Connally", "liver"], "metric_results": {"EM": 0.4375, "QA-F1": 0.562434074306887}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.8571428571428571, 0.6666666666666666, 0.8235294117647058, 0.0, 0.0, 0.6666666666666666, 0.7368421052631579, 0.0, 0.16666666666666669, 0.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1449", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-14780", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.4375, "CSR": 0.5582386363636364, "EFR": 0.9444444444444444, "Overall": 0.7145991161616162}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "a fowl", "lexicographer", "Islamic Republic", "One Flew Over the Cuckoo's Nest", "mayo-based white sauce from Alabama, and more", "Anne of Cleves", "Harpers Ferry", "a confeitaria Colombo", "the Nun's Priest's Tale", "Versailles", "Target", "meadow grasshopper", "the Tsardom of Russia", "\"The Undisputed World\"", "magnesium", "the Swamp Fox", "the Confederacy", "a German Shepherd", "peanuts", "Xinjiang Autonomous Region", "Parker House Rolls", "Damascus", "Central Missouri", "a hologram", "Thomas Gibson", "the 1906 earthquake", "Prince", "Mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "the Blue Fairy Book", "Diamond Jim Brady", "an axiom", "Princeton", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T.S. Eliot", "Andes Mountains", "Emeralds", "asteroids", "the Nutcracker", "a quake", "Labour Party", "the 1996 World Cup of Hockey", "minced meat ( commonly beef when named cottage pie or lamb when named shepherd's pie )", "Falstaff", "red hair", "Republican", "the Soldier Bear", "Bangor International Airport", "1995", "cancer", "12-hour-plus shifts", "a meeting with the president to discuss her son. Bush refused to see her, and she abandoned her protests in Texas last year."], "metric_results": {"EM": 0.5, "QA-F1": 0.5698660714285715}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-314", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.5, "CSR": 0.5557065217391304, "EFR": 1.0, "Overall": 0.725203804347826}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "seven", "the 21st century", "Mombasa, Kenya", "Tim Kaine", "18", "Adidas", "she was a young skater and desperately wanted to make her mother proud.", "the body of the aircraft", "18th century tapestries", "the United States", "Michigan", "the website on the 24th.", "Two", "Russia", "the Dr. Octopus", "$106,482,500", "Tuesday in Los Angeles.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "three", "tennis", "Toy Story", "Christmas", "90", "involved in an Internet broadband deal with a Chinese firm.", "$75", "a free laundry service", "the Blue Monster course at Doral", "Jeffrey Jamaleldine", "a mid-level Taliban commander", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million", "the former Kansas senator and 1996 GOP presidential nominee", "voters", "the traditional Jewish prayer recited during times of mourning.", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor.", "misdemeanor assault charges", "reiterated the Vatican's policy on condoms for married couples, one of whom has HIV, the virus that causes AIDS.", "people thought this was a small problem.", "Martin Aloysius Culhane", "a model of sustainability", "Kenyan", "more use of nuclear, wind and solar power", "Budapest", "the Isthmus of Corinth", "Gavin DeGraw", "Old Trafford", "Siddhartha", "Bobbi Kristina Brown", "Shayne Ward", "Christina Ricci", "Jermaine Lamarr Cole", "Hong Kong Film Award", "cooking sugar", "Henry Wadsworth", "18th century"], "metric_results": {"EM": 0.359375, "QA-F1": 0.49417340305687496}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.21428571428571427, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 0.9411764705882353, 0.4, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3478260869565218, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.359375, "CSR": 0.5475260416666667, "EFR": 1.0, "Overall": 0.7235677083333334}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "AKS", "Command Module design, workmanship and quality control.", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert A. Iger", "Regional League North", "November 5, 2002", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei", "Minneapolis, Minnesota", "Idisi", "French composer Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Royal Navy rank of Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "Columbia Records.", "Umar S. Israilov", "Derry City F.C.", "Fort Hood, Texas", "Bonny Hills", "London", "1999", "2006", "shooting guard", "Samuel Joel \" Zero\" Mostel", "October 13, 1980", "the Chechen Republic", "House of Commons", "1 May 1926", "Nikolai Morozov", "1968", "Bernd Bertie", "Girl Meets World", "January 15, 1975", "Pansexuality", "Javan leopard", "2,463,431", "Acts of the Apostles", "her castle", "Pyeongchang County, South Korea.", "Britain", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values.", "James Dean", "Glinda", "Persian Gulf"], "metric_results": {"EM": 0.5625, "QA-F1": 0.679946401637578}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 0.8235294117647058, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-3930", "mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-975", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.5625, "CSR": 0.548125, "EFR": 1.0, "Overall": 0.7236874999999999}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "warning the operators, who may then manually suppress the fire", "Northern Rail", "South Korea", "botulism", "golf", "Romania", "Pocahontas", "Matlock", "Washington", "Chile and Argentina", "The Blue Boy", "Morning Scriptures", "liriope", "Creation", "Pennsylvania", "eastern Pyrenees mountains", "Themes, Motifs & Symbols  Chapters III\u2013VI", "Dutch", "Salem witch trials", "Gryffindor", "Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Mase", "Superman: The Movie", "Jenkins", "inland republic in W Africa", "Billy Cox", "Javier Bardem", "Independence Day", "hydrogen", "Jordan", "So Solid Crew", "John Regis", "Magi", "albult", "Arts", "Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Moss", "Miracle Do Happen", "Poland", "Play style", "Guerrero", "albion", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "75 percent of utilities had taken steps to mitigate the Aurora vulnerability", "percipient", "amelia earhart", "Final Cut Pro"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6572916666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-5974", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.59375, "CSR": 0.5498798076923077, "EFR": 1.0, "Overall": 0.7240384615384615}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a vr\u0161ovci family", "between 100,000 and 180,000 light - years", "2016", "the angel Gabriel", "Mel Tillis", "Pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "a judge of the state's High court", "edd Kimber", "The Jewel of the Nile", "Orange Juice", "a photodiode", "September 9, 2010", "Jesse Frederick James Conaway", "a dromedary", "Dan Stevens", "Ben Fransham", "a warrior, Mage, or rogue coming from an elven, human, or dwarfven background", "1979", "June 5, 2017", "auctoritas", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Homer Banks, Carl Hampton and Raymond Jackson", "Jodie Foster", "Barry Watson", "Sanchez Navarro", "growing faster than the rate of economic growth", "1936", "British Columbia, Canada", "New York University", "2007", "2001", "Washington Redskins", "Hebrew Bible", "September 14, 2008", "Consular Report of Birth Abroad for children born to U.S. citizens ( who are also eligible for citizenship )", "Pasek & Paul and the book by Joseph Robinette", "the Chicago metropolitan area", "Francisco Pizarro", "1940", "Norman", "The Rose", "John Smith", "The eighth and final season", "1603", "neutrality", "he cheated on Miley", "banjo", "anabaptists", "chas chandler", "Taylor Swift", "chas chandler", "Michael Crawford", "$22 million", "14-day", "flooding was so fast that the thing flipped over", "pisco", "david", "fiscal"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5934064582272303}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.19047619047619044, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.375, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0909090909090909, 0.4444444444444445, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-474", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236"], "SR": 0.515625, "CSR": 0.5486111111111112, "EFR": 0.967741935483871, "Overall": 0.7173331093189964}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "complexity measures", "Best Supporting Actress", "around 300 patents worldwide", "Anvil firing", "2007", "Larry Richard Drake", "first to recognise the full potential of a \"computing machine\"", "London", "currently Ron Kouchi", "Hanford Site", "Native American", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "Eternity", "coffee cake", "Christies Beach", "eastern", "Arsenal", "Don Bluth and Gary Goldman", "torpedoes", "1969 until 1974", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defender", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube channel", "Daniel Andre Sturridge", "USS Essex (CV-9)", "Ron Cowen and Daniel Lipman", "1991's \"The Changing Scottish Landscape\"", "Captain while retaining the substantive rank of Commodore", "Giuseppe Verdi", "Andrzej Go\u0142ota and Tomasz Adamek", "Russell T Davies", "Geraldine Page", "Manchester", "3,000", "Umberto II", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan", "Jeux", "Justice of the Peace", "John Lennon", "The International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "11 February 2012", "changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music", "golden anniversary", "Australian", "stroke", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "Amanda Knox's aunt", "there could be 100,000 snakes in the Everglades, but no one knows for sure.", "brandy", "Sgt. Pepper", "North Carolina"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6111942527958153}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, false, false, true], "QA-F1": [0.2222222222222222, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 0.8, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6875000000000001, 1.0, 0.14285714285714288, 0.0, 0.25, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-1672", "mrqa_squad-validation-1546", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3907", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.484375, "CSR": 0.5463169642857143, "EFR": 1.0, "Overall": 0.7233258928571429}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "common flagellated protists that contain chloroplasts derived from a green alga", "Juliet", "Adidas", "Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "around 8 p.m. local time Thursday", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "A witness", "Adriano", "he eventually gave up 70 percent of his father-in-law's farm", "183", "American Civil Liberties Union", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "40", "Aldgate East", "137", "54-year-old", "Congressional auditors", "Jacob", "South Africa", "Markland Locks and Dam", "more than 4,000", "Baja California Language College in Ensenada, Mexico", "provided Syria and Iraq 500 cubic meters of water a second", "the Catholic League", "August 19, 2007", "10 years", "all three pleaded not guilty", "Japanese officials", "her daughter and granddaughter attend Oprah Winfrey's school in South Africa considers the talk-show host heaven-sent, despite allegations of abuse being investigated there.", "consumer confidence", "political and religious", "six", "nearly 28 years", "July 18, 1994", "Dan Brown", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "killed 17 U.S. sailors.", "along the Chao Phraya River and its many canals.", "two", "antihistamine and an epinephrine auto-injector", "more than 4,000", "he discussed foreplay, sexual conquests and how he picks up women", "Jean F Kernel", "around 10 : 30am", "1439", "tide-wise", "Christian Wulff", "Harnoncourt", "general secretary", "the George Washington Bridge", "Johns Creek", "junk", "Aristotle", "fish"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6211769217565588}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.6666666666666666, 0.5454545454545454, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.30769230769230765, 1.0, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.07142857142857144, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.3870967741935484, 0.0, 0.0, 1.0, 1.0, 0.5, 0.08333333333333334, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-2733", "mrqa_searchqa-validation-4780"], "SR": 0.484375, "CSR": 0.5441810344827587, "EFR": 0.9696969696969697, "Overall": 0.7168381008359457}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "July 7, 2015", "roseicollis", "Chicago", "monk seal", "Wilhelm", "quaerere", "Take Me Out to the Ballgame", "altoberfest", "\"What hath God wrought\"", "New Zealand", "Saint Elmo's Fire", "the alderman", "H. G. Wells", "Wayne", "illegible", "Scrabble", "Mussolini", "Valkyries", "rain", "bach", "Jodie Foster was a victim, gang-rape by men in a bar.", "Elysian Fields", "Firing Line", "Thomas Edison", "Manhattan Project", "Charles", "US Steel", "Enchanted", "the Liberty Bell", "mov", "Autobahn", "Destiny's Child", "Lord Byron", "a robin", "cortisone", "Margot Fonteyn", "a Coral reef fish", "McMillan & wife", "(Whizzer) White", "77 Sunset Strip", "Galileo Galilei", "Existentialism", "John Donne", "Nauru", "Karen", "murder", "Charles Lindbergh", "a queen", "neurons", "a candidate state", "James W. Marshall", "a single, implicitly structured data item", "South Korea", "slow", "M*A*S*H TV series", "Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "The e-mails", "HPV"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6427398989898989}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.8333333333333333, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-362", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-7230", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-14999", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-1372"], "SR": 0.53125, "CSR": 0.54375, "EFR": 1.0, "Overall": 0.7228125000000001}, {"timecode": 30, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.873046875, "KG": 0.4984375, "before_eval_results": {"predictions": ["ten minutes", "1892", "motivated students", "Nikolai Trubetzkoy", "June 1925", "bushwhackers", "British", "Santiago del Estero", "Baudot code", "Amtrak's \"Silver Service\"", "DTM", "Switzerland", "Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres from Adelaide station", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "Avoca Lodge", "Atlanta, Georgia", "Tampa Bay Devil Rays", "Scunthorpe", "2004", "Donald Sutherland", "Towards the Sun", "Cortina d'Ampezzo", "Angus Brayshaw", "an artist manager or a film or television producer", "Sufism", "January 30, 1930", "Sulla", "the Female Socceroos", "Jaguar Land Rover", "tempo", "Milk Barn Animation", "McLaren-Honda", "Timothy Dowling", "London", "Jane", "Tim Burton", "Otto Hahn and Meitner", "AMC Entertainment Holdings, Inc.", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins Sr.", "Jude", "twenty-three", "Gararish", "A. R. Rahman", "Whoopi Goldberg", "September 8, 2017", "volcanic activity", "Burbank, California", "a horse's life is often difficult.", "Heisenberg", "the White Sea Canal", "workers couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues.", "Eintracht Frankfurt", "Republican", "a poodle", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6213999542124542}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.3333333333333333, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.6666666666666666, 0.4, 0.3571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-286", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-6433", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032"], "SR": 0.546875, "CSR": 0.5438508064516129, "EFR": 1.0, "Overall": 0.7295514112903226}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Giuseppina Tuissi", "The Super Aguri F1 SA07", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics) and \"Martian Manhunter\"", "film and short novels", "Carson City", "The Nick Cannon Show", "\"Mickey's Christmas Carol\"", "age ten", "Alpine, New Jersey", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Frederick Alexander Lindemann,", "Rawhide", "astronomer and composer of German and Czech-Jewish origin", "Don \"Don\" DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Whitesnake", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Alison Swift", "Miller Brewing", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Clark County, Nevada", "Tampa Bay Storm", "Jango Fett", "High Court of Admiralty", "\"An All-Colored Vaudeville Show\"", "Lutheranism", "Lucy Muringo Gichuhi", "Valley Falls", "dice", "Nicholas \" Nick\" Offerman", "Dutch", "JackScanlon", "Leonard Bernstein", "19", "France", "kihydroxyketone", "amanuensis", "eight", "a nuclear weapon", "2005", "Beastie Boys", "Madison", "a car"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6450396825396826}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.16666666666666666, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-4489", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1534", "mrqa_triviaqa-validation-1394", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.515625, "CSR": 0.54296875, "EFR": 0.967741935483871, "Overall": 0.7229233870967742}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Colonel Tom Parker", "LSD", "King Henry I", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "Bubba", "football", "multiplayer online", "fondu", "Greece", "1932", "Andrei Rublev", "davies", "the Boston Marathon", "Carl Smith", "Humble Pie", "Jorge Lorenzo", "Walt Disney", "checkers", "Les Dawson", "Arthur, Prince of Wales", "the Grail", "Ronald Reagan", "Terry Bates", "temperature extremes", "the Coney Island Old Island Pier", "Hammer", "the liver", "Guildford Dudley", "the Amoco Cadiz", "Paul Keating", "David", "His Holiness", "12", "Cornell University", "Flybe", "Altamont Speedway Free Festival", "a fat like oil or lard", "Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "the senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives", "2006", "Central Avenue", "middleweight", "Jacob Zuma", "work is the hardest and least rewarding work", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "marsh"], "metric_results": {"EM": 0.625, "QA-F1": 0.655016447368421}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-6509", "mrqa_triviaqa-validation-3007", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.625, "CSR": 0.5454545454545454, "EFR": 0.9583333333333334, "Overall": 0.7215388257575758}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "diversity", "Ennis", "Stratfor's website", "the last few months,", "Jaime Andrade", "1 percent", "girls", "possible victims of physical and sexual abuse.", "the island's dining scene", "gasoline", "joesworld.", "\"AS IS/ HERE IS\"", "he won two Emmys for work on the 'Columbo' series starring Peter Falk.", "ice jam", "ozzy", "abduction of minors.", "vivian", "J. Crew.", "vivian liberto", "Florida", "Bhola", "Clifford Harris", "lower level", "nirvana", "Dr. Conrad Murray", "james polis", "it was based on the latter.", "he won it with unparalleled fundraising and an overwhelming ground game.", "between the ages of 14 to 17.", "Lana Clarkson", "misdemeanor", "1.2 million", "100,000", "Heshmatollah Attarzadeh", "crossfire by insurgent small arms fire,", "death", "if she would try to travel to Japan for summer vacation.", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "when people gathered outside as the conference in the building ended.", "shelling", "in the mouth.", "Atlantic Ocean", "movahedi", "Nepal", "Jiverly Wong,", "sexual assault with a minor", "the carrousel du Louvre", "September 21.", "Wilderness", "Aspirin", "The Yongzheng Emperor", "Narendra Modi", "steve davis", "75", "Justin Bieber", "scholarly analysis", "Randall Boggs", "Mick Jackson", "West Virginia", "Gary Oldman", "Paris"], "metric_results": {"EM": 0.5, "QA-F1": 0.5615544871794871}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5128205128205129, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.56, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-9569", "mrqa_triviaqa-validation-7244", "mrqa_hotpotqa-validation-4112", "mrqa_searchqa-validation-44"], "SR": 0.5, "CSR": 0.5441176470588236, "EFR": 0.96875, "Overall": 0.7233547794117647}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins,", "Robert A. Heinlein", "Mumbai", "Indiana", "animals,", "\"The Moonwalk, time all but stopped.", "Laos", "Vislor Turlough", "Westminster Abbey", "Battle of Agincourt", "white spirit", "King George III", "Kent", "Miss Prism", "Diptera", "a turkey", "transuranic", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "United Kingdom", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City", "Moscow", "Caracas", "Oil of Olay", "hair", "collage", "Bathsheba", "John Carpenter", "DitaVon Teese", "a collapsible support assembly", "Republican", "Argentina", "French", "dennis taylor", "the internal kidney structures", "a rabbit", "Rocky Marciano", "The Benedictine Order", "M69. Coventry", "tientsin", "Jack Klugman", "four", "2002 -- 2003 season", "2018", "a lightning strike", "Danny Glover", "Trey Parker and Matt Stone", "140 to 219", "Hundreds", "Democrats", "31 meters (102 feet)", "the dennis Galahad", "Sacramento, California", "The Ryukyu Islands"], "metric_results": {"EM": 0.625, "QA-F1": 0.7088541666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5333333333333333, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3995", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-10490", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.625, "CSR": 0.5464285714285715, "EFR": 1.0, "Overall": 0.7300669642857144}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "daiquiri", "calvary", "armadillos", "joe mercer", "Danielle Steel", "Absalom", "joe mercer", "The Goonies", "nellie", "quito", "Charenton", "wine", "Alyssa Milano", "bites a dog", "Star Spangled Banner", "The Rolling Stones", "London", "a knight", "Benjamin Franklin", "Bob Dylan", "a urinal", "moon landing", "Portugal", "Cadillac", "Matt Damon", "mama pynchon", "shalom", "white", "davie lemfour", "dictum", "Easton", "Scrabble", "joe mercer", "davie mercer", "an incubation chamber", "peter Sinclair", "Stephen Vincent Bent", "Brooke Ellen Bollea", "a war", "Nancy Sinatra", "David", "wine noir", "Robert Lowell", "forgo", "Richmond", "checks", "Amy Tan", "Florence", "pithos", "Grenada", "the Mahalangur Himal sub-range of the Himalayas", "Kusha", "Heroes and Villains", "sea", "argon", "1", "2015", "October 20, 2017,", "Columbus", "Gustav's top winds weakened to 110 mph,", "piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5887152777777778}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_newsqa-validation-2307"], "SR": 0.546875, "CSR": 0.5464409722222222, "EFR": 1.0, "Overall": 0.7300694444444444}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "(Pfc. Bowe) Bergdahl", "\"As I was walking back through the crowd it was the word on everyone's lips,\"", "a Columbian mammoth", "the Symbionese Liberation Army", "steamboat", "recall", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile on its launch pad,", "75", "prisoners", "homeless veterans and their entire family,\"", "CNN/Opinion Research Corporation", "Kingdom City", "an engineering and construction company with a vast personal fortune", "Ku Klux Klan", "President Felipe Calderon", "137", "3-3", "Dancing With the Stars", "love and loss.", "Michael Jackson", "\"a striking blow to due process and the rule of law\"", "Venezuela", "their business books", "the Nazi war crimes suspect", "a number of calls,", "Mandi Hamlin", "Iraq", "department assessment that suggested returning combat veterans could be recruited by right-wing extremist groups.", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole", "you can demonstrate they have been satisfactorily treated for at least 12 months.", "Tennessee", "contestants who look more like the viewing public,\"", "Malawi", "the Somali coast", "the skull", "six", "people of Gaza", "eight in 10", "one-shot victory in the Bob Hope Classic", "the Muslim north of Sudan", "(Jeannie) Longo-Ciprelli", "Clifford Harris,", "Kyra and Violet,", "Susan Boyle", "Florida", "UNICEF", "the United States, NATO member states, Russia and India", "27-year-old", "45", "(Carrie) Wakefield", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "Dick & Jane"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5682043650793651}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-640", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_hotpotqa-validation-2625", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010"], "SR": 0.484375, "CSR": 0.5447635135135135, "EFR": 1.0, "Overall": 0.7297339527027027}, {"timecode": 37, "before_eval_results": {"predictions": ["over $20 billion.", "the Veneto region of Northern Italy", "Preston, Lancashire, UK", "Daniel Auteuil", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "Fredric John Warburg", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "a chronological collection of critical quotations about William Shakespeare", "Terrence Jones", "fifth studio album, \"S&M\"", "one", "Evey, a young, working-class woman caught up in V's mission,", "O", "The Grandmaster", "highland regions of Scotland", "1960s", "Nobel Prize in Physics", "Russian Empire", "Cold Spring", "Hilary Duff", "Ogallala", "October 21, 2016,", "fifth studio album, \"My Beautiful Dark Twisted Fantasy\"", "Everything Is wrong", "Massapequa", "1988", "Dan Bilzerian", "Spitsbergen", "1967", "residential", "Macbeth", "band director", "1875", "$10\u201320 million", "Mandarin", "Uncle Fester,", "March", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "\" Wah - Wah ''", "The First Battle of Bull Run", "Alison Krauss", "Graham Henry", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars in", "Diamond", "Simon Legree", "Sideways", "pindar"], "metric_results": {"EM": 0.53125, "QA-F1": 0.633264652014652}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.7692307692307693, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_newsqa-validation-1312", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-8753"], "SR": 0.53125, "CSR": 0.544407894736842, "EFR": 1.0, "Overall": 0.7296628289473684}, {"timecode": 38, "before_eval_results": {"predictions": ["Turing machines", "Nepal", "Wang Chung", "Panama", "an gastropod", "Bhumibol Adulyadej", "Mary Kies", "the gizzard", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "dressage", "Benito Mussolini", "Santa Ana winds", "Fort Leavenworth", "INXS", "\"Longitudes and Attitudes: The World is Flat: A Brief History of the 21st Century\"", "an oxlike antelopes", "Extra-Terrestrial Intelligence", "Arthur", "Pablo Picasso", "Clara Barton", "Nine to Five", "an snake", "an elk", "Winnipeg", "Anastasio Somoza", "Arthur Miller", "Princess Margaret, Countess of Snowdon", "1937", "an algae", "feminism", "the Space Coast Convention Center", "the gallbladder", "the cousin of his wife, Mattie Silver", "midway", "Liechtenstein", "Custer", "the Laodicean Church", "salt", "Gloria Steinem", "Catherine de Medici", "Tonga", "Minos", "Gulliver", "rum", "SeaWorld", "an \"coup de grce\"", "Tyra Banks", "Richard Gephardt", "the Funky Chicken Hostel", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "attack on Pearl Harbor", "positive lens", "an inch", "American", "Province of Canterbury", "Valdosta", "Northern Rhodesia", "his son-in-law Cleve Landsberg,", "Goa", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to", "4"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6026785714285714}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-15746", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-7288", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-3579"], "SR": 0.53125, "CSR": 0.5440705128205128, "EFR": 1.0, "Overall": 0.7295953525641026}, {"timecode": 39, "before_eval_results": {"predictions": ["over the age of 18", "Nalini Negi", "Sundays", "The 1980 Summer Olympics", "the IB Primary Years Program", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing", "Nicole DuPort", "Brian Johnson", "Palmer Williams Jr.", "After World War I", "prospective studies", "Michigan State Spartans", "Franklin and Wake counties", "60", "RMS Titanic", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "chili con carne", "6 March 1983", "David Kaye", "James Arthur", "James Watson and Francis Crick", "Antarctica", "during the American Civil War", "Thomas Middleditch", "slavery", "Sir Ernest Rutherford", "Buddhist", "1832", "parthenogenesis", "on the two tablets", "Buffalo Bill", "$19.8 trillion", "Sleeping with the Past", "boy", "in the 1820s", "Soviet Union", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "2013", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` central '' or `` middle ''", "metamorphic rock", "Carmen", "waterfowl", "glass", "Rikki Farr's", "Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "the Gaslight Theater", "Dragnet", "Johnny Cash", "Louis Rukeyser", "Matthew Abraham \"Matt\" Groening"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6326082045721515}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.5, 0.0, 1.0, 1.0, 0.06451612903225806, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.2857142857142857, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-6940", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-1911", "mrqa_searchqa-validation-14218", "mrqa_hotpotqa-validation-5481"], "SR": 0.546875, "CSR": 0.544140625, "EFR": 0.9655172413793104, "Overall": 0.7227128232758621}, {"timecode": 40, "UKR": 0.646484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.7734375, "KG": 0.47578125, "before_eval_results": {"predictions": ["the architect or engineer", "Naples", "malaria", "Jefferson Davis", "Rubik's Cube", "a kettledrum", "the cake", "let (department manager) go, but can't do it until I have someone to replace him.", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "the National Motorcycle Museum", "One Hundred Years of Solitude", "Trotsky", "Aziraphale", "Wodehouse", "Sardinia", "the aetherial sphere", "(William) Pitt the Younger", "Popcorn", "Madonna", "weight class", "the yo-yo", "Charlotte", "\"There Is Nothin' Like A Dame\"", "Scotland", "(anaplasma, ehrlichia)", "defensive", "Columbine Columbine", "Italy", "kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "the Fantastical World Around You", "a petition signed by a certain... in 1891, permitting a certain number of citizens to make a request to amend a", "Chicago", "the Great Pyramid", "Herod", "Alaska", "more likely to be killed by a terrorist", "Asia", "anaphylaxis", "\"All children, except one, grow up\"", "Kuwait", "Quiz", "The Day of the Locust", "CONTINENTAL DRIFTING: Villarica", "Emilio Estevez", "The Call of the Wild", "Gibraltar", "the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Sizzurp,\"", "Larry Eustachy,", "Isabella II", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6117340686274509}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1729", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-16454", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-13067", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-5600", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-5758", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.515625, "CSR": 0.5434451219512195, "EFR": 1.0, "Overall": 0.6878296493902438}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "aurochs", "Israel", "Prince Rainier", "Walden", "Fred Astaire", "Humphrey Bogart", "(HONDA)", "Alan Bartlett Shepard Jr.", "(Antoine) Lavoisier", "(Gollancz)", "jacks", "Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "Ironside", "Aristotle", "(Prunella Scales)", "South Sudan", "Tuesday", "Moldova", "Lincoln", "along the east coast", "Antoine Lavoisier", "NOW Magazine", "Tuscany", "the Alamo", "Beaujolais Nouveau", "Edmund Cartwright", "Der Stern", "(Adam) van Noort", "the popes", "kautta", "(Gordon) Ramsay", "Wisconsin", "John Barbirolli", "Eton College", "Harrods", "Charles Dickens", "Ted Hankey", "General Joseph W. Stilwell", "the lamina", "sternum", "Portuguese", "mexico", "Greece", "Ed Miliband", "commitment", "an iron lung", "The Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Robin", "the Distinguished Service Cross", "Indian classical", "2013", "six", "\"an eye for an eye,\"", "Arabic, French and English", "Schwalbe", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6805803571428571}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-162", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-3241", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.609375, "CSR": 0.5450148809523809, "EFR": 1.0, "Overall": 0.6881436011904761}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman", "George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Banquo's", "Pakistan", "October 1, 2015", "digital transmission", "Isaiah Amir Mustafa", "Commander in Chief of the United States Armed Forces", "Paracelsus", "John C. Reilly", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "keratinocytes", "1898", "1770 BC", "360", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "216", "Justin Bieber", "the Indian Ocean", "ideology", "Gatiman express", "edible - nest swiftlets", "Andrew Garfield", "the 90s", "Gibraltar", "electrons", "cut off close by the hip, and under the left shoulder", "Lulu", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Tokyo for the 2020 Summer Olympics", "1972", "Virgil Tibbs", "Ethel Merman", "1961", "passwords", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Kate Flannery", "Lake Wales", "1560s", "Johannes Gutenberg", "Wichita", "Tina Turner", "Gianfranco Ferre", "Henry John Kaiser", "peter", "SARS", "tax credits", "her doctor", "23 million square meters (248 million square feet)", "neon", "the Prisoner of Azkaban", "the ark of acacia", "Basilan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5868562809739281}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.8571428571428571, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-10386", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_newsqa-validation-748", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.453125, "CSR": 0.5428779069767442, "EFR": 0.9714285714285714, "Overall": 0.6820019206810631}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a latte", "Sheffield United", "Lync", "Wat Tyler", "john john conner", "Scotland", "the Sun", "James Hogg", "Texas", "leeds rhinos", "Pears soap", "bulgaria", "Louis XVI", "john john connords", "fifty-six", "Uranus", "Plato", "the chord", "Chubby Checker", "Separate Tables", "Wilson", "luster", "Stephen of Blois", "the Antitrust Documents Group", "eukharistos", "Cubs", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "a tittle", "E. T. A. Hoffmann", "the Republic of Upper Volta", "Edvard Grieg", "an elephant", "the Creel Committee", "New Zealand", "Mendip Hills", "Street Art and Graffiti", "Jane Austen", "God bless America, My home sweet home", "Trade Mark Registration Act 1875", "boxing", "Benjamin Disraeli, 1st Earl of Beaconsfield", "The Jungle Book", "The Great Leap", "Jan van Eyck", "Rabin", "Shania Twain", "John Nash", "redox", "`` It ain't over'til it's over ''", "used as a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "Elvis' Christmas Album", "to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "Robert Park", "plus or minus 3", "Cairo", "Jackson Pollock", "moose", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5568166208791209}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.6, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.19047619047619047, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-1211", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1305", "mrqa_searchqa-validation-15709", "mrqa_newsqa-validation-1551"], "SR": 0.46875, "CSR": 0.5411931818181819, "EFR": 1.0, "Overall": 0.6873792613636363}, {"timecode": 44, "before_eval_results": {"predictions": ["1994\u20131999", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314 Premier League", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "1995 to 2012", "George Clooney, Thekla Reuten, Violante Placido, Irina Bj\u00f6rklund, and Paolo Bonacelli", "Rothschild", "China", "smith", "Gwyneth Paltrow, Ewan McGregor, Olivia Munn, Paul Bettany and Jeff Goldblum", "alternate uniform", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "\"The Sun on Sunday\"", "Christopher Tin", "Saint Louis", "Sullenberger III", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland, at the north-western corner of the county of Ayrshire", "first and only U.S. born world grand prix champion", "2015", "19th", "smith", "Lev Ivanovich Yashin", "Carrefour", "John Monash", "Benjam\u00edn Arellano F\u00e9lix", "Bank of China Tower", "first Spanish conquistadors in the region of North America now known as Texas", "Battle of Etowah", "12", "Margiana", "Heathrow", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County, Florida, United States", "honey bees", "squash", "smith", "soybeans", "Nineteen political prisoners", "How I Met Your Mother", "collapsed ConAgra Foods plant", "Mount Everest", "I.M. Pei", "Florence Nightingale", "the Citadel Hill and the Chain"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5992461046918768}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5714285714285715, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3060", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3252", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-16341"], "SR": 0.515625, "CSR": 0.540625, "EFR": 0.967741935483871, "Overall": 0.6808140120967742}, {"timecode": 45, "before_eval_results": {"predictions": ["pamphlets on Islam", "chicago", "Jesse of Bethlehem", "Oklahoma City", "insulin", "chico", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "Country Life Magazine", "The Lion King", "perfume empire", "Wyoming", "Benedictus", "chicago", "Javier Bardem", "1", "Lee Harvey Oswald", "virtual", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "chicago", "Rhine River", "Confucius", "Japan", "stewardi(i)", "chicago", "Christian Dior", "Phoenicia", "bobby Moore", "chicago", "Jerez de la Frontera", "ch\u0101\u2032b\u014d", "1864-65", "FC Porto", "chicago", "argument form", "chicago", "Portuguese", "Madagascar", "Helsinki", "Monopoly", "myxomatosis", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Mercedes -Benz GL - Class", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "1957", "district Attorney Larry Abrahamson", "South Africa", "teeth", "ABBA", "Phoenician Civilization", "Tom Coughlin"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5360215053763441}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false], "QA-F1": [0.5, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8387096774193548, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-4230", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-4976", "mrqa_triviaqa-validation-3330", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-5453", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-4760", "mrqa_hotpotqa-validation-3195"], "SR": 0.421875, "CSR": 0.5380434782608696, "EFR": 0.972972972972973, "Overall": 0.6813439152467685}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby School", "a modem", "Clinton", "George Herbert Walker Bush", "Penn State", "Luxor", "Vladimir Putin", "leviathan", "Mending Wall", "wombat", "our geology", "thunder", "Josephine Beauharnais", "The Three Musketeers", "the iTunes Store", "Neptune", "Annie", "The Comedy of Humours", "KLM", "Captain Marvel", "movie", "the retina", "a goat", "Planet of the Apes", "a knish", "In the 19th century, William Makepeace Thackeray", "the Reading Railroad", "Leon Trotsky", "cheese", "the Justice Department", "Melissa Etheridge", "Ignace Jan Paderewski", "jesse", "Charles Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "Rikki-Tikki Tavi", "mutual fund", "polygons", "Nashville", "lm", "a ferry", "New York Times", "The Oresteia", "clowns", "ewin Rommel", "Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "In 1946", "george terrier", "alligators", "Hindi", "London", "John Snow", "Ghana's Asamoah Gyan", "soldiers had not gone anywhere they were not permitted to be.", "Pakistan intelligence institutions and its army", "Los Angeles.", "1955"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6141369047619047}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-2185", "mrqa_searchqa-validation-1294", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1277"], "SR": 0.546875, "CSR": 0.5382313829787234, "EFR": 0.9655172413793104, "Overall": 0.6798903498716067}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "The Lord Mayor", "Shel Silverstein", "beers", "a streetcar", "Liverpool", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "George Harrison", "The Star Wars Effect", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "rehab", "Chad", "bicentennial", "midway", "George Gershwin", "alpacas", "Atlantic Ocean", "heredity", "The Bicentennial Man", "the rod", "a heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fulgencio Batista", "The Indianapolis 500", "the Twist", "(Rabbie) Burns", "cuckoos", "London", "red-orange, spotted beetles", "Joan of Arc", "palindrome", "quid", "Rob Van Winkle", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "the Ganges", "Thomas Mann", "Samuel, Kings & Chronicles", "Sing Sing", "Rajendra Prasad", "1945", "an edited version", "Bedfordshire", "Charles V", "The Lion, The Witch", "Lord's Resistance Army", "South Asia and the Middle East", "Netflix", "The government late Tuesday afternoon announced it would file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6893393065268065}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-10425", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.578125, "CSR": 0.5390625, "EFR": 0.9629629629629629, "Overall": 0.6795457175925925}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "the Dorchester Hotel", "Richard Burton", "(Alex) Cross", "Incredible", "a cheetah", "Charlie Brown", "Odin", "Japan", "a Sea-Monkeys", "a daffodils", "Dragnet", "Neil Simon", "Voyager 2", "a gull", "the Nez Perce", "Eva Peron", "a sackcloth", "the Hawkeyes", "Ivica Zubac", "Swiffer", "Huckleberry Hound", "Austria", "Jason Bourne", "Peru", "The Trojan Horse", "an atoll", "the Colosseum of Pompey", "Cambodia", "Dr. Hook and the Medicine Show", "Songs of Innocence", "Uvula", "a Sacraments of the", "Jacob", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "Zambezi", "serving the tea", "Deuteronomy", "The Police", "Jamestown", "Wild Cherry", "Robert Ford", "St. Francis of Assisi", "a Lemon meringue", "(Hugh) Williams", "Tarzan and Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stggle Larsson", "The Merchant of Venice", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.625, "QA-F1": 0.696606691919192}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-8217", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-4739", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-14611", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-13118", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-15335", "mrqa_triviaqa-validation-5851", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.625, "CSR": 0.5408163265306123, "EFR": 1.0, "Overall": 0.6873038903061224}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "The Stonemason's Yard", "Carmen Zapata", "The Shetland Islands", "Jerusalem Center for Jewish-Christian Relations", "International", "fourteen", "the kidneys", "apples", "Athina Onassis", "nadal", "Apollo 11", "five", "Kirk Douglas", "John Ford", "tin", "Longchamp", "Nihon-koku, \"State of Japan\"", "Ford", "joey", "Maine", "USS Missouri", "Pyrenees", "basketball", "Janis Joplin", "Mr. Stringer", "basketball", "South Africa", "JMoney41998", "Ed Miliband", "Scotland", "an aeoline", "Margaret Mitchell", "Burkinab\u00e9", "martina Navratilova", "40", "75 or older", "Winston Churchill", "John Masefield", "Rio de Janeiro", "\"Party of God\"", "Bengali", "Claire", "Guatemala", "Carousel", "Leicester", "Bobby Tambling", "radish", "martas rimmer", "Downton Abbey", "knives", "Garfield Sobers", "Herman Hollerith", "The 14th game of this series", "Golden Gate National Recreation Area", "Forbes", "English Electric Canberra", "Mark Fields", "pattern matching", "one of 10 gunmen who attacked several targets in Mumbai", "a rat", "tapas", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6866003787878787}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.4, 1.0, 1.0, 0.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-3654", "mrqa_triviaqa-validation-4967", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-1657", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_hotpotqa-validation-3207"], "SR": 0.59375, "CSR": 0.541875, "EFR": 0.9615384615384616, "Overall": 0.6798233173076923}, {"timecode": 50, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.83203125, "KG": 0.4546875, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "1980s", "Nodar Kumaritashvili", "Carpenter proceeded to advance to the million - dollar question without using any lifelines", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "The ladies'single figure skating competition of the 2018 Winter Olympics was held at the Gangneung Ice Arena", "The Walking Dead", "in Koine Greek : apokalypsis", "1962", "non-ferrous", "the state sector", "The sacroiliac joint or SI joint ( SIJ )", "Sunni Muslim family", "after World War II", "Cheshire", "The Massachusetts Compromise", "L.K. Advani", "a reserve unit in accordance with the military's needs", "Jason Marsden", "Louis Le Vau", "Ashrita Furman", "st. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "early 1960s", "602", "the beginning", "2013", "Diego Tinoco", "When is a perfect monotone function of the other", "January 2004", "Glenn Close", "cefal\u00f9, Caen, Durham, and elsewhere", "Johannes Gutenberg", "Dan Stevens", "Alex Ryan", "Kathleen Erin Walsh", "Carolyn Sue Jones", "De pictura", "the Formless All - pervasive Reality, made of stone, metal, or clay", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "birch", "push the food down the esophagus", "dolly parton", "westminster", "westmorland", "San Diego Stadium", "Black Abbots", "Prince Amedeo, 5th Duke of Aosta", "mental health", "Suba Kampong township", "for a full facial transplant since 2004.", "laryngitis", "pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6718565577940578}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.0, 0.14814814814814814, 0.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2294"], "SR": 0.59375, "CSR": 0.5428921568627452, "EFR": 0.9615384615384616, "Overall": 0.6992454986802413}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander", "the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "Scottish post-punk band Orange Juice", "1837", "The Vamps, Conor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "22 November 1914", "the Washington Wiz", "May 26, 2017", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "near Camarillo, California", "birmingham nambahu", "2018 and 2019", "in the ark of the covenant", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs", "15 February 1998", "Los Lonely Boys", "Thomas Alva Edison", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas '', cf. English Andrew", "`` Mirror Image ''", "two senators, regardless of its population, serving staggered terms of six years", "most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "1623", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes, OLD, PLH ( born August 26, 1954 ), nicknamed the Magician and Bata", "Jim Carrey", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee", "Herman Hollerith", "the ulnar nerve", "December 18, 2017", "Brooklyn, New York", "( 2008, 2009 )", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular", "16 May 2007", "chairman ( more usually now called the `` chair '' or `` chairperson '' ), who holds whatever title is specified in the bylaws or articles of association", "Germany", "an adopted daughter of Thanos", "Darlene Cates", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Bennett Cerf", "Matt Monro", "Fu#$@d", "birmingham", "cppola", "blood", "House of Fraser", "Venice", "Hyundai Steel", "birmingham", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5548377696447042}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.9090909090909091, 0.5714285714285715, 0.1, 0.5714285714285715, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 0.0, 0.0, 0.5185185185185185, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.1111111111111111, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 0.375, 0.0, 0.7999999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7894736842105263, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.390625, "CSR": 0.5399639423076923, "EFR": 0.8717948717948718, "Overall": 0.6807111378205127}, {"timecode": 52, "before_eval_results": {"predictions": ["Vilnius", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "UVM Agriculture Department and the Agricultural Experiment Station", "Pacific War", "1949", "The Dark Tower", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "S6", "Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "Bill Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "Macbeth", "Brad Silberling", "1973", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "seventh generation", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "the sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca", "Robert Moses", "Godiva Chocolatier", "Premier League club Manchester United", "\"Futurama\"", "Manhattan Project", "Russia", "Lush Ltd.", "Telugu", "1952", "Georgia Southern University", "Restoration Hardware", "1942", "Kansas City Chiefs", "Jesse Shannon", "C. H. Greenblatt", "Stephen Graham", "President alone, and the latter grants judicial power solely to the federal judiciary", "By functions ; introverted Sensing ( Si ), Extroverted Thinking ( Te )", "Belgium", "Jape", "Jackson Pollock", "Damon Bankston", "about 3,000 kilometers (1,900 miles)", "Casalesi clan", "Linda Darnell", "Scrabble", "Wendell, North Carolina", "a leap year"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7016712887806638}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.5, 0.125, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.5, 0.888888888888889, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-1835", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.59375, "CSR": 0.5409787735849056, "EFR": 1.0, "Overall": 0.7065551297169811}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "The cervical cancer vaccine,", "eight-day journey", "97 years of age", "The two men and a third hiker, Sarah Shourd,", "18", "Darrel Mohler", "Spc. Megan Lynn Touma,", "Operation Pipeline Express.", "The Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim \"came under fire\" after admitting they learned of the death from TV news coverage,", "a house party in Crandon, Wisconsin,", "the full Senate Sotomayor,", "the catamaran", "Grand Ronde, Oregon.", "a bag", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "14-day mission", "the fact that the teens were charged as adults.", "Kris Allen", "co-chair of the Genocide Prevention Task Force.", "state of the nation 15 years later", "Arsenal manager Arsene Wenger", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro on Sunday.", "Genocide Prevention Task Force", "Sheik Mohammed Ali al-Moayad", "The U.S. Food and Drug Administration Tuesday ordered the makers of certain antibiotics to add a \"black box\" label warning", "Jacob Zuma,", "the return of a fallen U.S. service member", "Sporting Lisbon", "The opposition group,", "Saturday.", "Jezebel.com's Crap E-mail From A Dude", "For more than 40 years and co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Democratic VP candidate", "Bergman", "Serie A", "three men with suicide vests who were plotting to carry out the attacks,", "between June 20 and July 20,\"", "Richard M. Nixon, right, and his Brazilian counterpart, Emilio Medici.", "Piedad Cordoba,", "Buddhism", "In the most high-profile amalgamation of Indian and western talent yet,", "Pakistani territory", "fight outside of an Atlanta strip club", "Britain's Got Talent", "Sen. Barack Obama", "Swamp Soccer", "the man facing up, with his arms out to the side. He is wearing socks but no shoes.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor, about 10 miles from Belfast.", "The military commissions", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "2018", "surfer", "arthropylum", "white", "2018 governor's race on August 9, 2017", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "fish", "a crust of mashed potato"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5912901959329389}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.14285714285714285, 1.0, 1.0, 0.0, 0.3333333333333333, 0.2424242424242424, 0.28571428571428575, 0.4, 0.0, 1.0, 1.0, 0.08695652173913043, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.4, 0.1904761904761905, 1.0, 0.6666666666666666, 0.30769230769230765, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.0, 0.0, 0.15384615384615385, 1.0, 0.9, 1.0, 1.0, 0.4615384615384615, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 0.72, 1.0, 0.11764705882352941, 0.0, 0.36363636363636365, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-1342", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1532", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-531", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-4204", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-12411", "mrqa_naturalquestions-validation-10616"], "SR": 0.453125, "CSR": 0.5393518518518519, "EFR": 1.0, "Overall": 0.7062297453703704}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "2004", "on the table", "the illegitimate son of Ned Stark", "Jesse L. Jackson", "Latavious Williams", "Ren\u00e9 Verdon", "31", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "declared neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in the pancreas", "In late - 2011", "Malibu, California", "desublimation", "eight", "the Anglo - Norman French waleis", "The three wise monkeys", "in lymph", "into the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Thomas Edison", "to refer to a former sexual or romantic partner, especially a former spouse", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "the pretribulation, premillennial, Christian eschatological interpretation of the Biblical apocalypse", "the topography and the dominant wind direction", "Development of Substitute Materials", "pagan custom", "in various submucosal membrane sites of the body", "2013", "John Garfield as Al Schmid", "ummat al - Islamiyah", "Lord Irwin", "the volume", "right to property is no longer a fundamental right", "Robert Gillespie Adamson IV", "18th century", "1998", "the left atrium of the heart", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the internal auditory canal of the temporal bone", "1858", "delivered appliances and other goods for department stores", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John Dustin Archbold", "misdemeanor assault charges", "$106,482,500", "introducing legislation Thursday to improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "real estate", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5762227182539683}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, true, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.21428571428571425, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.625, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.2, 1.0, 0.16, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.875, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-3624", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-1887", "mrqa_searchqa-validation-3606"], "SR": 0.453125, "CSR": 0.537784090909091, "EFR": 0.8857142857142857, "Overall": 0.6830590503246753}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth", "John Updike", "USA Today", "incense", "Jericho", "clean by.... for cleaning or spreading pitch on a ship's decks", "asteroids", "\"plankton\"", "In 1876, Rutherford B. Hayes won the election (by a margin of one electoral vote),", "Eleanor Roosevelt", "BATTLE OF LAKE ERIE", "Bangladesh", "The Secret", "medals", "Judd Apatow", "a laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pH", "the Aladdin Hotel", "9 to 5", "Jan & Dean", "force his", "ice cream", "Huckabee", "catherine the great", "Texas", "constellations", "AILD", "Celia", "Ross Perot", "the Thracian Sea", "C. S. Lewis", "Thomas Paine", "Back to the Future", "antelope", "Anne", "K'iche' kingdom of Q'umarkaj", "Dizzy", "\"Soup to nuts\"", "reasoning", "Fermi", "Icarus", "suspension bridge", "Tigger", "a repertoire", "the marathon", "Qwerty", "Deuteronomy 5", "collect menstrual flow", "13 May 1787", "nose", "Triumph", "Kansas", "the recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses", "March 17, 2015", "4.6 million", "the Dalai Lama", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6572945283882784}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-8573", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-6375", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.546875, "CSR": 0.5379464285714286, "EFR": 1.0, "Overall": 0.7059486607142857}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "bobby woodpecker", "liqueurs", "The Potteries", "stockton-on-Trent", "iron", "Little arrows", "Lorraine", "cats", "Reanne Evans", "Central African Republic", "the Battle of Camlann", "German mathematician David Hilbert", "1905", "Strasbourg, France", "Lab\u00e8que", "Jack London", "Sissy Jupe", "Muhammad Ali", "carbon", "The Bill", "M65", "Boxing Day", "cheers", "Taliban", "alpestrine", "a toad", "to make something better", "bokm\u00e5l", "skirts", "Australia", "Blucher", "Apollo", "Sachin Tendulkar", "55", "the Humber", "island of Tenerife", "South Africa", "bone", "Nutbush", "Lorraine Bottomley", "Shinto", "Cleckheaton", "the Greater Antilles", "Scotch", "Pluto", "pensioner Jim Branning (John Bardon)", "cryonics", "Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "American Idol", "\"Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World", "propofol", "Emmett Kelly", "Garfunkel", "Shakespeare in Love", "a desire to be reckoned with as an openly wounded and unabashedly portentous rock balladeer"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6358893557422969}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0588235294117647, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-6783", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6228", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_naturalquestions-validation-7270"], "SR": 0.578125, "CSR": 0.5386513157894737, "EFR": 0.9629629629629629, "Overall": 0.6986822307504873}, {"timecode": 57, "before_eval_results": {"predictions": ["against outside influences in next month's run-off election,", "Monday,", "eight-week", "which type of guy you should avoid.", "coalition forces in Afghanistan", "he has no plans to fritter his cash away on fast cars, drink and celebrity parties.", "Stratfor,", "from Paktika province in southeastern Afghanistan,", "Unseeded Frenchwoman Aravane Rezai", "murder in the beating death of a company boss who fired them.", "Kaka", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "kite surfers", "said that word of the secret talks between the two leaders about shaping Brazilian foreign policy filtered down to Brazilian military officers by a \"Cabinet leak.\"", "The opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "Saturday", "france, Russia, India, South Korea, China, South Africa, Brazil, Beirut and Poland.", "Dube,", "11", "hide his money,", "Bush said that both countries should be able to take part in NATO's Membership Action Plan, or MAP, which is designed to help aspiring countries meet the requirements of joining the alliance.", "Citizens", "the refusal or inability to \"turn it off\"", "Janet Napolitano", "nine newly-purchased bicycles at the scene,", "The alleged surviving attacker from last month's Mumbai terror attacks is seeking help from Pakistani officials, India said Monday.", "Pakistan from Afghanistan,", "the fact that the teens were charged as adults.", "Siri", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "BMW", "Sunday evening", "Landry", "Bush of a failure of leadership at a critical moment in the nation's history.", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "the Golden Gate Yacht Club of San Francisco", "boats", "Grease", "in some of the most hostile war zones,", "2002 for British broadcaster Channel 4", "because its facilities are full.", "the job bill's controversial millionaire's surtax,", "seven", "One of Osama bin Laden's sons", "in Africa", "Great Britain", "Manuel `` Manny '' Heffley", "2004", "jeremy", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "Chillingham Castle", "400th anniversary", "the river Liffey", "Scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.390625, "QA-F1": 0.527167690625508}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true], "QA-F1": [0.9333333333333333, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.23529411764705885, 0.4, 0.6666666666666666, 0.0, 0.0, 1.0, 0.25, 0.0, 0.15789473684210528, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 1.0, 0.0, 0.09523809523809522, 1.0, 1.0, 1.0, 0.33333333333333337, 0.05555555555555555, 0.5, 1.0, 1.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.2857142857142857, 0.0, 0.5714285714285715, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-7178"], "SR": 0.390625, "CSR": 0.5360991379310345, "EFR": 0.9743589743589743, "Overall": 0.7004509974580018}, {"timecode": 58, "before_eval_results": {"predictions": ["Ted 2", "1,467", "1989", "Meryl Streep", "14", "the National Basketball Development League (NBDL)", "Gust Avrakotos", "involuntary euthanasia", "astronaut, naval aviator, test pilot, and businessman", "a diving duck", "The Summer Olympic Games", "Glendale", "St. Louis Cardinals", "1992", "1993", "the University of Vienna", "Jack Ridley", "The Pennsylvania State University", "Chicago, Illinois", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australian", "suburb", "hydrogen fueled space rockets,", "The Savannah River Site", "swingman", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "23", "the Mach number (M or Ma)", "James Gay-Rees", "1872", "otion poetry", "Madonna", "conductor Ferrucio Calusio", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "non-alcoholic", "paper-based card", "White Horse", "Andrew Lloyd Webber, Jim Steinman, Nigel Wright with additional production by Franglen & Lupino", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Bumblebee", "riyadh", "Lady Gaga", "African violet", "three", "The museum", "in the Carrousel du Louvre,", "A Tale of Two Cities", "Gabriel", "William Wallace", "( Boss) Tweed"], "metric_results": {"EM": 0.5, "QA-F1": 0.6029513888888889}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true], "QA-F1": [0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_hotpotqa-validation-4874", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-3318", "mrqa_searchqa-validation-7521"], "SR": 0.5, "CSR": 0.5354872881355932, "EFR": 0.96875, "Overall": 0.6992068326271187}, {"timecode": 59, "before_eval_results": {"predictions": ["New Croton Reservoir", "connotations of the passing of the year", "John Barry", "Thespis", "Saronic Gulf", "2010", "Coroebus", "Ewan McGregor", "1952", "iron", "Jesse Frederick James Conaway", "tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Anne Murray", "1957", "certain actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet in 1876", "Have I Told You Lately", "2.4 %", "the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "season seven", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "the Roman Empire", "ordain presbyters / bishops", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "two Frenchmen : the engineer \u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau ''", "Felix Baumgartner", "1995", "2026", "Gupta Empire", "Abigail Hawk", "Hal Derwin", "Durban, South Africa", "in the 1970s", "1919", "23 September 1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "three levels", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "headdresses", "Wet Wet Wet", "the Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1927", "Bollywood", "Toofan 5", "to \"wipe out\" the United States if provoked.", "98.6", "Chicago", "Jonathan Swift", "an unusually impressive imposter"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6907343900858052}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true], "QA-F1": [0.5454545454545454, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5283018867924527, 0.26666666666666666, 1.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-4980", "mrqa_newsqa-validation-2663", "mrqa_newsqa-validation-213", "mrqa_searchqa-validation-2403"], "SR": 0.59375, "CSR": 0.5364583333333333, "EFR": 0.9230769230769231, "Overall": 0.6902664262820513}, {"timecode": 60, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.818359375, "KG": 0.47265625, "before_eval_results": {"predictions": ["Spanish Revolt of the Sergeants", "Anne Erin \"Annie\" Clark", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "theScanian War", "Sparky", "Fort Oranje", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "Crab Orchard Mountains", "Miss Universe 2011", "Accokeek, Maryland", "2008", "democracy and personal freedom", "Jordan Ridgeway", "French Canadians", "1964 to 1974", "Vanarama National League", "City Mazda Stadium", "Continental Army", "Matt Roller", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Sir Francis Nethersole", "The Panther", "British", "twelfth", "California State University", "City of Onkaparinga", "October 31, 1999", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950", "Raza Jaffrey", "David Letterman", "\u201cFor Gallantry;\u201d", "ArcelorMittal Orbit", "Government Accountability Office", "Joe Harn", "$249", "high and dry", "An American Tail", "Cats", "Peru"], "metric_results": {"EM": 0.625, "QA-F1": 0.6912202380952381}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-344", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-2096", "mrqa_newsqa-validation-4098"], "SR": 0.625, "CSR": 0.5379098360655737, "EFR": 1.0, "Overall": 0.7032850922131147}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "strawberry Blonde", "trout", "The Aidensfield Arms", "borneo", "France", "Manchester", "sky", "Susan Bullock", "Brian stewart", "November", "Wonga", "Brian Ladd", "Genghis Khan", "Kofi Annan", "jon stewart", "left side", "Istanbul", "lamb", "Space Oddity", "collie", "14 letters", "sharks", "florida", "Mike Hammer", "jon stewart", "Evelyn Glennie", "brain", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "a palla", "4.4 million", "Today", "Westminster Abbey", "Ralph Lauren", "Whitsunday", "Morgan Spurlock", "gruel", "Debbie Reynolds", "Caroline Aherne", "cations", "George Santayana", "Rudolf Nureyev", "Brian Gabbitas", "cat", "apple", "bromix", "Rodgers and Hammerstein", "part of a pre-recorded television program, Rendezvous with Destiny", "By 1770 BC", "The United States Secretary of State", "5", "Amal Clooney", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "the colony abandoned and looted,", "the Louvre", "Kansas City, Missouri", "YIVO"], "metric_results": {"EM": 0.609375, "QA-F1": 0.684375}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6210", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.609375, "CSR": 0.5390625, "EFR": 1.0, "Overall": 0.703515625}, {"timecode": 62, "before_eval_results": {"predictions": ["Curtis James Martin Jr.", "Gabriel Iglesias", "Walking in the Air", "Vikram Bhatt", "Helsinki, Finland", "Future", "Tommy Cannon", "Scottish national team", "203", "Ward Bond", "Illinois's 15 congressional district", "Buffalo", "between 7,500 and 40,000", "5,112 feet (1,559 m)", "Prof Media", "the lead roles of Timmy Sanders and Jack in the series \"Granite Flats\" and film \"King Jack\", respectively.", "four months in jail", "Michael Redgrave", "Sturt", "big Machine Records", "two Manhattan high school students who share a tentative month-long romance", "Europe", "Anandapala", "deadpan sketch group", "larger than a subcompact car but smaller than a mid-size car", "Spanish", "algernod Lanier Washington", "14,000 people", "in the western Highlands of Scotland", "37", "Taoiseach of Ireland", "137th", "Mr. Nice Guy", "Japan Airlines Flight 123", "tag team", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "video game", "United States of America (USA), commonly known as the United States (U.S.) or America ( USA),", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis, Tennessee", "Switzerland\u2013European Union relations", "New Jersey", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Tigris and Euphrates rivers", "in September 2000", "Woodrow Wilson", "our mutual friend", "buffalo", "Volkswagen", "Teresa Hairston", "Pope Benedict XVI", "in St. Louis, Missouri.", "mother of pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6653242807539682}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666665, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4444444444444444, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353"], "SR": 0.546875, "CSR": 0.5391865079365079, "EFR": 0.9655172413793104, "Overall": 0.6966438748631637}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil,", "Mokotedi Mpshe,", "apartment building", "July", "2005 & 2006 Acura MDX", "Ryan Adams.", "80 percent of the woman's face", "Olympia,", "27-year-old's", "next week", "April 26, 1913,", "7-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "his son, Isaac, and daughter, Rebecca.", "The Falklands, known as Las Malvinas in Argentina,", "\"everyone can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "two", "in the 1950s,", "Gary Player", "1 out of every 17 children under 3 years old", "\"Rin Tin Tin: The Life and the Legend\"", "a little coastal cleanup -- country style.", "President George Bush", "an average of 25 percent", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Ahmed", "2005", "\"He went there to receive this bullet. If he would not have been wounded; he wouldn't be in the hospital; he would be living in peace with his family.\"", "Johan Persson and Martin Schibbye", "Israel", "Sunday's", "Islamabad", "Jeffrey Jamaleldine", "The Rev. Alberto Cutie", "all day starting at 10 a.m.", "\"a fantastic five episodes.\"", "to make life a little easier for these families by organizing the distribution of wheelchair for Iraqi Kids.", "In a court filing for a protective order, Wimunc said that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "recite her poetry", "in the head", "350 U.S. soldiers", "neck", "The island's dining scene", "Andrew Garfield", "New England Patriots", "blood plasma and lymph in the `` intravascular compartment '' ( inside the blood vessels and lymphatic vessels )", "gold", "The Mystery of Edwin Drood", "The Mutiny on the Bounty", "Melbourne", "1998", "23 July", "Tuesday", "Evian", "Ashbury", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6416183827254306}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.21428571428571427, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.7659574468085107, 0.13333333333333333, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-5662", "mrqa_searchqa-validation-5963"], "SR": 0.53125, "CSR": 0.5390625, "EFR": 0.9666666666666667, "Overall": 0.6968489583333334}, {"timecode": 64, "before_eval_results": {"predictions": ["comedies he wrote in 1722\u20131723 for the Lille Gr\u00f8nnegade Theatre in Copenhagen", "Max Martin and Shellback", "Queen Victoria", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "West Cheshire Association Football League", "Transporter 3", "1983", "December 13, 1920", "The Norse\u2013Gaels", "265 million", "January 2004", "Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Dan Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "Coahuila, Mexico", "1968", "Holston River", "July 10, 2017", "London", "French-German science fiction drama", "Anno 2053", "Stephen Mangan", "the largest Mission Revival Style building in the United States", "Darci Kistler", "The Terminator", "Samoa", "The song, written by Lamar and Mike Will Made It and produced by the latter,", "Timo Hildebrand", "Netflix", "first flume ride in Ireland", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$75 for full-day class,", "\"Nu au Plateau de Sculpteur,\"", "nicotine", "The Bridges of Madison County", "James Madison", "a foreign exchange option"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7077459273182958}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2030", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4073", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.65625, "CSR": 0.5408653846153846, "EFR": 1.0, "Overall": 0.7038762019230769}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "leaves of the plant species Stevia rebaudiana", "Johannes Bell", "Universal Pictures", "May 2010", "T - Bone Walker", "the entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "four", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "William Shakespeare's play Romeo and Juliet", "Payaya Indians", "The Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "runoff will usually occur unless there is some physical barrier", "1940", "the pulmonary arteries start as the pulmonary trunk or main pulmonary artery", "Puente Hills Mall", "1977", "HTTP / 1.1 200 OK", "June 1992", "general taxation", "28 July 1914", "Richard Stallman", "the year 1 BC", "October 27, 1904", "December 25", "lizards", "Timomatic and Geri Halliwell", "the final scene of the fourth season", "Auburn Tigers football team", "during meiosis", "a contemporary drama in a rural setting", "Javier Fern\u00e1ndez", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant", "Jonathan Cheban", "2005", "computers or in an organised paper filing system", "bicameral Congress", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "\"Raging Bull\"", "Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "The controversial quote is part of a eight-page feature article about Hogan to be published in the magazine's Friday edition.", "fifth", "Tina Turner", "Bingo SOLO", "Amsterdam", "\"Salve\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.6063201404573687}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.08695652173913043, 1.0, 0.18181818181818182, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.0, 0.0, 0.8, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.515625, "CSR": 0.5404829545454546, "EFR": 1.0, "Overall": 0.703799715909091}, {"timecode": 66, "before_eval_results": {"predictions": ["Braille", "huggins", "polygon", "Steely Dan", "Strictly Come Dancing", "hirsch hanley", "about a mile north of the village of Dunvegan", "hladetina", "The opening line of perhaps the greatest romantic drama novel ever written", "the Iron Age", "Justin Bieber", "Tallinn", "1925 novel", "The Gunpowder Plot", "Moldova", "holly", "Edwina Currie", "sprite", "IKEA", "Pablo Picasso", "Some Like It Hot", "Ralph Vaughan Williams", "Tony Blair", "Pickwick", "360", "Caracas", "Ireland", "The Donington Grand Prix Collection is, quite simply, the largest showcase of Grand Prix racing cars in the world", "Jim Peters", "horse racing", "onion", "Pat Houston", "1948", "narwhal", "Sikhism", "giraffe", "kabuki", "email", "Zachary Taylor", "indigo", "Thursday", "for gallantry", "Swindon Town", "cricket", "jordan", "Burma", "Tottenham Court Road", "hongi", "basketball", "Snow White", "Italy", "Zane Lowe's show on BBC Radio 1 in June 2010, at the Rockstar offices in New York in July 2010, and at the Spike Video Game Awards in December 2010", "Buddhism", "eukaryotic cell", "Hechingen", "1986", "Charles L. Clifford", "Eleven people died and 36 were wounded in the Monday terror attack,", "Joe Pantoliano,", "Robert Barnett,", "Jeopardy", "The Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6873511904761904}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.05714285714285715, 0.5, 1.0, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.65625, "CSR": 0.5422108208955223, "EFR": 0.9090909090909091, "Overall": 0.6859634709972863}, {"timecode": 67, "before_eval_results": {"predictions": ["yann martel", "The Archers", "Tiffany and Co.", "ndebele", "Cambridge", "victoria", "1830", "clement", "sch schizophrenia", "george chaucer", "sports agent", "rough collie", "Edward M. Kennedy", "james may", "red squirrel", "r Richard Lester", "Buick", "polish", "gooseberry", "George W Bush", "The Color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1983", "victoria", "clement", "victoria", "Elizabeth I", "rye plomley", "Leon Baptiste", "360", "jeremy clems", "1123", "Mitford", "Sparta", "Hyundai", "clement", "Julian Fellowes", "haddock", "victoria", "tina Turner", "mainland China, Hong Kong and Macau", "Nowhere Boy", "victoria", "neck", "a quant", "lee clement", "35", "reed victoria", "victoria", "Christine", "South Asia", "Uralic languages", "New York City", "1942", "a card (or cards) during a card game", "Larry King", "murder in the beating death of a company boss who fired them.", "rebecca victoria", "Oakland Raiders", "the Mediterranean", "Isabella", "Turing"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5716517857142857}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.5, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-4878", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-6966", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-5098", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-7507", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-2128", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.453125, "CSR": 0.5409007352941176, "EFR": 1.0, "Overall": 0.7038832720588235}, {"timecode": 68, "before_eval_results": {"predictions": ["natural-language requests", "Philippines", "heavy turbulence", "Brian Smith.", "Tim Clark", "drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.", "people with ties to the U.S. Consulate in Ciudad Juarez, Mexico,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "E! News", "about 50", "two-state solution", "Yusuf Saad Kamel", "central London offices", "his father,", "Israel and the United States", "South Africa's", "the insurgency,", "Arlington National Cemetery's", "The Rosie Show", "Zapata Reyes,", "March 24,", "people never had to be far from their favorite tunes.", "mouth.", "100", "Anne Frank", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio,", "The father of Haleigh", "off the coast of Dubai", "the Somali coast", "municipal police officers", "businesses hiring veterans as well as job training for all service members leaving the military.", "shock, quickly followed by speculation about what was going to happen next,\"", "northwestern Montana", "Iran test-launched a rocket capable of carrying a satellite,", "without bail", "February 12", "Kim Jong Il", "a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro,", "Democratic VP candidate", "martial arts,", "people", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "65 years ago", "people have fled their homes in the Somali capital of Mogadishu as a result of a militant offensive against government forces,", "nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "horses", "k Kathryn C. Taylor", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5516799008986509}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.18181818181818182, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.8571428571428571, 0.4444444444444445, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9189189189189189, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-885", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-494", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-5208"], "SR": 0.4375, "CSR": 0.5394021739130435, "EFR": 0.9722222222222222, "Overall": 0.6980280042270531}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Americana Manhasset", "Mayfair", "Minister for Social Protection", "19 February 1927", "Arab", "the southern North Sea", "Larry Richard Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "Ezo", "\"Back to December\" is considered an orchestral country pop ballad and its lyrics are a remorseful plea for forgiveness for breaking up with a former lover", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Derry, Northern Ireland", "Daniel Wroughton Craig", "Hamburger SV", "Four Weddings and a Funeral", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "novel", "The Daily Stormer", "Fort Saint Anthony", "IT products and services,", "Japan", "1919", "Tak and the Power of Juju", "the western end of the National Mall in Washington, D.C.", "Len Wiseman", "Stephen Young", "\"My Backyard\" in Jacksonville, Florida,", "Gerry Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Bob Hurley", "September 1901", "Tuesday", "steroids, also known more properly as anabolic\u2013androgenic steroids", "North West England", "Football Bowl Subdivision (FBS; formerly Division I-A)", "\"Polovtsian Dances\"", "Kentucky", "1961", "1896", "1924", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "Earth", "diamonds", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "sumo wrestlers.", "Venezuelan President Hugo Chavez", "the Juilliard School", "dinosaurs", "the Boy Scouts of America", "Inuit"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7002079517704518}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.4444444444444444, 0.5, 0.0, 0.23076923076923078, 0.8, 1.0, 0.0, 0.8, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 0.5714285714285715, 0.8, 1.0, 1.0, 0.4, 1.0, 1.0, 0.2, 1.0, 0.2, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-2135", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-4943", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.546875, "CSR": 0.5395089285714285, "EFR": 1.0, "Overall": 0.7036049107142857}, {"timecode": 70, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.828125, "KG": 0.49296875, "before_eval_results": {"predictions": ["Arkansas", "early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "From Here to Eternity", "12", "port city of Aden", "Will Smith", "United States", "Patricia Veryan", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Queensland", "\"master builder\" of mid-20th century New York City", "Honolulu", "St. Louis County", "Badfinger", "his virtuoso playing techniques and compositions in orchestral fusion", "XVideos", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Currer Bell", "University of Nevada", "mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "ninth", "Hanna", "Manchester Victoria station", "Seb Rochford", "My Love from the Star", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "bass", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "prophets", "Bill Russell", "Andre Agassi", "city of Austria", "Boston Braves", "fill a million sandbags and place 700,000 around our city,\"", "Caster Semenya", "extend the usage of morphine sulfate oral solution 20 mg/ml.", "the Cuyahoga River", "uranium", "Peter Sellers", "river Elbe"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7763764880952381}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-5542", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.671875, "CSR": 0.5413732394366197, "EFR": 1.0, "Overall": 0.715071522887324}, {"timecode": 71, "before_eval_results": {"predictions": ["Nearly eight in 10", "Marie-Therese Walter", "help at-risk youth, victims of violent crimes and homeless children.", "Russian air force,", "a female soldier,", "Nearly eight in 10", "Goa", "Iran", "increased Dubai's shoreline by 100 percent", "Kenyan governments", "Susan Atkins", "Dominican Republic", "\"Operation Crank Call,\"", "228", "North Carolina", "National September 11 Memorial Museum", "Harlem luxury car dealer.", "\"I don't think I'll be particularly extravagant.\"", "Friday", "Elisabeth", "1959", "his", "269,000", "issued his first military orders as leader of North Korea", "iTunes,", "a group of teenagers.", "Six", "Luis Carlos Ameida", "27-year-old's", "outside influences in next month's run-off", "National Guard reallocated reconnaissance helicopters and robotic surveillance craft to the \"border states\" to prevent illegal immigration.", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million", "unwanted horses", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia and India", "1,500", "to cope with tough economic times.", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Francisco X. Pacheco,", "Sen. Barack Obama", "NeNe Leakes", "a motor scooter that goes about 55 miles per hour -- on 12-inch wheels.", "the shelling of the compound", "Guinea, Myanmar, Sudan and Venezuela", "pine beetles", "international aid agencies", "Supplemental oxygen", "February 28 or March 1", "Indo - Pacific", "putter", "quetzalcoatl", "Maryland", "2012", "\"Northeast Regional\"", "Crackle", "a porcupine", "oxygen", "the Apache", "Truman"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5070099888034671}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-974", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-3958", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-717", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2986", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-4809", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_hotpotqa-validation-3485", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.421875, "CSR": 0.5397135416666667, "EFR": 0.972972972972973, "Overall": 0.709334177927928}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997", "Sharyans Resources", "an vehicle which is both four - wheel - drive and primarily a road car", "Edward Seton", "Texas A&M University", "stromal connective tissue", "the Old Testament", "Anatomy ( Greek anatom\u0113, `` dissection '' )", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Gary Grimes", "very important", "Richard of Shrewsbury, Duke of York", "Ashrita Furman", "underworld criminal", "Jean Fernel", "2007", "in May 1980", "erosion", "English", "1960", "John F. Kennedy", "Johnny Logan", "revenge", "the misuse or `` taking in vain '' of the name of the God of Israel", "England and Wales", "1996", "c. 8000 BC", "North Dakota", "early Christians of Mesopotamia", "eight hours ( UTC \u2212 08 : 00 )", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr.", "Jay Baruchel", "Eleanor Parker", "merengue", "Butter Island off North Haven, Maine in the Penobscot Bay", "originated in Europe toward the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850", "during the 1890s Klondike Gold Rush", "secure communication over a computer network", "3", "1939", "BT Sport", "Ticket to Ride", "land - living organisms", "Felicity Huffman", "Roger Mortimer", "75", "j29", "Montana State University", "Sun Valley, Idaho", "president", "a bathing suit", "doctors", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "the Ganges-Brahmaputra", "a 529 account", "The Crow", "Madrid's Barajas International Airport"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5872549019607843}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 0.0, 0.3333333333333333, 1.0, 1.0, 0.13333333333333333, 1.0, 0.5, 1.0, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-8381", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-4656", "mrqa_triviaqa-validation-5787", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.484375, "CSR": 0.5389554794520548, "EFR": 0.9696969696969697, "Overall": 0.7085273648298049}, {"timecode": 73, "before_eval_results": {"predictions": ["Brazil", "The Fall Guy", "Crown", "Maria Montessori", "Kinsey Millhone", "the Department of Justice", "a science fiction novel", "March of the Crosby", "Adidas", "Fletcher Christian", "an ambulance", "a Condoleezza Rice", "Pakistan", "China", "liqueur", "Texas", "a Condor", "John James Audubon", "Pontius Pilate", "an \"C5 H4 N4 O3\"", "synaptic vesicles", "the halfpipe", "the Bitch", "a coffee drink", "Freakonomics", "George Washington Carver", "the Carboniferous Period", "Champagne", "Red Heat", "a black mayor", "the Dominican Republic", "a carrel", "a flop with chicks", "Prince William", "Sherlock Holmes", "ancistroid", "Orion", "India", "carbon monoxide", "King John", "a power drill", "horror", "Cambodia", "manslaughter", "programming techniques", "the Tennessee", "Hipparchus", "Billy Idol", "the Missouri Compromise", "the Rat", "a simple man with a low I.Q.", "to encounter antigens passing through the mucosal epithelium", "$1.528 billion in other countries", "in Western countries", "Conrad Murray", "Gryffindor", "czech Republic", "in Sochi, Russia", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month", "American Civil Liberties Union", "monthly and then quarterly men's magazine"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5169642857142858}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7908", "mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-16712", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-10889", "mrqa_searchqa-validation-13255", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-16646", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-12241", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_hotpotqa-validation-4076", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.453125, "CSR": 0.5377956081081081, "EFR": 1.0, "Overall": 0.7143559966216216}, {"timecode": 74, "before_eval_results": {"predictions": ["fructose", "Angela Rippon", "Anna Eleanor Roosevelt", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "javelin throw", "British Airways", "Bachelor of Science", "cirencester", "Pete Best", "Bonnie and Clyde", "Avatar", "Santiago", "St. Moritz", "Edmund Cartwright", "par", "Prometheus", "Japanese silvergrass", "April", "Sir Arthur", "Wolfgang Amadeus Mozart", "bees", "Sun Hill", "The Nutcracker", "Flyweight", "Adare", "Sesame Street", "photography", "Leslie Perowne", "Samuel Johnson", "Sports & Literature", "a bear", "ganga", "tabloid", "car door", "kolkata", "the odeon", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Dame Kiri Te Kanawa", "Churchill Downs", "Up stairs", "One Direction", "ulnar nerve", "Gibraltar", "111", "Merck & Co.", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "prisoners' rights", "November", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7000718390804597}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.671875, "CSR": 0.5395833333333333, "EFR": 0.9523809523809523, "Overall": 0.7051897321428571}, {"timecode": 75, "before_eval_results": {"predictions": ["Robert FitzRoy", "Martin O'Neill", "2012", "7\u00b056'", "Kind Hearts and Coronets", "Massachusetts", "Nippon Professional Baseball", "hiphop", "erotic thriller", "Eumolpus", "Backspacer", "James II of England", "Sir William McMahon", "Hopi tribe", "Digby, Lincolnshire", "Australian", "Annie Ida Jenny No\u00eb Haesendonck", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "NXT Tag Team Championship", "Chinese Coffee", "Love and Theft", "Marion", "4145 ft above mean sea level", "University of Georgia", "just over 1 million", "Indian", "The Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "25 October 1921", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller", "J. Cole", "Idisi", "The Books", "port of Mazatl\u00e1n", "Danish", "London, England", "Rochdale, North West England", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Koch Industries", "Liverpudlian", "Mindy Kaling", "3 October 1990", "Wednesday, September 21, 2016", "art photography of the band's performance and outdoor session pictures", "earache", "concrete", "cuckoo", "nearly $2 billion", "127 acres.", "\"That is the sort of thing that would be subject to a malpractice claim in the civilian world.\"", "Patrick", "the Tomb of the Unknown Soldier", "Mount Vesuvius", "city of San Pedro Garza Garcia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6283482142857143}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.4, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.2, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1908", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-13410", "mrqa_newsqa-validation-3554"], "SR": 0.515625, "CSR": 0.5392680921052632, "EFR": 0.967741935483871, "Overall": 0.7081988805178269}, {"timecode": 76, "before_eval_results": {"predictions": ["Pet Sounds", "on a nearby moor at Culloden", "gars gratia artis", "Liszt Strauss", "James Callaghan", "miscressaceae", "international", "Dublin", "pyrenees", "leprosy", "left", "Kenneth Williams", "avocado", "Anne of cleves", "The Double", "Relpromax Antitrust Inc.", "Supertramp", "crouch", "Octavian", "\"One Night / I Got Stung\"", "Heston Blumenthal", "united states", "The IT Crowd", "Some Like It Hot", "\"Mr Loophole\"", "Ken Purdy", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti art", "Friedrich Nietzsche", "Dee Caffari", "cheese", "Annie", "Kristiania", "piano", "Moby Dick", "snakes", "sacred Wonders of Britain", "Changing Places", "the pea", "Dr Tamseel, UK", "the Sea of Galilee", "1", "king menelaus", "memory-robbing disease", "The Firm", "1966", "an even break", "31536000", "Jordan", "arthropods", "in desperation, with only a small chance of success and time running out on the clock", "2018", "Miami Marlins", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "head for Italy.", "Rev. Alberto Cutie", "Michelle Obama", "an alto", "270", "place", "the American Red Cross"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5068181818181818}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.33333333333333337, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.060606060606060615, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-611", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-2838", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-5149", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.453125, "CSR": 0.5381493506493507, "EFR": 0.9714285714285714, "Overall": 0.7087124594155845}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player", "remains committed to British sovereignty", "Kirchners", "iPods", "45 minutes, five days a week.", "seeking a verdict of not guilty by reason of insanity that would have resulted in psychiatric custody.", "Kris Allen", "Jared Polis", "ore Gold,", "Zimbabwe", "Harry Nicolaides,", "Zhanar Tokhtabayeba,", "April 2010", "Zed's skull,", "a novel", "environmental", "his father's", "Iran", "head injury.", "\"Antichrist\"", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Frank's diary.", "The Da Vinci Code", "Matthew Fisher", "Swat Valley and South Waziristan.", "Colorado prosecutor", "Helmand province, Afghanistan.", "Climatecare,", "removal of his diamond-studded braces.", "Ennis, County Clare", "United States", "in Arabic, Russian and Mandarin that led police to 86 suspects in a series of raids that started Tuesday,", "Hamas,", "two pages -- usually high school juniors who serve Congress as messengers", "At least 40", "four", "Courtney Love,", "84-year-old", "signed a power-sharing deal with the opposition party's breakaway faction,", "three", "undergoing renovation.", "Naples home.", "Hanford nuclear site,", "November 26,", "sportswear,", "Beijing", "was unaware of problems with Zoe's Ark and thought they were on a humanitarian mission.", "get better skin, burn fat and boost her energy.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "Arousal regulation", "berath", "India and Pakistan", "allergic", "a lie detector", "a duet recorded by British rock band Coldplay and Barbadian singer Rihanna for Coldplay's fifth studio album \"Mylo Xyloto\"", "1963", "Black Abbots", "a nurse", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7143855263243808}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.21052631578947364, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.09523809523809523, 0.15384615384615383, 0.8823529411764706, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-1935", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.640625, "CSR": 0.539463141025641, "EFR": 1.0, "Overall": 0.7146895032051283}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O\"", "Silk Road", "Scandinavia", "George Rogers Clark", "a mole", "a coach dog", "Sweden", "Volleyball", "John Alden", "Ghost World", "Deuteronomy", "a locator map", "Alaska", "Madison Avenue", "Job", "a reg", "art deco", "Spider-Man", "Buddha", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "a lieutenant", "National Archives", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "a ford", "Cecilia Beaux", "the Mormon Tabernacle Choir", "The Scarlet Letter", "Siam", "Bangkok", "St. Paul", "a positron", "President Lyndon B. Johnson", "Jefferson", "Jerusalem", "Pushing Daisies", "Cranberry", "Tzatziki", "Ch'iu", "United Healthcare Workers East", "a sharlotka", "canals", "Abraham", "a self-appointed or mob-operated tribunal", "in ancient Mesopotamia", "Rachel Kelly Tucker", "makes Maria a dress to wear to the neighborhood dance", "London", "Kermadec Islands", "Julius Caesar", "Greek mythology, the Titaness daughter of the earth goddess Gaia and the sky god Uranus, and sister and wife to Cronus", "\"The Danny Kaye Show\"", "2012", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "Victor Mejia Munera,", "The oceans"], "metric_results": {"EM": 0.53125, "QA-F1": 0.615737907925408}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.3636363636363636, 0.07692307692307693, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-11290", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-2552", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.53125, "CSR": 0.5393591772151899, "EFR": 1.0, "Overall": 0.714668710443038}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "DeWayne Warren", "is actually wise", "Doug Pruzan", "a simple majority vote", "a byte", "Nehemiah 1 : 5", "September 19, 2017", "a solemniser", "17th Century sources referring to Cardinal Richelieu after he was named to head the royal council in 1624", "Hermann Ebbinghaus", "Agostino Bassi", "An error does not count as a hit", "Magnetically soft ( low coercivity ) iron", "Robert Cappucci and Joseph Wiley", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "40.5 metres ( 133 ft )", "Los Angeles Dodgers", "Emma Watson", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze YouTubers", "10 June 1940", "the citizens", "a nominating committee composed of rock and roll historians", "Amanda Fuller", "The Forever People", "1997", "mitochondrial membrane", "in the late 1980s", "American swimmer Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Josie ( Gabrielle Elyse )", "2002", "Evermoist", "Pangaea", "Selena Gomez", "Leslie and Ben", "the dress shop", "6,259 km ( 3,889 mi )", "September 6, 2007", "second from 1939 to 1960", "March 2, 2016", "the Mishnah", "the internal reproductive anatomy ( such as the uterus in females )", "short distance to terminate at Brundisium", "France", "Ukrainian", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "reduce the cost of auto repairs and insurance Premium"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6412946428571429}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0, 0.2857142857142857, 0.1111111111111111, 1.0, 1.0, 0.05128205128205128, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6153846153846153, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.3333333333333333, 0.3333333333333333, 0.33333333333333337, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.53125, "CSR": 0.5392578125, "EFR": 0.9666666666666667, "Overall": 0.7079817708333334}, {"timecode": 80, "UKR": 0.697265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.8125, "KG": 0.4828125, "before_eval_results": {"predictions": ["hilie ralston", "Miranda v. Arizona", "Dorian Gray", "Vancouver Island", "violin", "Utrecht", "Vietnam", "Jane Austen", "georgia fox", "Senior Training Manager", "Leadbetter", "glasnost", "CBS", "jazz", "Earthquake", "The Jungle Book", "jon Voight", "neoclassic designs of Robert Adam", "gal", "a great Dane", "to secure the subservience of priests and to rout out rebels and rogues", "Cambodia", "jujitsu", "74th Hunger Games", "head and neck", "11 years and 302 days", "New Zealand", "the Prussian 2nd Army", "kitty in Boots", "Whisky Galore", "Tunisia", "50", "Ted Kennedy", "eGrement", "short neck", "Google", "shoulder", "Iran", "Downton Abbey", "bird", "Rudyard Kipling", "backgammon", "Amy Dorrit", "(Albert) Einstein", "georgonzola", "barenboim", "exploits on the Island", "ear", "tree", "Imola Circuit", "trout", "Martin Lawrence", "Emmett Lathrop `` Doc '' Brown, Ph. D.", "North Atlantic Ocean", "September 6, 1961", "Boston Herald", "Lord Chancellor of England", "\"Britain's Got Talent,\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "so", "the East Asian Library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6012133699633699}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.05714285714285714, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-3623", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-4889", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-550", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_hotpotqa-validation-5154", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.515625, "CSR": 0.538966049382716, "EFR": 0.9354838709677419, "Overall": 0.6934056090700916}, {"timecode": 81, "before_eval_results": {"predictions": ["Dame Maggie Smith", "wulfstan", "monaco", "van", "Illinois", "belgian", "paul Maskey", "(Rafa) Federer", "tartar sauce", "the Three Graces", "satyrs", "geustavus III", "Congregational Quorum", "Martin Van Buren", "leeds", "george Webb", "Operation", "white", "Jay-Z", "Brian Clough", "honda", "runcorn", "Vietnam", "special administrative zones", "vincent van gogh", "sakhalin", "Croatia", "NBA", "steel", "(Prince) Bumpo", "kudi al-Fayed", "The Hustle", "a murre", "Samuel Johnson", "coointreau", "belgian", "Victor Hugo", "endosperm", "the Adriatic Sea", "heartburn", "Facebook Stories", "HMS Conqueror", "richard attenborough", "print letters", "Standard", "cynthia Nixon", "Hamlet", "Wat Tyler", "Patrick Henry", "steam engine", "Ukraine", "Eddie Murphy", "Pakistan", "Tom Hanks", "Thorgan Hazard", "Lithuanian national team", "kent Hovind", "almost 100", "improper or criminal conduct", "in critical condition", "Superman", "(Lief) Ericson", "The Towering Inferno", "member states"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5855902777777777}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2287", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.515625, "CSR": 0.5386814024390244, "EFR": 0.967741935483871, "Overall": 0.6998002925845791}, {"timecode": 82, "before_eval_results": {"predictions": ["laul daguerre", "Netherlands", "tarn", "wagen", "Sheffield", "subtropical", "piano", "charleston", "Pat Cash", "chile", "Wild Atlantic Way", "Kyoto Protocol", "web", "repechage", "steve biko", "charleston", "peacock", "rita hayworth", "Miss Honey", "imola", "Albania", "antelope", "snakes", "boreas", "Ivan Basso", "bullfighting", "one", "Playboy", "bulgaria", "Peter Ackroyd", "walford", "Sven Goran Eriksson", "thierry roussel", "mungo park", "death penalty", "Danny Alexander", "14", "Bangladesh", "adonis", "Papua New Guinea", "Lady Gaga", "Sunset Boulevard", "raging bull", "ars gratia artis", "pork scraps, or even chicken, turkey and beef", "All Things Must Pass", "fire", "Tet", "Arabah", "mark", "Lady Penelope", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "special guest performers Beyonc\u00e9 and Bruno Mars", "Greg Gorman and Helmut Newton", "American real estate developer, philanthropist and sports team owner", "Isabella II", "Mexico", "Marines and their families", "Arizona", "Frdric Chopin", "Indiana Jones", "Batavia", "the Cosmopolitan"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6651041666666667}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-2084", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-16678", "mrqa_hotpotqa-validation-668"], "SR": 0.640625, "CSR": 0.5399096385542168, "EFR": 0.9565217391304348, "Overall": 0.6978019005369304}, {"timecode": 83, "before_eval_results": {"predictions": ["(Johnny) Depp", "The Green Arrow", "a parable", "Juliet", "Spinal Tap", "Tennessee", "Detroit", "the Day Off", "the United States", "Giza", "Ruth Bader Ginsburg", "Article VII", "sense of touch", "Old Fashioned", "the Osmonds", "Bonnie and Clyde", "a crayfishes", "College of William and Mary", "a chimp", "Indian reservations", "John Updike", "Ganges", "vision", "Bright Lights", "to be the Bayou State's next governor", "coelacanth", "Northanger Abbey", "Cheers", "Heidi", "Crosby, Stills, Nash & Young", "Matt Leinart", "Group O", "Charles Edward Stuart", "the albatross", "Falklands", "taro", "a quip", "a lighthouse", "the afterimage", "Dan Rather", "Georgia", "Buffalo Bill", "the Big Bang Theory of Creation", "a dog", "Harvard", "neurons", "Hawaii", "Alexander Pope", "a dog", "a dragonfly", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "humble pie", "coming to America", "City and County of Honolulu", "Armidale, New South Wales", "1992", "publicly criticized his father's parenting skills.", "Steven Chu", "top designers, such as Stella McCartney,", "Rwanda"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7024553571428571}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-2050", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-4821", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-5245", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3660"], "SR": 0.609375, "CSR": 0.5407366071428572, "EFR": 1.0, "Overall": 0.7066629464285714}, {"timecode": 84, "before_eval_results": {"predictions": ["1970s", "Steveston Outdoor pool in Richmond, BC", "1930s", "Lenny Jacobson", "the status line", "each team", "overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "1991", "small packs, and in larger and smaller sizes", "687 ( Earth ) days", "a jazz funeral without a body", "the previous year's Palm Sunday celebrations", "Castleford", "the fourth C key from left on a standard 88 - key piano keyboard", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "winter", "Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the Octopus", "2001", "Lucius Verus", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "100 % owned by Xiu Li Dai and Yongge Dai", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg up to 7 ml", "gastrocnemius muscle", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "the mayor's home", "1945", "Pebble Beach", "andaman and Nicobar Islands", "midpiece", "646.38 metres ( 2,120.7 ft ) Warsaw Radio Mast", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Johnny Cash", "Rick Grimes ( Andrew Lincoln )", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "how to program computers to fruitfully process large amounts of natural language data", "10 years", "2026", "eleven", "Singing the Blues", "early 20th century", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "Play style", "echidna", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission", "would have significant public health experience and understand how these processes work, how meat enters the chain of commerce,\"", "a child carrier", "The Tin Drum", "Dwight D. Eisenhower", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5448618656529407}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.17647058823529413, 0.0, 0.0, 0.0, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8181818181818181, 1.0, 0.2, 1.0, 0.47058823529411764, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.5, 0.6153846153846153, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-4492", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-5609", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-4132"], "SR": 0.421875, "CSR": 0.5393382352941176, "EFR": 0.8648648648648649, "Overall": 0.6793562450317965}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "sprint", "ganga", "gerry adams", "mollusks", "Roy Rogers", "Steve Jobs", "Tommy Lee Jones", "Nirvana", "Donna Summer", "the hoof wall", "geese", "an authorization of the individual to fulfill a particular function or task", "Sheryl Crow", "hastings", "9801", "Franklin D. Roosevelt", "neurons", "Porridge", "Yoshi", "Swordfish", "cerumen", "george best", "faggots", "11", "brown", "Australia", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Genoa", "Vienna", "glee", "david hockney", "iron", "Japan", "bayern munish club", "denard", "Italy", "el Paso", "May Day", "chili peppers", "Madagascar", "Beaujolais", "david bercow", "kolkata", "dance show", "david nile Rodgers", "Nick Sager", "Chung", "Game 1", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "first grand Slam,", "propofol,", "bankruptcy", "Japan", "Zindine Zidane", "Augustus", "newt"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5936011904761904}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-5173", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-5256", "mrqa_naturalquestions-validation-8849", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-350"], "SR": 0.53125, "CSR": 0.5392441860465116, "EFR": 0.9666666666666667, "Overall": 0.6996977955426357}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "Tempo", "photographs, film and television", "Arthur Freed", "alt-right", "\"Runaways\" vol.  2 #7", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "8 May 1989", "Isfahan, Iran", "short interspersed nuclear elements (SINEs)", "murder", "London", "SBS", "quantum mechanics", "king Duncan", "January 16, 2013", "Forbes", "Anne and Georges", "David Villa", "Double Agent", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Bothtec", "Jim Thorpe", "De La Soul", "The Monster", "Shropshire Union Canal", "1670", "A skerry", "Oliver Parker", "FX", "Elizabeth Kekaaniau La\u02bbanui Pratt", "Pac-12 Conference", "Roots: The Saga of an American Family", "five", "William Scott Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "1907", "The Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "action role - playing video game developed and published by Nippon Ichi Software for the PlayStation 4", "nathan leopold Jr.", "Bill Haley & His Comets", "The Archbishop of Canterbury, the Most Rev and Rt Hon George Carey", "Amanda Knox", "Number Ones", "Jeddah, Saudi Arabia,", "E.B. White", "Andrew Jackson", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.625, "QA-F1": 0.6882575757575757}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.4, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3778", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-3788", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5047", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-1530"], "SR": 0.625, "CSR": 0.5402298850574713, "EFR": 1.0, "Overall": 0.7065616020114943}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "\"Hand of Thrawn\" novels", "1754", "May 10, 1976", "Hamlet", "Erick Avari", "Milwaukee Bucks", "Jenson Alexander Lyons", "\"Buffy the Vampire Slayer\"", "The Spiderwick Chronicles", "American reality documentary television series", "\"Alberta\", a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "Qualcomm", "water sprite", "the 10-metre platform event", "Cincinnati Bengals", "on the shore, associated with \"the Waters of Death\" that Gilgamesh had to cross to reach Utnapishtim, the far-away", "\"Guardians of the Galaxy Vol.  2\"", "November 15, 1903", "Bury St Edmunds,", "Rothschild banking dynasty", "Mr. Church", "\"Billboard\" Hot 100, \"Rich Girl\"", "Thomas Christopher Ince", "Peter 'Drago' Sell", "public", "Los Angeles", "\"The Future\"", "Prussian Lithuanian poet and philosopher Vyd\u016bnas", "al-Qaeda", "Darling", "Baldwin, Nassau County, New York", "Michael Fassbender", "House of Commons", "William Finn", "\"Love Letter\"", "Indian", "German", "Barnoldswick", "before 7 November 1435", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander Lindemann,", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "Conservative", "The Division of Cook", "\"Peshwa\" (Prime Minister)", "Prafulla Chandra Ghosh", "in section, the retina is no more than 0.5 mm thick", "United States from 1861 to 1865", "Western Samoa", "DeLorean", "amelia earhart", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "\"The New Promised Land: Silicon Valley.\"", "was the oldest tuatara ever to mate at the museum, on New Zealand's South Island,", "a snowmobile", "a snakes", "bone", "vasoconstriction of most blood vessels, including many of those in the skin, the digestive tract, and the kidneys"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6273548454469507}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3157894736842105, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.515625, "CSR": 0.5399502840909092, "EFR": 1.0, "Overall": 0.7065056818181819}, {"timecode": 88, "before_eval_results": {"predictions": ["george w. Bush", "\"La M\u00f4me Piaf\"", "george grant", "Apollon", "Richard Wagner", "Atticus Finch", "Peter Principle", "copper and zinc", "hammertone", "Dunfermline", "bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "the events of 16 September 1992", "Samoa", "george gorman", "The Daily Mirror", "copper", "Olympus Mons on Mars", "polish", "Dee Caffari", "calos", "Belize", "george clareer", "clairy", "prawns", "James Hogg", "MMORPG", "fermanagh", "Colombia", "Kevin Painter", "llyn Padarn", "Catherine of Aragon", "Muhammad Ali", "Carmen Miranda", "m Mishal Husain", "peter Sampras", "August 10, 1960", "Tallinn", "Sarajevo", "gluten", "enclosed", "arthur ransome", "muthia murlitharan", "Ridley Scott", "four", "Futurama", "adrian edmondson", "63 to 144 inches", "1925", "September 29, 2017", "Walter Brennan", "from 13 to 22 June 2012", "Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba", "the surge,", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "the devil's food cake", "Michelangelo", "Missouri", "The Jetsons"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5971590909090909}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.9090909090909091, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-4613", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-615", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-3589", "mrqa_hotpotqa-validation-3114", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.515625, "CSR": 0.5396769662921348, "EFR": 1.0, "Overall": 0.7064510182584269}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "a graphical user interface", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "large", "Nelly", "gladiators", "Finding Nemo", "the hyoid horns", "the Kite Runner", "a shark", "Kampala", "Oprah Winfrey", "Dixie Chicks", "apple pie", "California", "Best Buy", "the Mediterranean Sea", "Pope John Paul II", "Lobster Newburg", "Yemen", "DreamWorks", "chariots", "Neruda", "the 5th amendment", "a mite", "Saturn", "Nanny Diaries", "liquid crystal technology", "Robert Frost", "an authoritative pronouncement", "almonds", "Crete", "Father Brown", "reuben", "The Outsiders", "the waltz", "(Matt) Gaydos", "Jane Austen", "Wisconsin", "Charles Darnay", "Q's assistant", "When Harry Met Sally", "MEXICO", "pumice", "Molson Brewery", "Jan and Dean", "Robin Hood", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "andorra", "mike faraday", "germany fdr", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "upper respiratory infection,\"", "Fernando Gonzalez", "At least 14", "more than two years,"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7369791666666666}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-1293", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-16638", "mrqa_searchqa-validation-13703", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-14125", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-7462", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795"], "SR": 0.640625, "CSR": 0.5407986111111112, "EFR": 1.0, "Overall": 0.7066753472222222}, {"timecode": 90, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.841796875, "KG": 0.503125, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "\"Loch Lomond\"", "American reality television series", "Gweilo or gwailou", "\"The Royal Family\"", "\" Rosemary's Baby\"", "Mary MacLane", "was Ethiopia's regent from 1916 to 1930", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "Los Angeles Dance Theater", "johnnie ray", "Hampton University", "\"Krabby Road\"", "Jenji Kohan", "1", "God Save the King", "Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "Hard rock", "Louis Mountbatten of Burma", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "England, Scotland, and Ireland", "\"Coal Miner's daughter\"", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "mid-ninth-century Viking chieftain", "La Scala, Milan", "Orson Welles", "1987", "Schaffer", "Ryan Guno Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "Harlem River", "Turkish", "$1", "sulfur dioxide", "1913,", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "the Lord of the Rings", "Jaguar", "smut", "semi-autonomous organisational units"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7294642857142857}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-998", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1189", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_naturalquestions-validation-373"], "SR": 0.640625, "CSR": 0.5418956043956045, "EFR": 0.9130434782608695, "Overall": 0.7127065665312948}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "investment bank Friedman Billings Ramsey", "Robber Barons", "Robin Cousins", "Glory ( from the Latin gloria, `` fame, renown '' )", "LED illuminated display", "1954", "transmission, which contains a number of different sets of gears that can be changed to allow a wide range of vehicle speeds", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "membranes of the body's cells", "USS Chesapeake", "1977", "official residence of the President of the Russian Federation", "Charles Darwin", "graphical, games, and promotional materials", "Richard Stallman", "2004", "1940", "an armed conflict without the consent of the U.S. Congress", "cognitive bias", "oxidized, often gaseous products, in a mixture termed as smoke", "Spain", "two classes of organic compounds", "New England Patriots", "people in the 20th century who used obscure languages as a means of secret communication during wartime", "Zhu Yuanzhang", "1980 Summer Olympics", "Panic! at the Disco", "the posterior ( dorsal ) horn", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "following the 2017 season", "Julie Adams", "1881", "Music producer Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "600,000 cu mi", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Jane Addams", "August 5, 1937", "voters gathered as a tribe", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings, when Cowboys quarterback Roger Staubach ( a Roman Catholic and fan of The Godfather Part II ( 1974 ),", "Payson, Lauren, and Kaylie", "2015", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson near Portsmouth", "card game", "Sparta", "World Famous Gold & Silver Pawn Shop in Las Vegas", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "died in the Holmby Hills, California, mansion he rented.", "for weeks,", "Monroe", "Babel", "ternels", "Ponce de Leon"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5519160663429579}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6086956521739131, 0.0, 0.0, 1.0, 0.0, 1.0, 0.18750000000000003, 1.0, 1.0, 1.0, 0.3333333333333333, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.0, 0.9411764705882353, 0.3636363636363636, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-5243", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818"], "SR": 0.421875, "CSR": 0.5405910326086957, "EFR": 0.8108108108108109, "Overall": 0.6919991186839013}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the MacKenzie", "northwest of England", "electronic junk mail", "Tahrir Square", "David Frost", "Newbury Racecourse", "torture program", "Knutsford", "republic of cabo Verde", "Spongebob", "Farthings", "China", "Cynthia Kirkeby", "Thomas Cranmer", "George H. W. Bush", "the Caribbean Sea", "Jack Sprat", "Ronnie Kray", "conclave", "Dublin", "The Mayor of Casterbridge", "feet", "Amsterdam", "John Lennon", "Lusitania", "Anne of Cleves", "Australia", "mammal", "portugal", "Swaziland", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "Vickers Vimy", "Jinnah International Airport", "republic of India", "ethelbald I", "Peter Paul Rubens", "John Ford", "six", "Mendip Hills", "Burma", "Charles Taylor", "Pancho Villa", "a compact layout to combine keys which are usually kept separate", "Jerry Leiber and Mike Stoller", "the fourth season", "Karl Johan Schuster", "Worcester County", "Green Mountain overlook", "Lucky Dube,", "Middle East and North Africa", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "a surgical operation, prefrontal leukotomy"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6915441176470588}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-3629", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-7264", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_searchqa-validation-16895"], "SR": 0.640625, "CSR": 0.5416666666666667, "EFR": 1.0, "Overall": 0.7300520833333334}, {"timecode": 93, "before_eval_results": {"predictions": ["American heavy metal", "keeper of the Great Seal of Scotland", "1838", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "BBC Formula One coverage", "Coahuila, Mexico", "Atomic Kitten", "hetamine hydrochloride (shabu)", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australian", "the D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "Georgia Southern University", "Dutch", "1999", "Mudvayne", "1947", "Easter Rising", "Tuesday, January 24, 2012", "John Monash", "king edward the Elder", "Middlesbrough F.C.", "leftwing", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "a heavy metal band", "Cher", "125 lb (57 kg)", "chocolate-colored", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Queen Elizabeth II", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "a herons", "chariots", "Louisiana", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "a cat", "Gibraltar", "a numeric scale used to specify the acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6982886904761905}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false], "QA-F1": [0.5, 0.8333333333333334, 0.0, 0.8, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3786", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_naturalquestions-validation-8652"], "SR": 0.546875, "CSR": 0.5417220744680851, "EFR": 1.0, "Overall": 0.730063164893617}, {"timecode": 94, "before_eval_results": {"predictions": ["Aston Villa", "Guinea", "mayflower", "four", "Daily Mail Online", "tartan", "Toy Story", "GM Korea", "lungs", "nepal", "Left Book Club", "south africa", "St Columba", "Donald Sutherland", "New York", "nekemte", "portugal", "sternum", "pressure", "James Murdoch", "maine", "Fluids", "bach Cantatas", "Squeeze", "Altamont Speedway Free Festival", "Robert Plant", "Jerry Seinfeld", "stern tube", "kia", "mouse", "Sir Robert Walpole", "eight", "Principality of Andorra", "a braffin", "maine", "Kunsky", "Great Paul", "27", "Formula One World Champion", "squash", "Mary Decker", "karakorams 2", "nepal", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "Lady Godwin", "Festival of Britain", "welding boots", "farthingale", "1940s", "0.30 in ( 7.6 mm )", "in vitro", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4 loss,", "al Qaeda,", "UNICEF", "Corman", "Florence", "Saturn", "globalization"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6377604166666666}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5351", "mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-990", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-5055", "mrqa_triviaqa-validation-1469", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1521", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-185", "mrqa_triviaqa-validation-4256", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_newsqa-validation-3796", "mrqa_searchqa-validation-12145"], "SR": 0.578125, "CSR": 0.5421052631578948, "EFR": 1.0, "Overall": 0.730139802631579}, {"timecode": 95, "before_eval_results": {"predictions": ["American actor, producer, and director", "its air-cushioned sole", "local South Australian and Australian produced content", "Oryzomyini", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Van Diemen's Land", "Jim Kelly", "Frankfurt", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three acts", "Dra\u017een Petrovi\u0107", "Prussia", "David Wells", "north bank of the North Esk", "two", "Argentine cuisine", "13th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1993", "Jesus", "Sulla", "Riot Act", "Larry Gatlin Brothers", "right-hand batsman", "black nationalism", "\"Futurama\"", "Swiss", "Deftones", "\"Gangsta's Paradise\"", "Clitheroe Football Club", "Green Lantern", "\"Cleopatra\"", "The Fault in Our Stars", "Liesl", "how the Grinch Stole Christmas", "sheepskin", "White Horse", "banjo player", "Yellow fever", "Elise Marie Stefanik", "Francis Schaeffer", "Australia", "between 3.9 and 5.5 mmol / L ( 70 to 100 mg / dL )", "President pro tempore of the Senate", "davian Christie", "capture of Quebec", "cold comfort Farm", "red", "lightning strikes", "murders of his father and brother", "Guernsey", "Southern Christian Leadership Conference", "france", "brain and spinal cord"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6665871888528139}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.375, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.9523809523809523, 0.28571428571428575, 0.0, 0.5, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.578125, "CSR": 0.54248046875, "EFR": 1.0, "Overall": 0.73021484375}, {"timecode": 96, "before_eval_results": {"predictions": ["160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "a total of 360 members who are elected in single - member constituencies using the simple majority ( or first - past - the - post ) system", "Rukmini", "Carol Ann Susi", "the pyloric valve", "Jackie Van Beek", "the seven churches", "The Gaither Vocal Band", "Phillip Paley", "Germany", "Einstein", "1830", "pancreas", "100", "James Madison", "Woodrow Strode", "Baaghi", "Jenny Humphrey", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "epidermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract )", "1657", "The musical premiered on October 16, 2012, at Ars Nova ; directed by Rachel Chavkin the show was staged as an immersive production, with action happening around and among the audience", "American country music duo Brooks & Dunn", "September 15, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75 micrometers", "Miller Lite", "Oona Castilla Chaplin", "The speech compares the world to a stage and life to a play, and catalogues the seven stages of a man's life, sometimes referred to as the seven ages of man", "Lulu", "the NFL", "spacewar!", "flag of the United States of America", "Profit maximization", "Melbourne", "early morning of April 1", "city of San Antonio", "1,281,900 servicemembers, with an additional 801,200 people in the seven reserve components", "The Baltimore bullet", "royal oak", "The Krankies", "France", "Province of Syracuse", "June 11, 1986", "3-2", "200", "Republican Gov. Bobby Jindal", "\"reshit\"", "the prairie", "a shear", "curfew"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6334181162069126}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true], "QA-F1": [0.7499999999999999, 1.0, 0.0, 0.1, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.48275862068965514, 1.0, 0.3137254901960785, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06451612903225806, 1.0, 1.0, 1.0, 0.1739130434782609, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-6517", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2411", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-199", "mrqa_naturalquestions-validation-1479", "mrqa_triviaqa-validation-800", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334", "mrqa_searchqa-validation-16855"], "SR": 0.5625, "CSR": 0.5426868556701031, "EFR": 0.9285714285714286, "Overall": 0.7159704068483064}, {"timecode": 97, "before_eval_results": {"predictions": ["Pchaikovsky", "dark places", "the Konabar", "the boll weevil", "touchpad", "Wikipedia", "the Sundance Kid", "Japanese", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Edgar Allan Poe", "Sergey Brin", "Sanders", "Billy Corgan", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "the Stanza della Segnatura", "an ant", "birkenstock", "The Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Rumpole", "Bush", "Steve Austin", "Kurt Warner", "40", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "a Brown bear", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "Vietnamese", "Crayola", "The Man in the Gray Flannel Suit", "a process of an immigrant group adopting the ways of a new land", "orange", "Isaiah Amir Mustafa", "1999", "Americans acting under orders", "mike hammer", "\"The Crow,\"", "L. P. Hartley", "Temacine Tamazight", "the European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "It wasn't appreciated how much of an impact it can have on a patient's quality of life,\"", "Prada"], "metric_results": {"EM": 0.625, "QA-F1": 0.6625}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.06666666666666667, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-16570", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-10403", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.625, "CSR": 0.5435267857142857, "EFR": 1.0, "Overall": 0.7304241071428571}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Redblush", "a chargeback", "The Little Puffer", "the cornea", "lemonade", "Rumpole", "the guillotine", "the light bulb", "Spider-Man", "Atlanta", "China", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Rinzai", "El", "Zenith", "baboon", "wine", "\"What the hell did you trade Jay Buhner for?\"", "the Q- Tips", "natural selection", "Massachusetts", "Battle of the Bulge", "Shaft", "(W. Somerset) Maugham", "the Two Sicilies", "the Battle of Trafalgar", "republic", "Francis Drake", "the college of Charles", "Albert Einstein", "a bingo", "the pituitary Gland", "Alfred Hitchcock", "Hank Aaron", "reconnaissance", "Florida", "Ectoplasm", "Thomas Jefferson", "god of war", "Dante", "Christopher Columbus", "(Joseph) Haydn", "meringue", "Serena", "the Yakuza", "stones", "four", "James Hutton", "961", "michael william", "a curse", "michael seacole", "Orchard Central", "Fort Hood, Texas", "OutKast", "iPods", "suspend all", "Tuesday", "Nick Sager"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6416666666666667}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-6821", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-1611", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.546875, "CSR": 0.5435606060606061, "EFR": 0.9655172413793104, "Overall": 0.7235343194879833}, {"timecode": 99, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.826171875, "KG": 0.48671875, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "2020", "neuropsychology", "Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013 ( XLVIII )", "Ozzie Smith", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "the Beatles", "the Persian style of architecture", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "subtractive notation", "2014 -- 15", "Natural - language processing ( NLP )", "six - hoop game", "A request line", "It is run by The R&A, the same body that organises The Open Championship", "three high fantasy adventure films directed by Peter Jackson", "Sohrai", "the Jos Plateau", "celebrity alumna Cecil Lockhart", "James Long", "the fifth-most populous city in Florida", "March 23, 2018", "starting quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Disha Vakani", "Nickelback", "1999", "Queen M\u00e1xima of the Netherlands", "is a song recorded, written, and produced by American musician Lenny Kravitz for his second studio album, Mama Said ( 1991 )", "Deuteronomy 5 : 4 -- 25", "rotation axes ( / \u02c8\u00e6ksi\u02d0z / AK - seez )", "Ren\u00e9 Georges Hermann - Paul", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1996", "Fawcett", "charlie chaplin", "Francis Matthews", "hymenaeus", "2003", "1776", "Field Marshal Stapleton Cotton", "transit bombings", "eight-day", "102 new jobs for a minimum of nine weeks.", "Spain", "Elijah", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6467645202020202}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-3310", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-1831", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-791", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3692", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-2803", "mrqa_searchqa-validation-3524"], "SR": 0.59375, "CSR": 0.5440625, "EFR": 1.0, "Overall": 0.7131875}]}