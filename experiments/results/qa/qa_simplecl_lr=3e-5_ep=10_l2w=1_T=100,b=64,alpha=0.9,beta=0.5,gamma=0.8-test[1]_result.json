{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]', diff_loss_weight=1.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4080, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["April 20", "Paul Whiteman", "2005", "Islamism", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "the center of the curving path", "eight", "28", "global", "Fresno", "Amazonia: Man and Culture in a Counterfeit Paradise", "88%", "broken wing and leg", "Emmy Awards", "silt up the lake", "The TEU specifically excludes certain regions, for example the Faroe Islands, from the jurisdiction of European Union law", "legitimate medical purpose", "week 7", "Kromme Rijn", "Robert Boyle", "five", "Paul Samuelson", "The First Doctor encounters himself in the story The Space Museum", "until the age of 16", "80%", "Five", "threatened \"Old Briton\" with severe consequences", "less than 200,000", "Anheuser-Busch InBev", "Charles River", "Veni redemptor gentium", "northwestern Canada", "intracellular pathogenesis", "1998", "seven months old", "The Service Module was discarded", "July", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Article 17(3)", "Doctor Who \u2013 The Ultimate Adventure", "Eric Roberts", "electricity could be used to locate submarines", "1910\u20131940", "the owner", "2.666 million", "September 1969", "Apollo Program Director", "dreams", "Charles I", "more than 1,100 tree species", "complete the modules to earn Chartered Teacher Status", "true larvae", "apicomplexan-related diseases", "Rev. Paul T. Stallsworth", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "2009", "Daniel Diermeier", "PNU and ODM camps", "136", "12 to 15 million", "flax", "February 1, 2016", "2001", "lipophilic alkaloid toxins"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8867121848739495}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6614", "mrqa_squad-validation-4170", "mrqa_squad-validation-6426", "mrqa_squad-validation-8037", "mrqa_squad-validation-7774", "mrqa_squad-validation-3885", "mrqa_squad-validation-1492", "mrqa_squad-validation-5788", "mrqa_squad-validation-4343"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["Carolina Panthers", "1530", "Gallifrey", "ca. 2 million", "intractable problems", "effective planning", "pyrenoid and thylakoids", "Canterbury", "1873", "A bridge", "clergyman", "Commission v Italy", "sports tourism", "G", "eating both fish larvae and small crustaceans", "slow to complete division", "Astra's", "T. T. Tsui Gallery", "1905", "theatres", "those who proceed to secondary school or vocational training", "1521", "Search the Collections", "CD8", "3 January 1521", "a bill", "the University of Aberdeen", "2014", "Missy", "US$10 a week", "Super Bowl City", "Dudley Simpson", "esoteric", "temperature and light", "4000 years", "new entrance building", "Levi's Stadium", "tutor", "electricity", "2007", "Los Angeles", "zeta function", "adviser", "over $40 million", "When the Methodists in America were separated from the Church of England", "Lead fusible plugs", "occupational stress", "Synthetic aperture radar", "European Parliament and the Council of the European Union", "the coronation of Queen Elizabeth II", "O(n2)", "the Main Quadrangles", "35", "quadratic time", "antisemitic", "over-fishing and long-term environmental changes", "the Onon River and the Burkhan Khaldun mountain", "Hassan al-Turabi", "robbery", "Arlen Specter", "she is God-sent", "Venus Williams", "prisoners", "a globose pome"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7604166666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8691", "mrqa_squad-validation-3958", "mrqa_squad-validation-4648", "mrqa_squad-validation-8864", "mrqa_squad-validation-9454", "mrqa_squad-validation-7818", "mrqa_squad-validation-1272", "mrqa_squad-validation-2122", "mrqa_squad-validation-1530", "mrqa_squad-validation-1818", "mrqa_squad-validation-2525", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808", "mrqa_triviaqa-validation-5325"], "SR": 0.734375, "CSR": 0.796875, "EFR": 0.8823529411764706, "Overall": 0.8396139705882353}, {"timecode": 2, "before_eval_results": {"predictions": ["it developed into a major part of the Internet backbone", "best-known legend", "Egyptians", "quantity surveyor", "the \"missile gap\"", "Tanaghrisson", "1852", "1564", "Concentrated O2", "adenosine triphosphate", "300", "11.5 inches", "1964", "infected corpses", "CD4", "the Lutheran and Reformed states in Germany and Scandinavia", "Chinggis Khaan International Airport", "modern buildings", "May through September", "NBC", "Three", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Gymnosperms", "to punish Christians", "Word and Image", "Nafzger", "MPEG-4", "BSkyB", "chromalveolates", "embroidery", "three", "26", "Duran Duran", "The Mallee and upper Wimmera are Victoria's warmest regions with hot winds blowing from nearby semi-deserts", "long distance", "in the chloroplasts of C4 plants", "the violence", "primality", "divergent boundaries", "the Chancel Chapel", "Kurt H. Debus", "the construction of military roads", "in bays where they occur in very high numbers", "soap opera Dallas", "Ted Heath", "late 14th-century", "high-voltage", "contract", "Arabic numerals", "pamphlets on Islam", "draftsman", "1993\u201394", "$10 billion", "France", "20", "Iran", "the Koreans edge into second place in Asian qualifying Group 2 to finish ahead of Saudi Arabia on goal difference and seal their place in the finals", "the results by a chaplain", "56", "school", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory", "the judge in the federal trial of alleged \"underwear bomber\" Umar Farouk AbdulMutallab refused Tuesday to prevent the prosecution from calling the device he allegedly carried a \"bomb.\"", "coca wine", "Dissection"], "metric_results": {"EM": 0.703125, "QA-F1": 0.768780637254902}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0909090909090909, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4847", "mrqa_squad-validation-3088", "mrqa_squad-validation-3480", "mrqa_squad-validation-8905", "mrqa_squad-validation-3270", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565", "mrqa_squad-validation-9379", "mrqa_squad-validation-2906", "mrqa_squad-validation-2899", "mrqa_squad-validation-4322", "mrqa_squad-validation-9872", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1201"], "SR": 0.703125, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 3, "before_eval_results": {"predictions": ["\"vanguard of change and Islamic reform\"", "less than a year", "dampening the fire", "187 feet", "Zagreus", "Swynnerton Plan", "extra-legal", "Harvey Martin", "1895", "lion, leopard, buffalo, rhinoceros, and elephant", "Establishing \"natural borders\"", "drama series", "17", "sold", "income inequality", "pastor", "1534", "mesoglea", "20%", "Isaac Newton", "Miocene", "citizenship", "13", "force model", "Super Bowl City", "three", "Georgia", "Fort Caroline", "Horace Walpole", "Afrikaans", "adaptive immune system", "24", "applications such as on-line betting, financial applications", "United Kingdom", "issues related to the substance of the statement", "2007", "Luther's anti-Jewish works", "short-tempered and even harsher", "the First Minister", "two catechisms", "orientalism and tropicality", "John Dobson", "Super Bowl 50", "Beryl", "prima scriptura", "the Mongol Empire", "LOVE Radio", "\u201cLady\u201d or a \u201cWoman\u201d", "Robin's Brewery", "man's vehicle is a symbol of his manhood", "cutis anserina", "Bogota", "Artemis", "small", "Chile's second-largest city", "pilot", "three", "South Africa", "9", "Dorset", "The Daily Mirror", "Kat ( Jessie Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Paul Lynde", "adenosine diphosphate"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7318910256410256}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-1502", "mrqa_squad-validation-10466", "mrqa_squad-validation-3139", "mrqa_squad-validation-2486", "mrqa_squad-validation-9540", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-5104"], "SR": 0.6875, "CSR": 0.74609375, "EFR": 0.95, "Overall": 0.848046875}, {"timecode": 4, "before_eval_results": {"predictions": ["four days", "the International Fr\u00e9d\u00e9ric Chopin Piano Competition", "Nuda", "non-tertiary", "21", "The Christmas Invasion", "vertebrates", "Miocene", "Robert Underwood Johnson", "Cuba", "Horniman Museum", "the ability to pursue valued goals", "University of Erfurt", "curved", "Johann Sebastian Bach", "1524\u201325", "one of the most significant developments in a century of Anglo-French conflict", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "2009", "British", "2005", "Germany and Austria", "self-starting", "two tumen (20,000 soldiers)", "semi-Tijuana", "Henry Laurens", "planned projects sponsored by the National Science Foundation (NSF) beginning in 1985 to promote advanced research and education networking in the United States", "Go-Ahead", "international football", "second-largest", "biologist", "upper sixth", "arrest", "Pakistan", "1185", "Central business districts", "Hisao Yamada", "the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "13th century", "presence or (sometimes) absence", "co-NP", "between 1.4 and 5.8 \u00b0C above 1990 levels", "in 2007 Charlie Crist took over for Jeb Bush in the Terri Schiavo case.", "cornwall", "agie", "cornwall", "cornwall", "cornwall", "she gave military secrets to her brother", "oak leaf", "in dreams", "in 1397, Margrethe I had fought since 1375 to seize the power in Denmark, and later on in the two other kingdoms.", "conce conce conceals how He Made The Legendary Bob Dylan", "cornwall", "cornane", "santa fuss", "agdel's Lost Letter and P=NP", "chihiro", "agnostic", "cornwall", "Skat", "music business law", "Christopher Nolan", "gun"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5798251138229586}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.09090909090909091, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-6981", "mrqa_squad-validation-10145", "mrqa_squad-validation-7554", "mrqa_squad-validation-6046", "mrqa_squad-validation-2831", "mrqa_squad-validation-4848", "mrqa_squad-validation-5374", "mrqa_squad-validation-8372", "mrqa_squad-validation-6279", "mrqa_squad-validation-5178", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-9913", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-2062", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-13775", "mrqa_triviaqa-validation-1816"], "SR": 0.546875, "CSR": 0.70625, "EFR": 1.0, "Overall": 0.853125}, {"timecode": 5, "before_eval_results": {"predictions": ["wedding banquet", "respiration", "a deficit", "$216,000", "City of Malindi", "Apollo Applications Program", "trial division", "1965", "An attorney", "eight", "The Book of Discipline", "Olivier Messiaen", "1759-60", "The Rankine cycle", "deadly explosives", "first half of the eighteenth century", "5,560", "Tricia Marwick", "the main contractor", "ctenophores", "third most abundant", "an adjustable spring-loaded valve", "agriculture", "metamorphosed", "David Suzuki", "if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "June 1979", "4.95 mL", "December 12", "second half of the 20th Century", "Ten", "the edge railed rack and pinion Middleton Railway", "John Elway", "1598", "The Eleventh Doctor", "Tugh Temur", "\"War of Currents\"", "dampening the fire", "eight years", "the Dalai Lama", "a planned training exercise designed to help the prince learn to fly in combat situations", "a multibillion dollar arms deal", "the Dalai Lama", "lethal", "2-0", "Chesley \"Sully\" Sullenberger", "Stanford", "issued his first military orders", "July", "veterans and their entire family", "the coalition", "the Bronx", "Shanghai", "Fernando Gonzalez", "sportswear", "Jennifer Arnold and husband Bill Klein", "killing up to 280,000 people", "the sins of the members of the church", "humans", "a simple majority vote", "George Best", "June 26, 2018", "Mahler symphonies", "March 19, 2017"], "metric_results": {"EM": 0.75, "QA-F1": 0.8078258547008548}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.8, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4730", "mrqa_squad-validation-3555", "mrqa_squad-validation-3179", "mrqa_squad-validation-383", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-6817"], "SR": 0.75, "CSR": 0.7135416666666667, "EFR": 0.875, "Overall": 0.7942708333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["John Mayow", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "1 July 1851", "1562", "32.9%", "the architect's client and the main contractor.", "2009", "Katy\u0144 Museum", "Derek Wolfe and Malik Jackson", "silicon (silica SiO2, as found in granite and quartz), aluminium (aluminium oxide Al2O3, in bauxite and corundum)", "methotrexate or azathioprine", "over 100,000", "3.55 inches", "Bukhara", "Beyonc\u00e9", "its safaris", "Huntington Boulevard", "Northern Pride Festival", "eight", "Henry Laurens", "Roone Arledge", "the Florida legislature", "autoimmune", "Diarmaid MacCulloch", "permafrost", "University Athletic Association", "idolatry", "Protestant clergy to marry", "a suite of network protocols", "German creedal", "21 to 11", "eight", "\"Guilt implies wrong-doing.", "1130", "15", "50", "November 25, 2002", "the center of the Northern Hemisphere", "fovea centralis", "God", "Will", "MGM Resorts International", "Bobby Darin", "Gene MacLellan", "Matt Monro", "Chilka Lake is a brackish water lake along the eastern coastal plain. It lies in the state of Odisha and stretches to the south of the Mahanadi Delta.", "2013", "Wisconsin", "Jonathan Breck", "Neil Young", "milling", "an oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke.", "In a result, de jure racial segregation was ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution", "humid subtropical", "Sir Alex Ferguson", "Bachendri Pal", "2014", "1979", "Manet", "Justin Spitzer", "CNN", "Resting on or touching the ground or bottom.", "a lung disease that makes it hard to breathe.", "Quizlet"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7099198772718509}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.8, 0.5, 1.0, 0.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 0.8, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1166", "mrqa_squad-validation-291", "mrqa_squad-validation-3511", "mrqa_squad-validation-8400", "mrqa_squad-validation-6223", "mrqa_squad-validation-4673", "mrqa_squad-validation-2416", "mrqa_squad-validation-978", "mrqa_squad-validation-6913", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9961", "mrqa_searchqa-validation-10794", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-9370"], "SR": 0.640625, "CSR": 0.703125, "EFR": 0.9130434782608695, "Overall": 0.8080842391304348}, {"timecode": 7, "before_eval_results": {"predictions": ["The Hoppings", "Chicago", "I would be doing a service if I killed my father when he is hunting, made an alliance with Sultan Muhammad, brought this land to life and gave assistance and support to the Muslims.", "Inherited wealth may help explain why many Americans who have become rich may have had a \"substantial head start\"", "\"There is a world of difference between his belief in salvation and a racial ideology.", "StubHub Center", "WMO Executive Council and UNEP Governing Council", "trial division", "1999", "Gian Lorenzo Bernini, Grinling Gibbons, Daniel Marot, Louis Laguerre, Antonio Verrio, Sir James Thornhill, William Kent, Robert Adam, Josiah Wedgwood,", "local authorities", "dendritic cells, keratinocytes and macrophages", "Newcastle Mela", "ambiguity", "1665", "The Day of the Doctor", "punts", "1.6 kilometres", "\"winds up\" the debate by speaking after all other participants.", "2010", "around 100,000", "thermal expansion", "John Sutcliffe", "Owen Daniels", "incitement to terrorism", "Brookhaven", "A construction project", "Roy", "17 February 1546", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people)", "September 30, 1960", "Brian Steele", "the status line", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "a bond that the issuer has to pay a premium, the so - called call premium", "left coronary artery", "five", "Havana Harbor", "three", "In June 22, 1942", "Walter Mondale", "Mitch Murray", "Marie Fredriksson ( vocals ) and Per Gessle ( vocals and guitar )", "December 1, 2009", "virtual reality simulator", "Noah Schnapp", "Antonio Banderas", "ratio of the length s of the arc by the radius r", "Kanawha River", "Julie Stichbury", "In the U.S., tomato pur\u00e9e is a processed food product, usually consisting of only tomatoes, but can also be found in the seasoned form", "Michelle Stafford", "Spanish", "September 2017", "Krypton", "1990", "Russia", "phyronia tithonus", "Murcia", "India", "Wolfgang Amadeus Mozart", "Armin Meiwes", "Mot\u00f6rhead", "Bill Clinton"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6539340421469342}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.10526315789473685, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2608695652173913, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.8372093023255813, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.16216216216216214, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6294", "mrqa_squad-validation-7456", "mrqa_squad-validation-2608", "mrqa_squad-validation-8910", "mrqa_squad-validation-5534", "mrqa_squad-validation-5189", "mrqa_squad-validation-6402", "mrqa_squad-validation-9465", "mrqa_squad-validation-4385", "mrqa_squad-validation-6205", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3182", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-5845"], "SR": 0.5625, "CSR": 0.685546875, "EFR": 1.0, "Overall": 0.8427734375}, {"timecode": 8, "before_eval_results": {"predictions": ["Prague", "extremely high humidity", "Manning", "Sports Night", "2nd century BCE", "in the form of taxation", "destroy the antichrist", "collective bargaining, political influence, or corruption", "the Ilkhanate", "a second Gleichschaltung", "1864", "NL and NC", "over fifty", "Albert Einstein", "discarded", "state or government schools", "ash tree", "Procureur du Roi v Dassonville", "restored video, remaining in black and white, contains conservative digital enhancements and did not include sound quality improvements", "1687", "adaptive immune system", "case law by the Court of Justice", "Von Miller", "an archetypal \"mad scientist\"", "Dendritic cells", "Allston Science Complex", "writing a five volume book in his native Greek \u03a0\u03b5\u03c1\u03af \u03cd\u03bb\u03b7\u03c2 \u03b9\u03b1\u03c4\u03c1\u03b9\u03ba\u03ae\u03c2 in the 1st century AD", "wars", "in inorganic forms", "the aim was to solve South Africa's'' ethnic problems by creating complementary economic and political units for different ethnic groups", "Hirschman", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Sergeant Himmelstoss", "four", "bohrium", "a major fall in stock prices", "Buddhism", "March 31 to April 8, 2018", "Blind carbon copy to tertiary recipients who receive the message", "start fires, hunt, and bury their dead", "Ray Charles", "the story's themes of moral dilemma and choosing between the easy and the right decision", "the type of hazard ahead", "Rose Stagg", "continues the pre-existing appropriations at the same levels as the previous fiscal year", "Arnold Schoenberg", "Tiffany Adams Coyne", "Trace Adkins", "Andy Allo, Venzella Joy Williams, and Hannah Fairlight as Calamity, Serenity, Charity, and Veracity", "Southampton ( 1902, then in the Southern League )", "July 23, 2016", "Auburn Tigers football team", "Katherine Kiernan Maria", "1943", "Rumplestiltskin", "Kryptonite", "can integrate into the host genome", "Santiago", "Japan", "Lance Cpl. Maria Lauterbach", "Leonardo DiCaprio", "californium", "Peter Rovit", "the Pythagorean theorem"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5930932475290573}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.09523809523809525, 0.0, 0.0, 0.25, 1.0, 0.8, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 0.0, 1.0, 0.9142857142857143, 0.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.5, 0.5, 0.6, 1.0, 0.0851063829787234, 0.0, 0.0, 0.6896551724137931, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.7499999999999999, 1.0, 0.0, 1.0, 0.30303030303030304, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7561", "mrqa_squad-validation-4634", "mrqa_squad-validation-4331", "mrqa_squad-validation-4029", "mrqa_squad-validation-3113", "mrqa_squad-validation-6678", "mrqa_squad-validation-3946", "mrqa_squad-validation-1248", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-2351", "mrqa_newsqa-validation-2518", "mrqa_searchqa-validation-10924", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-2789"], "SR": 0.46875, "CSR": 0.6614583333333333, "EFR": 0.9411764705882353, "Overall": 0.8013174019607843}, {"timecode": 9, "before_eval_results": {"predictions": ["gold", "War of Currents", "12 research institutes", "Antoine Lavoisier", "United Kingdom, Australia, Canada and the United States", "well before Braddock's departure for North America", "Philip Webb and William Morris", "forming a 'A National Gallery of British Art',", "Kissinger's", "between 1980 and 1990s", "exceeds any given number", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "in the Channel Islands", "emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface", "they were nomads", "private southern Chinese manufacturers and merchants", "Owen Jones", "the Eleventh Doctor", "the BBC National Orchestra of Wales", "bilaterians", "the traditional Chinese autocratic-bureaucratic system", "August 1992", "NBC", "France's claim to the region was superior to that of the British, since Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier", "New South Wales", "November 17, 2017", "Daren Maxwell Kagasoff", "the Royal Air Force ( RAF ) defended the United Kingdom ( UK ) against large - scale attacks by Nazi Germany's air force, the Luftwaffe", "the Sunni Muslim family", "2,050 metres ( 6,730 ft )", "to form a higher alkane", "Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive", "James Watson and Francis Crick", "September 29, 2017", "11 p.m. to 3 a.m", "the Old English wylisc ( pronounced `` wullish '' ) meaning `` foreigner '' or `` Welshman ''", "204,408", "Lake Michigan", "an object that moves around an external axis is said to orbit", "one proton", "the frequency f, wavelength \u03bb, or photon energy E", "Spanish explorers", "the roofs of the choir side - aisles at Durham Cathedral", "the Indian Civil Service", "different parts of the globe", "Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences", "florida it is illegal to sell alcohol before 1 pm on any sunday", "4,840", "Louis XVIII", "bones of the vertebral column continue to grow, by about 12 months of age, the end of the cord reaches its permanent position at the level of L1 or L2 ( closer to the head )", "T'Pau", "the London Symphony Orchestra and London Philharmonic", "Julia Ormond", "the term was historically associated with fighters such as Benny Leonard and Sugar Ray Robinson who were widely considered to be the most skilled fighters of their day, to distinguish them from the generally more popular ( and better compensated ) heavyweight champions", "George Strait", "Amber Riley and her partner Derek Hough", "the fallopian tube", "study insects and their relationship to humans, other organisms, and the environment", "195029 June 1994", "Dana Andrews", "the western United States", "Hakeemullah Mehsud", "Bob Dylan", "the saffron yellow shawl"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5816978226848226}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.37037037037037035, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7027027027027027, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.4, 1.0, 0.8205128205128205, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.18181818181818182, 0.0, 0.06666666666666667, 1.0, 0.11764705882352941, 0.0, 0.16666666666666666, 0.07692307692307693, 0.0, 0.0, 0.0, 0.1875, 0.4, 0.0, 1.0, 0.2711864406779661, 1.0, 0.0, 1.0, 0.16666666666666669, 0.5, 0.8, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7888", "mrqa_squad-validation-9334", "mrqa_squad-validation-1131", "mrqa_squad-validation-8589", "mrqa_squad-validation-7771", "mrqa_squad-validation-8410", "mrqa_squad-validation-5733", "mrqa_squad-validation-10232", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-3868", "mrqa_hotpotqa-validation-940", "mrqa_hotpotqa-validation-1398", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-16618"], "SR": 0.453125, "CSR": 0.640625, "EFR": 1.0, "Overall": 0.8203125}, {"timecode": 10, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-940", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8859", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-996", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-16618", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10035", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10145", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10301", "mrqa_squad-validation-10359", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-10415", "mrqa_squad-validation-10466", "mrqa_squad-validation-1082", "mrqa_squad-validation-1109", "mrqa_squad-validation-1131", "mrqa_squad-validation-1151", "mrqa_squad-validation-1166", "mrqa_squad-validation-1180", "mrqa_squad-validation-1180", "mrqa_squad-validation-1195", "mrqa_squad-validation-121", "mrqa_squad-validation-1217", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1395", "mrqa_squad-validation-1468", "mrqa_squad-validation-1488", "mrqa_squad-validation-1492", "mrqa_squad-validation-1621", "mrqa_squad-validation-1630", "mrqa_squad-validation-1645", "mrqa_squad-validation-170", "mrqa_squad-validation-1719", "mrqa_squad-validation-1732", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1962", "mrqa_squad-validation-1971", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2082", "mrqa_squad-validation-2122", "mrqa_squad-validation-2159", "mrqa_squad-validation-217", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2361", "mrqa_squad-validation-2412", "mrqa_squad-validation-2416", "mrqa_squad-validation-2418", "mrqa_squad-validation-2457", "mrqa_squad-validation-2486", "mrqa_squad-validation-2548", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2608", "mrqa_squad-validation-2633", "mrqa_squad-validation-2667", "mrqa_squad-validation-2678", "mrqa_squad-validation-2843", "mrqa_squad-validation-2861", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2899", "mrqa_squad-validation-291", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-2985", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3088", "mrqa_squad-validation-3104", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3119", "mrqa_squad-validation-3162", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3209", "mrqa_squad-validation-3215", "mrqa_squad-validation-3241", "mrqa_squad-validation-3307", "mrqa_squad-validation-3355", "mrqa_squad-validation-3362", "mrqa_squad-validation-3407", "mrqa_squad-validation-3411", "mrqa_squad-validation-3413", "mrqa_squad-validation-3451", "mrqa_squad-validation-348", "mrqa_squad-validation-349", "mrqa_squad-validation-3522", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3559", "mrqa_squad-validation-3569", "mrqa_squad-validation-3585", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3822", "mrqa_squad-validation-383", "mrqa_squad-validation-3830", "mrqa_squad-validation-3841", "mrqa_squad-validation-3855", "mrqa_squad-validation-3882", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-3946", "mrqa_squad-validation-4008", "mrqa_squad-validation-4028", "mrqa_squad-validation-4046", "mrqa_squad-validation-4121", "mrqa_squad-validation-4172", "mrqa_squad-validation-423", "mrqa_squad-validation-4296", "mrqa_squad-validation-433", "mrqa_squad-validation-4331", "mrqa_squad-validation-4376", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4418", "mrqa_squad-validation-4430", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-4463", "mrqa_squad-validation-4495", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4648", "mrqa_squad-validation-4673", "mrqa_squad-validation-4704", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-477", "mrqa_squad-validation-4772", "mrqa_squad-validation-4803", "mrqa_squad-validation-4807", "mrqa_squad-validation-4841", "mrqa_squad-validation-4848", "mrqa_squad-validation-4936", "mrqa_squad-validation-4983", "mrqa_squad-validation-5023", "mrqa_squad-validation-5063", "mrqa_squad-validation-5083", "mrqa_squad-validation-513", "mrqa_squad-validation-513", "mrqa_squad-validation-5136", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5247", "mrqa_squad-validation-5250", "mrqa_squad-validation-5254", "mrqa_squad-validation-5265", "mrqa_squad-validation-5295", "mrqa_squad-validation-5307", "mrqa_squad-validation-5318", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5418", "mrqa_squad-validation-5445", "mrqa_squad-validation-5485", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5563", "mrqa_squad-validation-5564", "mrqa_squad-validation-5566", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5608", "mrqa_squad-validation-5653", "mrqa_squad-validation-5664", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5766", "mrqa_squad-validation-5782", "mrqa_squad-validation-5788", "mrqa_squad-validation-5821", "mrqa_squad-validation-5843", "mrqa_squad-validation-5852", "mrqa_squad-validation-5951", "mrqa_squad-validation-6046", "mrqa_squad-validation-6049", "mrqa_squad-validation-6067", "mrqa_squad-validation-6073", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6294", "mrqa_squad-validation-6310", "mrqa_squad-validation-638", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-653", "mrqa_squad-validation-6531", "mrqa_squad-validation-6535", "mrqa_squad-validation-6548", "mrqa_squad-validation-6567", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-6678", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6840", "mrqa_squad-validation-6841", "mrqa_squad-validation-6868", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6917", "mrqa_squad-validation-6959", "mrqa_squad-validation-6962", "mrqa_squad-validation-6981", "mrqa_squad-validation-703", "mrqa_squad-validation-7030", "mrqa_squad-validation-7142", "mrqa_squad-validation-7175", "mrqa_squad-validation-7211", "mrqa_squad-validation-7252", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-743", "mrqa_squad-validation-7435", "mrqa_squad-validation-7456", "mrqa_squad-validation-7554", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7632", "mrqa_squad-validation-7633", "mrqa_squad-validation-7678", "mrqa_squad-validation-7699", "mrqa_squad-validation-7704", "mrqa_squad-validation-7709", "mrqa_squad-validation-7716", "mrqa_squad-validation-7747", "mrqa_squad-validation-7766", "mrqa_squad-validation-7771", "mrqa_squad-validation-7774", "mrqa_squad-validation-7775", "mrqa_squad-validation-7800", "mrqa_squad-validation-7818", "mrqa_squad-validation-7831", "mrqa_squad-validation-7846", "mrqa_squad-validation-7863", "mrqa_squad-validation-7888", "mrqa_squad-validation-7899", "mrqa_squad-validation-795", "mrqa_squad-validation-7965", "mrqa_squad-validation-797", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8071", "mrqa_squad-validation-8163", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8365", "mrqa_squad-validation-8372", "mrqa_squad-validation-8410", "mrqa_squad-validation-8414", "mrqa_squad-validation-8441", "mrqa_squad-validation-8486", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8589", "mrqa_squad-validation-859", "mrqa_squad-validation-8598", "mrqa_squad-validation-8600", "mrqa_squad-validation-8666", "mrqa_squad-validation-8670", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8907", "mrqa_squad-validation-9020", "mrqa_squad-validation-9030", "mrqa_squad-validation-9050", "mrqa_squad-validation-9116", "mrqa_squad-validation-9121", "mrqa_squad-validation-9151", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9379", "mrqa_squad-validation-9401", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9454", "mrqa_squad-validation-9465", "mrqa_squad-validation-9484", "mrqa_squad-validation-9540", "mrqa_squad-validation-9590", "mrqa_squad-validation-9608", "mrqa_squad-validation-9689", "mrqa_squad-validation-9738", "mrqa_squad-validation-976", "mrqa_squad-validation-9760", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9954", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6711"], "OKR": 0.9140625, "KG": 0.409375, "before_eval_results": {"predictions": ["2009", "Huntington Boulevard", "since 1979", "William Hartnell and Patrick Troughton", "the concept Distributed Adaptive Message Block Switching", "1331", "Confucianism and promoting Chinese cultural values", "a computer network funded by the U.S. National Science Foundation (NSF)", "pseudo- Sciencesences", "a military coup d'\u00e9tat", "the Tyne Tunnel", "proteolysis", "a form of starch", "Eisleben", "break off the cathode, pass out of the tube, and physically strike him", "late night talk shows", "13\u20137", "around half", "the comprehensive institutions of the Great Yuan", "linear", "Chuck Howley", "with observations", "Annette", "Tim McGraw and Kenny Chesney", "a man who could assume the form of a great black bear", "a house edge of between 0.5 % and 1 %", "the sperm head disconnects from its flagellum", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "a sociological perspective", "6 January 793", "Colon Street", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Jane Fonda", "Robert Irsay", "Edgar Lungu", "UNESCO / ILO", "five", "Achal Kumar Jyoti", "the President", "Homer Banks, Carl Hampton and Raymond Jackson", "team managers", "BeBe Winans", "Detroit Red Wings", "after releasing Spike from the obligation to be Sweet's `` bride '', tells the group how much fun they have been", "six", "Yuzuru Hanyu", "a receptor or enzyme is distinct from the active site", "the geologist James Hutton", "Everywhere", "31 October 1972", "December 2, 1942", "September 8, 2017", "the Hollywood Masonic Temple", "Robert Duncan McNeill", "the Gaget, Gauthier & Co. workshop", "Renhe Sports Management Ltd", "Tangled", "a form of contactless communication between devices like smartphones or tablets", "Kenneth Hood \"Buddy\" MacKay Jr.", "a hard rock band", "Scotland", "she was a young skater and desperately wanted to make her mother proud", "prohibiting the expansion of slavery into a territory where slave status was favored", "Peyton Place"], "metric_results": {"EM": 0.5, "QA-F1": 0.5975364479270728}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.7272727272727273, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.375, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.2, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.21428571428571427, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_squad-validation-9831", "mrqa_squad-validation-5357", "mrqa_squad-validation-1388", "mrqa_squad-validation-434", "mrqa_squad-validation-625", "mrqa_squad-validation-8747", "mrqa_squad-validation-8530", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-9666", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-2277", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-15757"], "SR": 0.5, "CSR": 0.6278409090909092, "EFR": 0.96875, "Overall": 0.7336150568181818}, {"timecode": 11, "before_eval_results": {"predictions": ["The availability of the Bible in vernacular languages", "detention", "Western Xia", "by qualified majority", "carbon related emissions", "co-chair", "His wife Katharina", "three", "August 10, 1948", "the holy catholic (or universal) church", "30\u201360%", "Louis Paul Cailletet", "Class II MHC molecules", "crust and lithosphere", "orange", "electrons are fermions, they cannot occupy the same quantum mechanical state as other electrons", "stroke", "13 June 1525", "the type of reduction being used", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Chopper Nandi", "September 1947", "Spanish explorers", "Western Australia", "October 19, 2005", "April 10, 2018", "October 1, 2015", "number of games where the player played, in whole or in part", "cat", "a nobiliary particle indicating a noble patrilineality or as a simple preposition that approximately means of or from in the case of commoners", "Robert Kraft", "the country was called by the name of the dynasty", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Kida", "1960", "Bengal tiger", "Part 2", "a barrier that runs across a river or stream to control the flow of water", "De Wayne Warren", "lacteal", "1970", "autu", "Tom Brady", "Margaery Tyrell", "the Isle of Sheppey in England", "Lake Powell", "Clarence L. Tinker", "Bed and breakfast   Botel   Boutique hotel", "Trace Adkins", "Garbi\u00f1e Muguruza", "Keith Richards", "Hercules", "June 12, 2018", "Bart Howard", "111", "Leonard Bernstein", "The Hague", "historic buildings, arts, and published works", "Lisa", "Arnoldo Rueda Medina", "tax incentives for businesses hiring veterans as well as job training for all service members leaving the military.", "George Wallace", "Florida", "CNN"], "metric_results": {"EM": 0.5, "QA-F1": 0.5918670432916756}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.9743589743589743, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 0.4, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1111111111111111, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-8581", "mrqa_squad-validation-3474", "mrqa_squad-validation-4952", "mrqa_squad-validation-10483", "mrqa_squad-validation-3385", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-386", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1549", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-13057"], "SR": 0.5, "CSR": 0.6171875, "EFR": 0.9375, "Overall": 0.725234375}, {"timecode": 12, "before_eval_results": {"predictions": ["savanna or desert", "Downtown San Bernardino", "Allan Bloom, ''The Good War\" author Studs Terkel, American writer, essayist, filmmaker, teacher, and political activist Susan Sontag, analytic philosopher and Stanford University Professor of Comparative Literature Richard Rorty", "three", "Graz, Austria", "Sky Digital", "until 1796", "Einstein", "Gary Kubiak", "soy farmers", "The Sinclair Broadcast Group", "mercuric oxide (HgO)", "Guglielmo Marconi", "Luther's education", "wages and profits", "southern Europe", "the judicial branch", "biostratigraphers", "1999", "Miami Heat of the National Basketball Association ( NBA )", "the nasal septum", "a warrior, Mage, or rogue coming from an elven, human, or Dwarven background", "Bob Dylan", "1799", "interphase", "$2 million in 2011, with a winner's share of $315,600", "birch", "Roger Federer", "James Corden", "Pasek & Paul", "October 2", "1956", "Richard T. Jones", "Gwendoline Christie", "Orangeville, Ontario, Canada", "Guy Pemberton ( Hugh Dickson )", "RAM", "Afghanistan", "a fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Walter Brennan", "James Rodr\u00edguez", "8ft", "various submucosal membrane sites of the body", "1985", "the United States", "April 17, 1982", "Acid rain", "July 1, 2005", "a stem", "Thespis", "Ra\u00fal Eduardo Esparza", "By functions", "Thebes", "a constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "Consular Report of Birth Abroad for children born to U.S. citizens ( who are also eligible for citizenship ), including births on military bases in foreign territory", "a dragon", "the hindfoot", "southwestern", "\"From Dusk till Dawn\"", "between South America and Africa", "one", "a river", "snowflake curve", "the ruble"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6533576875710496}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.4, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.07142857142857142, 1.0, 1.0, 0.0, 0.41379310344827586, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.06666666666666667, 0.0, 0.0, 1.0, 0.0, 0.7142857142857143, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2644", "mrqa_squad-validation-8032", "mrqa_squad-validation-9895", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2644", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-559", "mrqa_hotpotqa-validation-371", "mrqa_newsqa-validation-2782", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-10635"], "SR": 0.59375, "CSR": 0.6153846153846154, "EFR": 0.9615384615384616, "Overall": 0.7296814903846154}, {"timecode": 13, "before_eval_results": {"predictions": ["return home", "socialist realism", "to spearhead the regeneration of the North-East", "sleep deprivation", "July 1977", "Anderson", "quantum electrodynamics (or QED)", "provisional elder/deacon", "antigenic variation", "kinematic measurements", "worker, capitalist/business owner, landlord", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Licensed Local Pastor", "feigned retreat to break enemy formations and to lure small enemy groups away from the larger group and defended position for ambush and counterattack", "Mt. Kenya as Mt. Kenia, 1862", "24 March 1879", "Rob Van Winkle", "electrons", "Edie Falco", "Leonard Bernstein", "Empire of the Sun", "James Polk", "Nicaragua", "Afghanistan", "Cambodia", "Aladdin", "Uncle Henry", "Lady and the Tramp", "a magic wand", "China", "solmn", "University of Hanoi", "Eleric", "Tsar Alexander III", "Beatrix Potter", "Louvre", "James Buchanan", "Volvic", "El Sarto", "Colorado", "Christopher Columbus", "Balfour Declaration", "a contingency fee", "a tortoise", "a biretta", "Shinto", "The Simpsons", "a blood", "Cho shalt remain", "a bead cord", "Hancock", "Philadelphia", "Charlie Chaplin Life Story", "The Fresh Prince of Bel-Air", "her abusive husband", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Sylvester the Cat", "Daniel Defoe", "Atlantic Ocean", "Battle of Britain and the Battle of Malta", "to the U.S. Consulate in Rio de Janeiro", "to make it so for all the camps", "three", "a night out in the clubs of Hollywood."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6767671275483775}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7407407407407407, 0.6666666666666666, 1.0, 1.0, 1.0, 0.923076923076923, 0.18181818181818182, 1.0, 0.25]}}, "before_error_ids": ["mrqa_squad-validation-8065", "mrqa_squad-validation-6255", "mrqa_squad-validation-8258", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-708", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-4663", "mrqa_searchqa-validation-4983", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-15238", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-4717", "mrqa_newsqa-validation-4188", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-2199"], "SR": 0.59375, "CSR": 0.6138392857142857, "EFR": 0.9615384615384616, "Overall": 0.7293724244505494}, {"timecode": 14, "before_eval_results": {"predictions": ["Huguenots", "1281", "Tower Theatre", "the Mi'kmaq and the Abenaki", "basic design typical of Eastern bloc countries.", "Palestine", "large, stiffened cilia that act as teeth", "the convenience of the railroad and worried about flooding", "Ireland", "18 April 1521", "Michael at Monte Gargano", "Warszawa", "in the Migration period", "2012", "35", "Trump", "a landmark", "the Blue Nile", "Solomon", "Betsey Johnson", "Eragon", "Rawhide", "p puppy", "Judy Garland", "Paul Hornung", "Chiffon", "Donna Summer", "von Doom", "John Mahoney", "Murder by Death", "Dave Brubeck", "Dalton Gang", "Washington", "Humphrey Bogart", "Rodney King", "Franklin D. Roosevelt", "rice straw, husks and other non-edible rice products", "Soba", "Sirhan Sirhan", "the Moon", "solstice", "dinner service", "Mountain Dew", "Omaha", "Van Halen", "actress", "the Erie Canal", "Raising Arizona.", "Baltic Sea", "senex", "Earvin \"Magic\" Johnson Jr.", "the Cathedral of Santa Maria del Fiore", "hog", "a syllable", "N\u0289m\u0289n\u0289\u0289", "Renishaw Hall, Derbyshire, England, UK", "Imola Circuit", "sheep", "to provide travel agencies in Japan with booking and ticketing capabilities for a wider range of international airlines.", "Berea College", "died", "Sharon Bialek", "European Council", "Barbados"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6211805555555556}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2943", "mrqa_squad-validation-4621", "mrqa_squad-validation-1062", "mrqa_squad-validation-9348", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-9432", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-534", "mrqa_naturalquestions-validation-129", "mrqa_triviaqa-validation-1936", "mrqa_hotpotqa-validation-5184", "mrqa_newsqa-validation-846", "mrqa_triviaqa-validation-2317"], "SR": 0.5625, "CSR": 0.6104166666666666, "EFR": 1.0, "Overall": 0.7363802083333333}, {"timecode": 15, "before_eval_results": {"predictions": ["easier and more efficient than anywhere else", "attempted to enter the test site knowing that they faced arrest", "Paul Revere", "the death of Elisabeth Sladen in early 2011", "3.6%", "nearly 42,000", "14", "the United States Census Bureau", "Asia", "19 April 1943", "actions-oriented", "neutralization by the immune system", "present-day Upstate New York and the Ohio Country", "the Roman Republic", "haiti", "South Africa", "Carling", "both the identity nor the affiliation of the speaker(s), nor that of any other participant", "haiti", "America Online Inc.", "Aramis", "bee hives", "The Firm", "the Streets", "violin", "a blazer", "the sound barrier", "the Titanic", "groin", "the gluteus maximus", "Luigi", "Georgia", "Massachusetts", "impressionist landscape, figure, circus genre, ballet", "Sc\u00e8nes de la Vie de Boh\u00e8me", "John Quincy Adams", "haiti", "Ethel Skinner", "Queen Victoria", "My Fair Lady", "Italy", "cellulose", "Adolphe Adam", "hays", "Barcelona", "a coelacanth", "mono", "cats", "Love Never Dies", "Elizabeth I", "haiti", "the crossword clue", "the ISS", "the Marcy Brothers", "1770 BC", "a costume party", "the oil platforms in the North Sea", "Sydney", "Chancellor Angela Merkel", "Charlie Moore", "haiti Fo", "the Professor", "Burundi", "the Howard Stern Show"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5608969155844156}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.7272727272727273, 0.6666666666666666, 1.0, 0.7272727272727273, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1830", "mrqa_squad-validation-6702", "mrqa_squad-validation-7872", "mrqa_squad-validation-10108", "mrqa_squad-validation-5893", "mrqa_squad-validation-6449", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-871", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-540", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-7024", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3616", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8359", "mrqa_hotpotqa-validation-1537", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-339", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-8487"], "SR": 0.4375, "CSR": 0.599609375, "EFR": 0.9444444444444444, "Overall": 0.7231076388888888}, {"timecode": 16, "before_eval_results": {"predictions": ["computational complexity theory", "leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies.", "tensions over slavery and the power of bishops in the denomination", "mannerist architecture", "489", "Grainger Town area", "the Great Fire of London", "Theory of the Earth", "Ward", "Newton", "bilaterians", "one of the daughters of former King of Thebes, Oedipus", "the forces of Andrew Moray and William Wallace", "differential erosion", "between two and 30 eggs", "biannually", "Arthur Chung", "18", "9 February 2018", "Yuzuru Hanyu", "dog", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff", "the 2013 non-fiction book of the same name by David Finkel", "Duck", "McKim Marriott", "thomas thomas", "31", "in Austin, Texas", "suprachiasmatic nucleus", "the Twelvers", "vasoconstriction of most blood vessels", "translocation Down syndrome", "Rockwell", "1 November", "Sylvester Stallone", "In Time", "Rodney Crowell", "0 '' trunk code", "in eukaryotic cells", "Melissa Disney", "Darren McGavin", "UNESCO", "Thaddeus Rowe Luckinbill", "ummat al - Islamiyah", "William DeVaughn", "a Spanish surname", "Brevet Colonel Robert E. Lee", "The Annunciation", "Santiago Ram\u00f3n y Cajal", "preteen", "seven", "novella", "southeast of the city", "21 June 2007", "four", "California", "England", "Roc Me Out", "\"Dear John,\"", "root out terrorists within its borders.", "Rudolf Nureyev", "43", "the Kurdish militant group in Turkey", "Mohamed Alanssi"], "metric_results": {"EM": 0.5, "QA-F1": 0.5943133344042264}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.08695652173913045, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7586206896551725, 0.0, 0.5, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.2, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.09523809523809523, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9525", "mrqa_squad-validation-6640", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5348", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-3649", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-4917", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-814"], "SR": 0.5, "CSR": 0.59375, "EFR": 0.9375, "Overall": 0.7205468749999999}, {"timecode": 17, "before_eval_results": {"predictions": ["in Sydney", "Doritos", "Hans Vredeman de Vries", "$20.4 billion", "\u00dcberlingen", "Finsteraarhorn", "seven", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "the Lippe", "the American Revolutionary War", "25", "Chuck Noland", "Donna Mills", "March 31, 2013", "pulmonary heart disease ( cor pulmonale )", "before the first year begins", "Kansas", "the President of the United States", "UNESCO / ILO", "asexually", "the eastern jungle and the nearby city of Ba\u00f1os de Agua Santa in the Ecuadorian Andes", "2008", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Marty Stuart", "March 16, 2018", "Cee - Lo", "The Sun", "a living prokaryotic cell ( or organelle )", "1966", "Charlotte Hornets", "Julie Adams", "April 29, 2009", "J. Presper Eckert", "201", "1983", "Thursdays at 8 : 00 pm ( ET )", "adenine ( A ), uracil ( U )", "MacFarlane", "to regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Hercules", "Angel Island", "Rufus and Chaka Khan", "moist temperate climates", "before they kill him", "7.6 mm", "sorrow", "in the ubi sunt tradition", "B.F. Skinner", "2020", "continues the pre-existing appropriations at the same levels as the previous fiscal year ( or with minor modifications ) for a set amount of time", "during World War II in Berlin", "the Friday of Sorrows along with other devotional writings, such as the reputed Marian apparitions attributed to Blessed Anne Catherine Emmerich", "Orangeville, Ontario, Canada", "o*la", "Switzerland", "Andrew Johnson", "St Augustine's Abbey", "if Russian long-range bombers should need to land in Venezuela, we would not object to that either.", "the insurgency", "Joe DiMaggio", "Steve Austin", "NBA 2K16", "Baltimore", "September 25, 2017"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6393717174369749}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, true, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 1.0, 0.72, 0.5714285714285715, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2998", "mrqa_squad-validation-9285", "mrqa_squad-validation-7700", "mrqa_squad-validation-7288", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-2682", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-7892", "mrqa_triviaqa-validation-2922", "mrqa_newsqa-validation-3489", "mrqa_searchqa-validation-10032", "mrqa_hotpotqa-validation-4735"], "SR": 0.515625, "CSR": 0.5894097222222222, "EFR": 0.9354838709677419, "Overall": 0.7192755936379929}, {"timecode": 18, "before_eval_results": {"predictions": ["1904", "to arrest Luther if he failed to recant", "Michael Heckenberger and colleagues of the University of Florida", "declined significantly", "\"quantized\", i.e. they appear in discrete portions\"", "prices", "Miller", "Super Bowl XX", "gold", "p", "2017", "to absorb menstrual flow", "protects it from infections coming from other organs ( such as lungs )", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "12 to 36 months old", "saline water", "her abusive husband", "HTTP / 1.1", "the east coast of Queensland", "2009", "December 2, 1942", "John Cooper Clarke", "1792", "Amitabh Bachchan", "Annette Strean", "the Bee Gees", "September 25, 1987", "The first official Twenty20 matches were played on 13 June 2003 between the English counties in the Twenty20 Cup", "the President", "a little girl ( Addy Miller )", "Ritchie Cordell", "9.0 -- 9.1 ( M )", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "2010", "James Corden", "development of electronic computers in the 1950s", "the internal reproductive anatomy ( such as the uterus in females )", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "Exodus 20 : 1 -- 17", "the population", "beta decay", "Rachel Sarah Bilson", "cut - throat competition", "March 5, 2014", "Haikou on the Hainan Island", "19 June 2018", "stromal connective tissue", "Sunday night", "Stephen Curry of Davidson", "Susie's father, Ben Willis", "Rufus and Chaka Khan", "Jodie Foster", "Rory McIlroy", "Aristotle", "the Chicago Cubs", "1919", "Lithuania", "President Obama's race in 2008", "Lou and Wilson", "oregon muni", "the internal organ systems in adults", "Pete Seeger", "Kiss Me, Kate", "Wigan"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6016908202413522}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 0.5454545454545454, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.75, 1.0, 0.0, 0.5714285714285715, 0.5, 1.0, 0.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5106382978723404, 1.0, 1.0, 0.0, 0.2222222222222222, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.888888888888889, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10810810810810811, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2268", "mrqa_squad-validation-10388", "mrqa_squad-validation-845", "mrqa_squad-validation-5", "mrqa_squad-validation-9213", "mrqa_naturalquestions-validation-2406", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-2904", "mrqa_triviaqa-validation-1259", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-3307", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-10336"], "SR": 0.46875, "CSR": 0.5830592105263157, "EFR": 0.9411764705882353, "Overall": 0.7191440112229102}, {"timecode": 19, "before_eval_results": {"predictions": ["phagosomal", "well before Braddock's departure for North America", "General Hospital", "destroyed", "at night", "assertive", "antagonistic", "whether or not to plead guilty", "parallelogram rule of vector addition", "Valley Falls", "22 July 1930", "Sam Raimi", "the Town of North Hempstead, Nassau County, New York, United States", "political thriller", "James Ellison", "Germaine, Sr.", "from 1986 to 2013", "the NYPD's 83rd Precinct", "Azeroth", "October 5, 1930", "1999", "musical research", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.,", "Michael Sheen,", "she blows on a horn", "the Mayor of the City of New York", "841", "My Boss, My Teacher", "Roy Spencer", "Bardney", "tragedy", "The Everglades", "Hibbing, Minnesota", "1891", "Eielson Air Force Base in Alaska", "Serhiy Paradzhanov", "Germany", "eight", "Carson City", "Lindsey Islands", "2008", "October 12, 1962", "Eli Roth", "Paige O'Hara", "Northern Ireland", "St. Louis, Missouri", "the first hole of a sudden-death playoff", "100 million", "every aspect of public and private life", "Burnley", "Russian Empire", "between 11 or 13 and 18", "due to the onset and progression of Alzheimer's disease", "Thai : \u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30, phayanchana )", "1 October 2006", "Imola", "Sufjan Stevens", "producing rock music with a country influence.", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "the Diamonds tutu", "Ethel Merman", "$1.5 million", "Sen. Barack Obama", "Tutsi ethnic minority and the Hutu majority"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6890500992063492}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.26666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-6024", "mrqa_squad-validation-10408", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-9467", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-12758", "mrqa_newsqa-validation-3659"], "SR": 0.609375, "CSR": 0.584375, "EFR": 1.0, "Overall": 0.731171875}, {"timecode": 20, "UKR": 0.755859375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1939", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4867", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-346", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-4917", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8487", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9432", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10151", "mrqa_squad-validation-10192", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10381", "mrqa_squad-validation-10388", "mrqa_squad-validation-10399", "mrqa_squad-validation-10408", "mrqa_squad-validation-1062", "mrqa_squad-validation-1082", "mrqa_squad-validation-1131", "mrqa_squad-validation-1180", "mrqa_squad-validation-1248", "mrqa_squad-validation-1272", "mrqa_squad-validation-1334", "mrqa_squad-validation-1348", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-150", "mrqa_squad-validation-1530", "mrqa_squad-validation-1551", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1939", "mrqa_squad-validation-2003", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-217", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2361", "mrqa_squad-validation-2416", "mrqa_squad-validation-2481", "mrqa_squad-validation-2511", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2667", "mrqa_squad-validation-2772", "mrqa_squad-validation-2861", "mrqa_squad-validation-2881", "mrqa_squad-validation-2906", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3148", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3270", "mrqa_squad-validation-3355", "mrqa_squad-validation-3385", "mrqa_squad-validation-3411", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3830", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4159", "mrqa_squad-validation-4186", "mrqa_squad-validation-423", "mrqa_squad-validation-4331", "mrqa_squad-validation-434", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4430", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4681", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-501", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5198", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5265", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-552", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5640", "mrqa_squad-validation-5653", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5849", "mrqa_squad-validation-5893", "mrqa_squad-validation-5986", "mrqa_squad-validation-6024", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6294", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-6728", "mrqa_squad-validation-680", "mrqa_squad-validation-6841", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6917", "mrqa_squad-validation-7029", "mrqa_squad-validation-7164", "mrqa_squad-validation-7175", "mrqa_squad-validation-7302", "mrqa_squad-validation-7362", "mrqa_squad-validation-7366", "mrqa_squad-validation-7435", "mrqa_squad-validation-744", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7709", "mrqa_squad-validation-7740", "mrqa_squad-validation-7766", "mrqa_squad-validation-7775", "mrqa_squad-validation-7818", "mrqa_squad-validation-7821", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8144", "mrqa_squad-validation-8163", "mrqa_squad-validation-828", "mrqa_squad-validation-8336", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8372", "mrqa_squad-validation-848", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8600", "mrqa_squad-validation-8687", "mrqa_squad-validation-8747", "mrqa_squad-validation-8759", "mrqa_squad-validation-8807", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-9050", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9348", "mrqa_squad-validation-9401", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9830", "mrqa_squad-validation-9831", "mrqa_squad-validation-9893", "mrqa_squad-validation-9930", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4825", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7542", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-871"], "OKR": 0.853515625, "KG": 0.46796875, "before_eval_results": {"predictions": ["one way streets", "since at least the mid-14th century", "Marlee Matlin", "biplane capable of taking off vertically (VTOL aircraft)", "Classic", "88", "826", "leishmaniasis", "Romulus, My Father", "Theodore Roosevelt Mason", "1996", "1964", "Kinnairdy Castle", "James Franco", "21 flights daily", "December 24, 1973", "Baden-W\u00fcrttemberg", "James Dean", "Rob Reiner", "Kansas City", "Westfield Tea Tree Plaza", "26,000", "Alemannic", "1992", "Darci Kistler", "the EN World web site", "(25 March 1948 \u2212 27 December 2013)", "Pigman's Bar-B- Que", "Columbus, Ohio", "Emilia-Romagna Region", "Miami Gardens, Florida", "Chaplain to the Forces", "John Sullivan", "Churros", "Franklin, Indiana", "Leslie James \"Les\" Clark", "mentalfloss.com", "Omega SA", "Flashback", "The Walter Reed Army Medical Center", "Kiernan Brennan Shipka", "1993", "Zaire", "Brett Ryan Eldredge", "pronghorn", "Walcha", "French", "FBI", "BMW X6", "David Michael Bautista Jr.", "Tennessee River", "about 15 mi", "Taylor Swift", "control purposes", "the final play of the 2017 / 18 Divisional Round game", "Batman & Robin", "Melbourne, Victoria, Australia", "Felipe Massa", "The Washington Post", "Constantine XI Palaiologos", "Viva Las Vegas", "bats", "Democritus", "Tennessee"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6087053571428571}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.4, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1459", "mrqa_squad-validation-6653", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4802", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3402", "mrqa_searchqa-validation-15983"], "SR": 0.484375, "CSR": 0.5796130952380952, "EFR": 1.0, "Overall": 0.731391369047619}, {"timecode": 21, "before_eval_results": {"predictions": ["between 1000 and 1900", "Colonialism", "beneficial", "2%", "Ted Ginn Jr.", "Harrods", "Coptic Cathedral", "Philadelphia", "Disco", "MMA", "the Kentucky River", "Augustus", "1995", "public", "December 21, 1956", "Robert FitzRoy", "Beno\u00eet Jacquot", "Nanna Popham Britton", "Sada Carolyn Thompson", "T. E. Lawrence", "Nelson Mandela", "Dissection", "German", "The A41", "Forbes", "Bronwyn Kathleen Bishop", "The Timekeeper", "Jena Malone", "water", "Hong Kong", "Four Weddings and a Funeral", "from 1993 to 1996", "July 16, 1971", "the music genres of electronic rock, electropop and R&B", "Theodore Anthony Nugent", "Reverend Timothy \"Tim\" Lovejoy", "Geet", "October 5, 1930", "Pamelyn Ferdin", "The Ninth Gate", "Comeng and Clyde Engineering", "neuro-orthopaedic", "Saint Motel", "About 200", "War Is the Answer", "Eleanor of Aquitaine", "Trilochanapala", "Rashida Jones", "The Division of Cook", "Peter Townsend", "Blue Origin", "Taylor Swift", "18 December 1975", "convergent plate boundary", "its vast territory was divided into several successor polities", "Kiri Te Kanawa", "melbourne", "30-minute", "the area was sealed off, so they did not know casualty figures", "The Waltons", "the wind band", "how now", "24 hours", "Nigel Lythgoe"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6749577245670996}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9904", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-4466", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4172", "mrqa_triviaqa-validation-6874", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5447", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2014"], "SR": 0.578125, "CSR": 0.5795454545454546, "EFR": 1.0, "Overall": 0.7313778409090909}, {"timecode": 22, "before_eval_results": {"predictions": ["Zorro", "Zagreus", "January 1985", "Computational complexity theory", "to plan the physical proceedings, and to integrate those proceedings with the other parts", "post-World War I", "Esteban Ocon", "New Orleans Saints", "Clarence Nash", "The Dressmaker", "1,800", "February 14, 1859", "Urijah Faber", "Mary-Kay Wilmers", "baeocystin", "a name in the 1623 First Folio of his plays", "Vaisakhi List", "Ars Nova Theater", "Monticello", "a large portion of rural Maine, published six days per week in Bangor, Maine", "\"Menace II Society\"", "16 March 1987", "the luxury Holden Calais (VF) nameplate", "Louis Silvie \"Louie\" Zamperini", "1968", "PlayStation 3", "the Cordillera Blanca in the Andes of Peru", "the Knight Company", "Parlophone Records", "Nicolas Winding Refn", "South America", "the Kingdom of Morocco", "Gerard Marenghi", "1958", "67,575", "31 July 1975", "orange", "Roseann O'Donnell", "\"There Is Only the Fight... : An Analysis of the Alinsky Model.\"", "SAVE", "29,000", "the extraterrestrial hypothesis ( ETH)", "Larnelle Steward Harris", "4,613", "a morir so\u00f1ando or orange Creamsicle", "Ramzan Kadyrov", "Albert", "Tetrahydrogestrinone", "5,042", "Captain B.J. Hunnicutt", "Victoria", "The Keeping Hours", "the Gilbert building", "dead plant and animal material", "Elena Anaya", "Rome", "The Archers", "a raven", "vitamin injections that promise to improve health and beauty", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented", "Daniel Radcliffe", "Milla Jovovich", "the bridge", "a beetle"], "metric_results": {"EM": 0.625, "QA-F1": 0.7151469827061259}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6956", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-460", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-5396", "mrqa_triviaqa-validation-1447", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-11933"], "SR": 0.625, "CSR": 0.5815217391304348, "EFR": 1.0, "Overall": 0.7317730978260869}, {"timecode": 23, "before_eval_results": {"predictions": ["the growth of mass production", "Edmonton, Canada", "Ollie Treiz", "Western Xia", "Richard Trevithick", "coughing and sneezing", "9", "mountain-climbing", "The Backstreet Boys", "November of that year", "the US Territory of Hawaii", "the American rock band Tool", "1919", "May 5, 2015", "Arthur Miller", "Edmonton, Alberta", "Brent Robert Barry (born December 31, 1971)", "1989 until 1994", "The LA Galaxy", "Nicholas Kristof", "film and short novels", "Flushed Away", "first baseman and third baseman", "the River Welland", "The Process", "200,167", "Bohemia", "\"The Brothers\" (2001)", "Armin Meiwes", "1933", "Dirk Werner Nowitzki (]", "Carl Michael Edwards II", "In a Better World", "the 45th Infantry Division in the Italian campaign of World War II", "Iron Man 3", "Julie 2", "Conservatorio Verdi in Milan", "6'5\"", "Artie turns out to be a terrible date", "June 26, 1970", "Lawrenceburg, Indiana", "14", "\"About a Boy\"", "1982", "1978", "domestic cat", "Denmark", "1999", "Uchinaanchu (\u6c96\u7e04\u4eba, Japanese: \"Okinawa jin\")", "Peter Thiel", "Summerlin, Nevada", "Cersei Lannister", "the contemporary finalists ( plus Emma ) and their all stars", "the inner edge of the galaxy", "Mahinda Rajapaksa", "iron", "naples", "Alias Smith and Jones", "It is done with the parents' full consent.", "he pulled a blanket over his head, \"pushed the plunger on the bomb and prepared to die,\"", "14", "1,000,000 milligrams", "naples", "the naples"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5768630108926162}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.31578947368421056, 0.0, 1.0, 0.0, 1.0, 0.5714285714285714, 0.08, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-2654", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-1564", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-2293", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-865", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-3074"], "SR": 0.46875, "CSR": 0.5768229166666667, "EFR": 1.0, "Overall": 0.7308333333333333}, {"timecode": 24, "before_eval_results": {"predictions": ["1965", "A job where there are many workers willing to work a large amount of time (high supply)", "A tundra", "article 30", "The mermaid", "in Now Zad in Helmand province, Afghanistan.", "to promote the attempts but simply to oversee them in a fair and independent manner and ratify successful efforts.", "daughter Paige, 15, and \"the crew\": Isaac, 8, Hope, 7, Noah, 5, Phoebe Joy, 3, Lydia Beth, 2, Annie, also 2,", "Kurt Cobain", "seven", "two years", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.\"", "piano", "Charlotte Gainsbourg and Willem Dafoe", "The Charlie Daniels Band", "can and will continue to help provide safety and physical security, but also could further assist with the reconstruction projects, such as building hospitals, schools, sanitation facilities and investment projects that would have direct impact on the socioeconomic development", "curfew", "Missouri", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards", "five victims by helicopter", "30-minute", "San Diego", "Iran", "cancer", "baseball bat", "U.S. Navy", "$249", "the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "sportswear", "\"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "eight", "the FAA received no reports from pilots in the air of any sightings", "1945", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "NATO fighters", "Jaime Andrade", "Somali President Sheikh Sharif Sheikh Ahmed", "\"I wasn't sure whether I was going to return to 'E! News' this week or after the new year.", "how sociopathic brains develop.", "Orbiting Carbon Observatory", "German Chancellor Angela Merkel", "Donald Trump", "Ryder Russell", "Scarlett Keeling", "the crew of the Bainbridge", "Joe Jackson", "Asashoryu", "the District of Columbia National Guard", "Interior Ministry", "Anil Kapoor", "$3 billion", "question people if there's reason to suspect they're in the United States illegally", "a young girl", "Elizabeth Dean Lail", "1926", "a squall", "Wyoming", "Kenny Everett", "Japan", "Richa Sharma", "The Frost Place Advanced Seminar", "Stringer Bell", "Valentina Tereshkova", "Baccarat"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5992967841944403}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 0.923076923076923, 0.10526315789473684, 0.0, 0.5, 1.0, 1.0, 0.06060606060606061, 1.0, 0.0, 1.0, 0.0784313725490196, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.8, 0.10256410256410256, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7407", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-2429", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3010", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6214", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-14187"], "SR": 0.515625, "CSR": 0.5743750000000001, "EFR": 0.9354838709677419, "Overall": 0.7174405241935483}, {"timecode": 25, "before_eval_results": {"predictions": ["phycobilisomes", "WatchESPN", "full independent prescribing authority", "over 10,000", "it infringed on democratic freedoms", "One of Osama bin Laden's sons", "a house party in Crandon, Wisconsin,", "chairman of the House Budget Committee", "in Amstetten,", "The Drug Enforcement Administration said Wednesday it's considering tighter restrictions on propofol, a drug reportedly found after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "anti-trust", "company Polo", "three empty vodka bottles, the inquest heard.The pathologist's tests revealed that Winehouse's blood-alcohol level was 416 milligrams per 100 milliliters of blood,", "Nkepile M abuse", "a Royal Air Force helicopter", "Juri Kibuishi, 23,", "McDonald's", "the administration's progress, while we also encourage him to 'evolve faster' on supporting full marriage equality,\"", "1981", "Jewish", "al Fayed's", "Teresa Hairston", "The pilot, whose name has not yet been released,", "18", "in July", "\"The Frisky\"", "Jared Polis", "debris", "a woman", "the Form Design Center in Hedmanska Garden.", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "21", "the heart of an urban center like Los Angeles.", "they would not be making any further comments, citing the investigation.No one was inside the apartment at the time of the fire, police said.", "North Korea intends to launch a long-range missile in the near future,", "the 3rd District of Utah", "House-passed bill that eliminates the 3% withholding requirement for government contractors", "the Russian word \"peregruzka,\"", "one", "Lee Probert.", "The listeria outbreak is the deadliest food-borne illness outbreak in the United States since 1998.", "23", "American", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.\"", "Monday.", "Frank Ricci", "Old Trafford", "China relishes its spot as a key global player. It is involved in major international disputes such as the showdowns with Iran and North Korea and has used its position on the U.N. Security Council", "the Carrousel du Louvre", "homicide by undetermined means", "Les Bleus", "Madison", "Gibraltar", "need to repent in time", "John McCarthy", "The Rime of the Ancient Mariner", "the narwhals", "Atlas ICBM", "Lionel Eugene Hollins", "Duchess Eleanor of Aquitaine", "John Irving", "diabetes", "the host"], "metric_results": {"EM": 0.359375, "QA-F1": 0.46413993073919546}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.28571428571428575, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 1.0, 0.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 0.06666666666666667, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8392", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-981", "mrqa_newsqa-validation-2745", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3726", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2673", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-5238", "mrqa_searchqa-validation-11369"], "SR": 0.359375, "CSR": 0.5661057692307692, "EFR": 0.9512195121951219, "Overall": 0.7189338062851782}, {"timecode": 26, "before_eval_results": {"predictions": ["Eero Saarinen", "according to a multiple access scheme.", "composite number", "Climate fluctuations during the last 34 million years", "Chicago Cubs", "The Salopian", "an \u201caerophone,\u201d", "Norman Hartnell", "four games.", "Millbank in London.", "Poland", "Adam Ant", "shallow seas,", "New Democracy", "Missouri", "six", "mushrooms", "Turkey", "1960", "the other characters residing outside of it.", "Walford East", "of Civil Law (B.C.L.)", "Laurence Olivier,", "four", "Leonard Bernstein", "\"I am trying get the hang of this new fangled writing machine, but am not making a shining success of it.", "are", "index fingers", "blood", "aircraft", "Passion fruit", "a jumper", "The Lone Ranger", "caridean shrimp", "Yemen", "Welles", "George IV.", "the Swindon Robins", "Joseph Smith, Jr.", "Herefordshire", "Kievan Rus (13th cent.)", "meat.", "wine", "rowing", "$100,000", "1970", "A Dangerous Man: Lawrence After Arabia", "'Lord Nelson' or a 'Nelson'", "Biblical", "Argentina", "lion", "Sinclair Lewis", "Phosphorus pentoxide", "the king of kalinga in the mauryan period", "Norwegian", "the County of York", "Cheshire County", "Atomic Kitten", "Phillip A. Myers.", "11th year in a row.", "Opryland,", "Heroes", "singer", "Alzheimer's disease"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5189867424242424}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1818181818181818, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-118", "mrqa_triviaqa-validation-1872", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-397", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-459", "mrqa_naturalquestions-validation-946", "mrqa_hotpotqa-validation-347", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-1554"], "SR": 0.46875, "CSR": 0.5625, "EFR": 0.9411764705882353, "Overall": 0.716204044117647}, {"timecode": 27, "before_eval_results": {"predictions": ["water-cooled", "Genghis Khan Mausoleum", "the most cost efficient bidder", "February 1", "the name announcement of Kylie Jenner's first child", "53", "Aman", "2005", "Elizabeth Dean Lail", "Anthony Caruso", "Peter Finch", "Jerry Ekandjo", "Guant\u00e1namo", "775", "north", "23 %", "\u0292wa d\u0259 viv\u0281", "American Horror Story : Roanoke", "American", "Central Germany", "King Saud University", "Sherwood Forest", "on the southeastern coast of the Commonwealth of Virginia in the United States", "Orlando", "epidemiology", "syrie Irving", "brain", "Paul Hogan", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "1996", "Dan Stevens", "in capillaries, alveoli, glomeruli, outer layer of skin", "September 6, 2019", "two", "Shenzi", "during the final stage of World War II", "3000 BC", "Isthmus of Corinth", "1976", "on the vaginal floor", "October 2008", "Broken Hill and Sydney", "artses liberales", "The Hunger Games : Mockingjay -- Part 1 ( 2014 )", "1773", "Escherichia coli", "9th century", "radioisotope thermoelectric generator", "Carol Worthington", "when viewed from different points on Earth", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "2014", "st ives", "Dorset", "pressure", "Frederick Martin \"Fred\" Mac Murray", "Franz Ferdinand", "authoritarian", "Evan Bayh", "tripplehorn,", "How I Met Your Mother", "Portugal", "grow old", "executive cow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6343591597058613}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 0.7999999999999999, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.06896551724137931, 0.4347826086956522, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-449", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-1180", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-3827", "mrqa_hotpotqa-validation-5286", "mrqa_newsqa-validation-1832", "mrqa_searchqa-validation-7864"], "SR": 0.515625, "CSR": 0.5608258928571428, "EFR": 0.9354838709677419, "Overall": 0.714730702764977}, {"timecode": 28, "before_eval_results": {"predictions": ["the father of the house when in his home", "Welsh", "to provide a standardized interface into and out of packet networks", "blue", "a wine grapes could not easily be grown", "Vietnam", "John Peel", "Moscow", "insect", "Estonia", "Siberia", "malaria", "1998", "Kent", "Arthur, Prince of Wales", "Israel", "a butterfly", "New Jersey", "Philippines", "a terrorist is one who practices terrorism,", "the number thirteen", "Aquae Sulis", "Eric Coates", "diggory Venn", "to make a furrow or furrows in.", "Brothers In Arms", "Mexico", "Aberystwyth", "Eric Morley", "Saskatchewan", "Frank Spillane", "Erik Aunapuu", "Frank Sinatra", "Galen Rupp", "Bozeman Daily Chronicle (Bozeman, MT)", "Niger", "the Lone Gunmen", "David Nixon", "piano", "a fish-tailed sea-god,", "Addis Ababa", "pascal", "heart", "Nova Scotia", "#JeSuisCharlie vigils around the world", "Len Hutton", "Nigeria", "Dead Sea", "40", "Gibraltar", "1929", "long", "Gerald Ford", "Spanish missionaries", "William Wyler", "rash", "1887", "Gateways", "Mohmand agency district.", "a plaque", "Phillip A. Myers.", "Priscilla Beaulieu", "Cold Mountain - No Animals Were Harmed", "a cure"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5956597222222222}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false], "QA-F1": [0.6, 1.0, 0.33333333333333337, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-3924", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4689", "mrqa_newsqa-validation-2885", "mrqa_searchqa-validation-1846", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-2066"], "SR": 0.546875, "CSR": 0.5603448275862069, "EFR": 1.0, "Overall": 0.7275377155172413}, {"timecode": 29, "before_eval_results": {"predictions": ["Albert C. Outler", "African Union chairman Jakaya Kikwete", "Drogo", "arthur Lincoln", "smith", "Australia", "chipmunk", "Amnesty International", "the Scud", "Australia", "Labrador Retrievers", "Rio Grande", "Carrie", "ecclesiastical", "Poland", "copenhagen", "Brooklyn", "Octavian", "copenhagen", "st petersburg", "pj Harvey", "Gryffindor", "archer", "The French Connection", "Thundercats", "arthur", "Rapa Nui", "copenhagen", "Azerbaijan", "ontario d'Agnolo di Francesco", "Oliver Twist", "Hinduism", "soles", "Alanis Morissette", "high cooking", "Herbert Henry Asquith", "sugarcane", "fingers", "linn", "handball", "a Short History of Tractors", "9", "function", "Sh Ontars Sister", "purple rain", "Fenn Street School", "fenella fielding", "alfa", "copenhagen", "copertina flessibile Acquisto", "times", "couscous", "March 31, 2017", "1987", "countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "Dutch", "1981", "Humberside", "were being discriminated against on the basis of nationality.", "copenhagen", "copenhagen", "Jefferson the Virginian", "tap", "copcorn"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5693857230392156}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 0.625, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8823529411764706, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8383", "mrqa_triviaqa-validation-7292", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-575", "mrqa_naturalquestions-validation-6764", "mrqa_hotpotqa-validation-2997", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2618", "mrqa_searchqa-validation-10920", "mrqa_searchqa-validation-15291"], "SR": 0.484375, "CSR": 0.5578125, "EFR": 0.9393939393939394, "Overall": 0.7149100378787878}, {"timecode": 30, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1132", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-394", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-478", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-998", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2364", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-913", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5986", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-9310", "mrqa_squad-validation-10141", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-1131", "mrqa_squad-validation-1272", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-1551", "mrqa_squad-validation-1639", "mrqa_squad-validation-1641", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1732", "mrqa_squad-validation-1830", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2849", "mrqa_squad-validation-2881", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3205", "mrqa_squad-validation-3270", "mrqa_squad-validation-3385", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3841", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4166", "mrqa_squad-validation-423", "mrqa_squad-validation-4385", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4655", "mrqa_squad-validation-4673", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4806", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5198", "mrqa_squad-validation-5234", "mrqa_squad-validation-5265", "mrqa_squad-validation-5331", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5590", "mrqa_squad-validation-5640", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5986", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6614", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-668", "mrqa_squad-validation-6727", "mrqa_squad-validation-6894", "mrqa_squad-validation-6956", "mrqa_squad-validation-7029", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-7435", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7740", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8144", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-839", "mrqa_squad-validation-848", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8600", "mrqa_squad-validation-8646", "mrqa_squad-validation-8656", "mrqa_squad-validation-8687", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-915", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-939", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9813", "mrqa_squad-validation-9878", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1021", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3512", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4717", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4858", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-884"], "OKR": 0.849609375, "KG": 0.4453125, "before_eval_results": {"predictions": ["cellular respiration", "Labor", "the most cost efficient bidder", "Prussian", "850 m", "Armani, Esprit", "Forbes billionaires", "Hanoi", "Stephen Mangan", "saloon", "\"Power 108\"", "Vixen", "Adult Swim", "Venice, Florida", "Tropical Storm Ana", "1974", "tokamak", "March 17, 2015", "American", "1958", "China Airlines", "Wayne County", "stanley maitland", "Democratic Party", "Greek-American", "Honey Irani", "2000", "Beauty and the Beast", "137th", "half of the Nobel Prize in Physics", "Future", "1956", "Francis Nethersole", "hiphop", "47,818", "Salisbury", "Lakshmibai", "Tony Aloupis", "sarod", "Anne Arundel County", "Austria Wien", "midnight sun", "Robert A. Iger", "Netherlands", "Dr. Alberto Taquini", "2 March 1972", "Terry the Tomboy", "Gracie Mansion", "Parlophone Records", "R-8 Human Rhythm Composer", "Obergruppenf\u00fchrer", "World War I", "84", "Linda Davis", "Sunni Muslim family", "Mona Lisa", "eight", "the highlands of the southeastern quarter of the Massif Central in the C\u00e9vennes range", "an open window", "staff sergeant", "\"It has never been the policy of this president or this administration to torture.\"", "trans fats", "Anna Mary Robertson", "stigmund Freud"], "metric_results": {"EM": 0.5, "QA-F1": 0.5844629329004329}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2885", "mrqa_hotpotqa-validation-3524", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-1122", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-1370", "mrqa_hotpotqa-validation-4594", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-2529", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-276", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4709", "mrqa_newsqa-validation-3819", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-15972"], "SR": 0.5, "CSR": 0.5559475806451613, "EFR": 1.0, "Overall": 0.7205645161290322}, {"timecode": 31, "before_eval_results": {"predictions": ["The \"freedom to provide services\"", "phagocytes", "Finland", "Gulf of Aden", "Natty Bumppo", "shanghai", "Amsterdam", "cellulose", "moles", "Ryan O\u2019 Neal", "Sicily", "Howard Keel", "Lilo & Stitch", "Charlie henderson", "Sweet Home Alabama", "alopecia universalis", "Percy Sledge", "The Bespoke Overcoat (1955), a Jewish reworking of a Russian tale by Gogol.", "\"Bard of Avon\"", "Man V Food", "1780s", "Kajagoogoo.com", "George Fox", "Croatian", "Manchester City", "The Exile", "heineken", "Esau", "South Africa", "Fidelio", "Hep Stars", "Some Like It Hot", "mercury", "Marcus Antonius", "fro-taj", "Enrico Caruso", "German war machine", "hydrogen", "nitric acid", "Tasmania", "flesh", "Mille Miglia", "tiger", "rhododendron", "Uranus", "Utrecht", "Puente del Arzobispo", "mousetrap", "stenodattilografo", "caliper", "arts", "Adolf Hitler", "dealer sets the cards face - down on the table near the player designated to make the cut", "2020", "Black Mesa Research Facility", "4,613", "Swiss Super League", "Benj Pasek and Justin Paul", "Shendi Mosteller, a friend of the Sanfords", "environmental efforts", "skyscrapers", "chicago", "The Planets", "The Swiffer Sweeper Vac"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5287809367167919}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.3157894736842105, 0.2857142857142857, 0.0, 1.0, 0.8571428571428571, 0.7499999999999999, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4462", "mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-3257", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-7201", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-7145", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-1690", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-745", "mrqa_searchqa-validation-199", "mrqa_searchqa-validation-10166"], "SR": 0.4375, "CSR": 0.55224609375, "EFR": 0.9166666666666666, "Overall": 0.7031575520833333}, {"timecode": 32, "before_eval_results": {"predictions": ["Torchwood", "Thomas Sowell", "wannabe", "toledigital-orlandosentinel", "monstatic", "October 31", "Wyoming", "Leicester", "hitler", "kangaroos", "lourdes", "1929", "hypopituitarism", "February", "piano", "Gloucestershire", "Jupiter Mining Corporation", "John Maynard Keynes", "Scooby snacks", "Yulia Tymochenko", "on Lake Union", "Adriatic Sea", "Bruce Springsteen and Jon Bon Jovi", "fife", "Goran Ivanisevic", "Francis Drake", "Wikia", "baku", "Truro", "hitler", "prairies", "Madness", "barings", "Anne Boleyn", "Mrs. Boddy", "Ken Norton", "Yann Martel", "cabbage", "John Denver", "toledo", "Claire Goose", "one-third", "pali and Prakrit", "hitler", "france", "Edward III", "Bill bryson", "The American Tobacco Company", "to meet and treat the same day", "Spanish", "Norman Mailer", "\"major science finding from the agency's ongoing exploration of Mars.\"", "between 1923 and 1925", "Pakistan", "capillaries", "sarod", "Laban Movement Analysis", "James I of England", "\"falling space debris,\"", "on the bench", "Bright Automotive, a small carmaker from Anderson, Indiana,", "will & Grace", "prehensile", "the English Channel"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5083333333333333}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.13333333333333333, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_triviaqa-validation-1725", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-952", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-3451", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2643", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-7073", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-1770", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-10495", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5403", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-2931"], "SR": 0.453125, "CSR": 0.5492424242424243, "EFR": 1.0, "Overall": 0.7192234848484849}, {"timecode": 33, "before_eval_results": {"predictions": ["the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea,", "1580s", "george smiley", "France", "kmflett", "wuthering Heights", "Portugal", "Everton", "vice-admiral", "Sweeney Todd", "smallbill", "benfica", "six", "1984", "11", "Buzz Aldrin", "mrs williams", "Archie Shuttleworth", "venus williams", "The IT Crowd", "bitter", "east Africa", "pokemon", "cold Comfort Farm", "the Eisbach", "Ruth Rendell", "wales", "Indian Love Call", "Dublin", "Beaujolais", "John Constable", "sheep", "linseed", "red hat", "nottingham", "jump", "1883", "Jessica Simpson", "diolchwch", "seaweed", "sea otter", "dot-com", "Tunisia", "Anna", "earache", "scar", "Vladimir Putin", "croquet", "Southwest Airlines", "Canary Wharf", "the best value diamond for your money", "wigan", "Taittiriya Samhita", "product-market fit", "Secretary of State", "Jos\u00e9 bispo Clementino dos Santos", "1979", "Umberto II", "Spc. Megan Touma,", "suppress the memories and to live as normal a life as possible", "40", "intelligent design", "charlie williams", "copper"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5974702380952381}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, true], "QA-F1": [0.47619047619047616, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-27", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-4782", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-748", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-6224", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-11614"], "SR": 0.5625, "CSR": 0.5496323529411764, "EFR": 0.9642857142857143, "Overall": 0.7121586134453781}, {"timecode": 34, "before_eval_results": {"predictions": ["Central business districts", "1973", "France", "3rd Sunday", "nairobi", "September", "Japanese", "james james stewartre", "james Garner", "Martin Luther King, Jr.", "petticoat", "indus River", "Puerto Rico", "180", "Charles Taylor", "bearded woman", "Ireland", "The Savoy", "niki lauda", "Finland", "india", "japan", "Massachusetts", "boutros", "the Rolling Stones", "Uranus", "mouse", "Aleister Crowley", "Greek", "Spain", "Mumbai", "kitsunes", "frottage", "collage", "tinker", "eriksson", "johan van Galen", "Angus Deayton", "Dean Martin", "Emily Davison", "Loki Laufeyiarson", "penny", "Nic Fiddian-Green", "ethel Skinner", "james stewart", "ghee", "cleopatra", "rambling", "commitment", "Anastasia Dobromyslova", "S\u00e8vres", "Procol Harum", "Victory gardens", "artes liberales", "Luke", "AT&T", "1998", "dachshund", "\"totaled,\"", "Iran of trying to build nuclear bombs,", "\"deeply intimate portrait will provide viewers with a raw and honest look inside a musical dynasty.\"", "Nashville", "\"reshit\"", "anemia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5427083333333333}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7088", "mrqa_triviaqa-validation-7015", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-6375", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-3914", "mrqa_triviaqa-validation-398", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-536", "mrqa_triviaqa-validation-2249", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-6010", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3770", "mrqa_hotpotqa-validation-4420", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-16252"], "SR": 0.515625, "CSR": 0.5486607142857143, "EFR": 1.0, "Overall": 0.7191071428571428}, {"timecode": 35, "before_eval_results": {"predictions": ["Seven Days to the River Rhine", "the Seerhein", "2,579 steps", "to collect menstrual flow", "men's basketball team from Seton Hall University", "the Near East", "on Saturday and then broadcast `` as live '' on the Sunday", "Juice Newton", "Hook", "Brian Steele", "nearby objects show a larger parallax than farther objects when observed from different positions", "four", "Leonard Bernstein", "Prince Henry", "9 February 2018", "1970s", "2001", "18th century", "her abusive husband", "moral", "provided majority of members present at that time approved the bill either by voting or voice vote", "a federal republic", "July 14, 1969", "Frank Langella", "Tennessee Titan", "Spanish", "April 26, 2005", "Castleford is a town in the metropolitan borough of Wakefield, West Yorkshire, England", "Action Jackson", "New England Patriots", "the world's first collected descriptions of what builds nations'wealth", "Patrick Warburton", "when the cell is undergoing the metaphase of cell division", "`` One Son ''", "Mara Jade", "revenge and karma", "20 year - old Kyla Coleman", "Nathan Hale", "Rachel Kelly Tucker", "a far lesser degree by blood capillaries extending to the outer layers of the dermis", "during World War II", "S\u00e9rgio Mendes", "a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole", "David Tennant", "Kelly Reno", "Brooke Wexler", "over $1.84 billion", "a patronymic surname", "Leonard Nimoy", "Billy Hill", "2005", "Pangaea or Pangea", "Kent", "petula Clark", "arm", "Tufts University", "2002", "May 4, 2004", "work together to stabilize Somalia and cooperate in security and military operations.", "Communist Party of Nepal (Unified Marxist-Leninist)", "Robert Barnett", "achilles", "Daniel Boone", "chicago"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6720246259112106}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.6666666666666665, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1904761904761905, 1.0, 0.6666666666666666, 0.6086956521739131, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.3333333333333333, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.21052631578947367, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.2857142857142857, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6190", "mrqa_triviaqa-validation-702", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-296"], "SR": 0.546875, "CSR": 0.5486111111111112, "EFR": 0.9310344827586207, "Overall": 0.7053041187739464}, {"timecode": 36, "before_eval_results": {"predictions": ["$5,000,000", "The Handmaid's Tale", "Austrian", "Archbishop of Canterbury", "1945", "102,984", "Emmanuel Ofosu Yeboah", "Clarence Nash", "Bulgarian", "Macomb County", "Dusty Dvoracek", "environmentalist", "1972", "Disney California Adventure", "Indiana", "Travis County, Texas", "the 78th PGA Championship", "orange", "Regionalliga Nord", "John Surtees", "ragby", "life insurance", "Ukrainian", "Cape Cod", "model", "BBC Focus", "George Clooney", "Joe Scarborough", "Tottenham ( ) or Spurs", "the attack on Pearl Harbor", "Alemannic", "Vienna", "Jesper Myrfors", "Paper", "Richard", "Ogallala Aquifer", "\"The Heirs\" (2013), \"Descendants of the Sun\" (2016)", "News Corp and 21st Century Fox", "the amount charged by a bookmaker, or \"bookie\", for taking a bet from a gambler.", "9", "Rockbridge County", "sexual activity", "English", "Gatwick", "Carlos Santana", "two Nobel Peace Prizes", "Dutch", "a game setting created by TSR, Inc. in the late 1980s", "Cannes Film Festival", "NXT Tag Team Championship", "DJ", "Bill Patriots", "Achal Kumar Jyoti", "a set of connected behaviors, rights, obligations, beliefs, and norms as conceptualized by people in a social situation", "jazz", "17", "plac\u0113b\u014d", "a federal judge in Mississippi on March 22,", "when times get tough", "\"It was perfect work, ready to go for the stimulus package,\"", "wounds", "walruses", "comoros", "Harry S. Truman"], "metric_results": {"EM": 0.5, "QA-F1": 0.6226073405760906}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.5333333333333333, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7272727272727273, 0.25, 0.09523809523809525, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-3731", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3145", "mrqa_naturalquestions-validation-3076", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-2776", "mrqa_newsqa-validation-379", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2449", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-13135"], "SR": 0.5, "CSR": 0.5472972972972974, "EFR": 1.0, "Overall": 0.7188344594594595}, {"timecode": 37, "before_eval_results": {"predictions": ["Robert Lane and Benjamin Vail", "pjaro carpintero", "20 feet", "biting", "College of William and Mary", "\"The core problem for the recalled toys was the design flaw,", "Bling-bling", "Biggie Smalls", "Marsha Hunt", "chinook", "Feodor", "Emily Hughes", "Sonnets to Orpheus", "Caesar salad", "Sheathed", "David Berkowitz", "jeopardy/2762_Qs.txt at master", "Taos Pueblo", "orca", "licorice stick", "A Moon for the Misbegotten", "Donovan", "blue rectangle", "Dublin", "mathematical", "George II", "Suzuki Grand Vitara", "The Yogi Bear Show", "a Lebanese military commander who served as president of Lebanon", "Judas Iscariot", "Christopher", "Stripes", "Little Red Riding Hood", "600 nm", "Daryl Hall & John Oates", "Cherokee", "The cause of the French people", "Gettysburg National Military Park", "\"Aladdin, or, The wonderful lamp\"", "Freddie Mercury", "\"sensible and responsible women don't want to vote", "Orange County, California", "Basil", "Arkansas", "Aeneas", "the Board", "The Devil Wears Prada", "Thomas Jefferson", "violins", "ethanol", "Adam Smith", "famous figures as Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "the church at Philippi, one of the earliest churches to be founded in Europe", "Pericles, the Athenians pursued a policy of retreat within the city walls of Athens, relying on Athenian maritime supremacy for supply", "red", "Cole Porter", "a non-speaking character", "New York City", "Westminster system", "Marilyn Martin", "nuclear", "Zahi Hawass, director of the Supreme Council of Antiquities,", "Jackson sitting in Renaissance-era clothes and holding a book", "from boxing, where a boxer who is still on their feet but close to being knocked down can be saved from losing by the bell ringing to indicate the end of the round"], "metric_results": {"EM": 0.265625, "QA-F1": 0.4034970238095238}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.19999999999999998, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.4, 0.4, 0.4, 0.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-12628", "mrqa_searchqa-validation-96", "mrqa_searchqa-validation-11078", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-11446", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12401", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-11435", "mrqa_searchqa-validation-2181", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-15249", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-4949", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-8404", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-10312", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-14370", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-13123", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-2068", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6949", "mrqa_searchqa-validation-822", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-7414", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4021", "mrqa_newsqa-validation-2611"], "SR": 0.265625, "CSR": 0.5398848684210527, "EFR": 1.0, "Overall": 0.7173519736842106}, {"timecode": 38, "before_eval_results": {"predictions": ["1954", "Zion", "cloves", "Melanie Griffith", "Abraham Lincoln", "St Martin", "a physician or surgeon", "Greenpeace", "a snake", "a cantae", "a canton", "a Palestinian city", "Caracas", "Memphis", "Breakfast at Tiffany's", "\"that will\"", "Faneuil Hall", "Babe", "devein", "a balsa", "Carter", "children of prostitutes", "an anteater", "New Orleans Saints", "Diana the Princess", "Tasmania", "Lakshmi Mittal", "cobalt", "Louisa May Alcott", "the wild outskirts of...", "Spider-Man 3", "grease", "insulin", "a fracas", "Sony", "Van Helsing", "Hugh Grant", "the Great Wall", "hand", "Hormel Foods", "UTC-08:00", "Clara Barton", "Kauai", "the esophagus", "Joseph", "Otsego County", "Sanford", "Colombia", "Thomas Paine", "Venezuela", "canticle", "Donna", "outside the playing season", "a fully centralized service with individual user accounts focused on one - on - one conversations", "Apollon", "Chicago", "Andrew Lloyd Webber", "1776", "Tim Howard", "Princess Jessica", "Hutus and Tutsis", "her father", "Monday and Tuesday", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5863715277777777}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5833333333333334]}}, "before_error_ids": ["mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-1238", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-9939", "mrqa_searchqa-validation-6793", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-3861", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-1716", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1534", "mrqa_searchqa-validation-14173", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-6886", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-4264", "mrqa_hotpotqa-validation-5676", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-4021"], "SR": 0.515625, "CSR": 0.5392628205128205, "EFR": 1.0, "Overall": 0.717227564102564}, {"timecode": 39, "before_eval_results": {"predictions": ["movements of nature", "retirement", "Hill Street Blues", "TB", "Ross Perot", "drowned", "fuchsia", "fracture", "Lance Armstrong", "Kung Fu", "elbow", "sienna", "Hindu", "the Voice", "Nacho Libre", "The Prince", "Cygnus", "the south coast", "a nougat", "a Scotch egg", "the Manhattan Project", "the Eiffel Tower", "Roger Federer", "a sculpere", "artery", "a cheddar", "Iran", "Florida", "The Virgin Spring", "(Ed) Dostoyevsky", "the Nome", "Leopold", "Atlanta", "tofu", "Zorro", "assume", "Jack Sprat", "offbeat", "uranium", "ismene", "food", "the Gannett Company", "phoebus", "aurore", "glaciers", "Dick Gephardt", "phoebus", "Lord Louis Mountbatten", "a master's degree", "the \"American Idol\"", "Robert Peary", "down to the ground", "Hellenism", "James W. Marshall", "(A) Ares", "willow", "The Sixth Sense", "the Soldier Bear", "Montreal, Quebec, Canada", "the Firth of Forth", "\"it's not your car.\"", "6-2", "\"What she's doing is she's humanizing the issue.", "$5,000"], "metric_results": {"EM": 0.40625, "QA-F1": 0.47135416666666663}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-16284", "mrqa_searchqa-validation-2577", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-12788", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-7469", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-14990", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-262", "mrqa_searchqa-validation-3491", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-4826", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-31", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-1152"], "SR": 0.40625, "CSR": 0.5359375, "EFR": 0.9736842105263158, "Overall": 0.7112993421052631}, {"timecode": 40, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2920", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3586", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-514", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-13508", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-2066", "mrqa_searchqa-validation-222", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4829", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9908", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10088", "mrqa_squad-validation-10388", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1248", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2608", "mrqa_squad-validation-2831", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3331", "mrqa_squad-validation-349", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4046", "mrqa_squad-validation-4155", "mrqa_squad-validation-4331", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4634", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5248", "mrqa_squad-validation-5307", "mrqa_squad-validation-5389", "mrqa_squad-validation-5469", "mrqa_squad-validation-5506", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5728", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6024", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6224", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6702", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7211", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-801", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-9140", "mrqa_squad-validation-915", "mrqa_squad-validation-9285", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9525", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.8515625, "KG": 0.5203125, "before_eval_results": {"predictions": ["toward the center of the curving path", "a random number generator", "Hinduism", "a turtle", "Donkey", "Jane Eyre", "Aiden", "William Shakespeare", "Louis XIV", "Montmartre", "The Dying Swan", "Jailhouse Rock", "a protractor", "voter registration", "the Kite Runner", "white granite", "Islamabad", "horseshoe", "Stephen Crane", "trespassing", "Jack Dempsey", "beheading", "Val Kilmer", "Kashmir", "Milwaukee", "a kiwi", "Pop-Tarts", "sugar", "Enrico Fermi", "Tiger Woods", "the Madding Crowd", "local broadcasters", "Grace Kelly", "a monkey's tail", "Bilbo", "Oliver Wendell Holmes", "the Constitution", "Proverbs", "a photon", "Maria Montessori", "orchid", "anparagus", "the Sun", "Michelangelo", "Aoki", "ale", "Superman", "a clause", "Brazil", "Puget Sound", "phylum", "Yahya Khan", "Prince Bao", "1038", "Bubba", "Easter Parade", "Thom Yorke", "beer", "Keeper of the Great Seal of Scotland", "Martin O'Malley", "1983", "The cause of the child's death will be listed as homicide by undetermined means", "five", "Edgehill"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7550595238095238}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8333333333333334, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-12159", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-7208", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-7222", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-4248", "mrqa_naturalquestions-validation-3485", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-694", "mrqa_newsqa-validation-2627"], "SR": 0.6875, "CSR": 0.5396341463414633, "EFR": 1.0, "Overall": 0.7319112042682926}, {"timecode": 41, "before_eval_results": {"predictions": ["metals", "2004", "to capitalize on her publicity", "Karen Gillan", "Moira Kelly", "Nick Kroll", "Albert Einstein", "Miami Heat", "Poems : Series 1", "shortwave radio", "the east African coast", "420 mg", "James Madison", "Tom Brady", "asphyxia", "2017", "Thomas Edison", "in a cell", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Flag Day in 1954", "Erica Rivera", "Afghanistan", "Gettysburg College", "the Geography of Oklahoma", "Eydie Gorm\u00e9", "into the bloodstream", "a fuel", "1 mile ( 1.6 km )", "1871", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "2018", "faster still in solids", "Presley Smith", "a combination of genetics and the male hormone dihydrotestosterone", "Norma's half - brother", "the types of instruments that are used in data collection", "Tami Lynn", "Christianity", "radians", "1931", "the fifth UK album", "May 2010", "24", "August 8, 1945", "Bibi", "31 March 1909", "increase consistency in United States federal sentencing", "the meridian", "Buffalo Springfield", "the form is properly a rotationally symmetric saltire", "within the chapters", "angevin", "lamps", "Mar del Sur", "Julie Taymor", "an organ", "Tony Aloupis", "American Civil Liberties Union", "Chevron", "attempted burglary", "Pancho Gonzales", "William Henry Harrison", "the orchids", "Britain"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6667264515455305}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24000000000000002, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.7368421052631579, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-413", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9237", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-7595", "mrqa_newsqa-validation-3427", "mrqa_newsqa-validation-2835", "mrqa_searchqa-validation-2827"], "SR": 0.578125, "CSR": 0.5405505952380952, "EFR": 0.9259259259259259, "Overall": 0.7172796792328043}, {"timecode": 42, "before_eval_results": {"predictions": ["high voltage", "water buffalo", "Texas", "Song of Solomon", "Zohan", "Sweden", "Battlestar Galactica", "Slayer Sainte-Marie", "sheep", "Mary I", "cape horn", "a blackbird", "Patty Duke", "Hoosiers", "Judas", "3,000th", "Grow old along with me", "savanna", "Primo Levi", "a long ton", "Kellogg's", "The Fall Guy", "cape horn", "Elizabeth Barret Browning", "Garland", "crocodiles", "Pakistan", "Slovenia", "a baboon", "Morris West", "outta nowhere", "El burlador de Sevilla", "Hawaii", "Empire State Building", "the League of Nations", "Sally Ride", "a bullet-proof vest", "a speech to the Troops at Tilbury", "a A 2.0 grade point average", "patchouli", "Civil War", "a Moon", "Greek Meatballs", "Holden Caulfield", "the Caucasus mountains, southwestern", "Firebird", "14", "Lecompton", "Midnight Cowboy", "Rosetta Stone", "Louisiana", "Malayalam", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "a scuffle with the Beast Folk", "Arabian Gulf", "Ross Kemp", "Marine One", "Juan Manuel Mata Garc\u00eda (] ; born 28 April 1988) is a Spanish professional footballer who plays as a midfielder for English club Manchester United and the Spain national team.", "Vietnam War", "Taylor Swift", "Herman Cain", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "on-loan David Beckham", "a 15-year-old boy that has left dozens injured and scores of properties destroyed."], "metric_results": {"EM": 0.46875, "QA-F1": 0.591323737026862}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.8, 0.375]}}, "before_error_ids": ["mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-7802", "mrqa_searchqa-validation-14410", "mrqa_searchqa-validation-9124", "mrqa_searchqa-validation-15998", "mrqa_searchqa-validation-8140", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-8429", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-6562", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-11902", "mrqa_searchqa-validation-1970", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-448", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-1406", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-1411", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-115"], "SR": 0.46875, "CSR": 0.5388808139534884, "EFR": 1.0, "Overall": 0.7317605377906977}, {"timecode": 43, "before_eval_results": {"predictions": ["Southeastern U.S.", "gin", "Leicester", "Gothenburg", "the Jets", "scurvy", "Japan", "azor", "falconry", "Niger", "Norman Brookes", "Billy Crystal", "Jaipur", "Goran Ivanisevic", "14", "Geoffrey Cox", "Spain", "Henry Hudson", "bridge", "a dove", "Much Ado About Nothing", "Felix", "Louis XV", "Elizabeth Mainwaring", "Australia", "Robert A. Heinlein", "Old Ironsides", "Aug. 24, 1572", "a sperm", "a crafty artisan", "Massachusetts", "Sherlock Holmes", "Delaware", "Olivia Smith", "Costa Concordia", "spiral", "orange juice", "Jim Morrison", "Mona Lisa", "graphite", "Bash Street", "Mercury", "Ireland", "Gandalf", "Moses", "Bogart", "Minder", "a star that is approximately the size of the earth, has undergone gravitational collapse,", "a turkey", "Eva Marie", "Tigger", "Saint Alphonsa", "Ku - Klip", "March 15, 1945", "Portsea", "1861", "McLaren", "\" Maria\"", "Diego Milito", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Zyrtec", "kids", "the diameter", "Leo Frank"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5833333333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666669, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-294", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5367", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-2978", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7617", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-5446", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2751"], "SR": 0.515625, "CSR": 0.5383522727272727, "EFR": 1.0, "Overall": 0.7316548295454546}, {"timecode": 44, "before_eval_results": {"predictions": ["Hoek van Holland", "Lori McKenna", "elm tree at Shackamaxon", "Marcus Atilius Regulus", "the state in which both reactants and products are present in concentrations which have no further tendency to change with time", "Lucknow", "Kristy Swanson", "Hendersonville, North Carolina", "hydrogen", "Abraham", "Kelly Osbourne", "1773", "the final episode of the series", "a god of the Ammonites", "the Han", "just after the Super Bowl", "1980s", "31 October 1972", "The Italian Agostino Bassi", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Brazil", "Barcelona", "LED", "BC Jean and Toby Gad", "24 hours", "cognitive bias", "2015", "UMBC", "Tommy Shaw", "November 2016", "Thomas Alva Edison", "B.R. Ambedkar", "the Indian Olympic Association ( IOA )", "Niles", "Judy Collins", "Oklahoma", "Grand Inquisition", "T'Pau", "Angel Island Immigration Station", "Johannes Gutenberg", "Terry Reid", "masons'marks", "Johannes Gutenberg", "Domhnall Gleeson", "the Soviet Union's 1976 achievement of thirteen gold medals", "Pandavas", "2020 National Football League ( NFL ) season", "1994 season", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "the church at Philippi", "Chuck Noland", "The Green Mile", "Cal Ripken, Jr.", "1905", "Workers' Party", "Revolver", "140 million", "contraband", "the International Space Station", "750", "lew springsteen", "nantucket", "Microsoft", "Love Never Dies"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6240405701754386}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 0.0, 0.10526315789473684, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5900", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-5597", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-7676", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-1345", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-3760"], "SR": 0.515625, "CSR": 0.5378472222222221, "EFR": 0.8709677419354839, "Overall": 0.7057473678315412}, {"timecode": 45, "before_eval_results": {"predictions": ["generally westward", "off the coast of Somalia", "Felipe Massa", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Tibet's independence from China", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "President Bush", "Brad Blauser, center,", "Sunday", "Vice President Joe Biden", "about 1,300 meters in the Mediterranean Sea", "only one", "raping and murdering a woman in Missouri", "Ryder Russell", "August 11, 12 and 13", "father, Osama bin Laden", "the club's board", "insect stings", "U.N. Security Council resolution in 2006", "mrs henderson,", "Fullerton, California", "Chinese", "$2.6 million", "Rev. Alberto Cutie", "ceo Herbert Hainer", "Kate Hudson's ex, Black Crowes rocker Chris Robinson", "269,000", "surrounding areas of the bustling capital", "North Korea", "state's attorney", "was arrested in a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola", "two", "Spanish", "remark made by his former caddy", "were stolen by the armed robbers.", "Oxbow, a town of about 238 people", "motor scooter", "Joan Rivers", "6,000", "gossip girl", "Public Citizen", "Diversity", "650", "Yemen", "South Africa", "Daniel Radcliffe", "President Sheikh Sharif Sheikh Ahmed", "cities throughout Canada", "Florida's Everglades, known as the River of Grass", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "Tyrion", "on June 8, 2009", "'Q'", "wolf", "polio", "mathematics", "the outdoors", "Mel Blanc", "giving mouth", "Colorado", "CIA", "Billy Bob Thornton"], "metric_results": {"EM": 0.375, "QA-F1": 0.5059028379708527}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 0.5454545454545454, 0.4, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.8, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.6666666666666666, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.08, 0.8717948717948718, 1.0, 0.0, 0.25, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.25, 0.0, 0.19999999999999998, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9261", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-3308", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-1120", "mrqa_triviaqa-validation-3086", "mrqa_hotpotqa-validation-4796", "mrqa_searchqa-validation-6261"], "SR": 0.375, "CSR": 0.5343070652173914, "EFR": 0.975, "Overall": 0.7258457880434783}, {"timecode": 46, "before_eval_results": {"predictions": ["every four years", "part - Samoyed terrier", "instruct Titus to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority ''", "duodenum by enterocytes of the duodenal lining", "warmth", "limited period of time", "during meiosis", "federal government", "alveolar process", "Katharine Hepburn -- Ethel Thayer", "Thorleif Haug", "multiple", "England, Northern Ireland, Scotland and Wales", "Norman Whitfield and Barrett Strong", "transform agricultural productivity, particularly with irrigated rather than dry - land cultivation in its northwest, to solve its problem of lack of food self - sufficiency", "January 2004", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection, capable of producing between 460 and 485 kW ( 620 -- 650 bhp )", "276", "the French CYCLADES project directed by Louis Pouzin", "when each of the variables is a perfect monotone function of the other", "Kansas City Chiefs", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "especially in Western cultures", "Husrev Pasha", "2018", "Squamish, British Columbia, Canada", "Mankombu Sambasivan Swaminathan", "change an annuity ticket to cash", "1962", "prophets and beloved religious leaders", "Best Art Direction, Best Makeup, and Best Visual Effects", "interstate communications by radio, television, wire, satellite, and cable", "Amen", "William Chatterton Dix", "1987", "Stefanie Scott", "Gamora on Ego", "in the fovea centralis", "Ephesus", "rabbinic Judaism's Oral Torah", "George H.W. Bush", "seven", "Uralic", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired, assumes", "scrolls", "British and French Canadian fur traders", "Lori McKenna", "2017", "October 27, 1904", "in the mid - to late 1920s", "Firoz Shah Tughlaq", "Runic", "Backgammon", "Malcolm Bradbury", "Boston Red Sox", "All-Star Game and All-NBA Team", "Den V\u00e6gelsindede", "Pakistan", "Maj. Nidal Malik Hasan, MD,", "February 12", "# Quiz", "the Capulets & the Montagues", "plutonium-238", "24"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5387764510393095}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.6153846153846153, 0.923076923076923, 0.6666666666666666, 0.30769230769230765, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.15789473684210525, 0.6666666666666666, 0.6451612903225806, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.37499999999999994, 0.0, 0.7499999999999999, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6122448979591837, 0.2857142857142857, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-1438", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-8299", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-1692", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-5895"], "SR": 0.390625, "CSR": 0.53125, "EFR": 0.9230769230769231, "Overall": 0.7148497596153847}, {"timecode": 47, "before_eval_results": {"predictions": ["Max Martin", "1995 Mitsubishi Lancer OZ Rally", "21 June 2007", "Lewis Carroll", "Puerto Rico Electric Power Authority ( PREPA ) -- Spanish : Autoridad de Energ\u00eda El\u00e9ctrica ( AEE )", "Austria - Hungary", "Mace Coronel", "libretto", "Jane Addams", "Harrys", "1800", "in the dress shop", "201", "Millerlite", "Instagram", "Experimental neuropsychology", "30 years after Return of the Jedi", "Selena Gomez", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W", "Christina Aguilera", "during initial entry training", "Eddie Murphy", "1997", "Bonnie Aarons", "1960", "during the summer of 1979", "Part XI of the Indian constitution", "the Constitution of India came into effect on 26 January 1950", "Rising Sun Blues", "halogenated paraffin hydrocarbons that contain only carbon, chlorine, and fluorine, produced as volatile derivative of methane, ethane, and propane", "the 1980s", "MacFarlane", "Singing the Blues", "1898", "a set of components that included charting, advanced UI, and data services ( Flex Data Services )", "Matt Jones", "brass band parades", "The Confederate States Army ( C.S.A. )", "three", "MFSK", "Zachary John Quinto", "Sanchez Navarro", "Have I Told You Lately", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "John Travolta", "Scherie or Phaeacia", "1 October 2006", "2017 Auburn Tigers football team", "Michael Kopelow", "Michael Clarke Duncan", "Origination Clause of the United States Constitution", "Gestapo", "Belgium", "heartburn", "Baltimore", "Vanessa Hudgens", "two", "Paris.", "voice-assistant software", "hardship for terminally ill patients and their caregivers", "The Iliad and The Odyssey", "Jeff Goldblum", "Prego", "Marillion"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5657471198981349}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.8000000000000002, 0.0, 0.08695652173913042, 0.5882352941176471, 1.0, 0.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.8, 0.25, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.07407407407407408, 0.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.0, 0.4, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-10381", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-8182", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-89", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-2606", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-886", "mrqa_searchqa-validation-16901"], "SR": 0.4375, "CSR": 0.529296875, "EFR": 0.9166666666666666, "Overall": 0.7131770833333333}, {"timecode": 48, "before_eval_results": {"predictions": ["Daylight Saving Time", "Cygnus", "Constantine", "Lautrec", "Nigeria", "Hawaii", "Jeremiah", "Paddock", "Don Knotts", "cinnamon Life", "Kbenhavn", "Porgy and Bess", "Dutchman", "Mars", "Cheers", "Robert Frost", "Thelma Dickinson", "dungeon", "cyclorama", "Laila Ali", "Fuchsia excorticata", "Epiphany", "Twin cities", "Fat Man, you shoot a great game of pool", "to lie idle, close or snug", "Led Zeppelin", "the Book of Kells", "a grandfather", "nuclear fission", "a Heisman", "Gulf of Tonkin", "Stephen Vincent Bent", "coelacanth", "Prague", "the Federal Reserve", "coal-fired power plant", "Afghanistan", "Cheetah", "Ambrose Bierce", "the American Lung Association", "croquet", "Aphrodite", "You get your house back", "Chico Rodriquez", "Budapest", "John Mahoney", "pythons", "Nittany", "Charles de Gaulle", "Beverly Cleary", "Afghanistan", "2004", "a recognized group of people who jointly oversee the activities of an organization", "since 3, 1, and 4", "eight", "the Young Men's Christian Association", "Liddell", "the theory of direct scattering and inverse scattering", "43rd", "South America", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "$150 billion", "fill a million sandbags and place 700,000 around our city.", "Gabbert Langer"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6056547619047619}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-4404", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-1117", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-5114", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-8359", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-8727", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-12165", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-3066", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-6317", "mrqa_hotpotqa-validation-1316", "mrqa_newsqa-validation-1791", "mrqa_triviaqa-validation-336"], "SR": 0.53125, "CSR": 0.5293367346938775, "EFR": 0.9666666666666667, "Overall": 0.7231850552721089}, {"timecode": 49, "before_eval_results": {"predictions": ["William Wyler", "Redford's adopted home state of Utah", "pelvic floor", "Kanawha Rivers", "amino acids glycine and arginine", "a alien mechanoid", "1937", "1920s", "832 BCE", "2005", "New York City", "Beorn", "Jonathan Cheban", "1992", "Montreal", "18", "Kristy Swanson", "230 million kilometres", "Camping World Stadium in Orlando", "the Bactrian", "sorrow regarding the environment", "the New York Yankees", "Woodrow Wilson", "Phosphorus pentoxide", "Brooklyn, New York", "Secretary of Commerce Herbert Hoover", "the Mishnah", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "Tom Brady", "Around 1200", "Walter Egan", "The Crossing", "a masculine name common in the Muslim World", "states, such as Taiwan, officially claim to hold continental territories but are de facto limited to control over islands", "origins of replication, in the genome", "Los Angeles", "2012", "Jackie Robinson", "Coton", "the American Civil War", "Pradyumna", "1979", "James Madison", "UVA", "Roger Federer", "17 - year - old", "Kylie Minogue", "5,534", "the semilunar pulmonary valve", "electron donors", "A rear - view mirror", "Geroge W. Bush", "Old Ironsides", "Lucas McCain", "Christian Maelen", "2014", "2014", "the idea that the Richmond students did nothing because of the \"bystander effect\":", "the Listeria monocytogenes bacteria", "United States, NATO member states, Russia and India", "Sagamore Hill", "hyperthyroidism", "a parrot", "UVB"], "metric_results": {"EM": 0.546875, "QA-F1": 0.64473165521494}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6976744186046512, 1.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-7770", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-558", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-14922", "mrqa_searchqa-validation-10011", "mrqa_triviaqa-validation-7608"], "SR": 0.546875, "CSR": 0.5296875, "EFR": 1.0, "Overall": 0.7299218749999999}, {"timecode": 50, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-561", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9724", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3331", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.828125, "KG": 0.48359375, "before_eval_results": {"predictions": ["alla capella", "\"leno\" fabric and then cutting the fabric into strips to make the chenille yarn", "Zeus", "7", "Lady Gaga", "John Alec Entwistle", "Esmeralda's Barn night  club", "george (Provis)", "wales", "December 18, 1958", "boudin", "Bristol", "Queen Victoria and Prince Albert", "a modern Townsend Thoresen car and passenger ferry", "Amanda Barrie", "Sharjah", "Madagascar", "Persian Gulf", "Miss Havisham", "oxygen", "Macbeth", "Gentlemen Prefer Blondes", "george w. Bush", "Japan", "the hose", "Lake Erie and all the Great Lakes", "John Key", "Subway's", "geodetics", "Dylan Thomas", "(Fred) Astaire", "the Tower of London", "Bridge", "quarter", "cirrus uncinus", "Manchester, England", "george allingham,", "cheese", "Klaus dolls", "Argentina", "Nick Faldo", "george bwerk", "the Count Basie Orchestra", "the Behemoth", "Matthew Boulton", "The Firm", "Elbow bursitis", "General Paulus", "isobar", "neapolitan", "Danish", "July 14, 1969", "1923", "Massillon, Ohio", "1937", "LA Galaxy", "2017 season", "British broadcasters,", "Frank Ricci,", "\"Three Little Beers,\"", "sweatshirt", "Susan B. Anthony dollar", "ivory", "in the north and west of the country,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.52281381302521}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.3529411764705882]}}, "before_error_ids": ["mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6534", "mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-5001", "mrqa_triviaqa-validation-7490", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1896", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-639", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-6275", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-3522", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2793", "mrqa_newsqa-validation-216", "mrqa_searchqa-validation-2495", "mrqa_newsqa-validation-2191"], "SR": 0.453125, "CSR": 0.5281862745098039, "EFR": 1.0, "Overall": 0.7129028799019608}, {"timecode": 51, "before_eval_results": {"predictions": ["manoah\u2019s wife", "smith", "Passenger Pigeon", "catalytic agent", "Sir Edwin Landseer", "Hitler", "Albert Einstein", "smith", "scales", "Judy Garland", "David Walliams", "tepuis", "sea burial", "erie lyle", "mare", "hypopituitarism", "elizabeth", "bees", "Treaty of Utrecht", "water", "deciduous", "stanley", "lyle", "mantle", "havre", "algae", "shines", "Algeria", "Churchill Downs", "The United States", "Leonard Bernstein", "Vladimir Putin", "shipwreck", "nahuatl", "20", "Ken Platt", "Jane Krakowski", "adios", "vikings", "Reginald Smith", "The Ghost of Christmas Yet To Come", "haf", "Daddy Mac", "creme fraiche", "convict", "barry lyle", "22", "havre", "Dublin", "Babylon", "Continental Marines", "Joe Spano", "Gary Grimes", "skeletal muscle", "CBS News", "Herman's Hermits", "United States House of Representatives", "\"Den of Spies\"", "Haeftling", "2-0", "Brunei", "a Pringles can", "a peanut butter cup", "1983"], "metric_results": {"EM": 0.421875, "QA-F1": 0.45807291666666666}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-805", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1815", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-7762", "mrqa_naturalquestions-validation-692", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2260", "mrqa_searchqa-validation-3853"], "SR": 0.421875, "CSR": 0.5261418269230769, "EFR": 1.0, "Overall": 0.7124939903846153}, {"timecode": 52, "before_eval_results": {"predictions": ["riding a real horse", "australia", "Belarus", "vodka", "Tony Manero", "Snarked", "Prussian 2nd Army", "silversmith", "Joshua Tree National Park", "carpathia", "Superman", "Letchworth", "velvet", "1", "nightmare on Elm Street", "Crackerjack", "white Ferns", "Utah", "carb", "As You Like It", "Labrador Retriever", "-shion", "delaware", "thief", "Clara wieck", "squeeze", "Apocalypse Now", "Boojum", "ski resort of St Moritz in Switzerland", "dave hockney", "Scafell Pike", "Edgar Allan Poe", "klaus smith", "Tony Blackburn", "Donna Summer", "france", "kiki", "wolf", "Titanic", "ken purdy", "trumpet player", "Andrew Lloyd Webber", "Malawi", "australia", "eric bristman", "mrs baryshnikov", "Norwich", "Ruth Rendell", "The Smiths", "ontario", "Ohio", "Miami Heat", "Joanna Page", "1961 during the Cold War", "1902", "FCI Danbury", "25 June 1971", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "Caster Semenya", "a \"happy ending\" to the case.", "universal and equal suffrage", "psychiatry", "Joe Montana", "Colonel"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6659722222222222}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.22222222222222224, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2767", "mrqa_triviaqa-validation-6648", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-4518", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3898", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5354", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-11787"], "SR": 0.609375, "CSR": 0.5277122641509434, "EFR": 0.96, "Overall": 0.7048080778301886}, {"timecode": 53, "before_eval_results": {"predictions": ["pigment", "Jack Ruby", "Google", "hugh Dowding", "squash", "Tennessee Williams", "Jim Smith", "injecting a 7 percent solution", "David Bowie", "hemp", "canoeing", "mozambique", "Christian Louboutin", "Ironside", "the need to toss logs across narrow chasms", "James Dean", "Mars", "once", "About Eve", "chicken", "George Orwell", "noel henderson", "homeless", "Derbyshire", "Cubism", "australia", "Virginia", "thomas hemingtha", "noel heulz", "Ruth Rendell", "nottingham", "chicago addams", "hugh grant", "sweden", "1921", "swynford", "Tears for Fears", "praseodymium", "Bruce Alexander", "slap", "spinal cord", "razor", "Arthur Hailey", "hart", "madonna", "Little arrows", "a nerve cell cluster", "seattle", "Haystacks", "27", "orchid", "104 colonists", "aiding the war effort", "benzodiazepines", "Cesar Millan", "2004 Paris Motor Show", "Brazilian-American mixed martial artist", "cell phones", "anil kapoor", "drug cartels", "Guinea", "steel", "place bet", "2012"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6588541666666667}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-3119", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-6040", "mrqa_triviaqa-validation-285", "mrqa_naturalquestions-validation-3962", "mrqa_hotpotqa-validation-3655", "mrqa_searchqa-validation-16488", "mrqa_searchqa-validation-14449"], "SR": 0.609375, "CSR": 0.529224537037037, "EFR": 1.0, "Overall": 0.7131105324074074}, {"timecode": 54, "before_eval_results": {"predictions": ["Big Mamie", "cinemas", "Canadian", "Sunyani West District", "six", "the Big 12 Conference", "armidale, New South Wales", "Alfred Preis", "Bowland Fells", "CAC/PAC JF-17 Thunder", "child 44", "Switzerland", "The Keeping Hours", "Jeffrey Adam \"Duff\" Goldman", "Ang Lee", "Mineola", "67,038", "Kentucky Wildcats", "eston Atkins", "Parliamentarians (\"roundheads\") and Royalists (\"Cavaliers\")", "Harry Potter series", "the Haitian Revolution", "alcoholic drinks", "john smith", "John Meston", "York County", "Cleveland Cavaliers", "Scottish rock band U2", "Hillsborough County", "Cartoon Network Too", "San Diego County Fair", "the fictional city of Quahog, Rhode Island", "mathematician and physicist", "January 19, 1943", "George Raft", "Chiba, Japan", "1944", "the British Army", "311", "Jennifer Taylor", "Cartoon Network", "Columbia Records", "British", "6,241", "October 17, 2017", "Ub Iwerks", "Phil Collins", "Hawaii", "Gregg Popovich", "1979", "Vincent Landay", "Lauren Tom", "~ 0.058 - 0.072 mm", "Spektor", "spain", "fort Sumter", "vanilla", "a Christian farmer", "three", "The Louvre", "relativity", "hemming", "contractions", "Capitoline Wolf"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6003720238095238}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-3161", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-1310", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5383", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2609", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-1776"], "SR": 0.515625, "CSR": 0.5289772727272728, "EFR": 1.0, "Overall": 0.7130610795454546}, {"timecode": 55, "before_eval_results": {"predictions": ["The 2016 United States Senate election in Nevada", "1952", "private Roman Catholic university", "voice-work", "400 MW", "Kerry Marie Butler", "Esp\u00edrito Santo Financial Group", "July 10, 2017", "Tuesday, January 24, 2012, at 8 p.m. ET/PT.", "leopard", "Salman Rushdie", "Tom Rob", "Big Bad Wolf", "Rockland County", "Personal History", "Australian-American", "Holston River", "1943", "1996", "Tamil Nadu", "Tallahassee City Commission", "A Boltzmann machine", "three different covers", "Hechingen in Swabia", "Floyd Mutrux and Colin Escott", "A Little Princess", "Marvel's Agent Carter", "2008", "from July 2, 1967 to August 21, 1995", "located on the northeast side of Steeles Avenue and Kennedy Road, right across the municipal border from the city of Toronto", "Mark Neveldine and Brian Taylor", "the fictional city of Quahog, Rhode Island", "Spiro Agnew", "The String Cheese Incident", "three", "Noel Gallagher", "Thorgan", "1692", "9", "The game introduced several innovations to the series: an open world split into zones, a seamless battle system, a controllable camera, a customizable \"gambit\" system which lets the player control the artificial intelligence", "Saudi Arabian", "26,000", "Northampton Town", "Indian National Congress", "American attorney, politician, and the principle founder of the Miami Dolphins", "punk rock", "Matt Lucas", "Duke", "Bill Clinton", "John D Rockefeller's Standard Oil Company", "richmondow", "pigs", "In Time", "The flag of Vietnam", "pulsar", "The Dogger Bank", "the first eight seasons", "\"totaled,\"", "Steven Gerrard", "ketamine", "martins", "The Southern Hemisphere", "Diebold", "Andr\u00e9s Iniesta"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6829545454545454}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [0.8333333333333333, 1.0, 0.4, 0.0, 1.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.5454545454545454, 0.19999999999999998, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-3340", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-5228", "mrqa_triviaqa-validation-3487", "mrqa_newsqa-validation-456", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-6593"], "SR": 0.59375, "CSR": 0.5301339285714286, "EFR": 1.0, "Overall": 0.7132924107142857}, {"timecode": 56, "before_eval_results": {"predictions": ["35,000", "Vishal Bhardwaj", "Macau", "Russian film industry", "no. 3", "The Government of Ireland", "Washington Street", "Eielson Air Force Base", "Clube Atl\u00e9tico Mineiro", "Steve Carell", "Michael Phelps", "Richard Strauss", "a married World War II nurse visiting Scotland", "Iynx", "David Starkey", "Richard II", "Anne Perry", "Ready to Die", "Nick Harper", "\"Darconville\u2019s Cat\"", "invoice, bill or tab", "October 16, 2015", "Dan Brandon Bilzerian", "The Andes", "Srinagar", "STS-51-C.", "Gregg Harper", "The Ones Who Walk Away from Omelas", "Minnesota's 8th congressional district", "three", "November 27, 2002", "Hank Azaria", "UFC Fight Pass", "The Sun", "Noel Paul Stookey", "Monty Python", "Ready Player One", "1,925", "7 October 1978", "June 2, 2008", "Ravenna", "John Meston", "the Battelle Energy Alliance", "a co-op of grape growers", "The Spiderwick Chronicles", "goalkeeper", "George Orwell", "Croatian", "Labour Party", "Giovanni", "Warsaw", "Texhoma", "the third extracellular compartment, the transcellular, is thought of as separate from the other two and not in dynamic equilibrium with them", "September 2017", "Ken Norton", "France", "Hastings", "the UK", "Al Nisr Al Saudi", "5 1/2-year-old", "cobalt", "Hollaback", "The bassoon", "time in exchange for detailed public disclosure of an invention"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6473713786213786}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-675", "mrqa_naturalquestions-validation-3261", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-6632"], "SR": 0.5625, "CSR": 0.5307017543859649, "EFR": 0.9642857142857143, "Overall": 0.7062631187343358}, {"timecode": 57, "before_eval_results": {"predictions": ["1961", "Ballon d'Or", "Elsie Audrey Mossom", "the first trans-Pacific flight from the United States to Australia", "House of Hohenstaufen", "BraveStarr", "White Knights of the Ku Klux Klan", "Benjam\u00edn Arellano F\u00e9lix", "lion", "Tony Award", "The German concept of Lebensraum", "China", "Esperanza Spalding", "1854", "Art Deco-style skyscraper", "Starachowice", "Debbie Harry", "England", "Mineola", "Osaka's Kansai International Airport", "Melbourne's City Centre", "B507 road", "bioelectricity", "German", "casting, job opportunities, and career advice", "ten episodes", "2012", "second generation", "Paul Hindemith", "Euripides", "first", "Clitheroe Football Club", "Peter Wooldridge Townsend", "Paper", "November 27, 2002", "Campbellsville University", "1837", "2006", "al-Qaeda", "Humberside Airport", "around 3,500,000", "racehorse breeder", "Nia Kay", "11 November 1918", "1891", "Bank of China Tower", "Lerotholi Polytechnic FC", "first", "872 to 930", "Lee Travis", "Richard Street", "diametrically opposite the South Pole", "a radius 1.5 times the Schwarzschild radius", "spot - type detectors", "Japan", "Apollo", "Leeds", "bragging about his sex life", "people are going to look at the content", "Fernando Torres", "Alicia", "Days Inns", "Napoleon's famous victories", "Gillis Grafstr\u00f6m"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6344246031746031}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.4, 0.5, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.1111111111111111, 0.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.3333333333333333, 0.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.5, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-4341", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4436", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-3499", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2325", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-6491", "mrqa_triviaqa-validation-4287"], "SR": 0.484375, "CSR": 0.5299030172413793, "EFR": 1.0, "Overall": 0.7132462284482759}, {"timecode": 58, "before_eval_results": {"predictions": ["Karl Kr\u00f8yer", "an open window that fits neatly around him", "Gyanendra", "Susan Atkins", "Palestinian prisoners", "\"I really hope that what I did will enable other women to come forward in similar situations,\"", "breast cancer", "a round", "are not for sale,", "10 municipal police officers", "his past and his future", "mpire of the Sun", "Carol Browner", "breast cancer", "Haeftling", "49,", "capital murder and three counts of attempted murder", "response by raising its alert level, while the country's media went into overdrive trying to predict how this oblique and erratic state would respond.", "some of the Awa", "16th grand Slam title.", "curfew", "\"Nu au Plateau de Sculpteur,\"", "claimed her third title since making her comeback last year after giving birth to baby daughter Jada,", "Glasgow, Scotland concert", "antihistamine and an epinephrine auto-injector", "Damon Bankston", "Afghanistan", "a ban on inflatable or portable signs and banners on public property", "Why do genocides and mass atrocities happen?", "sharia in Somalia is part of the laws for thousands of years, and we never had this kind of a thing.", "humans", "Vancouver, British Columbia", "prevent all civilians and laying down arms", "two years ago", "is a businessman, team owner, radio-show host and author.", "Sen. Barack Obama", "his new book.\"I need to distort not just leather and fabric, but also words,\"", "fake his own death", "his first presidential visit to a Muslim country", "one-of-a-kind navy dress with red lining by the American-born Lintner,", "The Rev. Alberto Cutie", "citizenship", "for housing, business and infrastructure repairs", "The nation's foremost concert producer, Charles Jubert, died. So did members of four bands who were practicing inside a studio that collapsed.", "mammoth's skull", "\"We are here to cooperate with anyone and everyone that will help us find the guilty party and return Lisa home safely,\"", "creation of an Islamic emirate in Gaza", "in the lawless southern provinces and especially in the Taliban stronghold of Helmand,", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "4-1.", "says Somalia's piracy problem was fueled by environmental and political events.", "Francisco Pizarro", "Owen Vaccaro", "2003", "Renault", "cheese", "The Straight Dope", "Grantham Canal", "War Is the Answer", "arts manager", "The Antarctic ice sheet", "Ms", "achieve", "prevent both freezing and melting"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4616987742398927}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 1.0, 0.4, 0.22222222222222224, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.29629629629629634, 1.0, 0.5, 1.0, 0.0, 0.5833333333333334, 0.0, 0.4, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.10526315789473685, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.07407407407407407, 0.0, 1.0, 0.0, 0.0, 0.9523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_newsqa-validation-112", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-901", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2182", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1055", "mrqa_triviaqa-validation-6184", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-4517", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-1521"], "SR": 0.359375, "CSR": 0.5270127118644068, "EFR": 0.975609756097561, "Overall": 0.7077901185923935}, {"timecode": 59, "before_eval_results": {"predictions": ["a female soldier,", "Sunday", "relatives of the five suspects,", "A witness", "U.N. Security Council", "Democratic VP candidate", "Choi", "second child", "Old Trafford", "leftist Workers' Party.", "Robert", "Christiane Amanpour", "Narayanthi Royal", "Mexico", "five", "going somewhere very special, far away, because under the Communist regime you didn't travel that much and Prague was \"wow.\"", "learning retreat, where they can learn in safer surroundings.", "Keating Holland.", "Asashoryu,", "Mutassim,", "Dr. Jennifer Arnold and husband Bill Klein,", "Lindsey oil refinery in eastern England.", "Sunday.", "future relations between the Middle East and Washington.", "prostate cancer,", "the southern city of Naples", "nearly 100 people", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "2002", "Spc. Megan Lynn Touma,", "how preachy and awkward cancer movies can get.\"", "'City of Silk' in Kuwait,", "strife in Somalia,", "Miami Beach, Florida,", "Charles Darwin", "it was unjustifiable \"for a project which does nothing more than perpetuate misconceptions about the state and its citizens.\"", "President Obama.", "one, you let them know what the case involves and they've heard it on the news,", "the approximately eight hours we spent carving in the middle of our Mountain View, California, campus.", "baseball bat", "Philippines", "\"directed donation\" from a deceased organ donor,", "Jaime Andrade", "Alfredo Astiz,", "ties", "The station", "The father of Haleigh Cummings,", "Adriano", "$8.8 million on its third weekend", "Tuesday", "Alan Graham", "Canada south of the Arctic", "Glenn Close", "Charles Crozat Converse", "Richard Krajicek", "Manchester", "Robert", "Big Kenny", "2013 Cannes Film Festival", "Yeah!", "3", "Rooster Cogburn", "Korea", "1936"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5879967833092833}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.07407407407407407, 0.3636363636363636, 0.0, 1.0, 1.0, 0.4444444444444445, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 0.3, 1.0, 0.0, 0.09090909090909091, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.5, 0.5, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-3828", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-1872", "mrqa_naturalquestions-validation-2512", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6864", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-53", "mrqa_searchqa-validation-11013"], "SR": 0.4375, "CSR": 0.5255208333333333, "EFR": 1.0, "Overall": 0.7123697916666666}, {"timecode": 60, "UKR": 0.720703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1899", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-382", "mrqa_hotpotqa-validation-4012", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-892", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-266", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2558", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1143", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5687", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6157", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7783", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.8125, "KG": 0.48671875, "before_eval_results": {"predictions": ["2017,", "1648 - 51 war", "Thomas Edison's assistants, Fred Ott", "after realizing Stefan's desperation to keep her alive, decides to complete her transition", "Master Christopher Jones", "Juliet", "the status line", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "The Maginot Line", "1937", "Fix You", "Hirschman", "Bobby Eli", "relieve families who had difficulty finding jobs during the Great Depression in the United States", "approximately 11 %", "Guy Berryman", "Austria - Hungary", "Sharyans Resources", "Babe Ruth", "in Seattle, Washington", "more than 420 locations", "Wylie Draper", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province )", "DNA was a repeating set of identical nucleotides", "Magnavox Odyssey", "a means of restarting play after a minor infringement", "19 December 2015", "tolled ( quota ) highways", "close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "innermost in the eye", "Saturday", "Glenn Close", "reservoirs at high altitudes", "aiding the war effort", "into smaller pulmonary arteries that spread throughout the lungs", "Nepal", "the symbol \u00d7", "1974", "Phillip Paley", "the tax rate paid by a small business", "the liver and kidneys", "the county seat and commercial center of Lee County, Florida, United States", "tenderness of meat", "Stephen Graham", "Florida", "2010", "Jeff Barry and Andy Kim", "infection", "ABC", "August 1991", "The Maidstone Studios in Maidstone, Kent", "two", "helium", "caliber", "Edward James Olmos", "Terrina Chrishell Stause", "Hilux", "Omar bin Laden,", "a man's lifeless, naked body", "jazz", "Falklands", "cement pond", "Seventy-six trombones", "Fifty Shades of Grey"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6069322454920281}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.8, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.962962962962963, 0.0, 0.0, 1.0, 0.5, 0.0, 0.28571428571428575, 0.5714285714285715, 0.2666666666666667, 0.8695652173913044, 0.15384615384615383, 1.0, 0.8333333333333333, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-4850", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-8998", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-6052", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1584", "mrqa_searchqa-validation-15624", "mrqa_searchqa-validation-893"], "SR": 0.484375, "CSR": 0.5248463114754098, "EFR": 0.9393939393939394, "Overall": 0.6968324251738699}, {"timecode": 61, "before_eval_results": {"predictions": ["17 - year - old", "The procedure can be performed at any level in the spine ( cervical, thoracic, or lumbar ) and prevents any movement between the fused vertebrae", "Dido", "international aid", "reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "January 15, 2007", "Gorakhpur railway station", "Roger Federer", "Battle of Antietam", "1933", "it `` never had any meaning other than the obvious one '' and is about the `` loss of innocence in children ''", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "John Travolta", "August 5, 1937", "Rory McIlroy", "Jonathan Breck", "the 4th century", "Smith Jerrod", "Acid rain", "2017 - 18 season", "Mulberry Street", "Pete Seeger", "the atrioventricular node, along the Bundle of His and through bundle branches to cause contraction of the heart muscle", "1994", "Wyatt and Dylan Walters", "10 May 1940", "Phillipa Soo", "Denver Broncos", "Judith Cynthia Aline Keppel", "Vincent Price", "the customer's account", "along the coast of northern California", "The film follows a child with Treacher Collins syndrome trying to fit in", "metaphase", "Sally Field", "Eric Clapton", "one's pay grade", "2001 Indian epic sports - drama film", "Colman", "Karen Gillan", "Easter", "Beijing, China", "either a marked ( `` - s '' ) or unmarked plural, as in : `` 1 lakh people ''", "Havana Harbor", "1824", "1977", "Qutab Ud - Din - Aibak", "Prince Henry", "the inferior thoracic border -- made up of the diaphragm", "United States customary system", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "floating ribs", "Cowboy Builders", "Romania", "Helensvale", "Kathryn Jean Martin \"Kathy\" Sullivan AM", "over 20 million", "to see my kids graduate from this school district.\"", "torture and indefinite detention", "iPhone 4S news,", "Chagas disease", "Jacob Marley", "John Hersey", "22 September 2015"], "metric_results": {"EM": 0.515625, "QA-F1": 0.641176837616458}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.12903225806451613, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809523, 0.07407407407407408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 0.9, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6, 0.75, 0.2758620689655173, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.4, 0.4, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8382", "mrqa_naturalquestions-validation-6091", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6596", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-451", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-2373", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-2249", "mrqa_searchqa-validation-4844", "mrqa_searchqa-validation-14622"], "SR": 0.515625, "CSR": 0.5246975806451613, "EFR": 0.967741935483871, "Overall": 0.7024722782258064}, {"timecode": 62, "before_eval_results": {"predictions": ["Dior", "cranberry", "the urban riverfront park", "fibula", "fort boyard", "a beached whale", "Mississippi River", "Dennis Quaid", "Lil Jon", "the Boer War", "the Colosseum", "Goldeneye", "fish stock", "termites", "Sir Winston Leonard Spencer-Churchill", "Salford in Greater Manchester", "the Lincoln Tunnel", "Pinta", "Louisiana's 3rd congressional district", "Billy the Kid", "Rembrandt", "Canada", "fort boyard", "Benedict XVI", "a US federal agency", "Prague", "Hanging Gardens", "data", "Two and a Half Men", "fort boyard", "Nacho Libre", "William & Mary", "fox", "the bassoon", "Montpelier", "Halo 4", "a pound of Antonio's flesh", "Iran", "Best Picture", "Hamlet", "Heroes", "Syria", "mead", "Atlanta Hawks", "Carnival", "dentures carved out of hippopotamus ivory", "Samson", "Dustin Hoffman", "Nittany", "Sicily", "Socrates", "more than 1,000", "the large area needed for effective gas exchange", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "fort boyard", "Honolulu, Hawaii", "fort boyard", "Esperanza Emily Spalding", "2000", "Sargent Shriver", "Daytime Emmy Lifetime Achievement Award.", "The Impeccable", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Gleneagles Golf Course"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5920095755693582}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.8695652173913043, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.47619047619047616, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-12748", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-13095", "mrqa_searchqa-validation-14699", "mrqa_searchqa-validation-1906", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-12588", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-1458", "mrqa_searchqa-validation-5976", "mrqa_searchqa-validation-1751", "mrqa_searchqa-validation-6371", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-11584", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-16438", "mrqa_searchqa-validation-8676", "mrqa_searchqa-validation-6211", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-14898", "mrqa_searchqa-validation-2392", "mrqa_searchqa-validation-8819", "mrqa_searchqa-validation-8062", "mrqa_searchqa-validation-546", "mrqa_searchqa-validation-15398", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6028", "mrqa_triviaqa-validation-7643", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-5895", "mrqa_newsqa-validation-3961", "mrqa_triviaqa-validation-1787"], "SR": 0.4375, "CSR": 0.5233134920634921, "EFR": 0.9722222222222222, "Overall": 0.7030915178571429}, {"timecode": 63, "before_eval_results": {"predictions": ["the Voting Rights Act", "a 919mm Parabellum pistol", "disabilities", "Harry Potter and the Chamber of Secrets", "Senator Mitchell", "Shampoos", "the Bronze Age", "the People's Party", "the Tower of London", "Daniel", "\"Cry-Baby\"", "the largest city", "Alaska", "Cosmopolitan", "David Beckham", "Minoan", "Japan", "rodeo", "a screwdrivers", "Vietnam", "Stanford", "a minor", "a prism schism", "Alaska", "Wallis Warfield Simpson", "a centipede", "Greek", "Sisters Rosensweig", "Brooklyn", "Penn State", "Easter Island", "President Nasser", "the Hell Is Hell", "Stephen Hawking", "Labor Day", "Mozambique", "landfills", "The Silence of the Lambs", "15", "Capone", "Paul McCartney", "Anne Murray", "Tennessee", "Buenos", "contagious", "Jane Austen", "wheat", "baking soda", "peanuts", "philosophy", "Byzantines", "The Paris Sisters", "Rent", "Miami Heat", "gluten", "a embroidered cloth", "Easter", "2004", "Pacific War", "Tsavo East National Park", "Casa de Campo International Airport in the Dominican Republic", "100", "\"vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "a great-grandfather of Miami Marlin Christian Yelich"], "metric_results": {"EM": 0.640625, "QA-F1": 0.733110119047619}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2857142857142857]}}, "before_error_ids": ["mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-3380", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-7830", "mrqa_searchqa-validation-2172", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-6957", "mrqa_searchqa-validation-4519", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-1226", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-4005", "mrqa_newsqa-validation-2082", "mrqa_hotpotqa-validation-798"], "SR": 0.640625, "CSR": 0.525146484375, "EFR": 1.0, "Overall": 0.709013671875}, {"timecode": 64, "before_eval_results": {"predictions": ["Malawi", "Ethiopia", "Sharpening steels", "the short-beaked echidna and the duck-billed platypus", "British Airways", "Yoshi", "1929", "The Blades", "the Kelly Gang", "Dante", "repechage", "uranium", "Wildcats", "d'Artagnan", "Abraham Lincoln", "The Merchant of Venice", "jennifer", "Paul Rudd", "Tanzania", "Julian", "Christian Louboutin", "clarinet", "Firecracker", "September", "Grantham", "Muriel Spark", "Turkey", "kvetch", "the Netherlands", "Lome", "Caviar", "Christian Dior", "Bobbi Kristina Brown", "King William IV", "Dutch", "Jack Lemmon", "Trainspotting", "Australia", "the north-west corner of the central Dutch district", "Diptera", "India and Pakistan", "obi", "Broccoli", "Heisenberg", "1976", "Cyclopes", "phrenology", "Stanley Kubrick's Full Metal jacket", "Tokyo", "California", "Windermere", "third", "the fictional elite conservative Vermont boarding school Welton Academy", "the Old French tailleur ( `` cutter '' )", "James Ager Wworthy", "11 or 13 and 18", "Franz Ferdinand Carl Ludwig Joseph Maria", "Chile", "the single-engine Cessna 206 went down,", "helping on the sandbags to keep the waters at bay.", "Rachel Carson", "Juno", "Toni Morrison", "Copts"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6967566287878788}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.4, 0.9090909090909091, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-2678", "mrqa_triviaqa-validation-4208", "mrqa_triviaqa-validation-825", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6308", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8858", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-3410", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-2435"], "SR": 0.59375, "CSR": 0.526201923076923, "EFR": 1.0, "Overall": 0.7092247596153846}, {"timecode": 65, "before_eval_results": {"predictions": ["amanda barrie", "soundgarden", "anser", "Tina Turner", "The Four Seasons", "barry Taylor", "1986", "port", "commercial", "Manson", "Uganda", "Ash", "temperature", "cartoonist", "Alaska", "iron", "william drogba", "c\u00e9vennes", "bagram", "Mnemosyne", "Brighton", "The Pillow Book", "arthur", "Iran", "jennifer henson", "smack", "Massachusetts", "a Nor'easter", "co-writer", "The Apprentice", "The Grateful Dead", "jennifer luigi", "brixham", "billy ray cyrus", "Ghana", "alla capella", "Nelson Mandela", "luigi", "Illinois", "sailing", "hat", "a mark", "Oman", "CBS", "noah beery, Jr.", "Northwestern University", "Sarajevo", "cast", "Beyonce", "jules Bernadotte", "MI5", "Games played", "the only way to go forward is to just keep living her life", "certiorari", "\"Little Dixie\" area", "American Way", "Al-Ghazali", "fluoroquinolone", "Alina Cho", "eight.\"", "Batman", "Passover", "Johann Strauss II", "John Lennon and George Harrison,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6170572916666667}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.125, 0.5, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-7165", "mrqa_triviaqa-validation-3599", "mrqa_triviaqa-validation-2184", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-5878", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3265", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-7950", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-1804", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-14825"], "SR": 0.546875, "CSR": 0.5265151515151515, "EFR": 0.9655172413793104, "Overall": 0.7023908535788924}, {"timecode": 66, "before_eval_results": {"predictions": ["sleepless in seattle", "Margaret Beckett", "Portugal", "typhoid fever", "Sheryl Crow", "Melvil Dewey", "Jamaica", "Pancho Villa", "the Wild Bunch", "mikhail gorbachev", "South Korea", "Harry Potter and the Half Blood Prince", "tiptoe through the Tulips", "eriertel (Karmeliter Quarter)", "charlesley", "Vengeance", "c\u00e9vennes", "les invalides", "1861", "a snowman", "w", "Dvorak", "a prophet", "aslan", "26", "the narwhal", "turkey bone", "photography", "Charlie Chan", "taekwondo", "phosphorus", "Hogwarts School of Witchcraft and Wizardry", "plutonium", "Mercury", "a tank", "Dorset", "J. S. Bach", "Groucho Marx", "lacrosse", "Queen Elizabeth I", "alberto laglin", "1804", "Belarus", "clydebank", "the Mad Hatter", "alisabeth bner", "heating device", "Chrysler", "closest approach to the original sound", "Burkina Faso", "london", "2010", "the National Football League ( NFL )", "percentage of ethanol in the blood in units of mass of alcohol per volume of blood", "the FAI Junior Cup", "the Knight Company", "north Miami-Dade County", "military veterans", "January 24, 2006.", "after giving birth to baby daughter Jada,", "two-parent family", "Ovid", "a serious or other-than", "Latin"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5677083333333334}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-2670", "mrqa_triviaqa-validation-458", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-999", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-4523", "mrqa_hotpotqa-validation-1030", "mrqa_newsqa-validation-2498", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-8081"], "SR": 0.546875, "CSR": 0.5268190298507462, "EFR": 1.0, "Overall": 0.7093481809701493}, {"timecode": 67, "before_eval_results": {"predictions": ["anunnaki", "Gandhi", "Battlestar Galactica", "sperm whale", "South Africa", "Brett Favre", "Peril", "Texas", "angioplasty", "anxiety", "a phaser", "Mary Pickford", "Rivera", "Sayonara", "a cat", "India", "the Lone Ranger", "Mars", "the Battle of Verdun", "a single death", "Andes", "India", "bbc", "Houston Rockets", "sirloin", "apartheid", "Boston", "horse", "Van Helsing", "\"elbow\"", "pesos", "shopping", "Captain William Bligh", "Urban Outfitters", "burt baskin", "Andrew Wyeth", "smallpox", "jimmy", "al jolson", "Risk", "recessive", "Don Quixote", "Richmond", "midnight", "The Age of Innocence", "Students for a Democratic Society", "the Bering Sea", "the Caucasus", "Coretta Scott", "tritonic", "bbc", "the Brewster family", "After World War II", "abbreviation", "Brighton", "Microsoft", "fleet street", "1966", "Juilliard School", "City of Starachowice", "success as a recording artist", "Bainbridge, Kenya,", "United Front for Democracy Against Dictatorship", "peninsulas"], "metric_results": {"EM": 0.625, "QA-F1": 0.6739583333333332}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-3288", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-7736", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4010", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-5960", "mrqa_searchqa-validation-5340", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-4308", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2674", "mrqa_naturalquestions-validation-2064"], "SR": 0.625, "CSR": 0.5282628676470589, "EFR": 0.9583333333333334, "Overall": 0.7013036151960785}, {"timecode": 68, "before_eval_results": {"predictions": ["Davenport", "a word", "Ulysses S. Grant", "gonads", "the High Plains", "the gap", "Saturday Night Fever", "Kansas", "a motor neuron", "John Fogerty", "a megawatt", "king", "electron", "the Communist Party", "Alfred Binet", "Bath", "the Indian Sub-continent", "the Billboard", "James Buchanan", "Hinduism", "the Miasa French Countryside Ruby Stemware", "2003", "Leon Trotsky", "Arkansas", "chromosomes", "Cuba", "The Computer Age", "airplanes", "medical schools", "Thomas Nast", "3", "freezing", "Picabo Street", "the National Security Agency", "Kiss Me Kate", "Honey Nut Cheerios", "Hercules", "5", "Rod Laver", "oatmeal", "Celia Humphrey", "Selma", "vote", "Herbert Hoover", "Joe Hill", "IHOP", "The Lottery by Shirley Jackson", "Lou Gehrig's disease", "Samuel Beckett", "William Pitt the Younger", "Independence Day", "Daya Jethalal Gada", "Kevin Sumlin", "Charles Carson", "Albert Einstein", "Orion", "the dodo", "1986", "James Gandolfini", "Mandarin Airlines", "at least two and a half hours.", "88-year-old", "Buenos Aires.", "Citizens for a Sound Economy"], "metric_results": {"EM": 0.625, "QA-F1": 0.6666666666666667}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-30", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-15032", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-5152", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-6721", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-2684", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_newsqa-validation-2945"], "SR": 0.625, "CSR": 0.5296648550724637, "EFR": 1.0, "Overall": 0.7099173460144927}, {"timecode": 69, "before_eval_results": {"predictions": ["Manitoba", "Santa Monica, California", "60 Minutes", "a fruitcake", "After Midnight", "Socrates", "Al Gore", "the Louvre", "(Henry) II", "the Desert Fox", "Baton Rouge", "Langston Hughes", "a manacle", "Wisconsin", "Cleveland", "slave", "Toronto", "Stevie Wonder", "Typhoid Mary", "an inch", "a payment plan", "Tokyo", "Tennessee Williams", "the United Arab Emirates", "Lurch", "the Chancellor", "Ben", "Old Yeller", "(John) Wesley", "2001: A Space Odyssey", "A Brief History of Time", "Prince", "Abraham", "Duncan", "Israel", "Redcliffe", "(Robert) Ludlum", "Flagellation", "the largest lakes and rivers", "Iowa", "Mephistopheles", "Samsonite", "the Marine Corps", "Vietnam", "Switzerland", "Punxsutawney, Pennsylvania", "Sports Illustrated", "Venus", "Shia LaBeouf", "Annapolis", "Princess Anne", "Hungary", "January 15, 2007", "Introverted Sensing ( Si )", "Aethelbert", "Trainspotting", "Stella McCartney", "1946", "Anne with an E on Netflix", "Daniil Shafran", "a Columbian mammoth", "Pixar", "cope in prison.", "then-Sen. Obama"], "metric_results": {"EM": 0.625, "QA-F1": 0.6642992424242424}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.18181818181818182, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6311", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-14167", "mrqa_searchqa-validation-3164", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-4905", "mrqa_searchqa-validation-13697", "mrqa_searchqa-validation-13570", "mrqa_searchqa-validation-6016", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-7011", "mrqa_searchqa-validation-6746", "mrqa_triviaqa-validation-6872", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-3564", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3695"], "SR": 0.625, "CSR": 0.5310267857142856, "EFR": 1.0, "Overall": 0.7101897321428571}, {"timecode": 70, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4260", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5757", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1840", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10156", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-1971", "mrqa_squad-validation-2082", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3215", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4299", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-513", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6067", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6917", "mrqa_squad-validation-6960", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9401", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.849609375, "KG": 0.53203125, "before_eval_results": {"predictions": ["Doc Holliday", "Frito-Lay", "the pale", "fowls", "a fruitcake", "Carrie Underwood", "the AI Behind Watson", "Christa McAuliffe", "Kilimanjaro", "Misbegotten", "a pumpkin", "Jumbo", "Stoke", "Jalisco", "Adolf Hitler", "Portland", "the imagist movement", "Diesel", "a palace", "a brig", "the Spanish Republic", "Ruth", "sugar", "Cheaper by the Dozen", "Hans Christian Andersen", "San Francisco", "(John) Cale", "Wanted", "a trumpet", "BORE", "Emma Peel", "the Homestead Act", "Ellis Island", "Max Factor", "a plantain", "Marie Curie", "Middle German", "Aladdin", "rain", "the Lion", "Toy Story", "the Tea Party", "George Sand", "Jim Jarmusch", "Afghanistan", "Minos", "salamanders", "Charles Van Halen", "Rosehips", "Led Zeppelin", "Nice", "Michigan", "Texas A&M Aggies", "Alex Burrall, Jason Weaver and Wylie Draper", "Leroy", "The Kentucky Derby", "Iwo Jima", "Charles L. Clifford", "Vince Staples", "Sam Bettley", "Al-Shabaab", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Lucky Dube,", "Florida"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7176674836601307}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.7000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-4747", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-8746", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-13954", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-6077", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-8455", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-1531", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-585"], "SR": 0.65625, "CSR": 0.5327904929577465, "EFR": 1.0, "Overall": 0.7324955985915492}, {"timecode": 71, "before_eval_results": {"predictions": ["The Beatles", "The Andy Griffith Show", "inconceivable", "The Anti-Lebanon Range", "trip", "watermelon", "a kart", "tanks", "Simon & Garfunkel", "(Alberto) Pujols", "the Andean bear", "Ordinal", "nebulae", "(George) W. Bush", "Eastwick", "The Who", "Cy Young", "Austin Powers", "a mime instructor", "conga drums", "a fern", "Nellie Bly", "IBM", "Pizza", "Athol Fugard", "Ricky Martin", "$100", "cloven", "Nicole Kidman", "Aristophanes", "Wimbledon", "Siddhartha Gautama", "a RESTRICTIVE", "a monkey", "a turquoise necklace", "Papua New Guinea", "Rooster Cogburn", "Halo 3", "Moses", "Sayonara", "A mechanoreceptor", "the Crescent", "The Moment of Truth", "anardvark", "Harpy", "Henry Fielding", "(James A. Garfield)", "a waterfowl taxidermy", "Howie Mandel", "China", "Henry Cavendish", "six degrees of freedom", "MFSK", "May 31, 2012", "Syria", "Ida Noddack", "(Bokm\u00e5l) or  (Nynorsk)", "World War II", "Peter Kay's Car Share", "Oneida Limited", "Zac Efron", "punishment", "brutal brutality", "solid rock layer"], "metric_results": {"EM": 0.625, "QA-F1": 0.6677083333333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.16666666666666669, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-11039", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-2234", "mrqa_searchqa-validation-10201", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-15752", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-1105", "mrqa_searchqa-validation-14945", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-5808", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-2382", "mrqa_triviaqa-validation-4231"], "SR": 0.625, "CSR": 0.5340711805555556, "EFR": 1.0, "Overall": 0.7327517361111111}, {"timecode": 72, "before_eval_results": {"predictions": ["133", "the United States", "Mediterranean", "the chaos and horrified reactions after the July 7, 2005, London transit bombings", "$2 billion", "\"Slumdog Millionaire\"", "17", "London's O2 arena,", "Joel \"Taz\" DiGregorio,", "second child", "in her home", "a fake license,", "U.S. 93", "a tracheotomy,", "64,", "Muslim festival", "figure out a way that was practical to get a drum set on a plane.", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "a satellite", "The leader of the African National Congress", "cross-country skiers", "Lisa Brown", "Saturn Sky", "the most important attacks on the church don't come from the sins of the members of the church,", "saying privately in 2008 that Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "Haeftling,", "severe flooding", "Acura", "Now Zad in Helmand province, Afghanistan.", "a clear strategy", "second", "through the weekend,", "Sen. Barack Obama", "African National Congress Deputy President", "the abduction of minors.", "three", "Former Argentina international defender Fernando Caceres", "Piers Morgan,", "the L'Aquila earthquake,", "David Russ,", "country directors", "on vacation", "\"We must eliminate the perceived stigma, shame and dishonor of asking for help,\"", "when the economy turns unfriendly,", "AC Milan", "BMW 3-Series", "Afghanistan,", "the bombers", "Mom", "Asashoryu,", "great king", "SIP ( Session Initiation Protocol )", "Ant & Dec", "Robert A. Heinlein", "a cheetah", "friends", "IT", "St. Louis Cardinals", "south-west", "popcorn", "6", "a white elephant", "Viscount Cranborne"], "metric_results": {"EM": 0.5, "QA-F1": 0.5665078411172162}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.0, 1.0, 1.0, 0.125, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-3044", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-1128", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-6294", "mrqa_triviaqa-validation-179", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-2653", "mrqa_searchqa-validation-8732"], "SR": 0.5, "CSR": 0.5336044520547945, "EFR": 1.0, "Overall": 0.7326583904109588}, {"timecode": 73, "before_eval_results": {"predictions": ["$40 and a loaf of bread.\"", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "\"Vaughn,\" which is what co-workers called him,", "Israel's Defense Secretary Ehud Barak,", "11,", "President Kennedy came down to Hoboken in '61 and I was only 10 years old.", "average of 25 percent", "April 2", "President Robert Mugabe intends to rig next week's elections in his favor,", "\"Buying a Prius shows the world that you love the environment and hate using fuel,\"", "OneLegacy,", "upper respiratory infection,\"", "10:30 p.m. October 3,", "Arabic, French and English,", "The United Nations is calling on NATO to do more to stop the Afghan opium trade", "\"Maude\" and the sardonic Dorothy on \"The Golden Girls,\"", "Bill,", "A man, one mission -- rescuing mutts", "Jewish", "a controversial theory about Mary Magdalene and Jesus.", "J.G. Ballard,", "protective shoes", "17", "2008.", "12 hours", "Now Zad in Helmand province, Afghanistan.", "Russian concerns that the defensive shield could be used for offensive aims.", "American", "Los Alamitos Joint Forces Training Base", "Al-Shabaab,", "safer surroundings.", "$60 billion on America's infrastructure.", "upper respiratory infection,\"", "test scores and graduation rates are improving faster in other nations than in the United States and this threatens our quality of life in a competitive world.", "Sri Lanka", "\"CNN Heroes: An All-Star Tribute\"", "1,073 immigration detainees", "nearly $162 billion in war funding", "scientific reasons.", "innovative, exciting skyscrapers", "concentration camps,", "Pakistani territory", "hanged in 1979 for the murder of a political opponent two years after he was ousted as prime minister in a military coup.", "Pew Research Center", "a member of the self-styled revolutionary Symbionese Liberation Army --", "38,", "\"stressed and tired force\"", "speed attempts", "Leo Frank,", "At least 38", "Israel", "Port Said to the southern terminus of Port Tewfik at the city of Suez", "Greek", "September 2017", "Dr John Sentamu", "Tony Cozier", "Crosspool", "Dulwich", "New York Giants", "South America", "Vulcan", "Beloved", "a flapjack", "Hungarian"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6139730476347325}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.07142857142857142, 0.0, 0.8, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.2608695652173913, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.28571428571428575, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-3641", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1268", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-10147", "mrqa_triviaqa-validation-6111", "mrqa_triviaqa-validation-666", "mrqa_searchqa-validation-1383"], "SR": 0.53125, "CSR": 0.5335726351351351, "EFR": 1.0, "Overall": 0.732652027027027}, {"timecode": 74, "before_eval_results": {"predictions": ["South Korea's newest plant claims it is designed to go green.", "1998 Arnold and husband Bill Klein,", "16", "the parents' full consent.", "five", "Marie-Therese Walter.", "Sixteen", "1979", "promised federal help for those affected by the fires.", "consumer confidence", "$81,8709", "Climatecare,", "a Florida girl", "state senators", "August 19, 2007.", "Ghana in Egypt.", "6,000", "Alan Kardec scored a spectacular second-half winner for Brazil against Costa Rica to set up a replay of the 1993 Under-20 World Cup final against Ghana in Egypt.", "Brian Smith.", "fight against terror will respect America's values.", "security as needed.\"", "Sunday,", "city of romance, of incredible architecture and history.", "1,500 Marines", "Atriss Deby hopes the journalists and the flight crew will be freed,", "Bill Haas", "Silicon Valley.", "people switched from the very bad category to the pretty bad category,", "at least nine people dead.", "British", "Cash for Clunkers is saving jobs up and down the auto supply chain: from dealers to assembly workers and parts markers.", "Yemen.", "the Dalai Lama", "800,000", "carving a pumpkin", "Russia", "one", "Democratic VP candidate", "Former U.S. soldier Steven Green", "Cologne, Germany,", "President George H.W. Bush", "Kurdistan Workers' Party,", "6-4", "Karen Floyd", "St Petersburg and Moscow,", "flights", "1-1 draw", "South African", "Alaska or Hawaii.", "30", "repeal of the military's \"don't ask, don't tell\" policy", "Dollree Mapp", "Matthias Schleiden and Theodor Schwann", "the American Civil War", "giraffe", "hiking and climbing", "World Bank", "Idaho", "Robert Matthew Hurley", "Ashanti Region", "Michael Kors", "Katherine Heigl", "Lenin", "Australia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.618497264119572}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.7058823529411764, 1.0, 1.0, 0.0, 0.0, 0.4, 0.2608695652173913, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.3333333333333333, 0.2222222222222222, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3773", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-1111", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-258", "mrqa_naturalquestions-validation-3705", "mrqa_triviaqa-validation-280", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-5300", "mrqa_triviaqa-validation-6077"], "SR": 0.515625, "CSR": 0.5333333333333333, "EFR": 1.0, "Overall": 0.7326041666666667}, {"timecode": 75, "before_eval_results": {"predictions": ["Bear Grylls", "vereinsthaler", "Adam Faith", "Elbe", "New South Wales", "Donald Sutherland", "Stockholm", "Leo", "Henry VIII", "a power outage", "Scott Glenn", "Pluto", "Deep Blue", "yellow-brown", "1996", "anchovy", "the duke of Monmouth\u2019s rebellion", "Isaac", "New Israel Shekel", "serbia", "The Lost Weekend", "Althorp", "the conquest of Peru", "June", "Persuasion", "Robert Taylor", "the AllStars", "Roman legions", "Norman Mailer", "fishes", "floating", "Florence", "hallmarks", "pascal", "\"gruppetto\"", "henry", "11", "Israel", "football", "the Porteous Riots", "English", "Gentlemen Prefer Blondes", "puppies", "Kenya", "Conrad Murray", "Shuttle Launch", "elia Earhart", "Spinach", "John Gorman", "Benedict XVI", "cirrocumulus", "Ariana Clarice Richards", "Turner Layton", "The Parlement de Bretagne", "Campbellsville University", "Daniel Craig", "Vanilla Air Inc.", "HPV", "\"This is not something that anybody can reasonably anticipate,\"", "21 percent suggesting that", "bottle", "Kansas", "Parkinson's disease", "John Gotti"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6338541666666666}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5929", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-771", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-5024", "mrqa_triviaqa-validation-5132", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-1221", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-3591", "mrqa_naturalquestions-validation-7021", "mrqa_hotpotqa-validation-684", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-10356"], "SR": 0.59375, "CSR": 0.5341282894736843, "EFR": 1.0, "Overall": 0.7327631578947369}, {"timecode": 76, "before_eval_results": {"predictions": ["Steve Jobs", "les Bleus", "Islamic militants", "Prince George's County Chief Executive Jack Johnson", "two women", "seven", "drug cartels", "suicide car bombing", "Illinois Reform Commission", "vice-chairman of Hussein's Revolutionary Command Council.", "about 4 meters (13 feet) high", "The recent violence -- which has included attacks on pipelines and hostage-taking --", "183", "80,", "her landlord", "Bob Bogle,", "back at work,\"", "Mexico", "Rev. Alberto Cutie", "California, Texas and Florida,", "3-3", "Iran of trying to build nuclear bombs,", "deployment of 30,000 additional U.S. troops to Afghanistan", "dancing", "Bridgestone Invitational", "Jenny Sanford,", "\"It was a wrong thing to say, something that we both acknowledge,\"", "Sheikh Sharif Sheikh Ahmed", "8,", "roadworthy.\"", "bipartisan rhetoric", "Haiti", "Mashhad", "10 percent", "Buenos Aires.", "Coast Guard", "Dennis Davern, the captain of yacht owned by Wood and her then-husband, actor Robert Wagner.", "oaxacan countryside of southern Mexico", "american third seed", "Aravane Rezai", "destroyed and his business is shattered, no one in his family died in the quake.", "\"the most dangerous precedent in this country, violating all of our due process rights.\"", "15,000", "the wife of Gov. Mark Sanford,", "on vacation", "The father of Haleigh Cummings,", "\"I am sick of life -- what can I say to you?\"", "E. coli", "southwestern Mexico,", "1,500", "vehicle identification numbers", "Jyoti Basu", "60 by West All - Stars", "Homer Banks, Carl Hampton and Raymond Jackson", "l Leeds", "edward ii", "seattle", "strings", "Dante", "1958", "(William Tecumseh Sherman", "(George) Bush", "air pressure", "eogyrinus"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5933620777888281}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 0.625, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.7272727272727272, 0.0, 0.0, 0.08, 0.08695652173913043, 0.5, 0.28571428571428575, 1.0, 0.3333333333333333, 0.3076923076923077, 1.0, 0.8, 1.0, 0.35294117647058826, 0.5, 0.33333333333333337, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-428", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2821", "mrqa_newsqa-validation-2362", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-4796", "mrqa_triviaqa-validation-1256", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12792", "mrqa_triviaqa-validation-1454"], "SR": 0.453125, "CSR": 0.5330762987012987, "EFR": 0.9714285714285714, "Overall": 0.7268384740259741}, {"timecode": 77, "before_eval_results": {"predictions": ["Bill Haas", "eight surgeons", "more than 200.", "Stephen Worgu", "American Civil Liberties Union", "a Florida girl", "\"The Cycle of Life,\"", "Ralph Lauren", "Charlie Chaplin", "behind the operator's station.", "Columbia", "four", "Jenny Sanford,", "London and Buenos Aires", "hundreds", "more than 78,000 parents", "St. Francis De Sales Catholic", "Hanin Zoabi,", "Buddhism", "in Seoul,", "President Obama", "Kim Jong Il", "strangulation and asphyxiation and had two broken bones in his neck,", "John and Elizabeth Calvert", "school funding", "terrorize", "Latvia,", "Michael Jackson", "Brazil", "Pixar's", "Pixar's", "1,500", "defeated Woods at the Bridgestone Invitational in Ohio in August.", "two United Arab Emirates based companies", "fascinating transformation that takes place when carving a pumpkin.", "best-of-three series.", "Transport Workers Union leaders", "Amanda Knox's aunt", "in the Muslim north of Sudan", "Melbourne", "cancer", "Al Shabaab may have been plotting an attack timed to coincide with the inauguration.", "the ship", "338", "finance", "Haeftling,", "bribing other wrestlers to lose bouts,", "in terms of the country's most-wanted list, Mejia Munera was one of Colombia's most sought-after criminals and ranked just below the leaders of Revolutionary Armed Forces of Colombia,", "outfit from designer", "350 U.S. soldiers", "\"Taz\" DiGregorio,", "active absorption", "Robin", "17 -- 15", "three Worlds", "the Sulu Sea", "british", "The satirical", "Valhalla Highlands Historic District", "boxer", "push", "rain", "Wynton Marsalis", "argon"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5524718915343916}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.42857142857142855, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.07407407407407407, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2633", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-394", "mrqa_triviaqa-validation-699", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12492", "mrqa_searchqa-validation-4358"], "SR": 0.453125, "CSR": 0.532051282051282, "EFR": 1.0, "Overall": 0.7323477564102564}, {"timecode": 78, "before_eval_results": {"predictions": ["3-2", "Aung San Suu Kyi", "1957", "Fort Bragg in North Carolina.", "emergency plans", "South Africa", "Al-Shabaab", "it", "assassination of President Mohamed Anwar al-Sadat", "Diego Milito's", "331", "Robert Barnett,", "More than 22 million people in sub-Saharan Africa", "the driver", "Asia and India.", "did not", "200 human bodies at various life stages -- from conception to old age, including embryos and fetuses taken from historic anatomical collections.", "Phillip A. Myers.", "Congress", "bipartisan", "a rally", "1912.", "Steven Green", "The plane, an Airbus A320-214,", "Casey Anthony,", "Lifeway's 100-plus stores nationwide", "way American men and women dress", "some deaths", "park bench facing Lake Washington", "The singer's personal security guard, Andrew Morris,", "Siri", "\"The Real Housewives of Atlanta\"", "an independent homeland", "Air traffic delays began to clear up Tuesday evening after computer problems left travelers across the United States waiting in airports,", "Chicago, Illlinois.", "Barbara Streisand's", "the death of a pregnant soldier", "raping", "five", "Takashi Saito,", "Bob Johnson", "$41.1 million", "the sanctuary is now the largest natural refuge of its kind in the United States.", "emergency aid", "Joe Jackson,", "red varnished cover with the word \"Album\" inscribed on it in gold lettering,", "the inspector-general", "anti-Israeli sentiment in Egypt in the past few months", "Demjanjuk", "following in Arizona's footsteps would take states in the wrong direction.", "Fourteen thoroughbred horses", "the Iraq War", "180th meridian or antimeridian", "instructions", "The Undertones", "Henry Mancini", "Fenn Street School", "acting", "the Vietnam War", "1967", "Ukraine", "capitals", "Francis Steegmuller", "genome"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5819493006993006}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0909090909090909, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-2947", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-584", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-4123", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-9852", "mrqa_hotpotqa-validation-5128", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-10570"], "SR": 0.46875, "CSR": 0.53125, "EFR": 0.9705882352941176, "Overall": 0.7263051470588235}, {"timecode": 79, "before_eval_results": {"predictions": ["Dick Rutan and Jeana Yeager", "20 years from the filing date subject to the payment of maintenance fees", "The eighth and", "Rigg", "March 2016", "brilliant celebrity with praise", "the name announcement of Kylie Jenner's first child", "2003", "orange collection boxes", "seven", "February 16, 2018", "Coroebus of Elis", "Jonathan Breck", "George Harrison", "Howard Ellsworth Rollins", "Necator americanus", "January 17, 1899", "eight hours ( UTC \u2212 08 : 00 )", "Thespis ( / \u02c8\u03b8\u025bsp\u026as / ; Greek : \u0398\u03ad\u03c3\u03c0\u03b9\u03c2 ; fl. 6th century BC )", "Eukarya", "Wednesday, September 21, 2016", "Robber baron", "three", "Tbilisi, Georgia", "off the rez", "1895", "King Harold Godwinson", "Sir Ronald Ross", "May 29, 2018", "1979", "In the television series's fourth season", "ancient Athens", "Russia", "the past record of geomagnetic reversals was noticed by observing the magnetic stripe `` anomalies '' on the ocean floor", "United Nations", "2018", "it culminates in a post as a Consultant, a General Practitioner ( GP )", "Real Madrid", "Angel Benitez", "capillary action", "1830", "Louis Mountbatten", "foreign investors", "Bhupendranath Dutt", "In June 22, 1942", "775 rooms", "July 2010", "senators", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Paul Lynde", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "a treaty of Waitangi", "James Garfield", "Burkina Faso", "England", "Sverdlovsk", "47,818,", "seven", "and Ghana in 1974.", "to close their shops during daily prayers, or they will be temporarily shut down,", "Versailles", "the University of Deseret", "(Bob) Dylan", "Mount Pelee"], "metric_results": {"EM": 0.53125, "QA-F1": 0.721364088157257}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false], "QA-F1": [0.5714285714285715, 0.5882352941176471, 0.5, 1.0, 1.0, 0.47058823529411764, 0.7999999999999999, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.6, 0.2222222222222222, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 0.5, 1.0, 0.5714285714285715, 1.0, 1.0, 0.5806451612903226, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-7848", "mrqa_triviaqa-validation-5704", "mrqa_triviaqa-validation-5769", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-18", "mrqa_searchqa-validation-13149", "mrqa_searchqa-validation-13773"], "SR": 0.53125, "CSR": 0.53125, "EFR": 0.8666666666666667, "Overall": 0.7055208333333333}, {"timecode": 80, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11088", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.8046875, "KG": 0.48984375, "before_eval_results": {"predictions": ["the 13th century", "14 December 1972 UTC", "China", "1919", "Abraham Gottlob Werner", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "September, 2016", "July 2014", "ase", "the lower the normal boiling point of the liquid", "Skat", "Claudia Grace Wells", "Bill Russell", "the British East India company to sell tea from China in American colonies without paying any taxes", "the Reverse - Flash", "in cell - mediated, cytotoxic innate immunity )", "Christopher Lloyd", "four", "1923", "San Francisco, California", "2018", "Lilian Bellamy", "Denver Broncos", "Zoe Badwi, Jade Thirlwall's cousin", "1997", "William Wyler", "Ray Harroun", "one person", "NIRA", "Las Vegas, New Mexico, USA", "Kaley Christine Cuoco", "Marshall Sahlins", "Cozonac", "the United Kingdom", "Gupta Empire", "the two - dimensional perspective projections ( or drawings ) of mutually parallel lines in three - dimensional space appear to converge", "James Madison", "Matt Monro", "Coton in the Elms", "Angola", "June 1992", "prokaryotic", "2014", "the Italian / Venetian John Cabot ( c. 1450 -- c. 1500 )", "the May Revolution of 1810", "Jason Flemyng", "blue", "an expression of unknown origin", "New Zealand to New Guinea", "1963", "Kaiser Chiefs", "Sarah Sawyer", "Claire Goose", "American Educ, author, businessman, and keynote speaker", "science fiction drama", "Earvin \"Magic\" Johnson Jr.", "President Sheikh Sharif Sheikh Ahmed", "snakes -- and one snake in particular.", "posting a $1,725 bail,", "Yitzhak Rabin", "Epidural Steroid Injection", "1936", "carbon"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6733865914786967}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5714285714285715, 0.3157894736842105, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-5789", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-1713", "mrqa_searchqa-validation-11202"], "SR": 0.609375, "CSR": 0.5322145061728395, "EFR": 1.0, "Overall": 0.7075366512345679}, {"timecode": 81, "before_eval_results": {"predictions": ["William the Conqueror", "1963", "Sachin Tendulkar", "The President of Zambia", "Ray Charles", "in Ephesus in AD 95 -- 110", "HTML", "in the 1970s", "Audra McDonald", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "North Dakota", "Tatsumi", "1969", "Liberia", "1937", "artes liberales", "Paul Lynde", "MSC Crociere S. p.A.", "2020", "2007", "Hodel", "the coffee shop Monk's", "Siddharth Arora / Vibhav Roy", "1991", "1900", "between 2 World Trade Center and 3 World Trade center", "Munich, Bavaria", "Steve Russell", "October 2, 2017", "Paul Lynde", "to regulate the employment and working conditions of civil servants", "Frank Oz", "Sam Waterston", "Sunday night", "Johnny Darrell", "the 7th century", "2018", "Will Turpin", "Ali Skovbye", "2013", "two", "Will Champion", "the class that teaches students defensive techniques to defend against the Dark Arts", "Pre-evaluation, strategic planning, operative planning and implementation", "Jackie Robinson", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "Brazil, Turkey and Uzbekistan", "the Lincoln Highway ran coast - to - coast from Times Square in New York City west to Lincoln Park in San Francisco", "two", "pia mater", "UN Charter", "the Kinks", "inishtrahull Island", "the Indus Valley", "the junction with Interstate 95", "Greek mythology", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "abusing its dominant position in the computer processing unit (CPU) market.", "American Lindsey Vonn", "5.5 million", "Ptolemy", "Dixie", "San Salvador", "Snickers bar"], "metric_results": {"EM": 0.5, "QA-F1": 0.6670314932544347}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.967741935483871, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9523809523809523, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6428571428571429, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 1.0, 0.7999999999999999, 0.4, 0.4615384615384615, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4210526315789474, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-1642", "mrqa_hotpotqa-validation-1861", "mrqa_newsqa-validation-3916", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3156", "mrqa_triviaqa-validation-7778"], "SR": 0.5, "CSR": 0.5318216463414633, "EFR": 0.9375, "Overall": 0.6949580792682927}, {"timecode": 82, "before_eval_results": {"predictions": ["the Four Seasons", "Kathleen Erin Walsh", "in 1603", "Dean Kelly", "in 2010", "Tulsa, Oklahoma", "beta decay", "ZZ Top", "1998", "John Adams", "Ford", "Grisha Alekandrovich Nikolaev", "Ed Furlong", "engraver", "July 2, 1928", "Dottie West", "in northern China", "Blood is the New Black", "1977", "arm", "Professor Eobard Thawne", "Jack Nicklaus", "arm regions", "St. John's, Newfoundland and Labrador", "by the early - to - mid fourth century", "a young husband and wife", "May 29, 2018", "Church of England", "September 27, 2017", "April 10, 2018", "Daniel Suarez", "six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "January 15, 2007", "the Soviet Union", "the optic chiasma", "Empire of Japan", "Bangladesh -- India border", "Germany", "Jenny Slate", "( 1 ) 2013", "Prince William, Duke of Cambridge", "Nicklaus", "in 1904", "Peter Greene", "Matt Monro", "warmth", "ATPase couples ATP synthesis during cellular respiration to an electrochemical gradient created by the difference in proton ( H ) concentration across the mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Gina Tognoni / to\u028an\u02c8jo\u028ani / ( born November 28, 1973 )", "energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments", "in Rome", "September 30", "Charles Darwin", "David Letterman", "Fiat", "five months", "Wanda", "April 24, 1934", "40", "55-year-old", "$2 billion", "Sex and the City", "coffee", "(Un regalo que te dio la vida)", "Oldham, in Greater Manchester, England"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6844538193248932}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.0, 0.5, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727272, 0.9090909090909091, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.25806451612903225, 0.4444444444444445, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9687", "mrqa_triviaqa-validation-7718", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2444", "mrqa_searchqa-validation-12962", "mrqa_triviaqa-validation-6822"], "SR": 0.484375, "CSR": 0.53125, "EFR": 0.9090909090909091, "Overall": 0.6891619318181819}, {"timecode": 83, "before_eval_results": {"predictions": ["American 3D computer-animated comedy film", "1964", "Hindi", "eight", "Flula Borg", "Comanche County, Oklahoma", "Swiss", "1999", "Congo River", "4,613", "George Washington Bridge", "Rose Mary Woods", "Guardians of the Galaxy Vol.  2", "2.1 million", "6'5\" and 190 pounds", "526 people per square mile", "North West England", "casting, job opportunities, and career advice", "Saint Elgiva", "Scotland", "2006", "Carrefour", "infinite sum of terms", "Daphnis et Chlo\u00e9", "Wayne Mark Rooney", "second half of the third season", "Johnson & Johnson", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "Blue Ridge Parkway", "The Backstreet Boys", "45th", "Foxborough, Massachusetts", "Wichita", "animation", "Margaret Thatcher", "sandstone", "Reginald Engelbach", "Dutch", "BC Dz\u016bkija", "Anne", "James Fox", "Rebirth", "\"Big Mamie\"", "Albert Bridge", "Betty Cohen", "Prince Alexander of Denmark", "King of France", "Alemannic", "Larry Alphonso Johnson Jr.", "Marlon St\u00f6ckinger", "Taylor Swift", "June 5, 2017", "very important", "the Tigris and Euphrates rivers", "Mozambique Channel", "Astor family", "Tashkent", "Pakistan's intelligence agency", "Goa", "a hunter", "Speed Racer", "Yogi Bear", "sakura", "fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid, which are then absorbed by the host"], "metric_results": {"EM": 0.625, "QA-F1": 0.7342294538429407}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9777777777777777]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-245", "mrqa_naturalquestions-validation-2680", "mrqa_triviaqa-validation-5311", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3680", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5496", "mrqa_naturalquestions-validation-7393"], "SR": 0.625, "CSR": 0.5323660714285714, "EFR": 0.9583333333333334, "Overall": 0.6992336309523809}, {"timecode": 84, "before_eval_results": {"predictions": ["As of January 17, 2018", "Atlanta", "1872", "1978", "Hodel", "Central Germany", "Kida", "the Outfield", "the seven stages of a man's life", "before title pages and titles", "Bill Irwin", "artes liberales", "the sinoatrial node", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "December 11, 2014", "A vanishing point", "Janie Crawford", "UTC \u2212 09 : 00", "mining", "Orange Juice", "blood plasma and lymph in the `` intravascular compartment ''", "nearly 92 %", "Missouri River", "OS X 10.0", "Jason Marsden", "The Chipettes", "Linda Davis", "Fats Waller", "Thespis", "Bill Belichick", "May 19, 2017", "season two", "about 375 miles ( 600 km ) south of Newfoundland", "the status line", "Samantha Jo `` Mandy '' Moore", "six doctors from Seattle Grace Mercy West Hospital", "Nodar Kumaritashvili", "The TLC - All That", "Welch, West Virginia", "Nicki Minaj", "Steve Russell", "Julia Roberts", "the church at Philippi", "control purposes", "to solve its problem of lack of food self - sufficiency", "Bed and breakfast", "The Lykan", "five", "Geothermal gradient", "soft contact lenses", "ringo Starr", "fire insurance", "Seoul, South Korea", "Tuesday", "Al Capone", "14 years", "ambassadors", "Russia", "the Dustbin", "the Equator", "Caesar", "There's no chance"], "metric_results": {"EM": 0.625, "QA-F1": 0.7380208333333333}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, false], "QA-F1": [0.8333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.7000000000000001, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-10122", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-1767", "mrqa_triviaqa-validation-6303", "mrqa_triviaqa-validation-5726", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-16205", "mrqa_newsqa-validation-2213"], "SR": 0.625, "CSR": 0.5334558823529412, "EFR": 0.9166666666666666, "Overall": 0.6911182598039216}, {"timecode": 85, "before_eval_results": {"predictions": ["the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "The uvea", "25 June 1932", "Felicity Huffman", "to control gene expression and mediate the replication of DNA during the cell cycle", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Roger Nichols and Paul Williams", "peninsular", "at least 28 vowel forms", "1957", "eleven separate regions of the Old and New World", "As of January 17, 2018, 201 episodes", "Tennessee Titans", "Monk's", "B.R. Ambedkar", "the superior meatus", "Cecil Lockhart", "Executive Residence of the White House Complex", "David Motl", "cuttlefish", "The sales area is primarily concentrated in the Southern United States, and has been sold as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "the New Croton Reservoir in Westchester and Putnam counties", "Professor Eobard Thawne", "Freddie Highmore", "the closing scene of the final episode of the first season", "1939", "74 languages", "during prenatal development", "Nebuchadnezzar", "A footling breech", "Robert Gillespie Adamson IV", "Waylon Jennings", "A rear - view mirror", "the fourth ventricle", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Doug Diemoz", "about $1.09 trillion", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "Frank Zappa", "1956", "Camping World Stadium in Orlando, Florida", "Louis XV", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "Jonathan Breck", "Jody Rosen", "The long - hair gene", "Wilt Chamberlain", "James Chadwick", "eretria", "Wyatt `` Dusty '' Chandler ( George Strait )", "Swiss", "trademark", "Las Vegas Boulevard, commonly referred to as the Las Vegas strip, or the strip,", "Hindi", "Brookhaven", "January 2004", "space shuttle Discovery", "dismissed all charges", "\"Empire of the Sun,\"", "frittata", "a crab", "a meter", "Matlock"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7115685478736948}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.35, 0.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6485", "mrqa_naturalquestions-validation-6066", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-5120", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-7683", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-1422", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-10904"], "SR": 0.640625, "CSR": 0.5347020348837209, "EFR": 0.9565217391304348, "Overall": 0.6993385048028311}, {"timecode": 86, "before_eval_results": {"predictions": ["Los Angeles", "a Polaroid picture", "Mikhail Baryshnikov", "filibustero", "Christo", "German songfacts", "Cyrus Hall McCormick", "the Ziegfeld Girl", "ballpoint pen", "a tabernacle", "a butterfly", "tequila", "the Chowan Indians", "Einstein", "American infantryman", "a roller-coaster", "George H.W. Bush", "Elvis Presley", "a flushing toilet", "pesticide chemical case 0097", "Gogol", "the Hudson River", "Banquo", "Virgo", "Texas", "a pardon", "an alligator", "General Mills", "the comb", "Magellan", "France", "white wine grapes", "\"The Drowsy Chaperone\"", "the death of King Duncan", "Greyhounds", "a circadian clock", "(John) Molson", "a lottery", "Al Lenhardt", "Tanzania", "Colorado", "Christopher Columbus", "the Caribbean", "Slovakia", "a polygon", "Frederic Remington", "The biography written by daughter Julie", "Evan Almighty", "going rate", "64", "a pillar of salt", "Johannes Gutenberg", "203", "4 January 2011", "Neighbours", "curb-roof", "Parsley the Lion", "Norway", "Big Machine Records", "The Campbell Soup Company", "Stoke City.", "Victor Mejia Munera", "in the heart of Los Angeles.", "Venezuela"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6375}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8333333333333333, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10854", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-4790", "mrqa_searchqa-validation-632", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-13536", "mrqa_searchqa-validation-5092", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6649", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-1516"], "SR": 0.5625, "CSR": 0.5350215517241379, "EFR": 1.0, "Overall": 0.7080980603448276}, {"timecode": 87, "before_eval_results": {"predictions": ["Shopgirl", "Easter Island", "True", "The Bridge on the River Kwai", "Motown Legends", "Virgin Atlantic", "duk", "Indira Gandhi", "pew", "Alfred Hitchcock", "the circulatory system", "the London Bridge", "Ho Chi Minh", "Mount Vesuvius", "Himalaya", "Kodachrome", "\"House M.D.\"", "Louis Armstrong", "Nepal", "A Prairie Home Companion", "Concord", "a bug", "the Thames", "Taipei", "Dame Nellie Melba", "the Caspian Sea", "Babe Ruth", "the Edo era", "Stromboli", "a pear", "chocolates", "laurel", "Chaucer's Prioress", "Japan", "Ben & Jerry", "a sweater", "Krakauer", "Ali", "Sweden", "John Glenn", "the Andes", "the ink-producing glands", "Mitt Romney", "Goofy", "Edsel II", "an inch", "Neptune", "Hawaii", "a ray", "Simon Cowell", "the Northern Mockingbird", "an enzyme known as ligase", "1898", "57 days", "Mr. Humphries", "James Mason", "Tinie Tempah", "Richard Arthur", "Ghana's", "Labour", "France's top court", "1,500", "South African President Thabo Mbeki,", "Louis XV"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6985119047619047}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10848", "mrqa_searchqa-validation-3533", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-7940", "mrqa_searchqa-validation-13410", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-9282", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-8024", "mrqa_searchqa-validation-2075", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-1736", "mrqa_searchqa-validation-6808", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-6890", "mrqa_searchqa-validation-6947", "mrqa_naturalquestions-validation-9670", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-301", "mrqa_newsqa-validation-3391"], "SR": 0.59375, "CSR": 0.5356889204545454, "EFR": 0.9615384615384616, "Overall": 0.7005392263986014}, {"timecode": 88, "before_eval_results": {"predictions": ["Edgar Allan Poe", "a fish", "(Arthur) Conan Doyle", "Grant Wood", "Probst", "All's Well That Ends Well", "the Black Sea", "Eggs Benedict", "lovebird", "Agatha Christie", "a church", "a biography", "the Mossad", "a backstroke", "Swahili", "a defibrillator", "Katrina", "an airbags", "a proscenium arch", "a yellowfin", "Pocahontas", "sinuses", "(James) Largo", "Jane Eyre", "Kandahr", "(Nolan) Ryan", "tofu", "Harrison", "clay", "the Jutland Peninsula", "(Al Alma) Thomas", "The Fourteen Points", "Misery", "(Cora) Munro", "a crossword clue", "the Osmonds", "a guitar", "Roberta Flack", "Belgium", "SHIELD", "Chicago", "an actuary", "a latkes", "Montana", "a dulcimer", "a ventriloquist dummy", "lead", "The Apocrypha", "a discus", "a beer", "Top Gun", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "H ions", "Nick Sager", "horse racing", "Uruguay", "Mauna Kea", "Deftones", "The Design Inference", "Dutch", "(Norra Grangesbergsgatan 4)", "3-0", "a bill in the Texas Legislature that would crack down on convicts caught with phones and allow prison systems to monitor and detect cell signals.", "Ronald Lyle \" Ron\" Goldman"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7042410714285714}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.9047619047619047, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-14174", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-13978", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-8423", "mrqa_searchqa-validation-10749", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-12604", "mrqa_searchqa-validation-2486", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-13957", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-2402", "mrqa_hotpotqa-validation-3258", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-692", "mrqa_hotpotqa-validation-2410"], "SR": 0.609375, "CSR": 0.5365168539325842, "EFR": 1.0, "Overall": 0.7083971207865167}, {"timecode": 89, "before_eval_results": {"predictions": ["Four bodies", "Pierre Mamboundou", "Kgalema Motlanthe,", "Rob Lehr, 26,", "that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.", "18th", "monarchy.", "throwing three punches but said only one connected.", "Dave Bego,", "near the equator,", "U.S. senators", "\"we have more work to do,\"", "a one-shot victory in the Bob Hope Classic", "that she had been lured from a club, forced into a men's bathroom at a university dormitory, bound and assaulted.", "Citizens", "death", "businessman", "his business dealings", "plastic surgery", "Meira Kumar", "The exact cause of IBS remains unknown,", "a nearby day care center whose children are predominantly African-American.", "The Mexican military", "The FBI's Baltimore field office", "the UK", "The Delta Queen", "Adidas,", "\"We must eliminate the perceived stigma, shame and dishonor of asking for help,\"", "Zac Efron", "he wants a \"happy ending\" to the case.", "about 1 percent", "4.6 million", "Rima Fakih", "the Airbus A330-200", "the Internet", "228", "London's O2 arena,", "The Georgia Aquarium", "London Heathrow's Terminal 5.", "4.5 pounds of heroin", "Bob Dole,", "near the grave.", "Muqtada al-Sadr,", "Leaders of more than 30 Latin American and Caribbean nations", "At least 38", "two years", "heavy turbulence", "alongside", "Florida", "tax", "Jose Miguel Vivanco,", "The genome", "2010", "Jurchen Aisin Gioro clan in Manchuria", "the Volga-Don canal", "William WymarkJacobs", "france", "wine", "Tsavo East National Park", "\"Lucky\"", "bananas", "Ron Markstein", "Midas", "Home Alone"], "metric_results": {"EM": 0.5, "QA-F1": 0.5659602759548881}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 0.5454545454545454, 0.0, 0.0, 1.0, 1.0, 1.0, 0.41379310344827586, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.0, 0.0, 1.0, 0.125, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-1036", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-7311", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-1968", "mrqa_triviaqa-validation-6077", "mrqa_hotpotqa-validation-3474", "mrqa_searchqa-validation-15665"], "SR": 0.5, "CSR": 0.5361111111111111, "EFR": 1.0, "Overall": 0.7083159722222222}, {"timecode": 90, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-11678", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.84375, "KG": 0.5078125, "before_eval_results": {"predictions": ["1974", "a small, hard, leather-cased ball with a rounded end wooden, plastic or metal bat", "Rebecca Lynn Forstadt", "\"Switzerland of England\"", "Ford Island", "the Food and Agriculture Organization", "Beno\u00eet Jacquot", "Lincoln", "Battle of the Philippines", "Les Temps modernes", "Agra", "1986", "\"King of Cool\"", "841", "Missouri", "1.23 million", "32", "Clarence Nash", "2015", "James Franco", "(born December 8, 1970)", "Sam Kinison", "1918", "Robert Gibson", "3730 km", "Washington State Cougars", "(1888-1954)", "Forbes", "Univision", "an English Anglican cleric and theologian", "As of the 2010 census", "Dallas", "237", "Tennessee", "Orlando", "Oklahoma City", "Dakota Johnson", "The School Boys", "\"n\u00e9e\" Amble", "2007", "Japan", "fixed-roof", "Sir Konstantin Sergeevich Novoselov", "Balloon Street, Manchester", "\"The Cleveland Show\"", "The National League", "Anne Erin \"Annie\" Clark", "Atlantic Ocean", "Geoffrey Hutchings", "Fort Hood, Texas", "Hong Kong", "William Jennings Bryan", "Pierre Chambrin", "Instagram's own account", "(1907)", "J. M. W. Turner", "geese", "Dr. Jennifer Arnold and husband Bill Klein,", "last summer.", "speed attempts", "Andrew Wyeth", "a calico", "biddy", "to the U.S. Holocaust Memorial Museum"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6500744047619047}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-3072", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-5733", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5341", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5813", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-7442", "mrqa_triviaqa-validation-6410", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2420"], "SR": 0.5625, "CSR": 0.5364010989010989, "EFR": 0.9285714285714286, "Overall": 0.7097913804945055}, {"timecode": 91, "before_eval_results": {"predictions": ["Leo Tolstoy", "Montana", "Tigger", "Charlesfort", "Super Mario Bros.", "Lexington", "a shark", "mint", "Roxanne", "salutatorian", "Roald Dahl", "ER", "Buffalo Bill", "a pager", "Hawaii", "Sir Isaac Newton", "Radiohead", "Cain", "Lignite", "the Vietnam War", "Algebra", "Catherine", "paul mcc McCartney", "Blondie", "Drumline", "Donnie Wahlberg", "cytokinesis", "the Unabomber", "Tom Petty", "Harry Potter", "The Sixth Sense", "American New Wave", "Tanqueray Gin", "Santa", "Prada", "Billy the Kid", "the Stone Age", "Cecil John Rhodes", "James Garner", "inn", "Michelle Pfeiffer", "a double take", "Michael Lewis Webster", "Michael Phelps", "Papua New Guinea", "Foehn", "a spectroscope", "sesame", "a quart", "hock", "Larry King", "UNICEF's global programing", "defense against rain rather than sun", "Vijaya Mulay", "cuzco", "Istanbul", "caspian seas", "Dante Bonfim Costa Santos", "George Raft", "Grace O'Malley", "Zed", "Communist", "an empty water bottle", "new wave rock band The Fixx"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7120535714285714}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9909", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-12917", "mrqa_searchqa-validation-6778", "mrqa_searchqa-validation-5080", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-14637", "mrqa_searchqa-validation-12260", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-3604", "mrqa_searchqa-validation-2465", "mrqa_searchqa-validation-11278", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11934", "mrqa_searchqa-validation-3638", "mrqa_searchqa-validation-6925", "mrqa_searchqa-validation-11404", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-3479", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2012"], "SR": 0.59375, "CSR": 0.5370244565217391, "EFR": 0.9615384615384616, "Overall": 0.7165094586120402}, {"timecode": 92, "before_eval_results": {"predictions": ["John Foster Dulles", "aluminum", "no cavities", "The New York Times", "the PATRIOT Act", "hock", "John Madden", "Ernest Hemingway", "a rubaiyat", "Judge Roy Bean", "Malaysia", "a yam", "the Mariner", "the Chunnel", "President Lincoln", "Smithfield", "Martin Scorsese", "Poland", "a pyramid", "a shepherd's pie", "the Tabernacle", "The Indianapolis 500", "the Galapagos", "Boston", "the Nautilus", "the Phoenician", "parez", "Angeles", "Athens", "Calvin Klein Eternity", "Cherry Jones", "Pennsylvania", "Cotton Mather", "the Berlin Wall", "Suzanne Valadon", "Sexuality", "elephants", "an axe", "Erwin Rommel", "Navarre", "wheat", "Vermont", "Mending Wall", "Thomas Jefferson", "copper", "Wrigley", "rum", "towels", "Brian Curtis Wimes", "steel", "Cormac McCarthy", "Barry Mann", "A turlough, or turlach", "6,259 km", "Tutankhamun", "Funchal", "Ruth Rendell", "Anthony Lynn", "The 2016 United States Senate election", "Martin Scorsese", "African National Congress Deputy President Kgalema Motlanthe,", "Angela Merkel", "September,", "Somali-based"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6085069444444444}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-13482", "mrqa_searchqa-validation-11", "mrqa_searchqa-validation-5947", "mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-16916", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4976", "mrqa_searchqa-validation-4760", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-1937", "mrqa_searchqa-validation-9291", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-10940", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-13222", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-335", "mrqa_newsqa-validation-1382"], "SR": 0.53125, "CSR": 0.5369623655913979, "EFR": 1.0, "Overall": 0.7241893481182796}, {"timecode": 93, "before_eval_results": {"predictions": ["Twister", "Romeo & Juliet", "doughboy", "Georgetown", "Plum", "the Cricket", "a duke", "Truman", "temples", "Lot", "a hull", "aluminum", "Kevin Smith", "Jabez Stone", "Major", "the Monitor", "Louis Pasteur", "parishes (or, in St. Ouen, cueillettes),", "Superior", "wheat", "Edgar Allan Poe", "Thomas Edison", "Manhattan", "(Curly) Lambeau", "T.E. Lawrence", "Cannes", "ape", "Eliot Spitzer", "stick", "the union", "a fox", "Poe", "a star", "impeachment", "Santo Domingo", "a Dagger", "French", "Nightingale", "'80s catchphrase", "Harvey Milk", "butterflies", "a computer", "the devil's food cake", "Goodyear", "corpulent", "Edinburgh", "a crumpet", "a bike", "the Great Smoky Mountains", "(Sir Walter) Scott", "Punjabi", "October 1986", "in the duodenum", "Anthony Hopkins", "six", "Dubai", "horse racing", "Abdul Razzak Yaqoob", "England", "\"Brotherly Leader\"", "nine-wicket", "Haiti,", "if Gadhafi suffered the wound in crossfire or at close-range,", "Roberto Micheletti,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.640788465007215}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11129", "mrqa_searchqa-validation-13621", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-15297", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3704", "mrqa_searchqa-validation-3150", "mrqa_searchqa-validation-1995", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-5066", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-6686", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-5257", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-221", "mrqa_hotpotqa-validation-3442", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2385"], "SR": 0.5625, "CSR": 0.5372340425531915, "EFR": 1.0, "Overall": 0.7242436835106383}, {"timecode": 94, "before_eval_results": {"predictions": ["John Mayer", "Benjamin Franklin", "the Amstel River", "Constantinople", "Georgie Porgie", "Superior", "Adam", "Puerto Rico", "hearth", "Quebec", "Belshazzar", "the Cincinnati Reds", "Once", "in darkness", "China", "Frederick Douglass", "Sitka", "the Amazons", "Debussy", "a prince", "Bojangles", "an Aunt Jemima", "Frank Sinatra", "a saucer", "surfing", "KLM", "(Frederick Victor) Zeller", "a carriage", "a vest", "The Adventures of Sherlock Holmes", "Ned Kelly", "a prostitution scandal", "World of Warcraft", "Shakespeare", "the Inca", "Alaska", "Bill Hicks", "the Wii", "high jump", "a red wine", "a new brush", "Danica Patrick", "an pancreas", "Midway Atoll", "stars", "Federalico Pea", "a sacristy", "the Great Seal", "Rihanna", "\"24\"", "Tom Brady", "a star", "the final scene of the fourth season", "Billy Hill", "iron", "Henry Ford", "saint bartholomew", "1943", "Battle of Britain and the Battle of Malta", "Kaep", "did not", "David Beckham", "the vicious brutality which accompanied the murders of his father and brother.", "B-movie queen Lana Clarkson"], "metric_results": {"EM": 0.640625, "QA-F1": 0.696704306722689}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-9190", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-2848", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-615", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-4359", "mrqa_searchqa-validation-5254", "mrqa_searchqa-validation-11261", "mrqa_searchqa-validation-8737", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-13716", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-5383", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-2196"], "SR": 0.640625, "CSR": 0.5383223684210526, "EFR": 0.9565217391304348, "Overall": 0.7157656965102974}, {"timecode": 95, "before_eval_results": {"predictions": ["The Buckwheat Boyz", "1998", "between the Eastern Ghats and the Bay of Bengal", "warming is the largest among the tropical oceans, and about 3 times faster than the warming observed in the Pacific", "Gatiman express", "Dr. Sachchidananda Sinha", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "no more than 4.25 inches ( 108 mm )", "the Gentiles", "art of the Persian Safavid dynasty", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "typically the player to the dealer's right", "his guilt in killing the bird", "February 6, 2005", "a global cultural icon of France and one of the most recognisable structures in the world", "in the episode `` Kobol's Last Gleaming ''", "Philippe Petit", "Terrell Suggs", "anion", "four", "1952", "User State Migration Tool", "2017", "restoring someone's faith in love and family relationships", "an illustration by Everest creative Maganlal Daiya back in the 1960s", "if the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "solemniser", "1961", "36 months", "an integral membrane protein that builds up a proton gradient across a biological membrane", "during the period of rest ( day )", "Ethel `` Edy '' Proctor", "Randy VanWarmer", "Isle Vierge", "Kevin Spacey", "Miller Lite", "the body - centered cubic ( BCC ) lattice", "1986", "their son Jack ( short for Jack - o - Lantern )", "Spektor", "in the evening, after 9pm ET ( UTC - 5 )", "Majo to Hyakkihei 2", "July 21, 1861", "Holden Nowell", "Muhammad Yunus", "The Bangles", "Juliet", "Texas, Oklahoma, and the surrounding Great Plains", "Ethel Merman", "The Catskill Aqueduct", "Judith Cynthia Aline Keppel", "Kenny Everett", "Trinidad", "Ghost", "Haleiwa", "William Bradford", "Johnny Lee Stallworth", "Scudetto", "three", "suicides", "John Henry", "Compost", "Two and a Half Men", "Paris Motor Show"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6410181250530516}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.16, 0.6666666666666666, 0.3333333333333333, 0.9428571428571428, 1.0, 0.0, 0.0, 1.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.5454545454545454, 0.2857142857142857, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.07272727272727272, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-6035", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-3217", "mrqa_newsqa-validation-2754", "mrqa_hotpotqa-validation-3319"], "SR": 0.515625, "CSR": 0.5380859375, "EFR": 0.9032258064516129, "Overall": 0.7050592237903226}, {"timecode": 96, "before_eval_results": {"predictions": ["Shetlands", "avant garde", "simple", "Pisces", "The Law Society", "Russell Crowe", "15", "Iran", "Clint Eastwood", "1921", "Spain", "David Bowie", "The Ten Commandments", "German", "volcanoes", "Porridge", "South Africa", "New Orleans", "the eye", "Pooh", "Ringo Starr", "John Mortimer", "bushfires", "Boise", "Danny DeVito", "Sweden", "four", "AllStars", "Rastafari", "Sydney", "400m", "Leo Tolstoy", "Stirling Moss", "the Mayflower", "Rick Wakeman", "Libya", "Brazil", "The Mary Tyler Moore", "Gordon Jackson", "Scottish", "Beyonce", "West Sussex", "Laputa", "Colombia", "The Addams", "a storm", "Tanzania", "Darby and Joan", "Robert Boyle", "Thailand", "Hugh Laurie", "Los Angeles", "sometime between 124 and 800 CE", "since been adopted by five other countries", "1 January 1788", "guitar feedback", "Stephen Lee", "Arroyo and her husband", "artificial intelligence.", "North Korea", "Vietnam", "white granite", "the humerus", "a pinch"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7417261904761905}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4799999999999999, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-1554", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6139", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-2788", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-1866", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-1433", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-1941", "mrqa_newsqa-validation-887", "mrqa_searchqa-validation-14431"], "SR": 0.671875, "CSR": 0.539465206185567, "EFR": 1.0, "Overall": 0.7246899162371134}, {"timecode": 97, "before_eval_results": {"predictions": ["40 militants and six Pakistan soldiers dead,", "recall communications", "mrs henderson", "two United Arab Emirates based companies", "Britain.", "The monarchy's end after 239 years of rule", "Kerstin Fritzl,", "56,", "\"Freshman Year\" experience", "more than 200.", "Princess Diana", "1825", "maintain an \"aesthetic environment\" and ensure public safety,", "Caylee Anthony,", "the elections", "Alexandre Caizergues,", "Arroyo and her husband", "\"It was terrible, it was gut-wrenching just to hear them say it,\"", "hackers.", "India", "non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "Haleigh Cummings,", "Glenn McConnell, Senate president pro tempore,", "Toffelmakaren.", "ended his playing career", "researchers", "in his late 30s and early 40s.", "\"bad taste\" and blamed the Louvre's directors for failing to prevent what could result in \"fragrances of fries drifting under Mona Lisa's nose\"", "3,000", "10 percent", "$250,000", "April 22.", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Yusuf Saad Kamel", "Clinton", "flights affected", "a charity, Wheelchair for Iraqi Kids.", "Jeffrey Jamaleldine", "more than two years,", "it was unjustifiable", "up to $50,000", "Citizens are picking members of the lower house of parliament,", "41,280 pounds", "Adam Lambert", "President Clinton.", "allegedly members of five organized crime rings with ties to Europe, Asia, Africa and the Middle East.", "Iran's nuclear program.", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "133", "\"He tried", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "Richard Parker", "to harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "electron pairs", "cricket", "wales", "roman numbers", "Hawaii", "stolperstein", "Paper", "Las Vegas", "the hypothalamus", "the orangutan", "mary blyton"], "metric_results": {"EM": 0.546875, "QA-F1": 0.663465254983112}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, true, false], "QA-F1": [0.923076923076923, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.06666666666666667, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6153846153846153, 0.6666666666666666, 1.0, 1.0, 0.12244897959183672, 0.8, 0.12121212121212123, 1.0, 1.0, 0.0, 1.0, 0.30303030303030304, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-4055", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-582", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7701", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-1532"], "SR": 0.546875, "CSR": 0.5395408163265306, "EFR": 0.9310344827586207, "Overall": 0.7109119348170303}, {"timecode": 98, "before_eval_results": {"predictions": ["it does not", "Swedish Prime Minister Fredrik Reinfeldt.", "through the Rockies and climb to 9,000 feet.", "Friday,", "Heshmatollah Attarzadeh", "Daniel Radcliffe", "Yusuf Saad Kamel", "digging at the site", "Susan Atkins,", "Pakistan's High Commission in India", "\"Wicked,\"", "Rolling Stone", "Zimbabwe Electoral Commission, after a long delay, ruled that neither candidate had won the required majority of votes, and scheduled a runoff election for June 27.", "The oceans", "Asashoryu", "an Airbus A320-214,", "autonomy.", "South Africa", "\"A good vegan cupcake has the power to transform everything for the better,\"", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "between 1917 and 1924", "Les Bleus", "Too many glass shards left by beer drinkers in the city center,", "collapsed ConAgra Foods plant lies atop parked cars Tuesday in Garner, North Carolina.", "40-year-old", "Krishna Rajaram,", "the island's dining scene", "in the U.S.", "toffelmakaren.", "Kurt Cobain's", "Turkey,", "Moody and sinister,", "United States, NATO member states, Russia and India", "Ryder Russell,", "spending billions to revitalize the nation's economy,", "Palestinian Islamic Army,", "Haeftling,", "off the coast of Dubai", "change course", "Sri Lanka,", "eight-week", "drug cartels", "on the bench", "Anil Kapoor.", "\"The Closer.\"", "38 feet", "St. Louis, Missouri.", "64,", "a reduction in repair costs for various auto parts, components and structures.", "black-happy sidekick", "order", "1977", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "nasdaq", "Van Rijn", "the kalavinka", "Little Big League", "hulder", "Lucille Ball", "Ziploc", "South Africa", "ivory", "kraftwerk"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7068452380952381}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-3636", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-2399"], "SR": 0.671875, "CSR": 0.5408775252525253, "EFR": 1.0, "Overall": 0.724972380050505}, {"timecode": 99, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1232", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13865", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-622", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8108", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1713", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3633", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3883", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.833984375, "KG": 0.53671875, "before_eval_results": {"predictions": ["\" Number Ones\"", "\"She wasn't the best \"coach,\"", "$40 billion", "\"I am sick of life -- what can I", "10 municipal police officers", "Lifeway's 100-plus stores nationwide", "truck safer,", "2009", "two years", "Shiza Shahid,", "gasoline", "James Whitehouse,", "North Korea,", "\"Tete de Cheval\" (\"Horse's Head\")", "Osama bin Laden's sons", "nude beaches.", "1-1", "Jason Chaffetz", "Martin \"Al\" Culhane,", "President Obama", "A member of the group dubbed the \"Jena 6\"", "Animal Planet", "Minerals Management Service Director Elizabeth Birnbaum", "Mandi Hamlin", "41,280", "Arlington National Cemetery's Section 60,", "Tim Clark, Matt Kuchar and Bubba Watson", "Caster Semenya", "Michael Jackson", "Elisabeth captive", "Djibouti,", "123 pounds of cocaine and 4.5 pounds of heroin,", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Brazil, Argentina, Mexico, Colombia and Venezuela.", "second-degree aggravated battery", "two Emmys", "Alina Cho", "The forward's lawyer", "Turkish President Abdullah Gul.", "children of street cleaners and firefighters.", "to secure more funds from the region.", "five female pastors", "14 bodies", "South Carolina Republican Party Chairwoman Karen Floyd", "African National Congress Deputy President", "1913,", "second", "Chinese President Hu Jintao", "Michael Arrington,", "asylum in Britain.", "Indian army", "New York City", "2009", "every colony except Massachusetts", "Phil Mickelson", "Quran", "Shanghai", "University of Columbia", "Indian", "zona glomerulosa of the adrenal cortex", "freezing", "Power", "Maine", "November 17, 2017"], "metric_results": {"EM": 0.625, "QA-F1": 0.7539434523809523}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.4, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.28571428571428575, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3337", "mrqa_newsqa-validation-3895", "mrqa_naturalquestions-validation-644", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-397"], "SR": 0.625, "CSR": 0.54171875, "EFR": 1.0, "Overall": 0.7281875}]}