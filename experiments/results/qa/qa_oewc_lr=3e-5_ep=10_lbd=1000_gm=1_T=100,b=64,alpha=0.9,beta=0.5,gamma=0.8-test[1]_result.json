{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[1]_result.json', stream_id=1, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4130, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["April 20", "Paul Whiteman", "2005", "Islamism", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d rather than its cells", "the center of the curving path", "eight", "28", "global", "Fresno", "Amazonia: Man and Culture in a Counterfeit Paradise", "88%", "broken wing and leg", "Emmy Awards", "silt up the lake", "The TEU specifically excludes certain regions, for example the Faroe Islands, from the jurisdiction of European Union law", "legitimate medical purpose", "week 7", "Kromme Rijn", "Robert Boyle", "five", "Paul Samuelson", "The First Doctor encounters himself in the story The Space Museum", "until the age of 16", "80%", "Five", "threatened \"Old Briton\" with severe consequences", "less than 200,000", "Anheuser-Busch InBev", "Charles River", "Veni redemptor gentium", "northwestern Canada", "intracellular pathogenesis", "1998", "seven months old", "The Service Module was discarded", "July", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "Article 17(3)", "Doctor Who \u2013 The Ultimate Adventure", "Eric Roberts", "electricity could be used to locate submarines", "1910\u20131940", "the owner", "2.666 million", "September 1969", "Apollo Program Director", "dreams", "Charles I", "more than 1,100 tree species", "complete the modules to earn Chartered Teacher Status", "true larvae", "apicomplexan-related diseases", "Rev. Paul T. Stallsworth", "Sumerian King Gilgamesh of Uruk and Atilla the Hun", "2009", "Daniel Diermeier", "PNU and ODM camps", "136", "12 to 15 million", "flax", "February 1, 2016", "2001", "lipophilic alkaloid toxins"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8867121848739495}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6614", "mrqa_squad-validation-4170", "mrqa_squad-validation-6426", "mrqa_squad-validation-8037", "mrqa_squad-validation-7774", "mrqa_squad-validation-3885", "mrqa_squad-validation-1492", "mrqa_squad-validation-5788", "mrqa_squad-validation-4343"], "SR": 0.859375, "CSR": 0.859375, "EFR": 1.0, "Overall": 0.9296875}, {"timecode": 1, "before_eval_results": {"predictions": ["Carolina Panthers", "1530", "Gallifrey", "ca. 2 million", "intractable problems", "effective planning", "pyrenoid and thylakoids", "Canterbury", "1873", "A bridge", "clergyman", "Commission v Italy", "sports tourism", "G", "eating both fish larvae and small crustaceans", "slow to complete division", "Astra's", "T. T. Tsui Gallery", "1905", "theatres", "those who proceed to secondary school or vocational training", "1521", "Search the Collections", "CD8", "3 January 1521", "a bill", "the University of Aberdeen", "2014", "Missy", "US$10 a week", "Super Bowl City", "Dudley Simpson", "esoteric", "temperature and light", "4000 years", "new entrance building", "Levi's Stadium", "tutor", "electricity", "2007", "Los Angeles", "zeta function", "adviser", "over $40 million", "Sunday Service of the Methodists in North America", "Lead fusible plugs", "occupational stress", "Synthetic aperture radar", "European Parliament and the Council of the European Union", "the coronation of Queen Elizabeth II", "O(n2)", "the Main Quadrangles", "35", "quadratic time", "antisemitic", "over-fishing and long-term environmental changes", "the Onon River and the Burkhan Khaldun mountain", "Hassan al-Turabi", "robbery", "Arlen Specter", "it is Oprah's daughters", "Venus Williams", "prisoners", "a globose pome"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7448529411764706}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8691", "mrqa_squad-validation-3958", "mrqa_squad-validation-4648", "mrqa_squad-validation-8864", "mrqa_squad-validation-9454", "mrqa_squad-validation-7818", "mrqa_squad-validation-1272", "mrqa_squad-validation-2122", "mrqa_squad-validation-1530", "mrqa_squad-validation-9977", "mrqa_squad-validation-1818", "mrqa_squad-validation-2525", "mrqa_squad-validation-6073", "mrqa_newsqa-validation-2834", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-808", "mrqa_triviaqa-validation-5325"], "SR": 0.71875, "CSR": 0.7890625, "EFR": 0.8888888888888888, "Overall": 0.8389756944444444}, {"timecode": 2, "before_eval_results": {"predictions": ["it developed into a major part of the Internet backbone", "The best-known legend", "Egyptians", "quantity surveyor", "the \"missile gap\"", "Tanaghrisson", "1852", "1564", "Concentrated O2", "adenosine triphosphate", "300 men", "11.5 inches (292.1 mm)", "1964", "the infected corpses over the city walls of Kaffa", "CD4 co-receptor", "the Lutheran and Reformed states in Germany and Scandinavia", "Chinggis Khaan International Airport", "modern buildings as well as structures dating from the 15th\u201318th centuries", "the warmest months from May through September, while the driest months are from November through April", "NBC", "Three", "the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies", "Gymnosperms", "to punish Christians by God", "Word and Image department", "Nafzger", "MPEG-4", "BSkyB", "chromalveolates", "embroidery", "three hundred years", "26", "Duran Duran", "The Mallee and upper Wimmera are Victoria's warmest regions with hot winds blowing from nearby semi-deserts", "long distance services", "the chloroplasts of C4 plants", "the violence that subsequently engulfed the country", "primality", "divergent boundaries", "the Chancel Chapel", "Kurt H. Debus", "the construction of military roads to the area by Braddock and Forbes", "In bays where they occur in very high numbers", "soap opera Dallas", "Ted Heath", "late 14th-century", "high-voltage", "A contract", "Arabic numerals", "pamphlets on Islam", "draftsman", "1993\u201394", "the collider is finally ready for an attempt to circulate a beam of protons the whole way around the 17-mile tunnel", "France", "20", "the U.N. General Assembly", "the Koreans edge into second place in Asian qualifying Group 2 to finish ahead of Saudi Arabia on goal difference and seal their place in the finals", "the results by a chaplain about 1:45 p.m., per jail policy", "56", "school", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory", "AbdulMutallab was in the bathroom for about 15 to 20 minutes, \"pushed the plunger on the bomb and prepared to die,\"", "coca wine", "Dissection"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7436701574569222}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5454545454545454, 1.0, 1.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.1818181818181818, 1.0, 1.0, 0.0909090909090909, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4847", "mrqa_squad-validation-3088", "mrqa_squad-validation-3480", "mrqa_squad-validation-8905", "mrqa_squad-validation-4772", "mrqa_squad-validation-3270", "mrqa_squad-validation-7211", "mrqa_squad-validation-1909", "mrqa_squad-validation-2565", "mrqa_squad-validation-2906", "mrqa_squad-validation-2899", "mrqa_squad-validation-4322", "mrqa_squad-validation-9872", "mrqa_squad-validation-2291", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-2281", "mrqa_newsqa-validation-1201"], "SR": 0.671875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 3, "before_eval_results": {"predictions": ["\"vanguard of change and Islamic reform\"", "less than a year", "dampening the fire", "187 feet (57 m)", "Zagreus", "Swynnerton Plan", "extra-legal", "Harvey Martin", "December 1895", "lion, leopard, buffalo, rhinoceros, and elephant", "Establishing \"natural borders\"", "drama series", "17 years", "sold", "income inequality", "pastor", "1534", "mesoglea", "20%", "Isaac Newton", "Miocene", "\"citizenship\"", "13", "force model", "Super Bowl City", "three", "Georgia", "Fort Caroline", "Horace Walpole", "Afrikaans", "adaptive immune system", "24", "applications such as on-line betting, financial applications", "United Kingdom", "issues related to the substance of the statement", "2007", "Luther's anti-Jewish works", "short-tempered and even harsher", "First Minister", "two catechisms", "orientalism and tropicality", "John Dobson", "Super Bowl 50", "Beryl", "prima scriptura", "the Mongol Empire", "LOVE Radio", "\u201cLady\u201d or a \u201cWoman\u201d", "Robinsons, Unicorn Bitter", "\"The Mullen \u00ef\u00bf\u00bd\"", "cutis anserina", "Bogota", "Artemis", "Malaxis paludosa", "concealpcion", "pilot", "\"Cup of tea!\"", "Mexico", "9", "Dorset", "The Daily Mirror", "Kat ( Jessica Wallace ), Little Mo ( Kacey Ainsworth ) and Zoe ( Michelle Ryan )", "Paul Lynde", "adenosine diphosphate"], "metric_results": {"EM": 0.6875, "QA-F1": 0.721474358974359}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-10466", "mrqa_squad-validation-3139", "mrqa_squad-validation-2486", "mrqa_squad-validation-9540", "mrqa_squad-validation-433", "mrqa_triviaqa-validation-7269", "mrqa_triviaqa-validation-4928", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-5104"], "SR": 0.6875, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 4, "before_eval_results": {"predictions": ["four days", "the International Fr\u00e9d\u00e9ric Chopin Piano Competition", "Nuda", "non-tertiary", "21", "The Christmas Invasion", "vertebrates", "Miocene", "Robert Underwood Johnson", "Cuba", "the Horniman Museum", "the ability to pursue valued goals", "University of Erfurt", "curved space", "Johann Sebastian Bach", "1524\u201325", "Spain's loss to Britain of Florida (Spain had ceded this to Britain in exchange for the return of Havana, Cuba)", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "2009", "British", "2005", "Germany and Austria", "self-starting design", "two tumen (20,000 soldiers)", "international metropolitan region", "Paul Revere", "projects sponsored by the National Science Foundation (NSF) beginning in 1985", "Go-Ahead", "football", "second-largest", "biologist", "upper sixth", "arrested", "Pakistan", "1185", "Central business districts", "Hisao Yamada", "Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "13th century", "organisms", "co-NP", "between 1.4 and 5.8 \u00b0C above 1990 levels", "cornwall", "stand by Me", "agie", "cornwall", "cornwall", "cornwall", "leopold II", "oak leaf", "cornwall", "cornwall", "cornwall", "cornwall", "cornwall", "The Killers", "cornwall", "Russell Crowe", "cornwall", "cornwall", "Skat", "cornwall", "Christopher Nolan", "gun"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6227678571428572}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.06666666666666667, 0.05714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-6981", "mrqa_squad-validation-1621", "mrqa_squad-validation-10145", "mrqa_squad-validation-7554", "mrqa_squad-validation-1360", "mrqa_squad-validation-6046", "mrqa_squad-validation-3119", "mrqa_squad-validation-4848", "mrqa_squad-validation-5374", "mrqa_squad-validation-6279", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-9913", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-2062", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-13775", "mrqa_triviaqa-validation-1816"], "SR": 0.59375, "CSR": 0.70625, "EFR": 1.0, "Overall": 0.853125}, {"timecode": 5, "before_eval_results": {"predictions": ["the wedding banquet", "respiration", "a deficit", "$216,000", "City of Malindi", "Apollo Applications Program", "trial division", "from the mid-sixties through to the present day", "An attorney", "eight", "The Book of Discipline", "Olivier Messiaen", "1759-60", "The Rankine cycle", "deadly explosives", "first half of the eighteenth century", "5,560", "Tricia Marwick", "the main contractor", "\"ctenes", "third most abundant chemical element", "adjustable spring-loaded valve", "agriculture", "metamorphosed", "David Suzuki", "the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "June 1979", "4.95 mL", "December 12", "second half of the 20th Century", "Ten", "the edge railed rack and pinion Middleton Railway", "John Elway", "1598", "The Eleventh Doctor", "Tugh Temur", "War of Currents", "dampening the fire", "eight years", "the Dalai Lama", "\"Battlefield helicopter crews routinely practice landing in fields and confined spaces away from their airfields", "corruption", "the Dalai Lama", "deaths", "2-0", "Chesley \"Sully\" Sullenberger", "Stanford University", "issued his first military orders", "July", "women", "McChrystal", "the Bronx", "Shanghai", "Fernando Gonzalez", "sportswear", "their surrogate", "killing up to 280,000 people", "the sins of the members of the church", "humans", "a simple majority vote", "George Best", "June 26, 2018", "Mahler Symphonies", "March 19, 2017"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7860253389550265}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.962962962962963, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0625, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.3333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7831", "mrqa_squad-validation-3559", "mrqa_squad-validation-4172", "mrqa_squad-validation-3555", "mrqa_squad-validation-3179", "mrqa_squad-validation-383", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-539", "mrqa_naturalquestions-validation-8633", "mrqa_hotpotqa-validation-5406", "mrqa_searchqa-validation-6817"], "SR": 0.703125, "CSR": 0.7057291666666667, "EFR": 0.9473684210526315, "Overall": 0.8265487938596492}, {"timecode": 6, "before_eval_results": {"predictions": ["John Mayow", "he was profoundly influenced by a math teacher Martin Sekuli\u0107", "1 July 1851", "1562", "32.9%", "the architect's client and the main contractor", "2009", "Katy\u0144 Museum", "Derek Wolfe", "silicates", "methotrexate or azathioprine", "over 100,000", "3.55 inches", "Bukhara", "Beyonc\u00e9", "safaris", "Huntington Boulevard", "Northern Pride Festival", "eight", "Henry Laurens", "Roone Arledge", "the Florida legislature", "autoimmune", "Diarmaid MacCulloch", "permafrost", "University Athletic Association", "idolatry", "Protestant clergy to marry", "a suite of network protocols", "German", "21 to 11", "eight", "\"Yes, I committed the act of which you accuse me.", "6800", "15", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "November 25, 2002", "the center", "fovea centralis", "Milcom", "Choroyuki Tagawa", "MGM Resorts International", "Bobby Darin", "Gene MacLellan", "Matt Monro", "Andhra Pradesh and Odisha", "2013", "Wisconsin", "Jonathan Breck", "Neil Young", "milling", "hot enough that light in the form of either glowing or a flame is produced", "de jure racial segregation was ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution", "hot summers and mild winters", "Sir Alex Ferguson", "Bachendri Pal", "April 2011", "1979", "Edouard Manet", "Justin Spitzer", "Dr. Paul Appelbaum", "Choir-laid", "Chronic obstructive pulmonary disease", "Jim Inhofe"], "metric_results": {"EM": 0.625, "QA-F1": 0.6934450965700966}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.5, 0.5, 1.0, 0.18181818181818185, 0.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1166", "mrqa_squad-validation-291", "mrqa_squad-validation-8400", "mrqa_squad-validation-6223", "mrqa_squad-validation-4673", "mrqa_squad-validation-2416", "mrqa_squad-validation-978", "mrqa_squad-validation-6913", "mrqa_squad-validation-5653", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-4865", "mrqa_newsqa-validation-130", "mrqa_searchqa-validation-10794", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-9370"], "SR": 0.625, "CSR": 0.6941964285714286, "EFR": 0.9166666666666666, "Overall": 0.8054315476190477}, {"timecode": 7, "before_eval_results": {"predictions": ["The Hoppings", "Chicago", "fear that the great Khan sent for his sons in the spring of 1223, and while his brothers heeded the order, Jochi remained in Khorasan", "Inherited wealth", "\"There is a world of difference between his belief in salvation and a racial ideology.", "StubHub Center", "WMO Executive Council and UNEP Governing Council", "trial division", "1999", "Gian Lorenzo Bernini", "housing stock", "dendritic cells, keratinocytes and macrophages", "Newcastle Mela", "ambiguity", "1665", "The Day of the Doctor", "punts", "1.6 kilometres", "\"winds up\" the debate", "2005", "100,000", "thermal expansion", "John Sutcliffe", "Owen Daniels", "incitement to terrorism", "Brookhaven", "A construction project", "Roy", "17 February 1546", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people), human inequality can be addressed", "September 30, 1960", "Brian Steele", "status line", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "call premium", "left coronary artery", "nine", "Havana Harbor", "four", "1932", "Walter Mondale", "Mitch Murray", "Marie Fredriksson", "December 1, 2009", "virtual reality simulator", "Noah Schnapp", "Antonio Banderas", "points on a sphere or angles in a circle are measured in units called?", "Kanawha River", "Julie Stichbury", "in consistency and content", "William J. Bell", "Spanish", "September 2017", "Krypton", "1990", "the Soviet Union", "gatekeeper", "Murcia", "India", "Wolfgang Amadeus Mozart", "Armin Meiwes", "Mot\u00f6rhead", "Bill Clinton"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6913241903116499}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65, 1.0, 1.0, 1.0, 0.8372093023255813, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.1, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6294", "mrqa_squad-validation-2608", "mrqa_squad-validation-8910", "mrqa_squad-validation-5189", "mrqa_squad-validation-6402", "mrqa_squad-validation-7632", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5010", "mrqa_triviaqa-validation-2674", "mrqa_searchqa-validation-5845"], "SR": 0.640625, "CSR": 0.6875, "EFR": 1.0, "Overall": 0.84375}, {"timecode": 8, "before_eval_results": {"predictions": ["Prague", "extremely high humidity", "Manning", "Sports Night", "2nd century BCE", "taxation", "destroy the antichrist", "collective bargaining, political influence, or corruption", "the Ilkhanate", "a second Gleichschaltung", "1864", "NL and NC", "over fifty", "Albert Einstein", "discarded", "state or government schools", "\"ash tree\" in Spanish, and an ash leaf is featured on the city's flag.", "Commission v France", "purported video, remaining in black and white, contains conservative digital enhancements and did not include sound quality improvements", "1689", "adaptive immune system", "case law by the Court of Justice", "Von Miller", "an archetypal \"mad scientist\"", "Dendritic cells", "Allston Science Complex", "writing a five volume book in his native Greek \u03a0\u03b5\u03c1\u03af \u03cd\u03bb\u03b7\u03c2 \u03b9\u03b1\u03c4\u03c1\u03b9\u03ba\u03ae\u03c2 in the 1st century AD.", "wars", "in inorganic forms, such as calcium carbonate", "to solve South Africa's'' ethnic problems", "Hirschman", "a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Sergeant Himmelstoss", "four", "bohrium", "a major fall in stock prices", "Buddhist missionaries", "March 31 to April 8, 2018", "The primary and secondary recipients may only see their own email address in Bcc", "start fires, hunt, and bury their dead", "Ray Charles", "the story's themes of moral dilemma and choosing between the easy and the right decision", "omitted and an additional panel stating the type of hazard ahead", "Valene Kane", "continues the pre-existing appropriations at the same levels as the previous fiscal year", "Arnold Schoenberg", "Tiffany Adams Coyne", "Trace Adkins", "Elizabeth Banks as Gail Abernathy - McKadden - Feinberger", "Southampton ( 1902, then in the Southern League )", "July 23, 2016", "Auburn Tigers football team", "Kate '' Mulgrew", "1943", "Rumplestiltskin", "Kryptonite", "a long - term infection that can be very difficult to eradicate", "Santiago", "Japan", "Lance Cpl. Maria Lauterbach", "the Demon Barber of Fleet Street", "the actinide berkelium", "(Heifetz, Perlman, etc.?)", "an isosceles triangle has two sides of equal length."], "metric_results": {"EM": 0.484375, "QA-F1": 0.6013658717701413}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.25, 1.0, 0.8, 1.0, 1.0, 0.6956521739130436, 1.0, 0.0, 0.0, 1.0, 0.8823529411764706, 0.0, 1.0, 1.0, 0.12500000000000003, 0.0, 0.5, 0.0, 0.6, 1.0, 0.0851063829787234, 0.0, 0.0, 0.6896551724137931, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 0.5263157894736842, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4634", "mrqa_squad-validation-4029", "mrqa_squad-validation-3113", "mrqa_squad-validation-6678", "mrqa_squad-validation-3946", "mrqa_squad-validation-1248", "mrqa_squad-validation-6310", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-1974", "mrqa_hotpotqa-validation-2351", "mrqa_newsqa-validation-2518", "mrqa_searchqa-validation-10924", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-2789"], "SR": 0.484375, "CSR": 0.6649305555555556, "EFR": 0.9696969696969697, "Overall": 0.8173137626262627}, {"timecode": 9, "before_eval_results": {"predictions": ["gold", "War of Currents", "12", "Antoine Lavoisier,", "the United Kingdom, Australia, Canada and the United States", "well before Braddock's departure for North America.", "Philip Webb and William Morris", "forming a 'A National Gallery of British Art'", "Kissinger's", "Throughout the 1980s and 1990s, demand for a Scottish Parliament grew, in part because the government of the United Kingdom was controlled by the Conservative Party, while Scotland itself elected relatively few Conservative MPs.", "exceeds any given number", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "in the Channel Islands", "an additional warming of the Earth's surface", "they were nomads", "private southern Chinese manufacturers and merchants.", "Owen Jones", "the Eleventh Doctor", "the BBC National Orchestra of Wales", "bilaterians", "the traditional Chinese autocratic-bureaucratic system", "August 1992", "NBC", "France's claim to the region was superior to that of the British, since Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.", "Buckland Valley near Bright", "November 17, 2017", "Daren Maxwell Kagasoff", "the Royal Air Force ( RAF )", "the Sunni Muslim family", "2,050 metres ( 6,730 ft )", "to form a higher alkane", "on about 62 acres west of Mandalay Bay at Russell Road and Hacienda Avenue and between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "James Watson and Francis Crick", "September 29, 2017", "11 p.m.", "the Old English wylisc ( pronounced `` wullish '' ) meaning `` foreigner '' or `` Welshman ''", "204,408", "Lake Baikal", "an object that moves around an external axis is said to orbit", "7000301604928199000", "inversely proportional to the wave frequency, so gamma rays have very short wavelengths that are fractions of the size of atoms, whereas wavelengths on the opposite end of the spectrum can be as long as the universe", "Spanish explorers", "the roofs of the choir side - aisles at Durham Cathedral", "the Indian Civil Service", "different parts of the globe", "statistical analysis of data, amend interpretation and dissemination of results ( including peer review and occasional systematic review )", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "5,534 registered hospitals in the United States", "Louis XVIII", "spinal spinal cord reaches its permanent position at the level of L1 or L2 ( closer to the head )", "T'Pau", "the actors", "Julia Ormond", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "George Strait", "Karina Smirnoff", "the fallopian tube", "insects and their relationship to humans, other organisms, and the environment", "195029", "Dana Andrews", "Hawaii", "Hakeemullah Mehsud", "Bob Dylan", "a mythical half-human and half-eagle creature"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6105220807028808}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1875, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7027027027027027, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.92, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 0.5, 0.18181818181818182, 0.0, 0.18518518518518515, 1.0, 0.11764705882352941, 0.0, 0.16666666666666666, 0.0, 0.0, 0.2857142857142857, 0.0, 0.3157894736842105, 0.4, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.8, 0.5, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9334", "mrqa_squad-validation-1131", "mrqa_squad-validation-7771", "mrqa_squad-validation-8410", "mrqa_squad-validation-5733", "mrqa_squad-validation-10232", "mrqa_squad-validation-2843", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-10283", "mrqa_triviaqa-validation-3868", "mrqa_hotpotqa-validation-940", "mrqa_hotpotqa-validation-1398", "mrqa_newsqa-validation-3354", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-16618"], "SR": 0.46875, "CSR": 0.6453125, "EFR": 0.9705882352941176, "Overall": 0.8079503676470587}, {"timecode": 10, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-917", "mrqa_hotpotqa-validation-940", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1525", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2734", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2945", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3188", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4501", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-5315", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7351", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8859", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-9291", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-996", "mrqa_naturalquestions-validation-9961", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1354", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-299", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3059", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-3418", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-968", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-1101", "mrqa_searchqa-validation-12469", "mrqa_searchqa-validation-13775", "mrqa_searchqa-validation-16618", "mrqa_searchqa-validation-3352", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-4461", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9477", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10035", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10145", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10301", "mrqa_squad-validation-10359", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-10415", "mrqa_squad-validation-10466", "mrqa_squad-validation-1082", "mrqa_squad-validation-1109", "mrqa_squad-validation-1131", "mrqa_squad-validation-1151", "mrqa_squad-validation-1166", "mrqa_squad-validation-1180", "mrqa_squad-validation-1180", "mrqa_squad-validation-1195", "mrqa_squad-validation-121", "mrqa_squad-validation-1217", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1395", "mrqa_squad-validation-1468", "mrqa_squad-validation-1488", "mrqa_squad-validation-1492", "mrqa_squad-validation-1621", "mrqa_squad-validation-1630", "mrqa_squad-validation-1645", "mrqa_squad-validation-170", "mrqa_squad-validation-1719", "mrqa_squad-validation-1732", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1962", "mrqa_squad-validation-1971", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2082", "mrqa_squad-validation-2122", "mrqa_squad-validation-2159", "mrqa_squad-validation-217", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2361", "mrqa_squad-validation-2412", "mrqa_squad-validation-2416", "mrqa_squad-validation-2418", "mrqa_squad-validation-2457", "mrqa_squad-validation-2486", "mrqa_squad-validation-2548", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2608", "mrqa_squad-validation-2633", "mrqa_squad-validation-2667", "mrqa_squad-validation-2678", "mrqa_squad-validation-2843", "mrqa_squad-validation-2861", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2899", "mrqa_squad-validation-291", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-2985", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3088", "mrqa_squad-validation-3104", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3119", "mrqa_squad-validation-3162", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3209", "mrqa_squad-validation-3215", "mrqa_squad-validation-3241", "mrqa_squad-validation-3307", "mrqa_squad-validation-3355", "mrqa_squad-validation-3362", "mrqa_squad-validation-3407", "mrqa_squad-validation-3411", "mrqa_squad-validation-3413", "mrqa_squad-validation-3451", "mrqa_squad-validation-348", "mrqa_squad-validation-349", "mrqa_squad-validation-3522", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3559", "mrqa_squad-validation-3569", "mrqa_squad-validation-3585", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3822", "mrqa_squad-validation-383", "mrqa_squad-validation-3830", "mrqa_squad-validation-3841", "mrqa_squad-validation-3855", "mrqa_squad-validation-3882", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-3946", "mrqa_squad-validation-4008", "mrqa_squad-validation-4028", "mrqa_squad-validation-4046", "mrqa_squad-validation-4121", "mrqa_squad-validation-4172", "mrqa_squad-validation-423", "mrqa_squad-validation-4296", "mrqa_squad-validation-433", "mrqa_squad-validation-4331", "mrqa_squad-validation-4376", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4418", "mrqa_squad-validation-4430", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-4463", "mrqa_squad-validation-4495", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4648", "mrqa_squad-validation-4673", "mrqa_squad-validation-4704", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-477", "mrqa_squad-validation-4772", "mrqa_squad-validation-4803", "mrqa_squad-validation-4807", "mrqa_squad-validation-4841", "mrqa_squad-validation-4848", "mrqa_squad-validation-4936", "mrqa_squad-validation-4983", "mrqa_squad-validation-5023", "mrqa_squad-validation-5063", "mrqa_squad-validation-5083", "mrqa_squad-validation-513", "mrqa_squad-validation-513", "mrqa_squad-validation-5136", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5247", "mrqa_squad-validation-5250", "mrqa_squad-validation-5254", "mrqa_squad-validation-5265", "mrqa_squad-validation-5295", "mrqa_squad-validation-5307", "mrqa_squad-validation-5318", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5418", "mrqa_squad-validation-5445", "mrqa_squad-validation-5485", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5563", "mrqa_squad-validation-5564", "mrqa_squad-validation-5566", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5608", "mrqa_squad-validation-5653", "mrqa_squad-validation-5664", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5766", "mrqa_squad-validation-5782", "mrqa_squad-validation-5788", "mrqa_squad-validation-5821", "mrqa_squad-validation-5843", "mrqa_squad-validation-5852", "mrqa_squad-validation-5951", "mrqa_squad-validation-6046", "mrqa_squad-validation-6049", "mrqa_squad-validation-6067", "mrqa_squad-validation-6073", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6224", "mrqa_squad-validation-6241", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6294", "mrqa_squad-validation-6310", "mrqa_squad-validation-638", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-653", "mrqa_squad-validation-6531", "mrqa_squad-validation-6535", "mrqa_squad-validation-6548", "mrqa_squad-validation-6567", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-6678", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6840", "mrqa_squad-validation-6841", "mrqa_squad-validation-6868", "mrqa_squad-validation-6876", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6917", "mrqa_squad-validation-6959", "mrqa_squad-validation-6962", "mrqa_squad-validation-6981", "mrqa_squad-validation-703", "mrqa_squad-validation-7030", "mrqa_squad-validation-7142", "mrqa_squad-validation-7175", "mrqa_squad-validation-7211", "mrqa_squad-validation-7252", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-743", "mrqa_squad-validation-7435", "mrqa_squad-validation-7456", "mrqa_squad-validation-7554", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7632", "mrqa_squad-validation-7633", "mrqa_squad-validation-7678", "mrqa_squad-validation-7699", "mrqa_squad-validation-7704", "mrqa_squad-validation-7709", "mrqa_squad-validation-7716", "mrqa_squad-validation-7747", "mrqa_squad-validation-7766", "mrqa_squad-validation-7771", "mrqa_squad-validation-7774", "mrqa_squad-validation-7775", "mrqa_squad-validation-7800", "mrqa_squad-validation-7818", "mrqa_squad-validation-7831", "mrqa_squad-validation-7846", "mrqa_squad-validation-7863", "mrqa_squad-validation-7888", "mrqa_squad-validation-7899", "mrqa_squad-validation-795", "mrqa_squad-validation-7965", "mrqa_squad-validation-797", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8071", "mrqa_squad-validation-8163", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8365", "mrqa_squad-validation-8372", "mrqa_squad-validation-8410", "mrqa_squad-validation-8414", "mrqa_squad-validation-8441", "mrqa_squad-validation-8486", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8589", "mrqa_squad-validation-859", "mrqa_squad-validation-8598", "mrqa_squad-validation-8600", "mrqa_squad-validation-8666", "mrqa_squad-validation-8670", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8907", "mrqa_squad-validation-9020", "mrqa_squad-validation-9030", "mrqa_squad-validation-9050", "mrqa_squad-validation-9116", "mrqa_squad-validation-9121", "mrqa_squad-validation-9151", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9379", "mrqa_squad-validation-9401", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9454", "mrqa_squad-validation-9465", "mrqa_squad-validation-9484", "mrqa_squad-validation-9540", "mrqa_squad-validation-9590", "mrqa_squad-validation-9608", "mrqa_squad-validation-9689", "mrqa_squad-validation-9738", "mrqa_squad-validation-976", "mrqa_squad-validation-9760", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9954", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1523", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2216", "mrqa_triviaqa-validation-2579", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-6711"], "OKR": 0.8984375, "KG": 0.4046875, "before_eval_results": {"predictions": ["2009", "Huntington Boulevard", "since 1979", "William Hartnell and Patrick Troughton", "Distributed Adaptive Message Block Switching", "1331", "Confucianism and promoting Chinese cultural values", "a computer network funded by the U.S. National Science Foundation (NSF) that began operation in 1981", "a variety of sciences", "a military coup d'\u00e9tat.", "the A1", "proteolysis", "a form of starch", "Eisleben", "break off the cathode, pass out of the tube, and physically strike him", "late night talk shows", "13\u20137", "around half", "the comprehensive institutions of the Great Yuan", "a D loop mechanism", "Chuck Howley", "with observations", "Annette", "Tim McGraw and Kenny Chesney", "a man who could assume the form of a great black bear", "Casino promotions such as complimentary matchplay vouchers or 2 : 1 blackjack payouts allow the player to acquire an advantage without deviating from basic strategy", "a sperm fusing with an ovum", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "a sociological perspective which developed around the middle of the twentieth century and that continues to be influential in some areas of the discipline", "6 January 793", "a historical street in downtown Cebu City that is often called the oldest and the shortest national road in the Philippines", "Woody Harrelson, Juliette Lewis, Robert Downey Jr., Tom Sizemore, and Tommy Lee Jones", "Jane Fonda", "Robert Irsay", "Edgar Lungu", "UNESCO / ILO", "five", "Achal Kumar Jyoti", "the President", "Homer Banks, Carl Hampton and Raymond Jackson", "The management team", "British R&B girl group Eternal", "Montreal Montreal", "When the others arrive, she divulges that Willow took her from heaven", "six", "Yuzuru Hanyu", "a receptor or enzyme is distinct from the active site", "a geologist James Hutton", "Fleetwood Mac", "31 October 1972", "December 2, 1942", "September 8, 2017", "Hollywood Masonic Temple", "Dolph Lundgren", "the Gaget, Gauthier & Co. workshop", "Renhe Sports Management Ltd", "Tangled", "smartphones and similar devices to establish radio communication with each other by touching them together or bringing them into close proximity, usually no more than a few centimetres", "Kenneth Hood", "a heavy metal band", "Scotland", "she was a young skater and desperately wanted to make her mother proud.", "prohibiting the expansion of slavery into a territory where slave status was favored", "Peyton Place"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5318145613563592}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.782608695652174, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 0.2, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.07692307692307693, 0.0, 1.0, 0.09523809523809523, 0.0, 0.10526315789473684, 0.375, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.2, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.0, 0.0, 0.0, 1.0, 0.07142857142857142, 0.0, 0.0, 1.0, 0.21428571428571427, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8160", "mrqa_squad-validation-4849", "mrqa_squad-validation-9831", "mrqa_squad-validation-5357", "mrqa_squad-validation-1388", "mrqa_squad-validation-434", "mrqa_squad-validation-625", "mrqa_squad-validation-8747", "mrqa_squad-validation-8530", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-5554", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-2208", "mrqa_triviaqa-validation-2277", "mrqa_hotpotqa-validation-1315", "mrqa_hotpotqa-validation-4906", "mrqa_newsqa-validation-3223", "mrqa_searchqa-validation-15757"], "SR": 0.453125, "CSR": 0.6278409090909092, "EFR": 0.9714285714285714, "Overall": 0.7254007711038961}, {"timecode": 11, "before_eval_results": {"predictions": ["The availability of the Bible in vernacular languages", "detention", "Western Xia", "by qualified majority", "carbon related", "co-chair", "Katharina", "three", "August 10, 1948", "the holy catholic (or universal) church", "30\u201360%", "Louis Paul Cailletet", "Class II MHC molecules", "crust and lithosphere", "orange", "electrons", "stroke", "1525", "the type of reduction being used", "a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam", "Tommy Nambahu", "1947", "Spanish explorers", "Western Australia", "Stan and Cartman accidentally destroy a dam, causing the town of Beaverton to be destroyed", "April 10, 2018", "October 1, 2015", "number of games where the player played, in whole or in part", "cat", "f\u0254n", "Tom Brady", "China in chinese is called zhongguo", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "Kida", "1960", "Bengal tiger", "Part 2", "flood", "De Wayne Warren", "triacylglycerol", "1971", "autu", "Tom Brady", "Margaery Tyrell", "Isle of Sheppey in England", "Lake Powell", "Clarence L. Tinker", "Bed and breakfast", "Trace Adkins", "Garbi\u00f1e Muguruza", "Keith Richards", "Hercules", "June 12, 2018", "Bart Howard", "111", "Leonard Bernstein", "The Hague", "historic buildings, arts, and published works", "Lisa", "Arnoldo Rueda Medina", "tax incentives", "Robert Fellmeth", "Florida", "CNN"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5983287545787546}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5, 1.0, 0.13333333333333333, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.08333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.4, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4074", "mrqa_squad-validation-8581", "mrqa_squad-validation-3474", "mrqa_squad-validation-4952", "mrqa_squad-validation-10483", "mrqa_squad-validation-2274", "mrqa_squad-validation-3385", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-1023", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-386", "mrqa_hotpotqa-validation-2436", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1549", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-13057"], "SR": 0.515625, "CSR": 0.6184895833333333, "EFR": 0.967741935483871, "Overall": 0.7227931787634408}, {"timecode": 12, "before_eval_results": {"predictions": ["savanna or desert", "Downtown San Bernardino", "Kurt Vonnegut", "three", "Graz, Austria", "Sky Digital", "until 1796", "Galileo Galilei and Sir Isaac Newton", "Gary Kubiak", "soy farmers", "The Sinclair Broadcast Group", "mercuric oxide (HgO)", "Guglielmo Marconi", "Luther's education", "wages and profits", "southern Europe", "The judicial branch", "biostratigraphers", "1999", "Miami Heat", "nasal septum", "third - person", "Baez", "1799", "interphase", "The purse, which is fixed in United States dollars, was $2 million in 2011, with a winner's share of $315,600", "birch", "Roger Federer", "James Corden", "Pasek & Paul", "October 2", "1956", "Thomas Lennon", "Gwendoline Christie", "Orangeville, Ontario, Canada", "Caroline Sterling, n\u00e9e Bone, formerly Pemberton ( born 3 April 1955 ; died 2017 ) ( Sara Coward )", "RAM", "Afghanistan", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Walter Brennan", "James Rodr\u00edguez", "8ft", "various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "1985, 2016, 2018", "any vessel approaching British waters", "April 17, 1982", "Acid rain", "July 1, 2005", "a stem", "Thespis", "Ra\u00fal Eduardo Esparza", "By functions", "Easter Panathenaea in 566 B.C.", "a constitutional monarchy in which the power of the Emperor is limited and is relegated primarily to ceremonial duties", "Consular Report of Birth Abroad for children born to U.S. citizens ( who are also eligible for citizenship )", "false", "hindfoot", "southwestern", "Planet Terror", "between South America and Africa", "one", "Easter", "snowflake curve, a geometric pattern repeated at smaller scales, has fractional dimensions; it's this type of shape related to the word \"fractional\"", "the ruble"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6573998917748918}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true], "QA-F1": [1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.07142857142857142, 1.0, 1.0, 0.0, 0.9777777777777777, 0.5, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0909090909090909, 0.0, 0.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2644", "mrqa_squad-validation-8032", "mrqa_squad-validation-10341", "mrqa_squad-validation-9895", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-2452", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-2644", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-6706", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-6998", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-559", "mrqa_newsqa-validation-2782", "mrqa_searchqa-validation-9535", "mrqa_searchqa-validation-10635"], "SR": 0.59375, "CSR": 0.6165865384615384, "EFR": 0.9230769230769231, "Overall": 0.7134795673076924}, {"timecode": 13, "before_eval_results": {"predictions": ["return home", "socialist realism", "to spearhead the regeneration of the North-East", "sleep deprivation", "July 1977", "Anderson", "quantum electrodynamics (or QED)", "provisional elder/deacon", "antigenic variation", "kinematic", "worker, capitalist/business owner, landlord", "the communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Licensed Local Pastor", "retreating retreat to break enemy formations and to lure small enemy groups away from the larger group and defended position for ambush and counterattack", "both Kenia and Kegnia", "24 March 1879", "Rob Van Winkle", "the Stanford Linear Accelerator in Menlo Park, CA.", "Edie Falco", "Leonard Bernstein", "Empire of the Sun", "James K. Polk", "Nicaragua", "Afghanistan", "Cambodia", "the Aladdin", "Uncle Henry", "Lady and the Tramp", "a magic wand", "China", "solmn", "Hanoi.", "Agliff", "Alexander Ulyanov", "Beatrix Potter", "solerix.ru", "George Washington", "a spring in the French Alps", "Andrea del Sarto", "soles", "Christopher Columbus", "the Balfour Declaration", "a contingency fee", "a tortoise", "a biretta", "Shinto", "The Simpsons", "a genetic disease that prevents blood from clotting", "a Grecian Urn", "a soling material", "the Bad Boys", "Philadelphia", "Robert Downey Jr.", "The Fresh Prince of Bel-Air", "her abusive husband", "a negro, whose ancestors were imported into ( the U.S. ), and sold as slaves '', whether enslaved or free, could not be an American citizen and therefore had no standing to sue in federal court", "Sylvester the Cat", "Daniel Defoe", "Atlantic Ocean", "Battle of Britain and the Battle of Malta", "the U.S. Consulate in Rio de Janeiro", "the club's board", "three", "solor"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6979580026455027}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7407407407407407, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8065", "mrqa_squad-validation-6255", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-13069", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-708", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-4663", "mrqa_searchqa-validation-4983", "mrqa_searchqa-validation-1122", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-11170", "mrqa_naturalquestions-validation-4762", "mrqa_triviaqa-validation-4717", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-2199"], "SR": 0.65625, "CSR": 0.6194196428571428, "EFR": 0.9545454545454546, "Overall": 0.7203398944805195}, {"timecode": 14, "before_eval_results": {"predictions": ["Protestantism", "1281", "Tower Theatre", "the Mi'kmaq and the Abenaki", "basic design typical of Eastern bloc countries.", "Palestine", "sticky cells", "the convenience of the railroad and worried about flooding", "Ireland", "18 April 1521", "Melus of Bari", "Warszawa", "in the Migration period", "2012", "35", "a keynote", "Jeopardy", "Blue Nile", "Solomon", "Betsey Johnson", "Eragon", "Rawhide", "p puppy", "Garland", "Paul Hornung", "silk", "Donna Summer", "a cosmic traveler", "John Mahoney", "Murder by Death", "Paul Desmond", "the Dalton Gang", "Washington", "Gilda", "Mandela", "Franklin D. Roosevelt", "rice", "soba", "Sirhan Sirhan", "the Moon", "Winter", "b", "Mountain Dew", "Omaha", "Van Halen", "actress", "the Erie Canal", "The _______ Family", "Baltic Sea", "senex", "Sexton", "cathedral of Santa Maria dei Fiori", "hog", "a syllable", "k\u0259\u02c8m\u00e6nt\u0283i\u02d0 / ( Comanche : N\u0289m\u0289n\u0289\u0289 )", "Renishaw Hall, Derbyshire, England, UK", "Imola Circuit", "sheep", "to provide travel agencies in Japan with booking and ticketing capabilities for a wider range of international airlines", "Berea College", "hanged in 1979 for the murder of a political opponent two years after he was ousted as prime minister in a military coup.", "Sharon Bialek", "The European Council", "the Antillen or the West Indies"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5958333333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2943", "mrqa_squad-validation-4621", "mrqa_squad-validation-1062", "mrqa_squad-validation-9348", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-6730", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-5304", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-10103", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-8575", "mrqa_searchqa-validation-9432", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-12619", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-534", "mrqa_naturalquestions-validation-129", "mrqa_triviaqa-validation-1936", "mrqa_hotpotqa-validation-5184", "mrqa_newsqa-validation-846", "mrqa_triviaqa-validation-2317"], "SR": 0.53125, "CSR": 0.6135416666666667, "EFR": 0.9666666666666667, "Overall": 0.7215885416666666}, {"timecode": 15, "before_eval_results": {"predictions": ["easier and more efficient than anywhere else", "arrested", "Francis Marion", "the death of Elisabeth Sladen in early 2011.", "3.6%", "42,000", "14", "the United States Census Bureau", "Asia", "19 April 1943", "actions-oriented", "rapidly evolve and adapt", "present-day Upstate New York and the Ohio Country", "the Roman Republic", "a Liberal Democrat", "South Africa", "Will Carling", "The Chatham House Rule", "haiti", "America Online Inc.", "the Comte de la F\u00e8re", "bees", "The Firm", "The Streets", "violin", "a bronze", "the sound of a bell", "the Titanic", "groin", "the gluteus maximus", "Luigi", "Georgia", "Massachusetts", "impressionist landscape, figure, circus genre, ballet", "La Boh\u00e8me", "(John) Adams", "guggul", "Ethel Skinner", "Queen Victoria", "My Fair Lady", "Austria", "lignin", "Adolphe Adam", "a hays", "Barcelona", "the coelacanth", "mono", "cats", "Love Never Dies", "Elizabeth I", "a blues band", "haiti", "the ISS", "the Marcy Brothers", "1770 BC", "costume party", "terrorist activity against Norwegian interests", "Sydney", "Chancellor Angela Merkel", "CNN", "haron Leon \" Mike\" Wallace", "the Professor", "haiti", "The Howard Stern Show"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5515963203463203}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, false, true, false, false], "QA-F1": [0.7272727272727273, 0.0, 0.0, 0.7272727272727273, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.5714285714285715, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1830", "mrqa_squad-validation-6702", "mrqa_squad-validation-3118", "mrqa_squad-validation-7872", "mrqa_squad-validation-5893", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-871", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1130", "mrqa_triviaqa-validation-7299", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-7024", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-3087", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3384", "mrqa_triviaqa-validation-6890", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-3616", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8359", "mrqa_hotpotqa-validation-1537", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-2590", "mrqa_searchqa-validation-339", "mrqa_searchqa-validation-14441", "mrqa_searchqa-validation-8487"], "SR": 0.453125, "CSR": 0.603515625, "EFR": 1.0, "Overall": 0.7262500000000001}, {"timecode": 16, "before_eval_results": {"predictions": ["computational complexity", "secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies", "tensions over slavery and the power of bishops in the denomination", "mannerist architecture", "489", "Grainger Town area", "the Great Fire of London", "Theory of the Earth", "Ward", "Newton", "bilaterians", "one of the daughters of former King of Thebes, Oedipus", "the forces of Andrew Moray and William Wallace", "differential erosion", "between two and 30 eggs", "biannually", "Arthur Chung", "18", "9 February 2018", "Yuzuru Hanyu", "Marley & Me", "26.617 \u00b0 N 81.617", "the 2013 non-fiction book of the same name by David Finkel", "Duck", "McKim Marriott", "Hanna Alstr\u00f6m", "31", "Austin, Texas", "postero", "the Twelvers", "vasoconstriction of most blood vessels", "attached to another chromosome", "Rockwell", "All Hallows", "Sylvester Stallone", "In Time", "Rodney Crowell", "9 or 10 national", "in the cell nucleus", "Melissa Disney", "Darren McGavin", "UNESCO", "Thaddeus Rowe Luckinbill", "ummat al - Islamiyah", "William DeVaughn", "a Spanish surname", "Brevet Colonel Robert E. Lee", "The Annunciation", "Santiago Ram\u00f3n y Cajal", "boy", "seven", "novella", "Southwest Florida International Airport ( RSW )", "21 June 2007", "four", "California", "England", "Roc Me Out", "\"Dear John,\"", "root out terrorists within its borders.", "Rudolf Nureyev", "43", "the Kurdish area of northern Iraq", "Mohamed Alanssi"], "metric_results": {"EM": 0.515625, "QA-F1": 0.617579816017316}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false], "QA-F1": [0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.27272727272727276, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1704", "mrqa_squad-validation-9525", "mrqa_squad-validation-6640", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-6524", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-3358", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-5348", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-3391", "mrqa_newsqa-validation-3649", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-4917", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-814"], "SR": 0.515625, "CSR": 0.5983455882352942, "EFR": 0.9354838709677419, "Overall": 0.7123127668406072}, {"timecode": 17, "before_eval_results": {"predictions": ["in Sydney", "Doritos", "Hans Vredeman de Vries", "$20.4 billion", "Lake \u00dcberlingen", "Finsteraarhorn", "seven", "the Doctor's archenemy, a renegade Time Lord who desires to rule the universe", "the Lippe", "the American Revolutionary War", "25", "Chuck Noland", "Donna Mills", "March 31, 2013", "pulmonary heart disease ( cor pulmonale )", "before the first year begins", "the much - decorated Adam Schumann ( Miles Teller )", "the President of the United States", "UNESCO / ILO", "asexually", "The Republic of Tecala", "2008", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Travis Tritt", "the fifteenth full - length studio album by the American band Yo La Tengo", "Cee - Lo", "the star at the center of the Solar System", "a living prokaryotic cell ( or organelle )", "1966", "Charlotte Hornets", "Julie Adams", "April 29, 2009", "J. Presper Eckert and John William Mauchly", "201", "1983", "October 12, 2017", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T )", "MacFarlane", "regulate the employment and working conditions of civil servants", "Hercules", "Angel Island ( California )", "Rufus and Chaka Khan", "moist temperate climates", "before they kill him", "7.6 mm", "sorrow regarding the environment", "1955", "John B. Watson", "2020", "continues the pre-existing appropriations at the same levels as the previous fiscal year ( or with minor modifications ) for a set amount of time", "during World War II", "the Gospels of Matthew, Mark, Luke and John", "Orangeville, Ontario, Canada", "semicubical parabola", "Switzerland", "Abraham Lincoln", "St Augustine's Abbey", "Russia announced it might hold joint naval maneuvers with Venezuela in the Caribbean.", "the insurgency", "Joe DiMaggio", "Steve Austin", "NBA 2K16", "Baltimore", "September 25, 2017"], "metric_results": {"EM": 0.546875, "QA-F1": 0.65337007959314}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.72, 0.5714285714285715, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.4444444444444445, 1.0, 0.0, 0.2, 0.25, 1.0, 1.0, 0.4444444444444445, 1.0, 0.5925925925925926, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.06896551724137931, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2998", "mrqa_squad-validation-7700", "mrqa_squad-validation-7288", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-7409", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5608", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-4653", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7892", "mrqa_triviaqa-validation-2922", "mrqa_hotpotqa-validation-538", "mrqa_newsqa-validation-3489", "mrqa_searchqa-validation-10032", "mrqa_hotpotqa-validation-4735"], "SR": 0.546875, "CSR": 0.5954861111111112, "EFR": 0.9655172413793104, "Overall": 0.7177475454980844}, {"timecode": 18, "before_eval_results": {"predictions": ["1904", "to arrest Luther if he failed to recant", "Michael Heckenberger and colleagues of the University of Florida", "declined significantly", "quantum mechanics", "prices", "Miller", "Super Bowl XX", "gold", "p", "2017", "to collect menstrual flow", "protects heart in mediastinum and limits its motion", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "12 to 36 months old", "the mixing of sea water and fresh water", "her abusive husband", "HTTP / 1.1", "the south coast of eastern New Guinea", "2009", "December 2, 1942", "John Cooper Clarke", "1792", "Amitabh Bachchan, Akshay Kumar, Bobby Deol, Divya Khosla Kumar, Sandali Sinha and Nagma", "Annette Strean", "the Bee Gees", "September 25, 1987", "the Surrey Lions defeating the Warwickshire Bears by 9 wickets in the final to claim the title", "the Senate", "the little girl ( Addy Miller )", "Tommy James and the Shondells", "9.0 -- 9.1 ( M )", "the right to peaceably assemble, or to petition for a governmental redress of grievances", "2010", "James Corden", "the development of electronic computers in the United States", "the internal reproductive anatomy ( such as the uterus in females )", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "the ark of the covenant", "senators, each of whom represents a single state in its entirety", "beta decay", "Rachel Sarah Bilson", "the NIRA", "March 5, 2014", "Haikou on the Hainan Island", "19 June 2018", "stromal connective tissue", "Sunday night", "Stephen Curry of Davidson", "Ben Willis", "Rufus and Chaka Khan", "Jodie Foster", "Rory McIlroy", "Aristotle", "the Chicago Cubs", "1919", "Lithuania", "President Obama's race in 2008", "the Impeccable", "the United States Treasury", "oregon Bates Summit Medical Center", "Pete Seeger", "Kiss Me, Kate", "Wigan"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6022178539715108}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.875, 0.0, 0.5714285714285715, 0.4444444444444445, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5106382978723404, 1.0, 1.0, 0.0, 0.2222222222222222, 0.9090909090909091, 0.0, 0.125, 1.0, 1.0, 1.0, 0.4, 0.888888888888889, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.10810810810810811, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2268", "mrqa_squad-validation-10388", "mrqa_squad-validation-845", "mrqa_squad-validation-5", "mrqa_squad-validation-9213", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-1930", "mrqa_triviaqa-validation-1259", "mrqa_hotpotqa-validation-4927", "mrqa_newsqa-validation-686", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-10336"], "SR": 0.484375, "CSR": 0.5896381578947368, "EFR": 0.9090909090909091, "Overall": 0.7052926883971292}, {"timecode": 19, "before_eval_results": {"predictions": ["the phagosomal membrane", "well before Braddock's departure for North America", "General Hospital", "destroyed", "at night", "assertive", "generally antagonistic", "whether or not to plead guilty", "parallelogram rule of vector addition", "Valley Falls", "23 December 2014", "Sam Raimi", "the Town of North Hempstead, Nassau County, New York", "political thriller", "Timothy McVeigh", "Germaine", "1995\u201396", "the NYPD's 83rd Precinct", "the world of Azeroth", "October 5, 1930", "1999", "Musicology", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Michael Sheen, and Laurence Fishburne", "she blows on a horn", "the Mayor of the City of New York", "841", "My Boss, My Teacher", "Roy Spencer", "Bardney", "tragedy", "the Everglades", "Saint Louis County", "1891", "Eielson Air Force Base in Alaska", "Serhiy Paradzhanov", "Germany", "eight teams", "Carson City", "Lindsey Islands", "the 2008 presidential election", "October 12, 1962", "Eli Roth", "Paige O'Hara", "Northern Ireland", "St. Louis, Missouri", "the first hole of a sudden-death playoff with Kentucky native Kenny Perry", "Prince", "the life of Kaji, a Japanese pacifist and socialist", "Burnley", "Russian Empire", "between 11 or 13 and 18", "the onset and progression of Alzheimer's disease", "phayanchana ), 15 vowel symbols ( Thai : \u0e2a\u0e23\u0e30, sara ) that combine into at least", "7 June 2005", "Imola", "Sufjan Stevens", "producing rock music with a country influence", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "the donkey named Bottom", "Irving Berlin", "$1.5 million", "Sen. Barack Obama", "Tutsi ethnic minority and the Hutu majority"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6778149801587301}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.8, 0.5, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-8687", "mrqa_squad-validation-6024", "mrqa_squad-validation-1551", "mrqa_squad-validation-10408", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-2585", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-206", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-904", "mrqa_hotpotqa-validation-5485", "mrqa_naturalquestions-validation-2629", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-7828", "mrqa_newsqa-validation-3659"], "SR": 0.53125, "CSR": 0.58671875, "EFR": 1.0, "Overall": 0.722890625}, {"timecode": 20, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2530", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-395", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4375", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-491", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5392", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10014", "mrqa_naturalquestions-validation-10019", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10148", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1298", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-1939", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2847", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-2918", "mrqa_naturalquestions-validation-3055", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3230", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3855", "mrqa_naturalquestions-validation-386", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4083", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4423", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4867", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-5558", "mrqa_naturalquestions-validation-5582", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7358", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7583", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-794", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-836", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-1127", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-13531", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15836", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-346", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4437", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-4917", "mrqa_searchqa-validation-534", "mrqa_searchqa-validation-5664", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5845", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6488", "mrqa_searchqa-validation-6817", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8487", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-902", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9432", "mrqa_squad-validation-10064", "mrqa_squad-validation-10141", "mrqa_squad-validation-10151", "mrqa_squad-validation-10192", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10232", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10381", "mrqa_squad-validation-10388", "mrqa_squad-validation-10399", "mrqa_squad-validation-10408", "mrqa_squad-validation-1062", "mrqa_squad-validation-1082", "mrqa_squad-validation-1131", "mrqa_squad-validation-1180", "mrqa_squad-validation-1248", "mrqa_squad-validation-1272", "mrqa_squad-validation-1334", "mrqa_squad-validation-1348", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-150", "mrqa_squad-validation-1530", "mrqa_squad-validation-1551", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1900", "mrqa_squad-validation-1909", "mrqa_squad-validation-1939", "mrqa_squad-validation-2003", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-217", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2361", "mrqa_squad-validation-2416", "mrqa_squad-validation-2481", "mrqa_squad-validation-2511", "mrqa_squad-validation-2560", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2667", "mrqa_squad-validation-2772", "mrqa_squad-validation-2861", "mrqa_squad-validation-2881", "mrqa_squad-validation-2906", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2926", "mrqa_squad-validation-2949", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3148", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3209", "mrqa_squad-validation-3270", "mrqa_squad-validation-3355", "mrqa_squad-validation-3385", "mrqa_squad-validation-3411", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3830", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4159", "mrqa_squad-validation-4186", "mrqa_squad-validation-423", "mrqa_squad-validation-4331", "mrqa_squad-validation-434", "mrqa_squad-validation-4381", "mrqa_squad-validation-4385", "mrqa_squad-validation-4430", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4681", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-501", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5178", "mrqa_squad-validation-5189", "mrqa_squad-validation-5198", "mrqa_squad-validation-5224", "mrqa_squad-validation-5234", "mrqa_squad-validation-5243", "mrqa_squad-validation-5265", "mrqa_squad-validation-5393", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-552", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5590", "mrqa_squad-validation-5600", "mrqa_squad-validation-5640", "mrqa_squad-validation-5653", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5849", "mrqa_squad-validation-5893", "mrqa_squad-validation-5986", "mrqa_squad-validation-6024", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-6223", "mrqa_squad-validation-6294", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-668", "mrqa_squad-validation-6685", "mrqa_squad-validation-6703", "mrqa_squad-validation-6727", "mrqa_squad-validation-6728", "mrqa_squad-validation-680", "mrqa_squad-validation-6841", "mrqa_squad-validation-6879", "mrqa_squad-validation-6894", "mrqa_squad-validation-6917", "mrqa_squad-validation-7029", "mrqa_squad-validation-7164", "mrqa_squad-validation-7175", "mrqa_squad-validation-7302", "mrqa_squad-validation-7362", "mrqa_squad-validation-7366", "mrqa_squad-validation-7435", "mrqa_squad-validation-744", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7709", "mrqa_squad-validation-7740", "mrqa_squad-validation-7766", "mrqa_squad-validation-7775", "mrqa_squad-validation-7818", "mrqa_squad-validation-7821", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8037", "mrqa_squad-validation-8144", "mrqa_squad-validation-8163", "mrqa_squad-validation-828", "mrqa_squad-validation-8336", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-8372", "mrqa_squad-validation-848", "mrqa_squad-validation-8487", "mrqa_squad-validation-8568", "mrqa_squad-validation-8578", "mrqa_squad-validation-8600", "mrqa_squad-validation-8687", "mrqa_squad-validation-8747", "mrqa_squad-validation-8759", "mrqa_squad-validation-8807", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-9050", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9151", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-9334", "mrqa_squad-validation-9348", "mrqa_squad-validation-9401", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9830", "mrqa_squad-validation-9831", "mrqa_squad-validation-9893", "mrqa_squad-validation-9930", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-1816", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2545", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-3353", "mrqa_triviaqa-validation-3607", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-387", "mrqa_triviaqa-validation-4278", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4825", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-4987", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7542", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-871"], "OKR": 0.8828125, "KG": 0.45390625, "before_eval_results": {"predictions": ["one way", "since at least the mid-14th century", "Marlee Matlin", "a biplane capable of taking off vertically (VTOL aircraft) and then be \"gradually tilted through manipulation of the elevator devices\" in flight", "Classic", "88", "826", "leishmaniasis", "Raimond Gaita", "Theodore Roosevelt Mason", "1996", "1964", "Kinnairdy Castle", "James Edward Franco", "Thai Air Asia X", "December 24, 1973", "Baden-W\u00fcrttemberg", "James Dean", "Rob Reiner", "Kansas City", "Westfield Tea Tree Plaza", "26,000", "Alemannic", "1992", "Darci Kistler", "the EN World web site", "(25 March 1948 \u2212 27 December 2013)", "Pigman's Bar-B- Que", "Columbus, Ohio", "Emilia-Romagna Region", "Miami Gardens, Florida", "The Tower of London", "John Sullivan", "churros", "Franklin, Indiana", "Ub Iwerks", "mentalfloss.com", "Omega SA", "Flashback", "The Walter Reed Army Medical Center", "Kiernan Shipka", "1993", "Zaire", "(Don't Ya\")", "pronghorn", "Walcha", "French", "FBI", "BMW X6", "David Michael Bautista", "Cherokee River", "15", "Taylor Swift", "( B ) channels and one data ( D ) channel for control purposes", "2017 / 18 Divisional Round", "Batman & Robin", "Melbourne, Australia", "Michael Schumacher", "The Washington Post", "Constantine XI Palaiologos", "Viva Las Vegas", "(Diaemus, or Desmodus, youngi)", "Democritus", "Tennessee"], "metric_results": {"EM": 0.484375, "QA-F1": 0.602091138028638}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.18181818181818182, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1459", "mrqa_squad-validation-6653", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-989", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-4445", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-1146", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-3052", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3402", "mrqa_newsqa-validation-1737", "mrqa_searchqa-validation-15983", "mrqa_triviaqa-validation-694"], "SR": 0.484375, "CSR": 0.5818452380952381, "EFR": 0.9696969696969697, "Overall": 0.7311678165584417}, {"timecode": 21, "before_eval_results": {"predictions": ["between 1000 and 1900", "Colonialism", "student-teacher relationships", "Approximately one million", "Ted Ginn Jr.", "Harrods", "Coptic Cathedral", "Philadelphia", "Disco", "MMA", "Kentucky River", "Augustus", "1995", "public", "G. Stanley Hall", "Fitzroya cupressoides", "L\u00e9a Seydoux", "Nanna Popham Britton", "Sada Carolyn Thompson", "T. E. Lawrence", "Nelson Mandela", "Dissection", "German", "A41", "Forbes", "Bronwyn Kathleen Bishop", "The Timekeeper", "Jena Malone", "water", "Hong Kong First Division League", "Four Weddings and a Funeral", "1993 to 1996", "July 16, 1971", "the music genres of electronic rock, electropop and R&B", "Ted Nugent", "Reverend Lovejoy", "Geet or The Song", "October 5, 1930", "Pamelyn Wanda Ferdin", "The Ninth Gate", "TransAd Adelaide", "neuro-orthopaedic", "Saint Motel", "About 200", "War Is the Answer", "Eleanor of Aquitaine", "Trilochanpala", "Amy Poehler", "The Division of Cook", "Peter Townsend", "Blue Origin", "Taylor Swift", "1975", "convergent plate boundary", "lost the strengths that had allowed it to exercise effective control", "Kiri Te Kanawa", "Christchurch", "30-minute", "they did not know how many people were onboard", "Robert Wightman", "the Prussian bandmaster Friedrich Wilhelm Wieprecht", "how now '' is a greeting", "once every 23 hours", "Nigel Lythgoe"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6350023674242424}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.8, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9904", "mrqa_squad-validation-1873", "mrqa_squad-validation-3060", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-123", "mrqa_hotpotqa-validation-5698", "mrqa_hotpotqa-validation-1831", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-4133", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-883", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-2419", "mrqa_naturalquestions-validation-10448", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-5447", "mrqa_searchqa-validation-8612", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-2014"], "SR": 0.546875, "CSR": 0.5802556818181819, "EFR": 1.0, "Overall": 0.7369105113636364}, {"timecode": 22, "before_eval_results": {"predictions": ["Zorro", "Zagreus", "January 1985", "Computational complexity theory", "to plan the physical proceedings, and to integrate those proceedings with the other parts", "post-World War I", "Esteban Ocon", "New Orleans Saints", "Clarence Nash", "The Dressmaker", "1,800", "from August 14, 1848", "Urijah Faber", "Mary-Kay Wilmers", "baeocystin", "the authorship of \"Titus Andronicus\"", "Vaisakhi List", "Ars Nova Theater in New York City", "Monticello", "H. Sawin Millett Jr. (born October 8, 1937) is a Maine politician", "\"Menace II Society\"", "16 March 1987", "the luxury Holden Calais (VF) nameplate", "Louis \"Louie\" Zamperini", "1968", "PlayStation 3", "the Andes or Andean Mountains", "the Knight Company", "Parlophone Records", "Nicolas Winding Refn", "South America", "the Kingdom of Morocco", "Gerard Marenghi", "1958", "67,575", "31 July 1975", "orange", "Roseann O'Donnell", "There Is Only the Fight", "SAVE", "29,000", "the extraterrestrial hypothesis", "Larnelle Steward Harris", "4,613", "a morir so\u00f1ando or orange Creamsicle", "Ramzan Kadyrov", "Albert", "Tetrahydrogestrinone", "5,042", "Captain B.J. Hunnicutt", "Victoria, Duchess of Kent", "the Keeping Hours", "the Gilbert building", "dead plant and animal material", "Isabel Maru", "Rome", "The Archers", "a raven", "intravenous vitamin \"drips\"", "after Michael Jackson's death in the Holmby Hills, California, mansion he rented.", "Daniel Radcliffe", "Milla Jovovich", "a male goose you might take", "a snout beetle, Anthonomus grandis"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6941867236024845}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.8, 0.0, 0.0, 0.0, 1.0, 0.6, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6956", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-460", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-2495", "mrqa_triviaqa-validation-1447", "mrqa_newsqa-validation-3611", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-11933"], "SR": 0.59375, "CSR": 0.5808423913043479, "EFR": 1.0, "Overall": 0.7370278532608696}, {"timecode": 23, "before_eval_results": {"predictions": ["the growth of mass production", "Edmonton, Canada", "Ollie Treiz", "Western Xia", "Richard Trevithick", "coughing and sneezing", "9", "his writings about the outdoors", "The Backstreet Boys", "November of that year", "Hawaii", "Tool", "1919", "May 5, 2015", "Arthur Miller", "Edmonton, Alberta", "(born December 31, 1971)", "from 1989 until 1994", "The LA Galaxy", "Nicholas Kristof", "film and short novels", "Flushed Away", "first baseman and third baseman", "The Wash", "The Process", "200,167", "Bohemia and Archduke of Austria", "\"The Brothers\" (2001)", "Armin Meiwes", "1933", "Dirk Werner Nowitzki (]", "Carl Edwards", "In a Better World", "the 45th Infantry Division", "Iron Man 3", "Julie 2 (2016)", "Conservatorio Verdi in Milan", "6'5\"", "terrible", "June 26, 1970", "Lawrenceburg, Indiana", "7", "About a Boy", "1982", "1978", "domestic cat", "Denmark", "1999", "The Ryukyuan people (\u7409\u7403\u6c11\u65cf, Ry\u016bky\u016b minzoku, Okinawan: \"Ruuchuu minzuku\")", "Peter Thiel", "Summerlin", "Cersei Lannister", "Kida", "about 26,000 light - years", "Mahinda Rajapaksa", "iron", "naples", "Alias Smith and Jones", "it is done with the parents' full consent.", "\"explosive device\" or \"bomb\"", "2.5 million", "1,000,000", "naples", "naples"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6220179354636591}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.25, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.4210526315789474, 0.0, 1.0, 0.0, 1.0, 0.5714285714285714, 0.0, 0.4, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-4373", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-3125", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-1564", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-3634", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-1368", "mrqa_triviaqa-validation-2293", "mrqa_newsqa-validation-3415", "mrqa_newsqa-validation-1203", "mrqa_newsqa-validation-865", "mrqa_searchqa-validation-5102", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-3074"], "SR": 0.53125, "CSR": 0.5787760416666667, "EFR": 1.0, "Overall": 0.7366145833333334}, {"timecode": 24, "before_eval_results": {"predictions": ["1965", "A job where there are many workers willing to work a large amount of time (high supply) competing for a job that few require (low demand)", "A tundra", "article 30", "The mermaid", "the eastern Afghan province of Logar", "a fair and independent manner and ratify successful efforts.", "Paige, 15, and \"the crew\": Isaac, 8, Hope, 7, Noah, 5, Phoebe Joy, 3, Lydia Beth, 2, Annie, also 2,", "Kurt Cobain", "seven", "two years", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "piano", "Charlotte Gainsbourg and Willem Dafoe", "The Charlie Daniels Band", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "curfew", "Missouri", "allegedly members of five organized crime rings with ties to Europe, Asia, Africa and the Middle East.", "five victims by helicopter", "30-minute", "San Diego", "Iran", "cancer", "baseball bat", "the USS Nimitz", "$249", "the coalition seeks to reduce tension between its military forces and Afghan civilians in an effort to maintain Afghan public support.", "sportswear", "\"It got me thinking about what I would want to do when I got out of the game.", "eight", "the FAA received no reports from pilots in the air of any sightings", "1941", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "NATO fighters", "Jaime Andrade", "Somali President Sheikh Sharif Sheikh Ahmed", "\"I wasn't sure whether I was going to return to 'E! News' this week or after the new year.", "that students often know ahead of time when and where violence will flare up on campus.", "Orbiting Carbon Observatory", "German Chancellor Angela Merkel", "Joan Rivers", "Ryder Russell", "Samson D'Souza", "the FBI", "his mother, Katherine Jackson, his three children and undisclosed charities.", "Asashoryu", "the District of Columbia National Guard", "Interior Ministry", "Anil Kapoor.", "$3 billion", "question people if there's reason to suspect they're in the United States illegally", "a young girl", "Elizabeth Dean Lail", "1926", "COCTOR", "Wyoming", "Kenny Everett", "November 23, 1996 in Japan", "Richa Sharma", "The Frost Place Advanced Seminar", "The Genius", "Valentina Tereshkova", "Baccarat"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6199259315563663}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.16, 1.0, 1.0, 1.0, 0.1818181818181818, 0.0, 0.0, 0.5, 1.0, 1.0, 0.06060606060606061, 1.0, 0.0, 1.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2608695652173913, 1.0, 0.0, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.8, 0.10256410256410256, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.888888888888889, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7407", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1453", "mrqa_newsqa-validation-710", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-3517", "mrqa_newsqa-validation-3010", "mrqa_naturalquestions-validation-1805", "mrqa_triviaqa-validation-6214", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-733", "mrqa_searchqa-validation-14187"], "SR": 0.546875, "CSR": 0.5775, "EFR": 0.9310344827586207, "Overall": 0.7225662715517241}, {"timecode": 25, "before_eval_results": {"predictions": ["phycobilisomes", "WatchESPN", "independent prescribing authority", "10,000", "it infringed on democratic freedoms", "U.N. High Commissioner for Refugees", "Crandon, Wisconsin", "chairman of the House Budget Committee", "in Amstetten,", "The Drug Enforcement Administration said Wednesday it's considering tighter restrictions on propofol,", "his comments", "the European Commission", "company Polo", "an inquest", "Nkepile M abuse", "a Royal Air Force helicopter", "Samuel Herr,", "McDonald's", "the president's", "1981", "Jewish", "al Fayed's security team.", "Lifeway Christian Stores", "Dan Parris, 25, and Rob Lehr, 26,", "18", "July", "\"Mad Men\"", "Jason Chaffetz", "debris", "a woman", "the Form Design Center.", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "14", "the heart of Los Angeles.", "they would not be making any further comments, citing the investigation.", "North Korea intends to launch a long-range missile in the near future,", "the 3rd District of Utah.", "Sen. Debbie Stabenow (D- Michigan)", "\"peregruzka,\"", "only one", "Lee Probert.", "1998.", "23", "American", "\"we take this issue seriously,\"", "Monday.", "Frank Ricci,", "Old Trafford", "Guinea, Myanmar, Sudan and Venezuela.", "the Carrousel du Louvre", "homicide by undetermined means", "Les Bleus", "Madison's", "Gibraltar, a British Overseas Territory,", "need to repent in time", "John McCarthy", "The Rime of the Ancient Mariner", "narwhals", "Atlas ICBM", "Lionel Eugene Hollins", "Duchess Eleanor of Aquitaine", "John Irving", "diabetes", "the South African gold mines"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5237441378066379}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.4, 0.45454545454545453, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6354", "mrqa_squad-validation-8392", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2907", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-1546", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-981", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3726", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2673", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-5238", "mrqa_searchqa-validation-11369"], "SR": 0.421875, "CSR": 0.5715144230769231, "EFR": 1.0, "Overall": 0.7351622596153847}, {"timecode": 26, "before_eval_results": {"predictions": ["Eero Saarinen", "according to a multiple access scheme", "prime number", "Climate fluctuations during the last 34 million years", "Chicago Cubs", "The Salopian", "a string, or the vibrating air column in the case of a brass instrument,", "Norman Hartnell", "canoeing", "on Millbank in London", "Poland", "Adam Ant", "shallow seas", "New Democracy", "Missouri", "six", "mushrooms", "Turkey", "1979", "a bear", "the fictional London Borough of Walford", "Civil Law", "Laurence Olivier", "four", "Leonard Bernstein", "is", "are", "index fingers", "blood", "aircraft", "passion fruit", "a jumper", "The Lone Ranger", "caridean shrimp", "Yemen", "Welles", "George IV.", "Barry Briggs", "Joseph Smith, Jr.", "Herefordshire", "Kievan Rus", "meat", "Beaujolais Nouveau", "rowing", "$5 $10 $20 $50 $100", "(1962\u201364)", "A Dangerous Man: Lawrence After Arabia", "'Lord Nelson'", "ancient Testament", "Argentina", "bees", "Sinclair Lewis", "Phosphorus pentoxide", "son of Bindusara", "Norwegian", "County of York", "Cheshire", "Atomic Kitten", "Phillip A. Myers.", "11th year in a row", "Opryland", "Noah Gray-Cabey", "singer", "Alois Alzheimer"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5411458333333334}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8918", "mrqa_triviaqa-validation-6262", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-1872", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-2496", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-428", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-4364", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-4905", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4118", "mrqa_naturalquestions-validation-946", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-347", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-1554"], "SR": 0.46875, "CSR": 0.5677083333333333, "EFR": 0.9705882352941176, "Overall": 0.7285186887254902}, {"timecode": 27, "before_eval_results": {"predictions": ["water-cooled", "Genghis Khan Mausoleum", "the most cost efficient bidder", "February 1, 2016", "Kylie Jenner", "53", "Valinor", "2012", "Elizabeth Dean Lail", "John Garfield", "Peter Finch", "Jerry Ekandjo", "Guant\u00e1namo Bay in Cuba", "775 rooms", "north", "23 %", "Joie de vivre", "Roanoke", "self - titled album", "Central Germany", "King Saud University", "Sherwood Forest", "southeastern coast", "Orlando", "epidemiology", "Kyrie Irving", "spinal cord", "Paul Hogan", "fifty small, white, five - pointed stars arranged in nine offset horizontal rows", "1996", "Emma Watson", "in capillaries, alveoli, glomeruli, outer layer of skin", "September 6, 2019", "two occasions", "Shenzi", "9, 1945", "3000 BC", "chalkidice", "1976", "on the vaginal floor", "October 2008", "Broken Hill and Sydney", "artes liberales", "The Hunger Games : Mockingjay -- Part 2 ( 2015 )", "1773", "Escherichia coli", "5 : 7 -- 8", "radioisotope thermoelectric generator", "Carol Worthington", "15 \u00b0, 90 \u00b0, 180 \u00b0, and 270 \u00b0", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "2014", "st ives", "Dorset", "pressure", "Frederick Martin", "Franz Ferdinand", "authoritarian tendencies", "st ives", "tripplehorn,", "How I Met Your Mother", "Portugal", "grow old", "Pauline Wayne"], "metric_results": {"EM": 0.5, "QA-F1": 0.6021334972634651}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 0.06896551724137931, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5161290322580644, 0.4347826086956522, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-10428", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-1180", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-3827", "mrqa_newsqa-validation-1832", "mrqa_searchqa-validation-7864"], "SR": 0.5, "CSR": 0.5652901785714286, "EFR": 0.875, "Overall": 0.7089174107142858}, {"timecode": 28, "before_eval_results": {"predictions": ["the father of the house when in his home", "Welsh", "data link", "blue", "Mead", "Vietnam", "John Robert Parker Ravenscroft", "Moscow", "insect", "the Soviet Union", "St. Petersburg", "malaria", "1992", "Kent", "Arthur, Prince of Wales", "Israel", "butterflies", "New Jersey", "the Philippines", "terrorist group's cause", "the number thirteen", "Aquae Sulis", "Eric Coates", "executive", "to make wrinkles in one's face,", "Brothers In Arms", "northern North America", "Aberystwyth", "Eric Morley", "Saskatchewan", "Mickey Spillane", "Erik Aunapuu", "Frank Sinatra", "Alberto Salazar", "the Washington Post", "Niger", "the Lone Gunmen", "Ivan Owen", "piano", "a fish-tailed sea-god", "Addis Ababa", "pascal", "heart", "Nova Scotia", "\u201clone wolf\u201d", "Len Hutton", "Nigeria", "Dead Sea", "40", "Gibraltar territory currently contains an 800 m long section of the isthmus that links the Rock with mainland Spain", "81", "penultima", "Gerald Ford", "Spanish missionaries", "William Wyler", "Nardwuar the Human Serviette", "1887", "Portal", "on various military check posts in Pakistan's border with Afghanistan", "a plaque at the home of his great-grandfather and by making Ali the first honorary \"freeman\" of the town.", "Phillip A. Myers", "Priscilla Beaulieu", "Cold Mountain", "Deadwood"], "metric_results": {"EM": 0.5, "QA-F1": 0.543969931722689}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, true, false], "QA-F1": [0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.125, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2318", "mrqa_squad-validation-4968", "mrqa_triviaqa-validation-5563", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-2307", "mrqa_triviaqa-validation-7723", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-4515", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-836", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-1782", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3536", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-396", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-3924", "mrqa_naturalquestions-validation-3348", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-388", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-3320", "mrqa_searchqa-validation-1846", "mrqa_searchqa-validation-2066"], "SR": 0.5, "CSR": 0.5630387931034483, "EFR": 0.96875, "Overall": 0.7272171336206897}, {"timecode": 29, "before_eval_results": {"predictions": ["Albert C. Outler", "Jakaya Kikwete", "Drogo", "times", "pemberley", "Australia", "chipmunk", "WWF", "scud", "africa", "Labrador Retriever", "Rio Grande", "Carrie", "times", "Poland", "colewine", "Brooklyn", "Nero", "proverbs", "copenhagen", "PJ Harvey", "Gryffindor", "arthur", "The French Connection", "Thundercats", "Stanley", "Rapa Nui", "copenhagen", "baku", "piero di cosimo", "Oliver", "Hinduism", "soles", "Alanis Morissette", "high sewing", "Herbert Henry Asquith,", "sugar", "index fingers", "ontario", "tennis", "tractors", "50", "function", "sh Ontars Sister", "purple rain", "Fenn Street School", "Fenella Fielding", "niner", "cox", "Jean-Paul Sartre", "temperature", "couscous", "March 31, 2017", "1987", "reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "Dutch", "1981", "Humberside", "the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", ", have received medication \"because of combative behavior with the imminent risk of danger to others and/or self,\"", "three full-length animated films.", "Jefferson", "tap", "copcorn"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5784420289855072}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.08695652173913043, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7292", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-860", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-7733", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-2373", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-4301", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-2997", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-124", "mrqa_newsqa-validation-2618", "mrqa_searchqa-validation-10920", "mrqa_searchqa-validation-15291"], "SR": 0.515625, "CSR": 0.5614583333333334, "EFR": 1.0, "Overall": 0.7331510416666667}, {"timecode": 30, "UKR": 0.751953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1070", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1132", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-1404", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1537", "mrqa_hotpotqa-validation-1556", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2354", "mrqa_hotpotqa-validation-2403", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2625", "mrqa_hotpotqa-validation-2642", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2933", "mrqa_hotpotqa-validation-3081", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3117", "mrqa_hotpotqa-validation-3149", "mrqa_hotpotqa-validation-3190", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3330", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3410", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3616", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-371", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-394", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-4195", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4425", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-478", "mrqa_hotpotqa-validation-4799", "mrqa_hotpotqa-validation-4861", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5184", "mrqa_hotpotqa-validation-5368", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5418", "mrqa_hotpotqa-validation-5476", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5583", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-998", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10113", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2701", "mrqa_naturalquestions-validation-2739", "mrqa_naturalquestions-validation-2839", "mrqa_naturalquestions-validation-2891", "mrqa_naturalquestions-validation-3018", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-321", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3548", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-379", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4094", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-438", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4924", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6358", "mrqa_naturalquestions-validation-6414", "mrqa_naturalquestions-validation-6445", "mrqa_naturalquestions-validation-6749", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7261", "mrqa_naturalquestions-validation-7308", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-7837", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8036", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8419", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1168", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1466", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1787", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1972", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2364", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2618", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-3116", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3307", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-343", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3591", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-652", "mrqa_newsqa-validation-766", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-913", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-981", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-11267", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-12758", "mrqa_searchqa-validation-12953", "mrqa_searchqa-validation-13057", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-15258", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-15517", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16626", "mrqa_searchqa-validation-16775", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3471", "mrqa_searchqa-validation-3563", "mrqa_searchqa-validation-357", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4216", "mrqa_searchqa-validation-4527", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5986", "mrqa_searchqa-validation-6080", "mrqa_searchqa-validation-630", "mrqa_searchqa-validation-6392", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6902", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7807", "mrqa_searchqa-validation-7928", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-9015", "mrqa_searchqa-validation-9022", "mrqa_searchqa-validation-9310", "mrqa_squad-validation-10141", "mrqa_squad-validation-10192", "mrqa_squad-validation-10207", "mrqa_squad-validation-10223", "mrqa_squad-validation-10242", "mrqa_squad-validation-10341", "mrqa_squad-validation-10399", "mrqa_squad-validation-10409", "mrqa_squad-validation-1131", "mrqa_squad-validation-1272", "mrqa_squad-validation-1333", "mrqa_squad-validation-1360", "mrqa_squad-validation-1492", "mrqa_squad-validation-1551", "mrqa_squad-validation-1639", "mrqa_squad-validation-1641", "mrqa_squad-validation-1719", "mrqa_squad-validation-1720", "mrqa_squad-validation-1732", "mrqa_squad-validation-1830", "mrqa_squad-validation-2003", "mrqa_squad-validation-205", "mrqa_squad-validation-2093", "mrqa_squad-validation-2118", "mrqa_squad-validation-221", "mrqa_squad-validation-2248", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2565", "mrqa_squad-validation-2565", "mrqa_squad-validation-2605", "mrqa_squad-validation-2849", "mrqa_squad-validation-2881", "mrqa_squad-validation-291", "mrqa_squad-validation-292", "mrqa_squad-validation-2951", "mrqa_squad-validation-2966", "mrqa_squad-validation-3078", "mrqa_squad-validation-3117", "mrqa_squad-validation-3119", "mrqa_squad-validation-3141", "mrqa_squad-validation-3205", "mrqa_squad-validation-3270", "mrqa_squad-validation-3385", "mrqa_squad-validation-3474", "mrqa_squad-validation-348", "mrqa_squad-validation-3480", "mrqa_squad-validation-3511", "mrqa_squad-validation-3538", "mrqa_squad-validation-3555", "mrqa_squad-validation-3572", "mrqa_squad-validation-3715", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3803", "mrqa_squad-validation-3841", "mrqa_squad-validation-3885", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4008", "mrqa_squad-validation-4121", "mrqa_squad-validation-4155", "mrqa_squad-validation-4166", "mrqa_squad-validation-423", "mrqa_squad-validation-4385", "mrqa_squad-validation-4433", "mrqa_squad-validation-4440", "mrqa_squad-validation-449", "mrqa_squad-validation-4567", "mrqa_squad-validation-4621", "mrqa_squad-validation-4648", "mrqa_squad-validation-4655", "mrqa_squad-validation-4673", "mrqa_squad-validation-4730", "mrqa_squad-validation-4739", "mrqa_squad-validation-4764", "mrqa_squad-validation-4806", "mrqa_squad-validation-4849", "mrqa_squad-validation-4927", "mrqa_squad-validation-5023", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5198", "mrqa_squad-validation-5234", "mrqa_squad-validation-5265", "mrqa_squad-validation-5331", "mrqa_squad-validation-5402", "mrqa_squad-validation-5506", "mrqa_squad-validation-5528", "mrqa_squad-validation-5533", "mrqa_squad-validation-5534", "mrqa_squad-validation-5535", "mrqa_squad-validation-5590", "mrqa_squad-validation-5640", "mrqa_squad-validation-5806", "mrqa_squad-validation-5843", "mrqa_squad-validation-5986", "mrqa_squad-validation-6058", "mrqa_squad-validation-6073", "mrqa_squad-validation-611", "mrqa_squad-validation-6205", "mrqa_squad-validation-632", "mrqa_squad-validation-6402", "mrqa_squad-validation-646", "mrqa_squad-validation-6508", "mrqa_squad-validation-6514", "mrqa_squad-validation-6531", "mrqa_squad-validation-6614", "mrqa_squad-validation-6640", "mrqa_squad-validation-6651", "mrqa_squad-validation-6677", "mrqa_squad-validation-668", "mrqa_squad-validation-6727", "mrqa_squad-validation-6894", "mrqa_squad-validation-6956", "mrqa_squad-validation-7029", "mrqa_squad-validation-7366", "mrqa_squad-validation-7369", "mrqa_squad-validation-7435", "mrqa_squad-validation-7561", "mrqa_squad-validation-7569", "mrqa_squad-validation-7658", "mrqa_squad-validation-7700", "mrqa_squad-validation-7740", "mrqa_squad-validation-7831", "mrqa_squad-validation-7832", "mrqa_squad-validation-7910", "mrqa_squad-validation-7975", "mrqa_squad-validation-8144", "mrqa_squad-validation-8346", "mrqa_squad-validation-8354", "mrqa_squad-validation-8358", "mrqa_squad-validation-839", "mrqa_squad-validation-848", "mrqa_squad-validation-8578", "mrqa_squad-validation-8587", "mrqa_squad-validation-8600", "mrqa_squad-validation-8646", "mrqa_squad-validation-8656", "mrqa_squad-validation-8687", "mrqa_squad-validation-8864", "mrqa_squad-validation-8884", "mrqa_squad-validation-8905", "mrqa_squad-validation-8908", "mrqa_squad-validation-901", "mrqa_squad-validation-9017", "mrqa_squad-validation-915", "mrqa_squad-validation-9236", "mrqa_squad-validation-9285", "mrqa_squad-validation-9333", "mrqa_squad-validation-939", "mrqa_squad-validation-9454", "mrqa_squad-validation-9590", "mrqa_squad-validation-9602", "mrqa_squad-validation-9689", "mrqa_squad-validation-9813", "mrqa_squad-validation-9878", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1021", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-1621", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1791", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2293", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2480", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2601", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2789", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3512", "mrqa_triviaqa-validation-3595", "mrqa_triviaqa-validation-3616", "mrqa_triviaqa-validation-3665", "mrqa_triviaqa-validation-3716", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3887", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4717", "mrqa_triviaqa-validation-4743", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4858", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5617", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5954", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6086", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6488", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6930", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7104", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7613", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-884"], "OKR": 0.84375, "KG": 0.42734375, "before_eval_results": {"predictions": ["cellular respiration", "The Greens", "the most cost efficient bidder", "Prussian", "850 m", "Armani, Esprit and Volvo", "three", "a region of Vietnam north of Hanoi that served as the Viet Minh's base of support during the First Indochina War", "Stephen Mangan", "saloon", "kWPW (107.9 FM, \"Power 108\")", "vixen", "Adult Swim", "Plantation", "Tropical Storm Ann", "1974", "plasma", "March 17, 2015", "American", "1958", "China Airlines", "Wayne County", "Keeper of the Great Seal of Scotland", "Dennis Kux", "Greek-American", "fitznil Joshi", "2000", "Beauty and the Beast", "137th", "half of the Nobel Prize", "Nayvadius DeMun Wilburn", "1956", "Francis Nethersole", "hiphop", "47,818", "Salisbury", "Lakshmibai", "Tony Aloupis", "sarod", "Anne Arundel County", "Austria wien", "midnight", "Robert A. Iger", "Netherlands", "Dr. Alberto Taquini", "1972", "Terry the Tomboy", "Gracie Mansion", "Parlophone Records", "R-8 Human Rhythm Composer", "obergruppenf\u00fchrer", "World War I", "714", "Sandy Knox and Billy Stritch", "Sunni Muslim family", "mona Lisa", "eight", "ardeche department", "an open window", "staff sergeant", "\"His treatment met the legal definition of torture,", "trans fats", "Anna Mary Robertson", "Vienna"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5817460317460318}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8333333333333334, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3524", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1083", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-1122", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-1370", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-4313", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-2529", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-276", "mrqa_naturalquestions-validation-3672", "mrqa_triviaqa-validation-3459", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-4709", "mrqa_newsqa-validation-3819", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-15972"], "SR": 0.484375, "CSR": 0.5589717741935484, "EFR": 1.0, "Overall": 0.7164037298387097}, {"timecode": 31, "before_eval_results": {"predictions": ["the treatment", "phagocytes", "Finland", "Gulf of Aden", "Natty Bumppo", "San Francisco", "Amsterdam", "cellulose", "moles", "Ryan O' Neal", "Sicily", "Howard Keel", "Lilo & Stitch franchise", "Charlie henderson", "Sweet Home Alabama", "alopecia universalis", "Quin Ivy", "The Boychick", "Poem Hunter", "Man V Food", "1780s", "Kajagoogoo", "George Fox", "Croatian", "Manchester City", "mike henderson", "beer", "Esau", "South Africa", "Fidelio", "Hep Stars", "Some Like It Hot", "mercury", "Cleopatra Selene", "fro-taj", "Enrico Caruso", "was Hitler right to", "hydrogen", "nitric acid", "Tasmania", "flesh", "Mille Miglia", "tiger", "rhododendron", "Uranus", "Utrecht", "mrs henderson", "a wedge of hard cheese", "stenodattilografo/a", "caliper", "arts", "Adolf Hitler", "right", "March 27, 2017", "Black Mesa Research Facility", "4,613", "Swiss Super League", "Benj Pasek and Paul", "jenny Sanford", "the sight of celebrity pontificating about the plight of the environment", "innovative, exciting skyscrapers", "Chicago", "The Planets", "mpping Pad Refills"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5651041666666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.0, 1.0, 0.8571428571428571, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1242", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-1033", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-3257", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-7201", "mrqa_triviaqa-validation-6723", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-2298", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-7145", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-10606", "mrqa_hotpotqa-validation-1690", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-3", "mrqa_searchqa-validation-199", "mrqa_searchqa-validation-10166"], "SR": 0.484375, "CSR": 0.556640625, "EFR": 0.9393939393939394, "Overall": 0.7038162878787879}, {"timecode": 32, "before_eval_results": {"predictions": ["K-9 and Company", "Thomas Sowell", "wannabe", "france", "pangrams", "October 31", "wales", "Leicester", "snakes", "joey", "Lisieux", "1929", "hypopituitarism", "February", "trumpet", "Gloucestershire", "Jupiter Mining Corporation", "John Maynard Keynes", "come Find Yourself", "the Hollywood elite", "the houseboat", "Adriatic Sea", "hitler", "fife", "Goran Ivanisevic", "Francis Drake", "Wikia", "baku", "Truro", "frasier", "tundras tundra", "Madness", "Barings Bank", "Anne Boleyn", "the House Un-American Activities Committee", "Ken Norton", "Yann Martel", "cabbage", "The Best of John Denver", "on Fleet Street", "hitler", "one-third", "france", "hitler", "sorrento", "King Edward III", "Bill bryson", "The American Tobacco Company", "Triumph and Disaster", "france", "Norman Mailer", "\"major science finding from the agency's ongoing exploration of Mars.\"", "between 1923 and 1925", "Pakistan, India, and Bangladesh", "alveoli", "sarod", "Laban Movement Analysis", "James I", "There were no reports of ground strikes or interference with aircraft in flight,", "on the bench", "Bright Automotive", "Will & Grace", "prehensile", "the English Channel"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5877083333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.08, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1725", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1781", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2643", "mrqa_triviaqa-validation-1618", "mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-7105", "mrqa_triviaqa-validation-2479", "mrqa_triviaqa-validation-1812", "mrqa_triviaqa-validation-7073", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-5902", "mrqa_triviaqa-validation-1770", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-10495", "mrqa_hotpotqa-validation-203", "mrqa_newsqa-validation-3994"], "SR": 0.546875, "CSR": 0.556344696969697, "EFR": 0.9655172413793104, "Overall": 0.7089817626698014}, {"timecode": 33, "before_eval_results": {"predictions": ["the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea,", "1580s", "intelligence officer", "France", "Ted Heath", "wuthering Heights", "Portugal", "will never play for another Premier League club", "vice-admiral", "venus", "smallbill", "benfica", "six", "1984", "11", "Buzz Aldrin", "mrs williams", "Archie Shuttleworth", "venus williams", "IT Crowd", "bitter", "brown wildebeest", "pokemon", "cold Comfort Farm", "Isar", "Ruth Rendell", "wales", "Indian Love Call", "eddie", "brouilly", "John Constable", "sheep", "linseed", "carmen Miranda", "nottingham", "jump jump", "1882", "joseph lachey", "powys", "sashimi", "sea otter", "dot-com", "Tunisia", "venus", "pain in the ear", "scar", "Vladimir Putin", "croquet", "low-cost", "Canary Wharf", "emeralds", "wigan Warriors", "Taittiriya Samhita", "product-market fit", "Secretary of State", "Jos\u00e9 bispo Clementino dos Santos", "1998", "Umberto II of Italy", "Spc. Megan Lynn Touma,", "suppress the memories and to live as normal a life as possible", "40", "intelligent design", "charlie williams", "copper"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5301339285714286}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true], "QA-F1": [0.47619047619047616, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-2199", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-3764", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-174", "mrqa_triviaqa-validation-27", "mrqa_triviaqa-validation-111", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-5250", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3957", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7609", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-7779", "mrqa_triviaqa-validation-984", "mrqa_triviaqa-validation-4808", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-4146", "mrqa_newsqa-validation-3534", "mrqa_searchqa-validation-11614"], "SR": 0.484375, "CSR": 0.5542279411764706, "EFR": 1.0, "Overall": 0.7154549632352941}, {"timecode": 34, "before_eval_results": {"predictions": ["Central business districts", "1973", "Brittany", "3rd Sunday", "Kenya", "September", "Japanese", "Albert Camus", "james Garner", "Martin Luther King", "petticoat", "indus", "Puerto Rico", "180 degrees", "Charles Taylor", "conchita wurst", "Ireland", "The Savoy", "niki lauda", "Finland", "india", "Japan", "Massachusetts", "boutros Ghali", "the Rolling Stones", "Uranus", "mole", "Aleister Crowley", "Greece", "Spain", "mumbai", "kitsune", "frottage", "collage", "cows", "eriksson", "jessica tromp", "Richard Wilson", "Dean Martin", "Emily Davison", "Loki", "penny", "eddie kirk burrell", "ethel Skinner", "Peter Blake", "ghee", "cleopatra Selene", "\"Statute of Rageman\"", "commitment", "trina gulliver", "S\u00e8vres", "Procol Harum", "Victory gardens", "arts", "Mark 3 : 13 -- 19, Matthew 10 : 1 -- 4, Luke 6 : 12 -- 16", "AT&T", "1998", "Dachshunds", "\"totaled,\"", "Iran of trying to build nuclear bombs", "\"It was quite surprising to learn of the request,\"", "nicholas", "\"reshit\"", "anemia"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6354166666666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-805", "mrqa_triviaqa-validation-7088", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4980", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-6496", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-7019", "mrqa_triviaqa-validation-3825", "mrqa_triviaqa-validation-2249", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-3770", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-1948", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-16252"], "SR": 0.609375, "CSR": 0.5558035714285714, "EFR": 1.0, "Overall": 0.7157700892857142}, {"timecode": 35, "before_eval_results": {"predictions": ["Seven Days to the River Rhine", "Seerhein", "2,579 steps", "to collect menstrual flow", "New Jersey Devils of the National Hockey League ( NHL )", "the Near East", "on BBC One on Saturday evenings", "jessica Newton", "Hook", "Brian Steele", "nearby objects show a larger parallax than farther objects when observed from different positions", "four", "Leonard Bernstein", "Christopher Columbus", "9 February 2018", "the 1970s", "the long - running Harry Potter film series", "the 18th century", "her abusive husband", "moral tale", "two - third of the total members present", "federal republic", "July 14, 1969", "Frank Langella", "Tennesseeitans", "a container often made of papier - m\u00e2ch\u00e9, pottery, or cloth", "April 26, 2005", "Castleford is a town in the metropolitan borough of Wakefield, West Yorkshire, England", "Action Jackson", "New England Patriots", "the beginning of the Industrial Revolution", "Patrick Warburton", "when the cell is undergoing the metaphase of cell division", "`` One Son ''", "Mara jr", "revenge", "20 year - old Kyla Coleman from Lacey, Washington", "Nathan Hale", "Rachel Kelly Tucker", "far lesser degree by blood capillaries extending to the outer layers of the dermis", "during World War II", "Joe Pizzulo and Leeza Miller", "1.5 times the Schwarzschild radius", "David Tennant", "Kelly Reno", "Brooke Wexler", "over $1.84 billion", "patronymic surname", "Leonard Nimoy", "Billy Hill", "2005", "Pangaea or Pangea", "Kent", "petula Clark", "arm", "Tufts University", "2002", "May 4, 2004", "work together to stabilize Somalia and cooperate in security and military operations.", "Communist Party of Nepal (Unified Marxist-Leninist)", "Robert Barnett", "macGyver", "Daniel Boone", "aiken"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6597054535376904}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 0.2857142857142857, 0.5, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.1904761904761905, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 0.0, 0.5, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.4, 0.0, 0.21052631578947367, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-1618", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8338", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-5435", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-10614", "mrqa_naturalquestions-validation-6190", "mrqa_triviaqa-validation-702", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-1567"], "SR": 0.53125, "CSR": 0.5551215277777778, "EFR": 0.9333333333333333, "Overall": 0.7023003472222222}, {"timecode": 36, "before_eval_results": {"predictions": ["$5,000,000", "The Handmaid's Tale", "Austrian", "Cardinal Reginald Pole", "1912", "102,984", "Emmanuel Ofosu Yeboah", "Clarence Nash", "Bulgarian", "Macomb County", "Dusty Dvoracek", "vice-president", "1972", "Disney California Adventure", "Indiana", "Travis County", "1996 PGA Championship", "orange", "Regional League North", "a constructor in Formula One, Formula 2 and Formula 5000 from 1970 to 1978", "ragby", "life insurance", "Ukrainian", "Cape Cod", "actress and model", "BBC Focus", "George Clooney", "Joe Scarborough", "Tottenham ( ) or Spurs", "the attack on Pearl Harbor", "Alemannic", "Vienna", "Jesper Myrfors", "Paper", "Amway", "Ogallala Aquifer", "\"The Heirs\"", "News Corporation", "taking a bet from a gambler", "12", "Rockbridge County", "sexual activity", "English", "Gatwick", "Carlos Santana", "two Nobel Peace Prizes", "Dutch", "military", "Cannes Film Festival", "Aiden English", "actor", "Bill Patriots", "Achal Kumar Jyoti", "a set of connected behaviors, rights, obligations, beliefs, and norms as conceptualized by people in a social situation", "jazz", "10", "placebo", "a federal judge in Mississippi", "when the economy turns unfriendly,", "\"It was perfect work, ready to go for the stimulus package,\"", "wounds", "pinnipeds", "Brazil", "Harry S Truman"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6125457875457876}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.3076923076923077, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.09523809523809525, 0.0, 0.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-5164", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2382", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-3145", "mrqa_naturalquestions-validation-3076", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-1124", "mrqa_newsqa-validation-2449", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-13135"], "SR": 0.53125, "CSR": 0.5544763513513513, "EFR": 1.0, "Overall": 0.7155046452702702}, {"timecode": 37, "before_eval_results": {"predictions": ["Robert Lane and Benjamin Vail", "woodpecker", "20", "cannibalism", "College of William and Mary", "China", "diamond", "Tupac Shakur", "Marsha Hunt", "chinook", "tsar", "Sarah Hughes, the runner-up at the U.S. championships in 2007", "Sonnets to Orpheus", "Caesar salad", "sheath", "Son of Sam Killer", "jeopardy/2762_Qs.txt at master", "pueblo", "Steelhead Marine", "licorice stick", "A Moon for the Misbegotten", "Superman", "a place name", "Dublin", "mathematical research", "John Andr", "Suzuki Grand Vitara", "Yogi Bear", "General Emile Lahoud", "a Redbud Tree", "Christopher", "Stripes", "Little Red Riding Hood", "a spectrophotometer", "Daryl Hall and John Oates", "Cherokee", "The cause of the French people", "Gettysburg National Military Park", "Morocco", "Jackie Kennedy", "President Grover Cleveland", "Los Angeles", "The Picture of Dorian Gray", "Arkansas", "Helen of Troy", "the Board of Civil Procedure", "Faust", "Thomas Jefferson", "violins", "ethanol", "Adam Smith", "famous figures as Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "the church at Philippi", "Athens became a breeding ground for disease and many citizens died including Pericles, his wife, and his sons Paralus and Xanthippus", "red", "Cole Porter", "a non-speaking character", "New York City", "Westminster system", "Phil Collins", "nuclear", "the head of the country's antiquities council", "\"The Book\"", "boxing"], "metric_results": {"EM": 0.3125, "QA-F1": 0.42300099206349207}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.5, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.4, 0.4, 0.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.08000000000000002, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.06666666666666667]}}, "before_error_ids": ["mrqa_squad-validation-1295", "mrqa_searchqa-validation-11502", "mrqa_searchqa-validation-1092", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-12401", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-11435", "mrqa_searchqa-validation-2181", "mrqa_searchqa-validation-9158", "mrqa_searchqa-validation-7563", "mrqa_searchqa-validation-6175", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-2565", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-4949", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-8404", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-10312", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-14370", "mrqa_searchqa-validation-235", "mrqa_searchqa-validation-13123", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-2068", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6949", "mrqa_searchqa-validation-822", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-10156", "mrqa_triviaqa-validation-7414", "mrqa_hotpotqa-validation-4081", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2534", "mrqa_naturalquestions-validation-9459"], "SR": 0.3125, "CSR": 0.548108552631579, "EFR": 1.0, "Overall": 0.7142310855263158}, {"timecode": 38, "before_eval_results": {"predictions": ["1954", "Zion", "cloves", "The Apartment", "Abraham Lincoln", "St Martin", "a physician or surgeon", "Greenpeace", "a golden eagle", "National Ice Cream Day", "a canton", "a canton", "Caracas", "Memphis", "Joseph Conrad", "horseplayers", "Faneuil Hall", "Babe", "Cajun", "balsa", "Anwar Sadat", "prostitutes", "The Apartment", "New Orleans Saints", "Princess Diana", "Tasmania", "the Taj Mahal", "cobalt", "Louisa May Alcott", "Settlers in Canada", "Spider-Man 3", "lubricants", "glucose", "The Apartment", "Sony", "Van Helsing", "Hugh Grant", "the Great Wall of China", "hand", "Hormel Foods", "UTC-08:00", "Clara Barton", "Kauai", "Esophagus", "Joseph", "Otsego County, New York", "Fred G. Sanford", "Colombia", "Thomas Paine", "Venezuela", "canticle", "Donna", "The claims process starts at noon Eastern Time and ends 24 hours later", "the first stand - alone instant messenger", "Apollon", "Chicago", "Andrew Lloyd Webber", "April 25, 1776", "Tim Howard", "Princess Jessica", "Hutus and Tutsis", "The father of Haleigh", "Monday and Tuesday", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5401323891625616}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4827586206896552]}}, "before_error_ids": ["mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-7187", "mrqa_searchqa-validation-8722", "mrqa_searchqa-validation-7041", "mrqa_searchqa-validation-6833", "mrqa_searchqa-validation-1238", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-12751", "mrqa_searchqa-validation-9939", "mrqa_searchqa-validation-6793", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-2019", "mrqa_searchqa-validation-3861", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16089", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-14763", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-1716", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1534", "mrqa_searchqa-validation-14173", "mrqa_naturalquestions-validation-215", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-4264", "mrqa_hotpotqa-validation-3786", "mrqa_hotpotqa-validation-5676", "mrqa_newsqa-validation-3774", "mrqa_naturalquestions-validation-4021"], "SR": 0.46875, "CSR": 0.546073717948718, "EFR": 0.9705882352941176, "Overall": 0.7079417656485671}, {"timecode": 39, "before_eval_results": {"predictions": ["movements of nature", "retirement", "Hill Street Blues", "bacteria", "Ross Perot", "(Little Caesar)", "a crayon", "fracture", "Lance Armstrong", "Kung Fu", "elbow", "sienna", "Hindu", "the Village Voice", "Nacho Libre", "The Beatles", "Cygnus", "La Mosquito Coast", "nougat", "a Scotch egg", "the Manhattan Project", "the Eiffel Tower", "Roger Federer", "sculpere", "arteries", "a cheddar", "the Anglo-Iranian (formerly Anglo-Persian) Oil Company", "Florida", "The Virgin Spring", "Fyodor", "the Nome Nugget", "(Arthur) Strutt", "Atlanta", "an egg", "Zorro", "assume", "Jack Sprat", "offbeat", "uranium", "Antigone", "Epicure", "( Christopher) Paolini", "Petruchio", "(George) Sand", "glaciers", "Dick Gephardt", "get better grades", "Lord Louis Mountbatten", "Master of Fine Arts", "the American Idol", "Robert Peary", "down to the ground", "Hellenism", "James W. Marshall", "Ares", "linseed", "John McClane", "the Soldier Bear", "Montreal, Quebec", "Yellowcraig", "\"Buying a Prius shows the world that you love the environment and hate using fuel.\"", "58 minutes", "\"What she's doing is putting a personal and human face on the issue", "$31,000"], "metric_results": {"EM": 0.484375, "QA-F1": 0.53125}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-7265", "mrqa_searchqa-validation-16284", "mrqa_searchqa-validation-2577", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15995", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-12788", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-7469", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-9039", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-10806", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-262", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-4826", "mrqa_naturalquestions-validation-6856", "mrqa_triviaqa-validation-6258", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-825", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-1152"], "SR": 0.484375, "CSR": 0.54453125, "EFR": 1.0, "Overall": 0.713515625}, {"timecode": 40, "UKR": 0.728515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2247", "mrqa_hotpotqa-validation-2359", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2621", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2778", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-2920", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3375", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3586", "mrqa_hotpotqa-validation-3674", "mrqa_hotpotqa-validation-3680", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-5181", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5406", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10156", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10408", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-1214", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-421", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-5039", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-514", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-5999", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6633", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9730", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1316", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1587", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2611", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4119", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12707", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-13508", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-1643", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16846", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-2066", "mrqa_searchqa-validation-222", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2829", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-394", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4753", "mrqa_searchqa-validation-4829", "mrqa_searchqa-validation-5261", "mrqa_searchqa-validation-5434", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-6339", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6707", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7241", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9173", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-9717", "mrqa_searchqa-validation-9908", "mrqa_searchqa-validation-9913", "mrqa_squad-validation-10088", "mrqa_squad-validation-10388", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1248", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1779", "mrqa_squad-validation-181", "mrqa_squad-validation-1830", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2295", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2608", "mrqa_squad-validation-2831", "mrqa_squad-validation-2868", "mrqa_squad-validation-2881", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3104", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3331", "mrqa_squad-validation-349", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4046", "mrqa_squad-validation-4155", "mrqa_squad-validation-4331", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4634", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5248", "mrqa_squad-validation-5307", "mrqa_squad-validation-5389", "mrqa_squad-validation-5469", "mrqa_squad-validation-5506", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5728", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6024", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6224", "mrqa_squad-validation-6272", "mrqa_squad-validation-6279", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6702", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7211", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-801", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-9140", "mrqa_squad-validation-915", "mrqa_squad-validation-9285", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9525", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_squad-validation-9977", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1342", "mrqa_triviaqa-validation-1425", "mrqa_triviaqa-validation-1432", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2193", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-272", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3325", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4593", "mrqa_triviaqa-validation-4620", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5729", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-658", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6888", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.841796875, "KG": 0.4953125, "before_eval_results": {"predictions": ["toward the center of the curving path", "the Liberty Bell", "Hinduism", "turtle", "Donkey", "Jane Eyre", "Aiden", "Alles gut", "Queen Victoria", "Montmartre", "The Dying Swan", "Elvis Presley", "a protractor", "voter registration", "The Kite Runner", "white granite", "Islamabad", "horseshoe crabs", "Stephen Crane", "trespassing", "Jack Dempsey", "beheading", "Val Kilmer", "Pakistan", "Milwaukee", "a kiwi", "Pop-Tarts", "sugar", "Enrico Fermi", "Tiger Woods", "the Madding Crowd", "The Broadcaster's Calendar of Important Regulatory Dates", "Grace Kelly", "an arm", "Bilbo", "Oliver Wendell Holmes", "the Constitution", "The Church of Jesus Christ", "a photon", "Maria Montessori", "an orchid", "an arm", "the sun", "Michelangelo", "Spain", "ale", "Superman", "each clause", "Brazil", "Puget Sound", "phylum", "Yahya Khan", "Qianlong", "In 1038", "Bubba", "Easter Parade", "Thom Yorke", "beer", "Keeper of the Privy Seal of Scotland", "Martin Joseph O'Malley", "1983", "The cause of the child's death will be listed as homicide by undetermined means", "five", "at the Battle of Edgehill"], "metric_results": {"EM": 0.625, "QA-F1": 0.7087053571428572}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_searchqa-validation-5719", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-5559", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-7659", "mrqa_searchqa-validation-12159", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-4898", "mrqa_searchqa-validation-7222", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-5968", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-4248", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-10551", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-4500", "mrqa_newsqa-validation-2627", "mrqa_triviaqa-validation-5693"], "SR": 0.625, "CSR": 0.5464939024390244, "EFR": 1.0, "Overall": 0.7224237804878049}, {"timecode": 41, "before_eval_results": {"predictions": ["metals", "2004", "to capitalize on her publicity", "Karen Gillan", "Moira Kelly", "Nick Kroll", "Albert Einstein", "Miami Heat", "in Poems : Series 1", "transceivers", "tropical and subtropical latitudes from the Red Sea and the east African coast across the Indian Ocean", "200 to 500 mg up to 7 mL", "James Madison", "Tom Brady", "asphyxia", "1947", "Thomas Edison", "Unwinding of DNA at the origin", "to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Flag Day in 1954", "Erica Rivera", "Afghanistan", "Gettysburg College", "the Geography of Oklahoma", "thia Weil", "into the gastrointestinal tract through a series of ducts", "an oxidant, usually atmospheric oxygen", "1 mile ( 1.6 km )", "1871", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "1994", "343 m / s in air", "Presley Smith", "a combination of genetics and the male hormone dihydrotestosterone", "Norman's half - brother", "the types of instruments that are used in data collection", "Tami Lynn", "Christianity", "radians", "1931", "Help!", "May 2010", "24", "August 8, 1945", "Bichu", "16 December 1908", "the Comprehensive Crime Control Act of 1984", "the meridian", "Stephen Stills", "the times sign", "within the chapters as well", "a family tree", "lighting at least as far back as the 7th century", "\"southern sea\"", "Julie Taymor", "an organ", "Tony Aloupis", "the American Civil Liberties Union", "Chevron", "Enchautegui's death", "Pancho Gonzales", "William Henry Harrison", "the AOU Checklist of North", "Britain"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6235873090967838}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.5957446808510638, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.20000000000000004, 0.8125000000000001, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-2578", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-7212", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2650", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6816", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-2671", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9237", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-3294", "mrqa_triviaqa-validation-7595", "mrqa_newsqa-validation-3427", "mrqa_searchqa-validation-2827"], "SR": 0.546875, "CSR": 0.5465029761904762, "EFR": 0.896551724137931, "Overall": 0.7017359400656814}, {"timecode": 42, "before_eval_results": {"predictions": ["high voltage", "water buffalo", "Texas", "Song of Solomon", "israel", "Sweden", "Battlestar Galactica", "Sainte-Marie", "sheep", "Mary Tudor", "Quiz", "a blackbird", "Patty Duke", "Hackman", "Judas Iscariot", "3,000th", "grow old along with me", "savanna", "Luciano", "freight ton", "Kellogg's", "Fall Guy", "cape", "depth and breadth", "Judy Garland", "crocodiles", "Pakistan", "Bosnia and Herzegovina, FYR", "a baboon", "a young Russian Cardinal", "outta nowhere", "El burlador de Sevilla", "paddle boarding", "Empire State Building", "the League of Nations", "Sally Ride", "bullet proof", "the Spanish Armada", "75% on percentile scale", "Eau de Parfum Spray", "the Civil War", "a planet", "Greek Meatballs", "Holden Caulfield, a smart but troubled teenager", "the Caucasus mountains", "Firebird", "14", "Lecompton, Kansas", "Midnight Cowboy", "Rosetta Stone", "Louisiana", "Malayalam", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "the half - finished puma woman rips free of her restraints and escapes from the lab", "Arabian Gulf", "Ross Kemp", "Marine One", "Juan Manuel Mata Garc\u00eda (] ; born 28 April 1988) is a Spanish professional footballer who plays as a midfielder for English club Manchester United and the Spain national team.", "Vietnam War", "Taylor Swift", "Cain", "raping her in a Milledgeville, Georgia, bar during a night of drinking in March.", "on-loan David Beckham", "the killing of a 15-year-old boy"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5740638990638991}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14814814814814817, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-14410", "mrqa_searchqa-validation-9124", "mrqa_searchqa-validation-15998", "mrqa_searchqa-validation-8140", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-8842", "mrqa_searchqa-validation-8429", "mrqa_searchqa-validation-6960", "mrqa_searchqa-validation-4534", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-9724", "mrqa_searchqa-validation-6562", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-2141", "mrqa_searchqa-validation-933", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-11902", "mrqa_searchqa-validation-1970", "mrqa_searchqa-validation-9663", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-448", "mrqa_searchqa-validation-10127", "mrqa_searchqa-validation-7377", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-1406", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-894", "mrqa_triviaqa-validation-1411", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-1912"], "SR": 0.421875, "CSR": 0.5436046511627908, "EFR": 1.0, "Overall": 0.721845930232558}, {"timecode": 43, "before_eval_results": {"predictions": ["Southeastern U.S.", "gin", "Leicester", "mile run", "the Jets", "scurvy", "Japan", "Razor", "falconry", "Niger", "Norman Brookes", "Billy Crystal", "Jaipur", "Goran Ivanisevic", "36", "\"The News at Ten\"", "Spain", "Henry Hudson", "bridge", "a raven", "Errors", "Felix", "Louis XV", "Mrs Mainwaring", "Australia", "Robert A. Heinlein", "USS Constitution", "Aug. 24, 1572", "fertilization", "lacquer", "Massachusetts", "Sherlock Holmes", "Delaware", "Olivia Smith", "Costa Concordia", "elliptical", "orange juice", "Jim Morrison", "mrs henderson presents", "graphite", "\"Bash Street Kids", "Mercury", "Ireland", "Gandalf", "Moses Sithole", "Bogart", "Arthur Daley", "a neutron", "turkey", "Eva Marie", "Tigger", "Saint Alphonsa", "Ku - Klip", "March 15, 1945", "Portsea", "1861", "McLaren", "\" Maria\"", "Diego Milito", "\"I'm certainly not nearly as good of a speaker as he is.\"", "Zyrtec", "\" Dustbin lids\"", "diameter", "Leo Frank"], "metric_results": {"EM": 0.53125, "QA-F1": 0.578125}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4019", "mrqa_triviaqa-validation-2326", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-2956", "mrqa_triviaqa-validation-3076", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-7617", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-6266", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-5446", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-2751", "mrqa_searchqa-validation-14791"], "SR": 0.53125, "CSR": 0.5433238636363636, "EFR": 1.0, "Overall": 0.7217897727272727}, {"timecode": 44, "before_eval_results": {"predictions": ["the inclusion of Lake Constance and the Alpine Rhine", "lizabeth McKenna", "the local Lenape to be on good terms with the Native Americans and ensure peace for his colony", "Xanthippus", "when the forward reaction proceeds at the same rate as the reverse reaction", "Lucknow", "Kristy Swanson", "Hendersonville, North Carolina", "pH 7 ( 25 \u00b0 C )", "Abraham", "Kelly Osbourne", "1773", "the final episode of the series", "a god of the Ammonites", "the Han", "just after the Super Bowl", "the 1980s", "31 October 1972", "The Italian Agostino Bassi", "graduation with a Bachelor of Medicine, Bachelor of surgery degree", "won by Germany, who beat Argentina 1 -- 0 after extra time", "Real Madrid", "LED illuminated display", "BC Jean", "about 24 hours", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "2015", "No. 1 seed Virginia", "Tommy Shaw", "November 2016", "Thomas Alva Edison", "B.R. Ambedkar", "the Indian Olympic Association ( IOA )", "Niles", "Judy Collins", "the Joads, a poor family of tenant farmers driven from their Oklahoma home by drought, economic hardship, agricultural industry changes, and bank foreclosures forcing tenant farmers out of work", "Grand Inquisition", "T'Pau", "Angel Island Immigration Station", "Johannes Gutenberg", "Terry Reid", "the name of a work gang", "Johannes Gutenberg", "Domhnall Gleeson", "Russia", "the Pandavas", "2020 National Football League ( NFL ) season", "1994 season", "a judge who lacks compassion is repeatedly approached by a poor widow, seeking justice", "the church at Philippi", "Chuck Noland", "The Green Mile", "Cal Ripken, Jr.", "1905", "Workers' Party", "Revolver", "140 million", "contraband", "the International Space Station", "750", "lew springsteen", "nantucket", "Microsoft", "love Never Dies"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6506890527950311}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true], "QA-F1": [0.0, 0.5, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.42857142857142855, 1.0, 0.2857142857142857, 0.5714285714285715, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.07142857142857142, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9113", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-935", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-8161", "mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-4592", "mrqa_triviaqa-validation-7676", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-1345", "mrqa_searchqa-validation-7952", "mrqa_searchqa-validation-3760"], "SR": 0.53125, "CSR": 0.5430555555555556, "EFR": 0.8666666666666667, "Overall": 0.6950694444444444}, {"timecode": 45, "before_eval_results": {"predictions": ["westward", "the United States, NATO member states, Russia and India", "Felipe Massa.", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "frees up a place for another non-European Union player in Frank Rijkaard's squad.", "Tibet", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple", "President Bush", "Brad Blauser, center, created the program.", "Sunday", "President Obama", "about 1,300 meters in the Mediterranean Sea", "one", "raping and murdering a woman in Missouri", "Ryder Russell", "13", "his father", "the club's board", "insect stings", "it is provocative action", "\"Disney\"", "Fullerton, California", "Chinese", "40 lash", "The Rev. Alberto Cutie", "ceo Herbert Hainer", "Chris Robinson", "269,000", "surrounding areas of the bustling capital faced further inundation at the next high tide.", "North Korea", "the state's attorney", "was arrested in a federal sting after his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "\"deep sorrow\" at the death of two women killed in a stampede at one of his events in Angola", "two", "Spain", "comments he made after his new boss, golffer Adam Scott, defeated Woods at the Bridgestone Invitational in Ohio in August.", "theft in Switzerland of two paintings by Pablo Picasso, Bjoern Quellenberg, a spokesman for the Kunsthaus, a major art museum in Zurich", "Oxbow, Minnesota", "motor scooter", "Donald Trump", "6,000", "gossip Girl", "the FDA", "Diversity", "650", "Yemen", "South Africa", "Daniel Radcliffe", "President Sheikh Sharif Sheikh Ahmed", "cities throughout Canada", "Florida's Everglades", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "Tyrion", "June 8, 2009", "'Q'", "wolf", "iron lung", "mathematics", "the outdoors", "Mel Blanc", "given mouth", "Colorado", "CIA", "Billy Bob Thornton"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5538939093994241}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5454545454545454, 0.4, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.23529411764705882, 0.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.08, 0.8717948717948718, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-2384", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-3308", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-600", "mrqa_newsqa-validation-1360", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-1581", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-1800", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3910", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-5370", "mrqa_hotpotqa-validation-4796", "mrqa_searchqa-validation-6261"], "SR": 0.453125, "CSR": 0.5411005434782609, "EFR": 1.0, "Overall": 0.7213451086956522}, {"timecode": 46, "before_eval_results": {"predictions": ["every four years", "part husky or other Nordic breed, and possibly part terrier", "Benedict to ordain presbyters / bishops and to exercise general oversight, telling him to `` rebuke with all authority ''", "enterocytes of the duodenal lining", "warmth", "A patent is a set of exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time in exchange for detailed public disclosure of an invention", "during meiosis", "federal government", "The alveolar process ( / \u00e6l\u02c8vi\u02d0\u0259l\u0259r / )", "Katharine Hepburn", "Thorleif Haug", "multiple", "each country provides public healthcare to all UK permanent residents that is free at the point of use, being paid for from general taxation", "Norman Whitfield and Barrett Strong", "to solve its problem of lack of food self - sufficiency", "January 2004", "5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "291", "the French CYCLADES project directed by Louis Pouzin", "when each of the variables is a perfect monotone function of the other", "the Kansas City Chiefs", "in Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "especially in Western cultures", "Husrev Pasha", "2001", "Squamish, British Columbia, Canada", "Mankombu Sambasivan Swaminathan", "either Georgia, New Jersey, and Texas, players must choose, in advance, whether they wish to collect a jackpot prize in cash or annuity", "1962", "prophets and beloved religious leaders", "Best Picture, Best Director for Fincher, Best Actor for Pitt and Best Supporting Actress for Taraji P. Henson", "interstate communications by radio, television, wire, satellite, and cable", "trinitarian formula", "William Chatterton Dix", "as the B - side of the `` Tramp '' single in 1987, and as its own single in 1988", "Stefanie Scott", "Thanos", "in the fovea centralis", "Ephesus", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Ferraro", "seven", "Uralic languages", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired, assumes", "comic", "British and French Canadian fur traders", "Lori McKenna", "2017", "October 27, 1904", "in the mid - to late 1920s", "Firoz Shah Tughlaq", "Runic", "Backgammon", "Malcolm Bradbury", "Wade Boggs", "all-Star Game", "Holberg's comedy \"Den V\u00e6gelsindede\"", "Pakistan", "Malcolm X", "February 12.", "Charles Dickens", "the Montague & the Montagues", "plutonium", "24"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5748569227217019}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.6399999999999999, 0.7272727272727273, 0.6666666666666666, 0.4615384615384615, 0.5714285714285715, 0.0, 0.8, 1.0, 0.0, 0.0, 0.16, 0.0, 0.16666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.7499999999999999, 0.09999999999999999, 1.0, 0.0, 1.0, 0.125, 1.0, 0.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.6122448979591837, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3818", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-1171", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-6363", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-8702", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10509", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-1692", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1308", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-13101"], "SR": 0.4375, "CSR": 0.5388962765957447, "EFR": 0.9444444444444444, "Overall": 0.7097931442080377}, {"timecode": 47, "before_eval_results": {"predictions": ["Max Martin", "1995 Mitsubishi Eclipse", "2005", "Lewis Carroll", "Puerto Rico Electric Power Authority ( PREPA ) -- Spanish : Autoridad de Energ\u00eda El\u00e9ctrica ( AEE )", "Austria - Hungary", "Mace Coronel", "libretto", "Theodore Roosevelt", "The Order of the Phoenix", "1800", "in the dress shop", "As of January 17, 2018", "Miller Lite", "Instagram's own account", "Experimental neuropsychology", "2015", "Selena Gomez", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W", "Christina Aguilera", "during initial entry training", "Prince Akeem Joffer", "1997", "Bonnie Aarons", "1960", "during the summer of 1979", "Part XI of the Indian constitution", "the Constitution of India came into effect on 26 January 1950", "The oldest known recording of the song, under the title `` Rising Sun Blues ''", "halogenated paraffin hydrocarbons", "the 1980s", "MacFarlane", "Frankie Laine's `` I Believe '' in 1953", "1898", "a set of components that included charting, advanced UI, and data services ( Flex Data Services )", "Matt Jones", "the members of the actual club with the parading permit as well as the brass band", "The Confederate States Army ( C.S.A. )", "three", "Olivia", "Zachary John Quinto", "Sanchez Navarro", "Have I Told You Lately", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "John Travolta", "Scheria", "1 October 2006", "2017 Georgia Bulldogs", "Eda Reiss Merin", "Michael Clarke Duncan", "Origination Clause of the United States Constitution", "Gestapo", "Belgium", "heartburn", "from the port of Baltimore west to Sandy Hook", "ballroom expert Mary Murphy", "two", "at a relative's house,", "a sort of robot living inside.", "hardship for terminally ill patients and their caregivers", "The Iliad", "David Cronenberg", "prego", "marillion"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5752709294219445}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.08695652173913042, 0.5882352941176471, 1.0, 0.0, 1.0, 0.14285714285714288, 0.0, 1.0, 0.8, 0.8333333333333333, 1.0, 0.5714285714285715, 0.6666666666666666, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.5, 0.07407407407407408, 0.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9333333333333333, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-9959", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-10381", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-8182", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-2190", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-6782", "mrqa_triviaqa-validation-89", "mrqa_hotpotqa-validation-3255", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-2606", "mrqa_newsqa-validation-588", "mrqa_newsqa-validation-886", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-3520"], "SR": 0.4375, "CSR": 0.5367838541666667, "EFR": 0.9166666666666666, "Overall": 0.7038151041666667}, {"timecode": 48, "before_eval_results": {"predictions": ["Daylight Saving time", "Cygnus", "Constantine", "Lautrec", "Nigeria", "Hawaii", "Jeremiah", "fencing", "Don Knotts", "cinnamon Life", "Copenhagen", "Porgy and Bess", "Dutchman", "Deimos", "Cheers", "Robert Frost", "Geena Davis", "dungeon", "cyclorama", "Laila Ali", "New Zealand", "The Tempest", "San Diego", "Fat man, you shoot a great game of pool", "to show affection", "Led Zeppelin", "the Book of Kells", "tatarabuelo", "nuclear fission", "Heisman", "the Gulf of Tonkin", "Stephen Vincent Bent", "the coelacanth", "Prague", "the Federal Reserve", "coal-fired power plant", "Afghanistan", "The Cheetah", "Ambrose Bierce", "the American Lung Association", "croquet", "Aphrodite", "You get your house back, your wife comes back to life, and you get out of prison.", "Chico Rodriquez", "Budapest", "John Mahoney", "pythons", "Nit-A-Nee", "Charles de Gaulle", "Beverly Cleary", "Afghanistan", "2004", "a recognized group of people who jointly oversee the activities of an organization", "3 / 14 in the month / day format ) since 3, 1, and 4 are the first three significant digits of \u03c0", "10", "The Young Men's Christian Association", "Lutwidge Dodgson", "the theory of direct scattering and inverse scattering", "43rd", "South America", "energy-efficient light-emitting diodes that will illuminate the ball's more than 600 crystals.", "$150 billion over 10 years in clean energy.", "fill a million sandbags and place 700,000 around our city.", "Angel Cabrera"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6119047619047618}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3479", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-4404", "mrqa_searchqa-validation-12001", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-7598", "mrqa_searchqa-validation-8359", "mrqa_searchqa-validation-3900", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-8727", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-12165", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-3066", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4738", "mrqa_hotpotqa-validation-1316", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-675"], "SR": 0.53125, "CSR": 0.5366709183673469, "EFR": 0.9666666666666667, "Overall": 0.7137925170068027}, {"timecode": 49, "before_eval_results": {"predictions": ["William Wyler", "Redford's adopted home state of Utah", "pelvic floor", "Kanawha Rivers", "amino acids glycine and arginine", "an alien mechanoid", "1937", "the 1920s", "931 BCE", "2005", "New York City", "Beorn", "Jonathan Cheban", "1992", "Montreal", "18", "Kristy Swanson", "687 ( Earth ) days", "Camping World Stadium in Orlando", "the Bactrian camel", "poignant anthems of sorrow regarding the environment", "the New York Yankees", "the notable exception of Herbert Hoover", "Phosphorus pentoxide", "Brooklyn, New York", "Republican Secretary of Commerce Herbert Hoover", "the Mishnah", "a deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "Tom Brady", "Around 1200", "Walter Egan", "The Crossing", "to stay, abide", "the Federated States of Micronesia", "origins of replication, in the genome", "the rules of the Southern California Timing Association", "2012", "Jackie Robinson", "Coton in the Elms", "the American Civil War", "Pradyumna", "1979", "James Madison", "UMBC", "Roger Federer", "17 - year - old", "Kylie Minogue", "As of 2018, there were 5,534 registered hospitals in the United States", "the left and right main pulmonary arteries", "electron donors", "A rear - view mirror", "George W Bush's", "the United States Constitution", "Lucas McCain", "Martin Scorsese", "2014", "June 6, 1959", "the immorality of these deviant young men", "the Listeria monocytogenes bacteria", "the United States, NATO member states, Russia and India", "the Wilcox Mansion", "hyperthyroidism", "a parrot", "UVB rays"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6481283453303803}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.2857142857142857, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6976744186046512, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.28571428571428575, 1.0, 1.0, 0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-10259", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-1399", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-3175", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-7770", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-2074", "mrqa_searchqa-validation-8320", "mrqa_searchqa-validation-14922", "mrqa_searchqa-validation-10011", "mrqa_triviaqa-validation-7608"], "SR": 0.546875, "CSR": 0.536875, "EFR": 1.0, "Overall": 0.7205}, {"timecode": 50, "UKR": 0.7109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1050", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3331", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5710", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-706", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1369", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-197", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3083", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4245", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4762", "mrqa_naturalquestions-validation-4863", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-650", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8076", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-130", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-2675", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10336", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10924", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-11990", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14061", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-15238", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15972", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16896", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1705", "mrqa_searchqa-validation-1837", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3055", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4066", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-561", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-6855", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8205", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_searchqa-validation-9724", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-10491", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3331", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4751", "mrqa_squad-validation-4772", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5224", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-6075", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-6449", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-680", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-7863", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8486", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-9121", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9893", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-233", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4854", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-760", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.794921875, "KG": 0.47421875, "before_eval_results": {"predictions": ["cappella", "caterpillar", "Zeus", "7", "Lady Gaga", "John Alec Entwistle", "The Blind Beggar  public house, Whitechapel Road.", "a fearful man, all in coarse gray with a great iron on his leg", "Yonne", "December 18, 1958", "(long, curled up)", "Bristol", "Queen Victoria and Prince Albert", "Car ferry", "Sherrie Hewson", "Sharjah", "Madagascar", "Arabian Gulf", "Miss Havisham", "oxygen", "(Mac) Dagger", "Gentlemen Prefer Blondes", "Nikita Khrushchev", "Netherlands New Guinea", "the hose", "Hudson River", "John Key", "Subway's", "geodetics", "Dylan Thomas", "Jeff Bridges", "the Tower of London", "Bridge", "quarter note", "cirrus uncinus", "Manchester", "Passchendaele", "cheese", "Klaus dolls", "Argentina", "Colin Montgomerie", "(Dave) Lamb", "Count Basie Orchestra", "The Behemoth", "Sir John Houblon,", "The Firm", "elbow", "(C) N Trueman", "isohyet", "naples", "Danish", "July 14, 1969", "1923", "Massillon, Ohio", "1937", "The LA Galaxy", "2017", "British broadcasters,", "Frank Ricci,", "The Stooges comedic farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "sweatshirt", "Susan B. Anthony dollar", "elephant tusks", "in the lawless southern provinces and especially in the Taliban stronghold of Helmand, poppy production"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5283668154761905}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.375, 1.0, 0.8571428571428571, 0.0, 0.9166666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-6534", "mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3072", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-4103", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-1411", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-3120", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-5001", "mrqa_triviaqa-validation-7490", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-2760", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-1896", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1927", "mrqa_triviaqa-validation-3522", "mrqa_hotpotqa-validation-5451", "mrqa_hotpotqa-validation-2793", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-4112", "mrqa_searchqa-validation-2495", "mrqa_searchqa-validation-12831", "mrqa_newsqa-validation-2191"], "SR": 0.4375, "CSR": 0.5349264705882353, "EFR": 1.0, "Overall": 0.7030009191176471}, {"timecode": 51, "before_eval_results": {"predictions": ["Rembrandt Harmensz. van Rijn", "smith", "Passenger Pigeon", "catalyst", "Sir Edwin Landseer", "Hitler", "physicist", "smith", "scales", "Judy Garland", "David Walliams", "tepuis", "inhumation", "erie smith", "long rider", "hypopituitarism", "Delaware", "bees", "Treaty of Utrecht", "stalag stalagmites", "usually one year of age", "lyle", "france", "mantle", "rugs", "algae", "shine", "Algeria", "Churchill Downs", "The United States", "Leonard Bernstein", "Vladimir Putin", "shipwreck", "netherlands", "20", "Ken Platt", "Jennifer Aniston", "red rock", "netherlands", "Reginald Smith", "muppet Christmas Carol", "netherlands", "jump jump", "creme anglaise", "electric chair", "sugar ray robinson", "10", "netherlands", "Dublin", "Babylon", "the Continental Marines", "Joe Spano", "Katherine Allentuck", "skeletal muscle", "CBS News", "Herman's Hermits", "the United States Congress", "\"revolution of values\"", "clothes", "2-0 up", "Brunei", "a Pringles can", "a peanut butter cup", "1983"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5799851190476191}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5, 0.8, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-3828", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-2529", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-713", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-5922", "mrqa_triviaqa-validation-3477", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-7762", "mrqa_naturalquestions-validation-692", "mrqa_hotpotqa-validation-4406", "mrqa_newsqa-validation-2202", "mrqa_newsqa-validation-2260", "mrqa_newsqa-validation-4091", "mrqa_searchqa-validation-3853"], "SR": 0.46875, "CSR": 0.5336538461538461, "EFR": 0.9705882352941176, "Overall": 0.6968640412895927}, {"timecode": 52, "before_eval_results": {"predictions": ["horse", "australia", "sweden", "vodka", "Tony Manero", "Snarked", "Prussian 2nd Army", "silversmith", "Joshua Tree National Park", "carpathia", "Superman", "Letchworth", "velvet", "1", "eric scissorhands", "crackerjack", "white Ferns", "Utah", "carburetors", "As You Like It", "Labrador Retriever", "what", "Delaware", "thieves", "Clara wieck", "squeeze", "Apocalypse Now", "Boojum", "St Moritz", "the Queen", "Scafell Pike", "Edgar Allan Poe", "Chris Moyles", "Tony Blackburn", "Donna Summer", "Strasbourg, France", "kiki", "wolf", "the Titanic", "stanley purdy", "piano", "Andrew Lloyd Webber", "Malawi", "australia", "sue barker", "mrs baryshnikov", "Norwich", "Ruth Rendell", "The Smiths", "ontario", "Ohio", "Miami Heat", "Joanna Page", "20 July 2015", "23 October 2003", "FCI Danbury", "25 June 1971", "\"Even though I moved a tad slower than usual today, everyone welcomed me back with open arms and it was a wonderful homecoming,\"", "Caster Semenya", "a \"happy ending\"", "suffrage", "Tom Cruise", "joseph smith", "Colonel"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6484375}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7014", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-870", "mrqa_triviaqa-validation-2767", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-4676", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-6261", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-4518", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3898", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-5354", "mrqa_newsqa-validation-4178", "mrqa_searchqa-validation-8349", "mrqa_searchqa-validation-1384", "mrqa_searchqa-validation-11787"], "SR": 0.59375, "CSR": 0.5347877358490566, "EFR": 1.0, "Overall": 0.7029731721698113}, {"timecode": 53, "before_eval_results": {"predictions": ["albinism", "Jack Ruby", "Google", "hugh Dowding", "squash", "Tennessee Williams", "Jim Smith", "injecting a 7 percent solution intravenously three times a day", "David Bowie", "wheat,  hemp, and other crops.", "canoeist", "mocambique", "Christian Louboutin", "Ironside", "Scottish", "James Dean", "Mars", "once every two weeks", "About Eve", "chicken", "George Orwell", "noel edmonds", "homeless", "Derbyshire, England", "Cubism", "polynesian", "Virginia", "lothbrok", "noel edmonds", "Ruth Rendell", "nottingham", "Charles Addams", "hot Chocolate", "zambia", "1921", "france", "Everybody Wants To Rule The World", "praseodymium", "Bruce Alexander", "bloom", "the brain and the spinal cord", "azor", "Arthur Hailey", "hart", "madonna", "little arrows", "a nerve cell cluster  or a group of nerve cell bodies located in the autonomic nervous system", "noel edmonds", "ernestacks", "27", "orchids", "104 colonists", "aiding the war effort", "anterograde amnesia and dissociation", "Cesar Millan", "2004 Paris Motor Show", "Brazilian-American mixed martial artist and Brazilian jiu-jitsu practitioner", "cell phones", "anil kapoor", "drug cartels", "Guinea-Bissau", "steel", "place bet", "2012"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5898897058823529}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-3136", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6447", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-1081", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-3119", "mrqa_triviaqa-validation-294", "mrqa_triviaqa-validation-6040", "mrqa_triviaqa-validation-5551", "mrqa_triviaqa-validation-285", "mrqa_triviaqa-validation-4525", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-4442", "mrqa_hotpotqa-validation-3655", "mrqa_searchqa-validation-14449"], "SR": 0.5625, "CSR": 0.5353009259259259, "EFR": 1.0, "Overall": 0.7030758101851852}, {"timecode": 54, "before_eval_results": {"predictions": ["Big Mamie", "dancer", "Canadian", "Sunyani West District", "six", "basketball", "Adelaide", "Alfred Preis", "Forest of Bowland", "Chengdu Aircraft Corporation", "Daniel Espinosa", "Switzerland", "sleepless hours", "Jeffrey Adam \"Duff\" Goldman", "Ang Lee", "Mineola", "1,382", "Kentucky Wildcats", "judy holliday", "Parliamentarians (\" roundheads\") and Royalists (\"Cavaliers\")", "Harry Potter series", "Haitian Revolution", "alcoholic drinks", "sonic", "John Meston", "York County", "Cleveland Cavaliers", "judy holliday", "Hillsborough County", "Cartoon Network Too", "San Diego County Fair", "the fictional city of Quahog, Rhode Island", "physicist", "1970", "George Raft", "Chiba, Japan", "1944", "the British Army", "311", "Jennifer Taylor", "Cartoon Network", "Columbia Records", "British", "6,241", "October 17, 2017", "Leslie James \"Les\" Clark", "phil Collins", "Hawaii", "Gregg Popovich", "1979", "john Malkovich", "Lauren Tom", "~ 55 - 75", "Spektor", "augusta", "monta", "vanilla", "a Christian farmer who took exception to her \"inappropriate behavior\" while filming a music video on his land.", "four", "The Louvre", "albert", "hemming", "will", "The Capitoline Wolf"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5994791666666666}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-796", "mrqa_hotpotqa-validation-5245", "mrqa_hotpotqa-validation-3161", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-4828", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-4856", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-1310", "mrqa_naturalquestions-validation-8962", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-5383", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-2609", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-9920", "mrqa_searchqa-validation-1776"], "SR": 0.515625, "CSR": 0.5349431818181818, "EFR": 1.0, "Overall": 0.7030042613636364}, {"timecode": 55, "before_eval_results": {"predictions": ["The 2016 United States Senate election in Nevada", "1952", "private Roman Catholic university located in Great Falls, Montana within the Diocese of Great Falls\u2013Billings", "voicing Liquid Snake in the \" Metal Gear\" series", "400 MW", "Kerry Marie Butler", "Esp\u00edrito Santo Financial Group", "September 5, 2017", "Tuesday, January 24, 2012, at 8 p.m. ET/PT.", "leopard", "Salman Rushdie", "Tom Rob", "Big Bad Wolf", "Rockland County", "Personal History", "Australian-American", "Holston River", "1943", "1996", "India", "Tallahassee City Commission", "A Boltzmann machine", "three different covers", "Hechingen in Swabia", "Floyd Mutrux and Colin Escott", "A Little Princess", "\"\", and the 2013 Marvel One- Shot short film of the same name", "2008", "from July 2, 1967 to August 21, 1995", "Kennedy Road", "Mark Neveldine and Brian Taylor", "the fictional city of Quahog, Rhode Island", "Spiro Agnew", "The String Cheese Incident", "three", "Don't Look Back in Anger", "Thorgan", "1692", "9", "three", "Asian Indoor and Martial Arts Games", "26,000", "Northampton Town", "Indian state of Gujarat", "General Manager of the Miami Dolphins", "\"punk rock\"", "\"Little Britain\"", "The 1990\u201391 UNLV Runnin' Rebels", "Bill Clinton", "John D Rockefeller's Standard Oil Company", "Barack Obama's middle name", "pigs", "In Time", "The flag of Vietnam", "pulsar", "The Dogger Bank", "the first eight seasons", "\"totaled,\"", "Steven Gerrard", "ketamine", "the Caspian tern", "The Southern Hemisphere", "Diebold", "Andr\u00e9s Iniesta"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6455762987012987}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true], "QA-F1": [0.8333333333333333, 1.0, 0.13333333333333333, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1379", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-4859", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-3512", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-3595", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-2859", "mrqa_hotpotqa-validation-5228", "mrqa_triviaqa-validation-3487", "mrqa_newsqa-validation-456", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-6593"], "SR": 0.578125, "CSR": 0.5357142857142857, "EFR": 1.0, "Overall": 0.7031584821428571}, {"timecode": 56, "before_eval_results": {"predictions": ["35,000", "Vishal Bhardwaj", "Macau", "Russian film industry", "no. 3", "The Government of Ireland", "Washington Street", "Eielson Air Force Base", "Galo (], \"Rooster\"", "Tom Shadyac", "Michael Phelps", "Richard Strauss", "Claire Fraser", "Iynx", "David Starkey", "Richard II of England", "Anne Perry", "September 13, 1994, by Bad Boy Records and Arista Records", "Ben Miller", "\"Darconville\u2019s Cat\"", "An invoice, bill or tab", "October 16, 2015", "Dan Brandon Bilzerian", "The Andes", "Srinagar", "STS-51-C", "Gregg Harper", "The Ones Who Walk Away from Omelas", "Minnesota's 8th congressional district", "(Polish: \"Trzy kolory\"", "November 27, 2002", "Henry Albert \"Hank\" Azaria", "UFC Fight Pass", "The Sun", "Peter Yarrow", "The Frost Report", "Ready Player One", "1,925", "7 October 1978", "June 2, 2008", "Ravenna", "John Meston", "the Battelle Energy Alliance", "a co-op of grape growers", "The Spiderwick Chronicles", "a Welsh football goalkeeper", "George Orwell", "Croatian", "Labour Party", "Fortunino Francesco Verdi", "Warsaw", "Texhoma", "interstitial fluid in the `` interstitial compartment ''", "September 2017", "Ken Norton", "Dordogne Valley of France", "Hastings", "the UK", "Al Nisr Al Saudi", "5 1/2-year-old son, Ryder Russell", "cobalt", "that track", "The bassoon", "time in exchange for detailed public disclosure of an invention"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6998435592185592}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 0.28571428571428575, 1.0, 0.4615384615384615, 0.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-484", "mrqa_hotpotqa-validation-246", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-784", "mrqa_hotpotqa-validation-1364", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3900", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-675", "mrqa_triviaqa-validation-6104", "mrqa_newsqa-validation-544", "mrqa_searchqa-validation-6632"], "SR": 0.59375, "CSR": 0.5367324561403508, "EFR": 1.0, "Overall": 0.7033621162280702}, {"timecode": 57, "before_eval_results": {"predictions": ["1961", "Ballon d'Or", "Elsie Audrey Mossom", "the first trans-Pacific flight from the United States to Australia", "the House of Hohenstaufen", "BraveStarr", "White Knights of the Ku Klux Klan", "Benjam\u00edn Arellano F\u00e9lix", "lion", "Tony Award", "the belief that the German people needed to reclaim historically German areas of Eastern Europe into which they could expand, is tied to it", "China", "Esperanza Spalding", "1854", "Art Deco-style skyscraper", "Starachowice", "Debbie Harry", "England", "Mineola, New York", "Tokyo's Narita International Airport", "Essendon Airport (IATA: MEB, ICAO: YMEN) is a 305 ha public airport serving scheduled commercial, corporate-jet, charter and general aviation flights", "Abbey Road", "bioelectromagnetics", "German", "casting, job opportunities, and career advice", "ten episodes", "2012", "second generation North American Ford Falcon", "Igor Stravinsky, Carl Orff, Paul Hindemith, Richard Strauss, Luigi Nono, Krzysztof Penderecki and Joaqu\u00edn Rodrigo", "Euripides", "second segment", "Clitheroe Football Club", "Peter Wooldridge Townsend", "Paper", "November 27, 2002", "Campbellsville", "1875", "2007", "al-Qaeda", "Humberside Airport", "around 3,500,000", "a successful racehorse breeder and owner", "Nia Kay", "11 November 1918", "1891", "the Bank of China Tower", "Lerotholi Polytechnic FC", "a former American football player", "872", "Lee", "Richard Street", "diametrically opposite the South Pole", "a radius 1.5 times the Schwarzschild radius", "A photoelectric, or optical smoke detector", "Japan", "Apollo", "leeds", "bragging about his sex life on television", "people look at the content of the speech, not just the delivery", "Fernando Torres", "Alicia Keys", "Tybee Island, Ga.", "the night before", "Yulia Lipnitskaya"], "metric_results": {"EM": 0.5, "QA-F1": 0.6525486894319131}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, true, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.4, 0.5, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.2222222222222222, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.5, 0.4210526315789474, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-4341", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-4762", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4436", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-3499", "mrqa_newsqa-validation-203", "mrqa_searchqa-validation-4002", "mrqa_searchqa-validation-6491", "mrqa_triviaqa-validation-4287"], "SR": 0.5, "CSR": 0.5360991379310345, "EFR": 1.0, "Overall": 0.7032354525862069}, {"timecode": 58, "before_eval_results": {"predictions": ["his invention with the Dutch patent office", "an open window that fits neatly around him", "King Birendra", "Susan Atkins", "Palestinian prisoners", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997", "can be as good as the technician and the doctor who read it, so you want to go to a good place.\"", "a round that pierced his hip on the left side and tore through his abdomen.", "The iconic Abbey Road music studios made famous by the Beatles are not for sale,", "10 municipal police officers", "his past and his future", "mpire of the Sun", "Nancy Sutley", "\"Mammograms are known to be uncomfortable,\"", "Haeftling", "55-year-old", "capital murder and three counts of attempted murder", "shock, quickly followed by speculation about what was going to happen next,\"", "the Awa", "16th grand Slam title.", "curfew", "\"Nude, Green Leaves and Bust\"", "made her comeback last year after giving birth to baby daughter Jada, who was watching her mum from the stands again on Saturday.", "Glasgow, Scotland concert", "an antihistamine and an epinephrine auto-injector", "Damon Bankston", "Afghanistan's", "an \"aesthetic environment\" and ensure public safety,", "Why do genocides and mass atrocities happen?", "sharia in Somalia is part of the laws for thousands of years, and we never had this kind of a thing.", "humans", "Vancouver, British Columbia", "an independent homeland for the country's ethnic Tamil minority", "two years ago", "is president and CEO of Ripken Baseball,", "Sen. Barack Obama", "social issues like homelessness and AIDS.", "fake his own death", "made some progress in patching up damaged relations with the powerful Turkish military.", "navy dress with red lining by the American-born Lintner", "The Rev. Alberto Cutie", "citizenship", "housing, business and infrastructure repairs", "The nation's foremost concert producer, Charles Jubert, died. So did members of four bands who were practicing inside a studio that collapsed.", "a mammoth's skull", "\"We are here to cooperate with anyone and everyone that will help us find the guilty party and return Lisa home safely,\"", "the creation of an Islamic emirate in Gaza", "in the lawless southern provinces and especially in the Taliban stronghold of Helmand, poppy production was going on largely unchecked.", "The 19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "2-1", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "Francisco Pizarro", "Owen Vaccaro", "2003", "French manufacturer Renault", "cheese", "Alan Freed", "Woolsthorpe-by-Colsterworth", "War Is the Answer", "arts manager", "ice sheet", "Mrs", "can accomplish, succeed, or achieve", "freeze fish blood"], "metric_results": {"EM": 0.296875, "QA-F1": 0.42389229155076014}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, false, false, false], "QA-F1": [0.0, 0.4444444444444445, 0.0, 1.0, 0.4, 0.0, 0.11764705882352941, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.4827586206896552, 0.0, 0.4, 0.0, 1.0, 0.923076923076923, 0.0, 0.0, 1.0, 0.0, 0.8333333333333333, 1.0, 0.26666666666666666, 1.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.07407407407407407, 0.0, 0.7586206896551725, 0.0, 0.0, 0.08, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-112", "mrqa_newsqa-validation-3857", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-1403", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-358", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-2735", "mrqa_newsqa-validation-2191", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1055", "mrqa_triviaqa-validation-4416", "mrqa_hotpotqa-validation-2315", "mrqa_hotpotqa-validation-4517", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-6685", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-1521"], "SR": 0.296875, "CSR": 0.5320444915254237, "EFR": 0.9777777777777777, "Overall": 0.6979800788606403}, {"timecode": 59, "before_eval_results": {"predictions": ["2nd Lt. Holley Wimunc.", "Sunday", "five suspects,", "A witness", "U.N.", "the Democratic VP candidate", "Ken Choi,", "second child", "on the bench", "leftist Workers' Party.", "Robert", "Christiane Amanpour", "Narayanthi Royal", "france", "five", "a city of romance, of incredible architecture and history.", "a 10-day retreat,", "Keating Holland", "Asashoryu,", "Mutassim,", "Dr. Jennifer Arnold and husband Bill Klein,", "Lindsey oil refinery in eastern England.", "Sunday.", "future relations between the Middle East and Washington.", "prostate cancer,", "the southern city of Naples", "at least 300", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "2002", "Spc. Megan Lynn Touma,", "how preachy and awkward cancer movies can get.\"", "some truly mind-blowing structures", "strife in Somalia,", "Miami Beach, Florida,", "Charles Darwin", "\"Jersey Shore\"", "President Obama.", "\"One, you let them know what the case involves and released a picture of the girl,", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "baseball bat", "in his native Philippines", "Los Angeles.", "Jaime Andrade", "Alfredo Astiz,", "tie salesman", "The station", "Ronald Cummings,", "Adriano", "$8.8 million", "11:30 p.m.", "Alan Graham", "Canada south of the Arctic", "Glenn Close", "Joseph M. Scriven", "Richard Krajicek", "Manchester", "Robert", "John Rich", "2013 Cannes Film Festival", "Yeah!", "three", "Rooster Cogburn", "Korea", "1936"], "metric_results": {"EM": 0.46875, "QA-F1": 0.622296626984127}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, true], "QA-F1": [0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.4444444444444444, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1059", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-81", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-831", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-1278", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3095", "mrqa_naturalquestions-validation-1872", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6864", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-53", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-11013"], "SR": 0.46875, "CSR": 0.5309895833333333, "EFR": 1.0, "Overall": 0.7022135416666667}, {"timecode": 60, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1027", "mrqa_hotpotqa-validation-1062", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1095", "mrqa_hotpotqa-validation-1360", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-1899", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2436", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2694", "mrqa_hotpotqa-validation-2724", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2959", "mrqa_hotpotqa-validation-3047", "mrqa_hotpotqa-validation-3105", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-374", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-382", "mrqa_hotpotqa-validation-4012", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4225", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4618", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4964", "mrqa_hotpotqa-validation-5048", "mrqa_hotpotqa-validation-5127", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5260", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-54", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-5517", "mrqa_hotpotqa-validation-5520", "mrqa_hotpotqa-validation-5543", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5814", "mrqa_hotpotqa-validation-5837", "mrqa_hotpotqa-validation-584", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-825", "mrqa_hotpotqa-validation-892", "mrqa_hotpotqa-validation-99", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10139", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10205", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10348", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1657", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2052", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-2336", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2380", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3232", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-4103", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4195", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4479", "mrqa_naturalquestions-validation-4495", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-519", "mrqa_naturalquestions-validation-5371", "mrqa_naturalquestions-validation-571", "mrqa_naturalquestions-validation-5782", "mrqa_naturalquestions-validation-5868", "mrqa_naturalquestions-validation-5968", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6276", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-6856", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7240", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7893", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-825", "mrqa_naturalquestions-validation-8290", "mrqa_naturalquestions-validation-8318", "mrqa_naturalquestions-validation-8339", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8737", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8883", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-9438", "mrqa_naturalquestions-validation-9511", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-9813", "mrqa_naturalquestions-validation-9985", "mrqa_newsqa-validation-11", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1152", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1285", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-1514", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1758", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1832", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-216", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-266", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-331", "mrqa_newsqa-validation-3515", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3551", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-376", "mrqa_newsqa-validation-3792", "mrqa_newsqa-validation-3993", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-548", "mrqa_newsqa-validation-650", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-865", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-960", "mrqa_searchqa-validation-1001", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-10131", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-10643", "mrqa_searchqa-validation-10762", "mrqa_searchqa-validation-10979", "mrqa_searchqa-validation-11056", "mrqa_searchqa-validation-11194", "mrqa_searchqa-validation-11362", "mrqa_searchqa-validation-11369", "mrqa_searchqa-validation-11772", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-12514", "mrqa_searchqa-validation-12835", "mrqa_searchqa-validation-12854", "mrqa_searchqa-validation-12930", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13780", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-1413", "mrqa_searchqa-validation-14711", "mrqa_searchqa-validation-14845", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15479", "mrqa_searchqa-validation-15496", "mrqa_searchqa-validation-1554", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-15991", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-16291", "mrqa_searchqa-validation-16306", "mrqa_searchqa-validation-16310", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16600", "mrqa_searchqa-validation-16901", "mrqa_searchqa-validation-1838", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-1942", "mrqa_searchqa-validation-1964", "mrqa_searchqa-validation-2558", "mrqa_searchqa-validation-2761", "mrqa_searchqa-validation-2930", "mrqa_searchqa-validation-3066", "mrqa_searchqa-validation-3105", "mrqa_searchqa-validation-3129", "mrqa_searchqa-validation-3239", "mrqa_searchqa-validation-3451", "mrqa_searchqa-validation-3570", "mrqa_searchqa-validation-3628", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4054", "mrqa_searchqa-validation-4248", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5377", "mrqa_searchqa-validation-5744", "mrqa_searchqa-validation-5767", "mrqa_searchqa-validation-6142", "mrqa_searchqa-validation-6261", "mrqa_searchqa-validation-6345", "mrqa_searchqa-validation-6623", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7299", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8219", "mrqa_searchqa-validation-8558", "mrqa_searchqa-validation-8832", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-936", "mrqa_searchqa-validation-9370", "mrqa_squad-validation-10088", "mrqa_squad-validation-10409", "mrqa_squad-validation-1079", "mrqa_squad-validation-1109", "mrqa_squad-validation-121", "mrqa_squad-validation-1388", "mrqa_squad-validation-1530", "mrqa_squad-validation-1544", "mrqa_squad-validation-181", "mrqa_squad-validation-1836", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2268", "mrqa_squad-validation-2276", "mrqa_squad-validation-2291", "mrqa_squad-validation-2364", "mrqa_squad-validation-2469", "mrqa_squad-validation-2486", "mrqa_squad-validation-2831", "mrqa_squad-validation-2885", "mrqa_squad-validation-2906", "mrqa_squad-validation-292", "mrqa_squad-validation-30", "mrqa_squad-validation-3078", "mrqa_squad-validation-3139", "mrqa_squad-validation-3179", "mrqa_squad-validation-3785", "mrqa_squad-validation-3796", "mrqa_squad-validation-3830", "mrqa_squad-validation-4155", "mrqa_squad-validation-4343", "mrqa_squad-validation-4358", "mrqa_squad-validation-4509", "mrqa_squad-validation-4567", "mrqa_squad-validation-4849", "mrqa_squad-validation-4952", "mrqa_squad-validation-5034", "mrqa_squad-validation-5083", "mrqa_squad-validation-5178", "mrqa_squad-validation-5307", "mrqa_squad-validation-5469", "mrqa_squad-validation-5517", "mrqa_squad-validation-5535", "mrqa_squad-validation-5566", "mrqa_squad-validation-5600", "mrqa_squad-validation-5723", "mrqa_squad-validation-5733", "mrqa_squad-validation-5782", "mrqa_squad-validation-5821", "mrqa_squad-validation-611", "mrqa_squad-validation-6272", "mrqa_squad-validation-6402", "mrqa_squad-validation-6421", "mrqa_squad-validation-6437", "mrqa_squad-validation-646", "mrqa_squad-validation-6554", "mrqa_squad-validation-6640", "mrqa_squad-validation-6653", "mrqa_squad-validation-6894", "mrqa_squad-validation-7040", "mrqa_squad-validation-7142", "mrqa_squad-validation-7302", "mrqa_squad-validation-743", "mrqa_squad-validation-7554", "mrqa_squad-validation-7569", "mrqa_squad-validation-7709", "mrqa_squad-validation-7788", "mrqa_squad-validation-7832", "mrqa_squad-validation-797", "mrqa_squad-validation-828", "mrqa_squad-validation-840", "mrqa_squad-validation-8598", "mrqa_squad-validation-8656", "mrqa_squad-validation-8691", "mrqa_squad-validation-8759", "mrqa_squad-validation-915", "mrqa_squad-validation-9379", "mrqa_squad-validation-9428", "mrqa_squad-validation-9830", "mrqa_squad-validation-9904", "mrqa_triviaqa-validation-1002", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1057", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1143", "mrqa_triviaqa-validation-1198", "mrqa_triviaqa-validation-1243", "mrqa_triviaqa-validation-1500", "mrqa_triviaqa-validation-1522", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-1679", "mrqa_triviaqa-validation-1684", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-171", "mrqa_triviaqa-validation-1811", "mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2272", "mrqa_triviaqa-validation-2383", "mrqa_triviaqa-validation-2448", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-2482", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-2541", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-2818", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-3017", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-3069", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3130", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3251", "mrqa_triviaqa-validation-3293", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3714", "mrqa_triviaqa-validation-3741", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3822", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4123", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4344", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4644", "mrqa_triviaqa-validation-4687", "mrqa_triviaqa-validation-485", "mrqa_triviaqa-validation-4936", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5342", "mrqa_triviaqa-validation-5406", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-553", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-559", "mrqa_triviaqa-validation-5643", "mrqa_triviaqa-validation-5687", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-585", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5983", "mrqa_triviaqa-validation-6064", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-6145", "mrqa_triviaqa-validation-6157", "mrqa_triviaqa-validation-6234", "mrqa_triviaqa-validation-6258", "mrqa_triviaqa-validation-6277", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-6511", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6575", "mrqa_triviaqa-validation-6711", "mrqa_triviaqa-validation-6931", "mrqa_triviaqa-validation-6955", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-7021", "mrqa_triviaqa-validation-7373", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7406", "mrqa_triviaqa-validation-741", "mrqa_triviaqa-validation-7508", "mrqa_triviaqa-validation-753", "mrqa_triviaqa-validation-7617", "mrqa_triviaqa-validation-764", "mrqa_triviaqa-validation-7754", "mrqa_triviaqa-validation-7770", "mrqa_triviaqa-validation-7783", "mrqa_triviaqa-validation-884", "mrqa_triviaqa-validation-936"], "OKR": 0.802734375, "KG": 0.490625, "before_eval_results": {"predictions": ["2017", "1648 - 51 war", "Thomas Edison's assistants, Fred Ott", "season four", "Master Christopher Jones", "William Shakespeare's play Romeo and Juliet", "the status line", "in the 18th century in the United Kingdom when members of parliament disparagingly used the title in reference to Sir Robert Walpole", "The Maginot Line", "1937", "Fix You", "Hirschman", "Bobby Eli", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "approximately 11 %", "Guy Berryman", "Austria - Hungary", "Sharyans Resources", "Babe Ruth", "Seattle, Washington", "420", "Wylie Draper", "Pinar del R\u00edo Province ( now in Artemisa Province )", "DNA was a repeating set of identical nucleotides", "Magnavox Odyssey", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "Saturday evenings", "tolled ( quota ) highways", "close by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "innermost in the eye while the photoreceptive cells lie beyond", "Saturday", "Glenn Close", "reservoirs at high altitudes", "aiding the war effort", "into smaller pulmonary arteries that spread throughout the lungs", "Nepal", "the form is properly a rotationally symmetric saltire", "Ace", "Phillip Paley", "the tax rate paid by a small business", "the liver and kidneys", "Lee County, Florida, United States", "The onset of rigor mortis and its resolution partially determine the tenderness of meat", "Stephen Graham", "Florida", "2010", "Jeff Barry and Andy Kim", "infection, irritation, or allergies", "ABC", "August 1991", "The Maidstone Studios in Maidstone, Kent", "three", "hydrogen", "inches (5.588 mm)", "Edward James Olmos", "Days of Our Lives", "Toyota Innova", "Omar bin Laden,", "a man's lifeless, naked body", "jazz", "Falkland Islands", "cement pond", "Seventy-six trombones", "Fifty Shades of Grey"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5792662545787546}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.8, 0.5714285714285715, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.3076923076923077, 0.0, 0.2666666666666667, 0.761904761904762, 0.15384615384615383, 1.0, 0.0, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-3470", "mrqa_naturalquestions-validation-6166", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-4999", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-8998", "mrqa_triviaqa-validation-1947", "mrqa_triviaqa-validation-6052", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1584", "mrqa_searchqa-validation-11279", "mrqa_searchqa-validation-15624", "mrqa_searchqa-validation-893"], "SR": 0.46875, "CSR": 0.529969262295082, "EFR": 0.9705882352941176, "Overall": 0.70214274951784}, {"timecode": 61, "before_eval_results": {"predictions": ["17 - year - old", "The procedure can be performed at any level in the spine ( cervical, thoracic, or lumbar ) and prevents any movement between the fused vertebrae", "Dido", "international aid", "at least US $2 trillion by GDP in nominal or PPP terms", "January 15, 2007", "Gorakhpur railway station", "Roger Federer", "the Battle of Antietam", "1933", "Puff the Magic Dragon is not about drugs", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "John Travolta", "August 5, 1937", "Rory McIlroy", "Jonathan Breck", "the 4th century", "Smith Jerrod", "Acid rain", "2018 NCAA Division I Men's Basketball Tournament for their third National Championship", "Mulberry Street", "Lucille Simmons", "the sinoatrial node", "1994", "Wyatt and Dylan Walters", "10 May 1940", "Phillipa Soo as Natasha, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne, Brittain Ashford as Sonya, Nick Choksi as Dolokhov, Shaina Taub as Mary", "Denver Broncos", "Judith Aline Keppel", "Swedien and Jones", "the customer's account", "Redwood National and State Parks", "The film follows a child with Treacher Collins syndrome trying to fit in", "when the cell is undergoing the metaphase of cell division ( where all chromosomes are aligned in the center of the cell in their condensed form )", "Sally Field", "Eric Clapton", "senior enlisted sailor ( `` E-9 '' )", "Lagaan", "Billy Colman", "Karen Gillan", "Bulgarian and Romanian sweet leavened bread, which is a type of Stollen", "Beijing, China", "5 lakh", "Havana Harbor", "the 1830s", "2017", "Qutab Ud - Din - Aibak", "Prince Henry", "the inferior thoracic border -- made up of the diaphragm", "United States customary", "the majority opinion of the court which gives rise to its judgment", "floating ribs", "Glamour Model", "Romania", "The Gold Coast", "Kathryn Jean Martin \"Kathy\" Sullivan AM", "25 million", "to give my kids a better environment.\"", "torture and indefinite detention", "the iPods", "Chagas disease", "Jacob Marley", "John Hersey", "22 September 2015"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6879264265234379}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.12903225806451613, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07407407407407408, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.9, 0.0909090909090909, 1.0, 1.0, 0.22222222222222224, 0.16666666666666669, 0.6666666666666666, 1.0, 0.11764705882352942, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6, 0.8571428571428571, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5439", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-8382", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-9867", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-6596", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8159", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-5168", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-5817", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-451", "mrqa_hotpotqa-validation-3930", "mrqa_hotpotqa-validation-123", "mrqa_newsqa-validation-1110", "mrqa_searchqa-validation-4844", "mrqa_searchqa-validation-14622"], "SR": 0.515625, "CSR": 0.5297379032258065, "EFR": 0.967741935483871, "Overall": 0.7015272177419355}, {"timecode": 62, "before_eval_results": {"predictions": ["Christian Dior", "cranberry", "the Danforth Foundation", "fibula", "fort boyard", "a beached whale", "the Mississippi", "Dennis Quaid", "Lil Jon", "the Boers", "the Colosseum", "Goldeneye", "soup", "tarmes", "Winston Churchill", "Salford in Greater Manchester,", "the Lincoln Tunnel", "Pinta", "Louisiana", "Billy the Kid", "Rembrandt", "Canada", "Steve Hackett", "Benedict XVI", "Strategic Petroleum Reserve", "Prague", "Hanging Gardens", "electricity", "Harry Dean Stanton", "fort boyard", "Nacho Libre", "the College of William and Mary", "the Sheepeater", "the bassoon", "Montpelier", "Halo 4", "pound of Antonio's flesh", "Iran", "Best Picture", "Hamlet", "Heroes", "Russia", "mead", "the Atlanta Hawks", "a large gay crowd", "ivory", "Samson", "Dustin Hoffman", "Nittany", "Sicily", "Socrates", "more than 1,000", "the large area needed for effective gas exchange", "Harry Potter's first year at Hogwarts School of Witchcraft and Wizardry", "fort boyard", "Honolulu, Hawaii", "fort boyard", "Esperanza Emily Spalding", "2000", "Sargent Shriver", "Daytime Emmy Lifetime Achievement Award.", "Impeccable", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Gleneagles Golf Course"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6389237359703607}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.5, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.8695652173913043, 0.0, 0.6666666666666666, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5263157894736842, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-7337", "mrqa_searchqa-validation-13095", "mrqa_searchqa-validation-4677", "mrqa_searchqa-validation-14699", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-1906", "mrqa_searchqa-validation-6044", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-5976", "mrqa_searchqa-validation-1751", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-16438", "mrqa_searchqa-validation-8676", "mrqa_searchqa-validation-6211", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-14898", "mrqa_searchqa-validation-8819", "mrqa_searchqa-validation-8062", "mrqa_searchqa-validation-15398", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-1325", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-6028", "mrqa_triviaqa-validation-7643", "mrqa_hotpotqa-validation-2367", "mrqa_hotpotqa-validation-5895", "mrqa_newsqa-validation-3961", "mrqa_triviaqa-validation-1787"], "SR": 0.484375, "CSR": 0.5290178571428572, "EFR": 0.9696969696969697, "Overall": 0.7017742153679654}, {"timecode": 63, "before_eval_results": {"predictions": ["the Voting Rights Act", "a pistol", "disabilities", "Harry Potter and the Chamber of Secrets", "Senator Mitchell", "a shower", "the Bronze Age", "the People's Party", "the Tower of London", "Daniel", "the Jinx", "Vermont", "California", "Cosmopolitan", "David Beckham", "Knossos", "Japan", "rodeo", "a screwdrivers", "Vietnam", "Stanford", "a Funeral March", "a prism schism", "Alaska", "Wallis Warfield Simpson", "a centipede", "Greek", "Sisters Rosensweig", "Brooklyn", "Penn State", "Easter Island", "Nasser", "the Hell Is Hell", "Stephen Hawking", "Labor Day", "Mozambique", "landfills", "The Silence of the Lambs", "15", "Capone", "Paul McCartney", "Anne Murray", "Tennessee", "Buenos", "contagious", "Jane Austen", "riticale", "baking soda", "peanuts", "philosophy", "Byzantium", "Icona Pop", "waiting tables at the Moondance Diner to support himself", "Miami Heat of the National Basketball Association ( NBA )", "gluten", "embroidered cloth", "Friday", "2004", "Pacific War", "Tsavo East National Park, Kenya", "Casa de Campo International Airport in the Dominican Republic", "almost 100", "vehicle insurers have an incentive and a method to reach car owners who haven't complied fully with recalls.", "a great-grandfather of Miami Marlin Christian Yelich"], "metric_results": {"EM": 0.625, "QA-F1": 0.7226934523809524}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.5, 0.888888888888889, 1.0, 1.0, 0.3333333333333333, 0.2857142857142857]}}, "before_error_ids": ["mrqa_searchqa-validation-2858", "mrqa_searchqa-validation-13708", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-3380", "mrqa_searchqa-validation-13891", "mrqa_searchqa-validation-2172", "mrqa_searchqa-validation-16208", "mrqa_searchqa-validation-7562", "mrqa_searchqa-validation-6957", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-11643", "mrqa_searchqa-validation-175", "mrqa_searchqa-validation-14995", "mrqa_searchqa-validation-16029", "mrqa_searchqa-validation-4368", "mrqa_searchqa-validation-11314", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-1226", "mrqa_naturalquestions-validation-4454", "mrqa_triviaqa-validation-7085", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-2352", "mrqa_newsqa-validation-2362", "mrqa_hotpotqa-validation-798"], "SR": 0.625, "CSR": 0.530517578125, "EFR": 1.0, "Overall": 0.708134765625}, {"timecode": 64, "before_eval_results": {"predictions": ["Malawi", "Asia", "sharpening steels", "the short-beaked echidna and the duck-billed platypus", "British Airways", "Yoshi", "1929", "The Blades", "Peter Carey", "Dante", "repechage", "uranium", "Wildcats", "Milady de Winter", "Abraham Lincoln", "Merchant of Venice", "inches", "Paul Rudd", "Tanzania", "Julian", "Christian Louboutin", "saxophone", "Firecracker Maria Elena (Penelope Cruz)", "September", "Grantham,", "Muriel Spark", "Turkey", "kvetching", "the Netherlands", "Lome", "what", "French couture designer Christian Dior", "Pat Houston", "King William IV", "Dutch", "Jack Lemmon", "Trainspotting", "Australia", "the north-west corner of the central business district", "Diptera", "India and Pakistan", "obi", "ccoli", "Heisenberg", "1976", "Cyclopes", "phrenology", "Full Metal jacket", "Tokyo", "California", "Windermere", "third", "the fictional elite conservative Vermont boarding school Welton Academy", "Norman occupational surname ( meaning tailor ) in France", "Daniel Richard", "between 11 or 13 and 18", "Franz Ferdinand Carl Ludwig Joseph Maria", "Spanish", "the single-engine Cessna 206 went down,", "helping on the sandbags to keep the waters at bay.", "Rachel Carson", "the white flag of surrender", "Toni Morrison", "Copts"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5995535714285714}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-3067", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-326", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-4208", "mrqa_triviaqa-validation-825", "mrqa_triviaqa-validation-1712", "mrqa_triviaqa-validation-7398", "mrqa_triviaqa-validation-1407", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-6308", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-1819", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-4966", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8858", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-1926", "mrqa_searchqa-validation-4044", "mrqa_newsqa-validation-2435"], "SR": 0.546875, "CSR": 0.5307692307692308, "EFR": 0.9655172413793104, "Overall": 0.7012885444297082}, {"timecode": 65, "before_eval_results": {"predictions": ["amanda barrie", "luigi", "spump", "Tina Turner", "The Four Seasons", "the BBC", "1986", "port", "commercial", "Charles Manson", "Uganda", "Ash", "a steep slope", "satirical cartoons, drawings, stage design", "Alaska", "iron", "william drogba", "c\u00e9vennes", "bagram", "Mnemosyne", "lincoln", "The Pillow Book", "isabella", "ethiopia", "john denver", "smack", "Massachusetts", "jet streams", "wellington", "The Apprentice", "Altamont", "jumpin Jack Flash", "brixham", "billy ray cyrus", "Ghana", "alla capella", "Mandela", "luigi", "Illinois", "sailing", "waterproof", "two-thirds", "Oman", "CBS", "Noah Beery, Jr.", "lincoln", "Sarajevo", "luigi", "Beyonce", "Carl Johan", "MI5", "Games played", "Something to Sing About", "petition for a writ of certiorari", "Little Dixie", "American Way", "Arab", "fluoroquinolone", "Alina Cho", "eight.\"", "The Penguin", "Passover", "Johann Strauss II", "John Lennon and George Harrison,"], "metric_results": {"EM": 0.625, "QA-F1": 0.66640625}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-2184", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-5539", "mrqa_triviaqa-validation-2177", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-7203", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-7708", "mrqa_triviaqa-validation-940", "mrqa_triviaqa-validation-2918", "mrqa_triviaqa-validation-5878", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-6095", "mrqa_naturalquestions-validation-7950", "mrqa_hotpotqa-validation-150", "mrqa_newsqa-validation-1804", "mrqa_searchqa-validation-14825"], "SR": 0.625, "CSR": 0.5321969696969697, "EFR": 1.0, "Overall": 0.708470643939394}, {"timecode": 66, "before_eval_results": {"predictions": ["sleepless in seattle", "Margaret Beckett", "Portugal", "typhoid fever", "Sheryl Crow", "melvil Dewey", "pennsylvania", "Pancho Villa", "the Wild Bunch", "mikhail gorbachev", "south Korea", "prince bride", "tiptoe through the Tulips", "london", "soprano", "the Greek Goddess of Revenge", "c\u00e9vennes", "Les Invalides", "1861", "p Larson brown", "london", "Dvorak", "Samuel", "aslan", "26", "the narwhal", "wishbone", "photographer", "Charlie Chan", "taekwondo", "phosphorus", "Hogwarts School of Witchcraft and Wizardry", "plutonium", "Mercury", "tanks", "dorset", "j. S. Bach", "Groucho Marx", "lacrosse", "Queen Elizabeth I", "london london", "1804", "london", "Clydebank", "mad hatter", "Bette Davis", "heating device", "Chrysler", "\"The closest approach to the original sound\"", "kataria", "pennsylvania alabama", "2010", "Pepsi", "the country", "FAI Junior Cup", "the Knight Company", "north Miami-Dade County", "world leaders", "January 24, 2006.", "after giving birth to baby daughter Jada,", "family", "Ovid", "any conditions or", "Latin"], "metric_results": {"EM": 0.578125, "QA-F1": 0.59375}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4624", "mrqa_triviaqa-validation-4413", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2153", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-580", "mrqa_triviaqa-validation-4502", "mrqa_triviaqa-validation-458", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-5492", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-3425", "mrqa_triviaqa-validation-1834", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-4907", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-1794", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-4523", "mrqa_hotpotqa-validation-1030", "mrqa_searchqa-validation-8081"], "SR": 0.578125, "CSR": 0.5328824626865671, "EFR": 0.9629629629629629, "Overall": 0.701200335129906}, {"timecode": 67, "before_eval_results": {"predictions": ["the Ring", "Gandhi", "Battlestar Galactica", "humpback whale", "South Africa", "Brett Favre", "Peril", "Texas", "angioplasty", "anxiety", "a phaser", "Mary Pickford", "Rivera", "Sayonara", "a cat", "india", "Jack Benny", "Mars", "the Battle of Verdun", "statistic", "the Andes", "India", "Cecil John Rhodes", "Houston Rockets", "sirloin", "apartheid", "Boston", "bianco", "Van Helsing", "\"elbow\"", "pesos", "Shop", "Lt. Bligh", "Urban Outfitters", "Burt Baskin", "Andrew Wyeth", "smallpox", "jimmy", "Al Jolson", "Risk", "recessive", "Don Quixote", "Richmond", "midnight", "The Age of Innocence", "Students for a Democratic Society", "the Bering Sea", "the Caucasus", "Coretta Scott", "tritonic", "Yellow Submarine", "the Brewster family", "After World War II", "abbreviation", "brighton", "Microsoft", "fleet street", "1966", "Juilliard School", "City of Starachowice", "success as a recording artist", "Mombasa, Kenya,", "the opposition group,", "peninsulas"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7121527777777777}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11546", "mrqa_searchqa-validation-2378", "mrqa_searchqa-validation-15118", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-4607", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4010", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-9669", "mrqa_searchqa-validation-5960", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-9122", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-4308", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2674", "mrqa_naturalquestions-validation-2064"], "SR": 0.65625, "CSR": 0.5346966911764706, "EFR": 0.9545454545454546, "Overall": 0.6998796791443851}, {"timecode": 68, "before_eval_results": {"predictions": ["Davenport", "an idiom", "Ulysses S. Grant", "the ovaries", "Oregon", "the gap", "Grease", "Kansas", "neurons", "the Old Man Down the Road", "a kilomega- (kM-)", "king", "an electron", "the Communist Party", "Alfred Binet", "Bath", "the Atlantic Ocean", "the Billboard", "James Buchanan", "Hindu", "the Miasa French Countryside Ruby Stemware", "Wayne Brady", "Frida Kahlo", "Arkansas", "chromosomes", "Cuba", "The Computer Age", "airplanes", "the Mortimer D. Sackler, M.D. Prize for Distinguished Achievement in", "Thomas Nast", "3", "freezing", "Picabo Street", "the National Security Agency", "Kiss Me Kate", "Honey Nut Cheerios", "Hercules", "the Gallic War", "Rod Laver", "an Eczema Therapy Bath Treatment", "Ivy Dickens", "Selma Blair", "vote", "Herbert Hoover", "Joe Hill", "IHOP", "The Lottery by Shirley Jackson", "Lou Gehrig's disease", "Samuel Beckett", "William Pitt the Younger", "Independence Day", "Daya Jethalal Gada", "Kevin Sumlin", "Charles Carson", "Albert Einstein", "Orion", "the dodo", "1986", "Michael Rispoli", "Mandarin Airlines", "at least two and a half hours.", "6 feet 6 inches,", "Buenos Aires", "Citizens for a Sound Economy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5833333333333334}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-30", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-15848", "mrqa_searchqa-validation-6898", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-15032", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-9338", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-5152", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-8453", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-6721", "mrqa_searchqa-validation-6749", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-8996", "mrqa_searchqa-validation-2684", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-14015", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_hotpotqa-validation-2003", "mrqa_newsqa-validation-2945"], "SR": 0.53125, "CSR": 0.5346467391304348, "EFR": 1.0, "Overall": 0.7089605978260869}, {"timecode": 69, "before_eval_results": {"predictions": ["Manitoba", "Santa Monica, California", "60 Minutes", "a fruitcake", "After midnight", "Socrates", "Al Gore", "the Louvre", "(Henry) II", "the Desert Fox", "Baton Rouge", "Langston Hughes", "a manacle", "Wisconsin", "Cleveland", "slave", "lively", "Stevie Wonder", "Typhoid Mary", "an inch", "from 700 to 2000", "Tokyo", "Tennessee Williams", "the United Arab Emirates", "Lurch", "the President", "Big Ben", "Old Yeller", "a mammal", "A Space Odyssey", "A Brief History of Time", "Prince", "Genesis", "Duncan", "Yitzhak Rabin", "Brisbane", "Jason Bourne", "Flagellants", "a globe", "Iowa", "Mephistopheles", "Samsonite", "the Marine Corps", "Vietnam", "Bern", "Punxsutawney", "Sports Illustrated", "Venus", "Shia LaBeouf", "Annapolis", "Princess Anne", "( Hungarian : Magyarorsz\u00e1g z\u00e1szlaja )", "January 15, 2007", "Introverted Sensing ( Si )", "Glastonbury Abbey", "Trainspotting", "Stella McCartney", "1902", "Anne of Green Gables", "Daniil Shafran", "a Columbian mammoth", "Pixar", "helps the convicts find calmness in a prison culture lively with violence and chaos.", "then-Sen. Obama"], "metric_results": {"EM": 0.65625, "QA-F1": 0.709375}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.2, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6601", "mrqa_searchqa-validation-6311", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-9482", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-4836", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-14167", "mrqa_searchqa-validation-12673", "mrqa_searchqa-validation-3164", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-15830", "mrqa_searchqa-validation-3297", "mrqa_searchqa-validation-7011", "mrqa_naturalquestions-validation-6020", "mrqa_triviaqa-validation-6872", "mrqa_hotpotqa-validation-3564", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3695"], "SR": 0.65625, "CSR": 0.5363839285714286, "EFR": 1.0, "Overall": 0.7093080357142857}, {"timecode": 70, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2351", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2897", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3440", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4086", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4104", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4260", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5757", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-591", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9857", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1706", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1752", "mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3957", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-10001", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12101", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15105", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15196", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15355", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15553", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1840", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4751", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4867", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5138", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9310", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10156", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-1971", "mrqa_squad-validation-2082", "mrqa_squad-validation-2186", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2318", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3205", "mrqa_squad-validation-3215", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4299", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4807", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-513", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5673", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6067", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-680", "mrqa_squad-validation-6917", "mrqa_squad-validation-6960", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9401", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_squad-validation-9830", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1278", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2150", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3181", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3428", "mrqa_triviaqa-validation-3455", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4014", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4389", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-557", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5907", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6241", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-768", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.85546875, "KG": 0.53125, "before_eval_results": {"predictions": ["Charles", "Fritos", "a cross", "the fowls", "a fruitcake", "Carrie Underwood", "Night", "Christa McAuliffe", "the Kilimanjaro", "the Misbegotten", "a pumpkin", "Jumbo", "Stoke", "Mexico", "Adolf Hitler", "Portland", "imagism", "Rudolf Diesel", "a palace", "a brig", "the Spanish Republic", "Ruth", "sugar", "Cheaper by the Dozen", "Hans Christian Andersen", "San Francisco", "the Velvet Underground", "Wanted", "the trumpet", "dull", "Emma Peel", "the Homestead Act", "Liberty Island", "Max Factor", "the plantain", "Marie Curie", "Middle German", "Aladdin", "rain", "the Samson", "Toy Story", "the Tea Party", "George Sand", "Jim Jarmusch", "Afghanistan", "Zeus", "salamanders", "Charles", "the rose hips", "Led Zeppelin", "city of Germenicia", "Michigan", "Texas A&M Aggies", "Alex Burrall, Jason Weaver and Wylie Draper", "Jackson", "The Kentucky Derby", "Iwo Jima", "Charles L. Clifford", "Vince Staples", "Sam Bettley", "Al-Shabaab", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Lucky Dube,", "Florida"], "metric_results": {"EM": 0.640625, "QA-F1": 0.709110936041083}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.7000000000000001, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3842", "mrqa_searchqa-validation-8680", "mrqa_searchqa-validation-4747", "mrqa_searchqa-validation-1917", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-2880", "mrqa_searchqa-validation-11834", "mrqa_searchqa-validation-14430", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-8856", "mrqa_searchqa-validation-2455", "mrqa_searchqa-validation-13954", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-6077", "mrqa_searchqa-validation-8455", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-1531", "mrqa_triviaqa-validation-1148", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-585"], "SR": 0.640625, "CSR": 0.5378521126760563, "EFR": 1.0, "Overall": 0.7325704225352112}, {"timecode": 71, "before_eval_results": {"predictions": ["The Beatles", "The Andy Griffith Show", "impractical", "the Pocono Mountains", "trip", "watermelon", "a kart", "tanks", "Simon & Garfunkel", "Albert Pujols", "the Andean bear", "ordinal", "nebulae", "(George) W. Bush", "Shirlington", "The Who", "Cy Young", "Austin Powers", "mime instructor", "drums", "a redwood", "Nellie Bly", "IBM", "meat", "Athol Fugard", "menudo-mdo", "money", "cloven", "Nicole Kidman", "Aristophanes", "Wimbledon", "Prince Siddhartha Gautama", "restrictive", "a monkey", "a turquoise bead", "Papua New Guinea", "Rooster Cogburn", "Halo 3", "Boz", "Sayonara", "touch", "the crescent", "The Moment of Truth", "an ant bear", "Harpy", "Henry Fielding", "James A. Garfield", "a duck", "Howie Mandel", "a Chinese orbital carrier rocket", "(Henry) Cavendish", "three degrees", "Olivia", "May 31, 2012", "Syria", "Ida Noddack", "(Bokm\u00e5l) or  (Nynorsk)", "World War II", "Peter Kay's Car Share", "Oneida Limited", "Zac Efron", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "the murders of his father and brother.\"", "mantle"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6220953525641025}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11101", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-15754", "mrqa_searchqa-validation-1282", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-9006", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-5133", "mrqa_searchqa-validation-2234", "mrqa_searchqa-validation-7031", "mrqa_searchqa-validation-16168", "mrqa_searchqa-validation-10201", "mrqa_searchqa-validation-12152", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-8888", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-3908", "mrqa_searchqa-validation-8196", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-1105", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-2850", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-5808", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-2382"], "SR": 0.578125, "CSR": 0.5384114583333333, "EFR": 1.0, "Overall": 0.7326822916666667}, {"timecode": 72, "before_eval_results": {"predictions": ["133", "the United States", "Mediterranean", "the chaos and horrified reactions", "$2 billion", "\"Slumdog Millionaire\"", "17", "London's O2 arena,", "Joel \"Taz\" DiGregorio,", "second child", "her home", "a fake license", "U.S. 93", "a tracheotomy", "64,", "Muslim festival", "travel really light so that we can roll into town, hire the crew, and also hire local equipment.", "without the restrictions", "the immediate release into the United States of 17 Chinese Muslims", "a satellite", "President Thabo Mbeki", "cross-country skiers", "Lisa Brown", "Christmas", "the reality he has seen is \"terrifying.\"", "Obama could be successful as a black candidate in part because of his \"light-skinned\" appearance and speaking patterns \"with no Negro dialect, unless he wanted to have one.\"", "stylish clothes designed and produced by prison inmates.", "flooding", "Acura", "Now Zad in Helmand province, Afghanistan.", "a clear strategy", "second", "through the weekend", "Barack Obama", "deputy president", "the abduction of minors", "three", "Fernando Caceres", "Joan Rivers", "the L'Aquila earthquake,", "David Russ", "country directors", "Iran", "we must eliminate the perceived stigma, shame and dishonor of asking for help,\"", "as a way for people to be able to buy things or get things and do it economically,\"", "Filippo Inzaghi", "BMW", "Afghanistan", "the bombers", "mother", "Asashoryu", "king", "a horizontal control layer that isolates the access network from the service layer", "Ant & Dec", "Robert A. Heinlein", "a falcon", "friends", "IT", "St. Louis Cardinals", "south-west", "Popcorn", "2", "a white elephant", "Lord Robert Cecil"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5822403999233502}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.21052631578947367, 0.6666666666666666, 0.9411764705882353, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.9152542372881356, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.125, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-893", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-1348", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-2693", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-686", "mrqa_newsqa-validation-2255", "mrqa_newsqa-validation-1475", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1373", "mrqa_newsqa-validation-631", "mrqa_newsqa-validation-3332", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-1128", "mrqa_naturalquestions-validation-6294", "mrqa_triviaqa-validation-179", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-2653", "mrqa_searchqa-validation-8732", "mrqa_hotpotqa-validation-364"], "SR": 0.46875, "CSR": 0.537457191780822, "EFR": 1.0, "Overall": 0.7324914383561645}, {"timecode": 73, "before_eval_results": {"predictions": ["$40 and a bread.", "Justicialist Party, or PJ by its Spanish acronym,", "\"Vaughn,\"", "Efraim Kam,", "11,", "Ralph Cifaretto on the HBO series \"The Sopranos,\"", "an average of 25 percent", "February 5,", "President Robert Mugabe", "love the environment and hate using fuel,\"", "OneLegacy,", "an upper respiratory infection,\"", "10:30 p.m. October 3,", "Arabic, French and English,", "Antonio Maria Costa, executive director of the U.N. Office on Drugs and Crime,", "\"Maude\" and the sardonic Dorothy on \"The Golden Girls,\"", "Bill,", "great tenderness toward the people who devoted themselves to Rin Tin Tin Tin", "ancient Jewish tradition", "\"The Da Vinci Code\"", "J.G. Ballard,", "protective shoes", "17", "2008", "12 hours", "Now Zad in Helmand province, Afghanistan.", "Russia and some European countries have expressed concerns about the missile defense system. While Poland and the Czech Republic have agreed to host parts of the system, others in Europe share Russian concerns that the defensive shield could be used for offensive aims.", "American", "Los Alamitos Joint Forces Training Base", "Al-Shabaab,", "safer surroundings.", "A McCain spokesman attacked Obama's plan, saying the Democrat's \"agenda to raise taxes and isolate America from foreign markets will not get our economy back on track or create new jobs.\"", "an upper respiratory infection,\"", "teacher Ronald F. Ferguson", "Sri Lanka", "\"CNN Heroes: An All-Star Tribute\"", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "$162 billion in war funding", "scientific reasons.", "innovative, exciting skyscrapers", "fled to Holland to escape the wave of anti-Semitism spreading across Europe as the Nazis rose to power in Germany ahead of the Second World War.", "Pakistani territory", "assassinated Thursday in Rawalpindi,", "Pew Research Center", "self-styled revolutionary Symbionese Liberation Army", "38,", "a \"stressed and tired force\"", "speed attempts", "Leo Frank,", "At least 38", "Iran", "Port Said to the southern terminus of Port Tewfik at the city of Suez", "Greek", "September 2017", "Dr John Sentamu", "Tony Cozier", "Sheffield", "Dulwich", "Tom Coughlin", "South America", "Vulcan", "Beloved", "a flapjack", "Hungarian"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5980565767973856}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false], "QA-F1": [0.7499999999999999, 0.9411764705882353, 0.0, 0.0, 1.0, 0.2857142857142857, 0.4, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.44000000000000006, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.20000000000000004, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.28571428571428575, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1999", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-1569", "mrqa_newsqa-validation-2266", "mrqa_newsqa-validation-3280", "mrqa_newsqa-validation-1793", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2763", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-10147", "mrqa_triviaqa-validation-6111", "mrqa_hotpotqa-validation-3195", "mrqa_searchqa-validation-1383"], "SR": 0.46875, "CSR": 0.5365287162162162, "EFR": 1.0, "Overall": 0.7323057432432433}, {"timecode": 74, "before_eval_results": {"predictions": ["South Korea's newest plant", "their surrogate,", "16", "the program was made with the parents' full consent.", "five", "Marie-Therese Walter.", "Sixteen", "1979", "federal help", "consumer confidence", "$81,3404", "Climatecare, one of Europe's most experienced providers of carbon", "Haleigh Cummings,", "state senators", "August 19, 2007.", "Ghana in Egypt.", "6,000", "1-0", "Brian Smith.", "Barack Obama sent a message that fight against terror will respect America's values.", "to provide security as needed.\"", "Sunday,", "a city of romance, of incredible architecture and history.", "the first of 1,500 Marines", "appealed urgently to be rescued, fearing the crew could be harmed or killed,", "Bill Haas", "Silicon Valley.", "things are better in their own lives than in the rest of the country,\"", "five", "British", "cash for Clunkers", "Yemen,", "the Dalai Lama", "800,000", "going through a metamorphosis from blobs of orange to art as night falls.", "Russia", "one", "Democratic VP candidate", "former U.S. soldier Steven Green", "Cologne, Germany,", "President George H.W. Bush", "Kurdish militant group in Turkey", "6-4", "Karen Floyd", "St Petersburg and Moscow,", "flights", "1-1 draw", "Bahrain's", "Alaska or Hawaii.", "30", "repeal of the military's \"don't ask, don't tell\" policy", "Dollree Mapp", "Matthias Schleiden and Theodor Schwann", "the American Civil War", "giraffe", "hiking and climbing", "World Bank", "Dallas/Fort Worth Metroplex", "Bob Hurley", "Ashanti Region", "Michael Kors", "Katherine Heigl", "Lenin", "Lebanon"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6593045112781954}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9473684210526316, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.10526315789473685, 1.0, 1.0, 0.2, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-4199", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-1861", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-1256", "mrqa_newsqa-validation-1763", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-4118", "mrqa_newsqa-validation-910", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-258", "mrqa_naturalquestions-validation-3705", "mrqa_triviaqa-validation-280", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-5300", "mrqa_triviaqa-validation-6077"], "SR": 0.5625, "CSR": 0.536875, "EFR": 0.9642857142857143, "Overall": 0.7252321428571429}, {"timecode": 75, "before_eval_results": {"predictions": ["The Wiltshire adventurer and TV personality Bear Grylls", "1 vereinsthaler", "Terry", "The Elbe", "New South Wales", "Robert Agar ( Donald Sutherland)", "Stockholm", "Leo", "Henry VIII", "a power outage Sunday,", "Roger Corman", "Pluto", "Deep Blue", "yellow-brown colour", "1996", "squid", "the failure of the duke of Monmouth\u2019s rebellion", "Ishmael", "New Israel Shekel", "serbia", "The Lost Weekend", "Althorp", "the conquest of Peru", "June", "Persuasion", "Robert Taylor", "the AllStars", "naval power", "Norman Mailer", "fishes", "floating", "Florence", "Touchstone Gold test", "pascal", "\"gruppetto\"", "Frankie Valli", "11", "Israel", "football", "murder, madness and seduction", "English", "Gentlemen Prefer Blondes", "puppies", "Kenya", "Conrad Murray", "Morph", "Amelia Earhart", "Spinach", "John Gorman", "Pope Benedict XVI", "cirrus", "Ariana Clarice Richards", "Turner Layton", "The Parlement de Bretagne", "Campbellsville University", "Daniel Craig", "Vanilla Air", "HPV", "\"This is not something that anybody can reasonably anticipate,\"", "21 percent suggesting that things are going well.", "the bottle", "Ohio", "Parkinson's disease", "John Gotti"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6194444444444445}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true], "QA-F1": [0.4444444444444445, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5621", "mrqa_triviaqa-validation-5929", "mrqa_triviaqa-validation-4049", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-771", "mrqa_triviaqa-validation-2332", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5227", "mrqa_triviaqa-validation-4227", "mrqa_triviaqa-validation-5132", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-1221", "mrqa_triviaqa-validation-6770", "mrqa_triviaqa-validation-2958", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-1490", "mrqa_naturalquestions-validation-7021", "mrqa_hotpotqa-validation-684", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-1303", "mrqa_searchqa-validation-13638"], "SR": 0.546875, "CSR": 0.5370065789473684, "EFR": 1.0, "Overall": 0.7324013157894737}, {"timecode": 76, "before_eval_results": {"predictions": ["Steve Jobs", "way too much ball.", "Islamic", "a prisoner killed in a Maryland county jail", "two women", "seven", "drug cartels", "suicide car bombing", "Illinois Reform Commission", "a vice-chairman of Hussein's Revolutionary Command Council.", "about 4 meters (13 feet) high", "crude oil", "183", "80,", "Lisa Brown", "Bob Bogle,", "back at work.", "Mexico", "The Rev. Alberto Cutie -- sometimes called \"Father Oprah\"", "California, Texas and Florida,", "3-3 draw", "Iran of trying to build nuclear bombs,", "\"There is no imminent threat of the government being overthrown, but the Taliban has gained momentum,\"", "dancing", "golf", "mosteller's brother-in-law", "\"It was a wrong thing to say,", "Sheikh Sharif Sheikh Ahmed", "8,", "safety issues in the company's cars", "bipartisan", "Haiti", "Mashhad", "10 percent", "Buenos Aires.", "U.S. Navy", "Natalie Wood", "oaxaca", "american third seed Venus Williams", "Aravane Rezai", "destroyed and his business is shattered,", "\"the most dangerous precedent in this country, violating all of our due process rights.\"", "15,000", "the state's first lady,", "on vacation", "The father of Haleigh Cummings,", "\"I am sick of life -- what can I say to you?\"", "E. coli", "Michoacan state,", "1,500", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "Jyoti Basu", "60 by West All - Stars", "Stax Records songwriters Homer Banks, Carl Hampton and Raymond Jackson", "leeds", "edward ii", "seattle", "strings", "Xherdan Shaqiri", "1958", "William Tecumseh Sherman", "George W. Bush", "air pressure", "tetrapods"], "metric_results": {"EM": 0.46875, "QA-F1": 0.562636883354994}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false, false, true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 0.6666666666666666, 1.0, 0.08333333333333333, 1.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.11764705882352942, 0.08695652173913043, 0.5, 0.0, 1.0, 0.3333333333333333, 0.3076923076923077, 1.0, 0.0, 1.0, 1.0, 0.5, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1227", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3985", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-3372", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-3968", "mrqa_newsqa-validation-3785", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3282", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3767", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-4796", "mrqa_hotpotqa-validation-1902", "mrqa_searchqa-validation-10640", "mrqa_searchqa-validation-12792", "mrqa_triviaqa-validation-1454"], "SR": 0.46875, "CSR": 0.5361201298701299, "EFR": 0.9705882352941176, "Overall": 0.7263416730328495}, {"timecode": 77, "before_eval_results": {"predictions": ["Bill Haas", "a team of eight surgeons", "more than 200.", "The forward's lawyer", "former detainees of Immigration and Customs Enforcement", "Misty Cummings, then known asMisty Croslin,", "Dr. Death in Germany", "Ralph Lauren", "Charlie Chaplin", "at the site of a Metro train crash in Washington.", "Columbia", "73,", "Jenny Sanford,", "Desire Petroleum", "hundreds", "more than 78,000 parents", "St. Francis De Sales Catholic Church", "The Israeli Navy", "Buddhism", "in Seoul,", "President Obama", "the United States", "strangulation and asphyxiation and had two broken bones in his neck,", "John and Elizabeth Calvert", "more than $2 billion in disaster assistance", "hanging a noose in a campus library,", "Spain", "Michael Jackson", "Brazil", "Pixar's", "Pixar", "1,500", "won the Bridgestone Invitational in Ohio in August.", "Two United Arab Emirates based companies", "fascinating transformation that takes place when carving a pumpkin.", "the best-of-three series.", "Transport Workers Union leaders", "Amanda Knox's aunt", "in the Muslim north of Sudan", "in Melbourne", "cancer", "school officials made some readjustments to inauguration security as a precaution and did not", "a U.S. ocean surveillance ship", "338", "finance", "Haeftling,", "bribing other wrestlers to lose bouts,", "in terms of the country's most-wanted list,", "outfit from designer", "350 U.S. soldiers", "\"Taz\" DiGregorio,", "Active non-osmotic water absorption", "Robin", "17 -- 15", "wayverning body of Jehovah's Witnesses", "the Sulu Archipelago", "Egypt", "The satirical", "Valhalla Highlands Historic District", "boxer", "push", "rain", "Wynton Marsalis", "argon"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5068700396825397}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.2666666666666667, 0.4, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-2633", "mrqa_newsqa-validation-1290", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3986", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-4208", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2997", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-1768", "mrqa_newsqa-validation-2463", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-3310", "mrqa_newsqa-validation-4020", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-8444", "mrqa_triviaqa-validation-7062", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-2388", "mrqa_searchqa-validation-12492", "mrqa_searchqa-validation-4358"], "SR": 0.40625, "CSR": 0.5344551282051282, "EFR": 1.0, "Overall": 0.7318910256410256}, {"timecode": 78, "before_eval_results": {"predictions": ["3-2", "Aung San Suu Kyi", "1957,", "Fort Bragg in North Carolina.", "their emergency plans", "South Africa", "Al-Shabaab,", "it -- you know -- black is beautiful,\"", "the assassination of President Mohamed Anwar al-Sadat", "Diego Milito's", "11 countries,", "Robert Barnett,", "More than 22 million people in sub-Saharan Africa", "the driver", "in the Intertropical Convergence Zone,", "did not go into further detail about her heart condition or the medical procedure.", "200", "Phillip A. Myers.", "Congress", "bipartisan", "a rally", "1912.", "Steven Green", "The plane, an Airbus A320-214,", "Jennifer Kobilinsky,", "Lifeway's", "minimalism", "some deaths", "Seattle", "Dr. Christina Romete,", "SRI International,", "\"The Real Housewives of Atlanta\"", "an independent homeland", "Air traffic delays began to clear up Tuesday evening after computer problems left travelers across the United States waiting in airports,", "Iowa,", "Barbara Streisand's", "the death of a pregnant soldier", "raping", "five", "Takashi Saito,", "Black Entertainment Television founder Bob Johnson", "$41.1 million", "in order to accommodate the public,", "natural disasters", "Joe Jackson,", "red varnished cover with the word \"Album\" inscribed on it in gold lettering,", "the inspector-general of the House", "peace with Israel", "John Demjanjuk,", "following in Arizona's footsteps would take states in the wrong direction.", "Fourteen thoroughbred horses", "a couple broken apart by the Iraq War", "180th meridian or antimeridian", "instructions", "The Undertones", "Johnny Mercer", "Fenn Street School", "American actor and former fashion model", "the Vietnam War", "1967", "Ukraine", "capitals", "Francis Steegmuller", "genome"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5660943223443223}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-915", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-889", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-2831", "mrqa_newsqa-validation-768", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-584", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-4123", "mrqa_naturalquestions-validation-5583", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-9852", "mrqa_triviaqa-validation-6200", "mrqa_hotpotqa-validation-5128", "mrqa_searchqa-validation-6021", "mrqa_searchqa-validation-10570"], "SR": 0.46875, "CSR": 0.533623417721519, "EFR": 0.9411764705882353, "Overall": 0.7199599776619509}, {"timecode": 79, "before_eval_results": {"predictions": ["Dick Rutan and Jeana Yeager", "20 years from the filing date subject to the payment of maintenance fees", "The eighth and", "Rigg", "March 2016", "great celebrity", "the name announcement of Kylie Jenner's first child", "2003", "in the U.S.", "seven", "February 9, 2018", "Coroebus of Elis", "Jonathan Breck", "George Harrison", "Howard Ellsworth Rollins Jr", "Ancy Lostoma duodenale", "January 17, 1899", "UTC \u2212 09 : 00", "Thespis", "biological taxonomy", "Wednesday, September 21, 2016", "Robber baron", "three", "Tbilisi, Georgia", "off the rez", "1895", "King Harold Godwinson", "Sir Ronald Ross", "May 29, 2018", "1979", "In the television series's fourth season", "ancient Athens", "Russia", "the magnetic stripe `` anomalies '' on the ocean floor", "United Nations", "2018", "it culminates in a post as a Consultant, a General Practitioner ( GP ), or some other non-training post, such as a Staff grade or Associate Specialist post", "Real Madrid", "Angel Benitez", "by capillary action", "1830", "1st Earl Mountbatten", "foreign investors", "Bhupendranath Dutt", "In June 22, 1942", "19 state rooms", "In 1984", "senators", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Paul Lynde", "electric potential generated by muscle cells when these cells are electrically or neurologically activated", "treaty of Waitangi", "James Garfield", "The Gambia", "England", "Sverdlovsk,", "47,818", "seven", "Samoa", "to close their shops during daily prayers,", "Versailles", "Westminster College", "Bob Dylan", "Mount Pelee"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7513671377641966}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [0.5714285714285715, 0.5882352941176471, 0.5, 1.0, 1.0, 0.13333333333333336, 0.7999999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6, 0.0, 1.0, 0.8571428571428571, 0.4, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3592", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-8530", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-6865", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-306", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9242", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-7848", "mrqa_triviaqa-validation-5704", "mrqa_searchqa-validation-13149", "mrqa_searchqa-validation-13773"], "SR": 0.609375, "CSR": 0.5345703125, "EFR": 0.84, "Overall": 0.6999140625}, {"timecode": 80, "UKR": 0.708984375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2162", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-3034", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3463", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-5360", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-101", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3053", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-340", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4013", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-5950", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3327", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11088", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-11356", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13490", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2382", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3216", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-484", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6593", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-76", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-10491", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-2605", "mrqa_squad-validation-3179", "mrqa_squad-validation-3270", "mrqa_squad-validation-3461", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3885", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-6727", "mrqa_squad-validation-6981", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8589", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1082", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1428", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-200", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-2817", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3682", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4209", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4681", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-583", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6265", "mrqa_triviaqa-validation-635", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7247", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694", "mrqa_triviaqa-validation-7733"], "OKR": 0.78515625, "KG": 0.48125, "before_eval_results": {"predictions": ["the 13th century", "14 December 1972 UTC", "China", "1919", "Abraham Gottlob Werner", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "September, 2016", "July 2014", "peroxidase", "at a given temperature", "Skat", "Claudia Grace Wells", "Bill Russell", "the Sons of Liberty in Boston, Massachusetts", "the Reverse - Flash", "in vertebrate's immune system", "Melanie Lynskey", "four", "1923", "San Francisco, California", "2018", "Lilian Bellamy", "Denver Broncos", "Zoe Badwi, Jade Thirlwall's cousin", "1997", "William Wyler", "Ray Harroun", "one person", "the NIRA", "Santa Fe, New Mexico, USA", "Kaley Christine Cuoco", "Marshall Sahlins", "Cozonac ( Romanian pronunciation : ( kozo\u02c8nak ) ) or Kozunak", "the United Kingdom", "the Gupta Empire", "the oculus, or `` eye point ''", "James Madison", "Matt Monro", "Burton upon Trent", "Algeria", "June 1992", "prokaryotic cell ( or organelle )", "in 1942", "the English", "the May Revolution of 1810", "Jason Flemyng", "blue", "unknown origin", "the NW North American Plate", "1963", "Kaiser Chiefs", "Donna Jo Napoli", "claire goose", "Stephen Richards Covey", "road movie", "Earvin \"Magic\" Johnson Jr.", "Sheikh Sharif Sheikh Ahmed", "snakes", "posting a $1,725 bail,", "Israel", "spinal stenosis", "1936", "carbon"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6081056096681097}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.38095238095238093, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2382", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-5711", "mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-5332", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-681", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-10188", "mrqa_triviaqa-validation-4432", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-2802", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-1713", "mrqa_searchqa-validation-8396", "mrqa_searchqa-validation-11202"], "SR": 0.53125, "CSR": 0.5345293209876543, "EFR": 0.9666666666666667, "Overall": 0.6953173225308642}, {"timecode": 81, "before_eval_results": {"predictions": ["William the Conqueror", "1963", "Alastair Cook", "The President of Zambia", "Ray Charles", "AD 95 -- 110", "HTML", "in the 1970s", "Dan Stevens", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "North Dakota", "Tatsumi", "1969", "Ethiopia", "1937", "artes liberales", "Debbie Reynolds", "MSC Crociere S. p.A.", "2020", "2007", "Hodel", "the coffee shop Monk's", "Siddharth Arora / Vibhav Roy", "1991", "1900", "the World Trade Center Transportation Hub", "Munich, Bavaria", "Steve Russell", "October 2, 2017", "Paul Lynde", "to regulate the employment and working conditions of civil servants, oversee hiring and promotions, and promote the values of the public service", "Frank Oz", "Sam Waterston", "Sunday night", "Johnny Darrell", "the 7th century at Rendlesham in East Anglia", "2018", "Will Turpin", "Ben Rosenbaum", "2013", "two", "Will Champion", "Bartemius Crouch Jr impersonating Alastor `` Mad - eye '' Moody ( book four )", "Pre-evaluation, strategic planning, operative planning, implementation, and post-evaluated", "Jackie Robinson", "Stephen Stills'former girlfriend, singer / songwriter Judy Collins", "Brazil, Turkey and Uzbekistan", "Times Square in New York City west to Lincoln Park in San Francisco", "two parallel planes", "the pia mater", "United Nations Peacekeeping Operations", "the Kinks", "Inishtrahull Island,", "the Indus Valley Civilization", "Interstate 95", "sacred to Cybele,", "an infinite sum of terms that are calculated from the values of the function's derivatives at a single point", "violating anti-trust laws.", "American Lindsey Vonn", "5.7 million", "Ptolemy", "Dixie's Land", "San Salvador", "Snickers candy bars"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6436777111629881}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.967741935483871, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.37499999999999994, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 1.0, 0.7999999999999999, 0.4, 0.631578947368421, 0.5, 1.0, 0.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.4210526315789474, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-4506", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-9830", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-10501", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-5862", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-1861", "mrqa_newsqa-validation-1728", "mrqa_newsqa-validation-3156", "mrqa_searchqa-validation-14076", "mrqa_triviaqa-validation-7778"], "SR": 0.484375, "CSR": 0.5339176829268293, "EFR": 0.9393939393939394, "Overall": 0.6897404494641538}, {"timecode": 82, "before_eval_results": {"predictions": ["the Four Seasons", "Kathleen Erin Walsh", "in 1603", "Stephen Graham", "in 2010", "Tulsa, Oklahoma", "beta decay", "Nickelback", "1998", "John Adams", "Ford", "Grisha Alekandrovich Nikolaev", "Linda Hamilton's real - life son Dalton Abbot", "( Pierre - Auguste Renoir)", "July 2, 1928", "Dottie West", "in northern China", "Blood is the New Black", "in 1977", "crossbar", "Professor Eobard Thawne", "Jack Nicklaus", "arm regions", "St. John's, Newfoundland and Labrador", "by the early - to - mid fourth century", "a young husband and wife", "May 29, 2018", "Church of England", "September 27, 2017", "April 10, 2018", "Daniel Suarez", "the slogan used to describe the six nations that have had sovereignty over some or all of the current territory of the U.S. state of Texas", "January 15, 2007", "the Western Bloc ( the United States, its NATO allies and others )", "the optic chiasma", "Empire of Japan", "the International Border ( IB )", "Ole Einar Bj\u00f8rndalen", "Jenny Slate", "2013", "Prince William, Duke of Cambridge, the Prince of Wales's elder son", "Nicklaus", "in 1904", "Jim Carrey", "Matt Monro", "a little warmth", "in the thylakoid lumen through the thyleakoid membrane and into the chloroplast stroma", "( born November 28, 1973 )", "energy from light is absorbed by proteins called reaction centres", "in Rome in 336", "March 29, 2018", "Charles Darwin", "David Letterman", "Fiat", "five months", "Wanda", "April 24, 1934", "40", "49,", "$2 billion", "\"Sex and the City\"", "coffee", "\"Bryan Adams\"", "Oldham, in Greater Manchester, England"], "metric_results": {"EM": 0.515625, "QA-F1": 0.7179401327838828}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.5, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.7272727272727272, 0.9090909090909091, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 0.9523809523809523, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.8, 0.4, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-6252", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3233", "mrqa_naturalquestions-validation-7641", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-5022", "mrqa_naturalquestions-validation-3830", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-2337", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-312", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-2832", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9687", "mrqa_triviaqa-validation-7718", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-2444", "mrqa_triviaqa-validation-6822"], "SR": 0.515625, "CSR": 0.5336972891566265, "EFR": 0.9032258064516129, "Overall": 0.6824627441216479}, {"timecode": 83, "before_eval_results": {"predictions": ["3D computer-animated comedy", "1964", "Hindi", "eight", "Flula Borg", "Comanche County, Oklahoma", "Swiss", "1999", "Congo River", "4,613", "George Washington Bridge", "H. R. Haldeman", "Guardians of the Galaxy Vol.  2", "2.1 million", "6'5\" and 190 pounds", "526 people per square mile", "North West England", "casting, job opportunities, and career advice", "an Anglo-Saxon saint", "Scotland", "2006", "Carrefour", "infinite sum of terms", "Stravinsky's \"Daphnis et Chlo\u00e9\"", "Chelsea", "second half of the third season", "Johnson & Johnson", "novelty songs, comedy, and strange or unusual recordings dating from the early days of phonograph records to the present.", "Brown Mountain Overlook", "Backstreet Boys", "45th", "Foxborough, Massachusetts", "Wichita", "animation", "Margaret Thatcher", "sandstone", "Reginald Engelbach", "Dutch", "BC Dz\u016bkija", "Anne", "James Fox", "Rebirth", "\"Big Mamie\"", "Albert Bridge, London", "Maxwell Atoms", "King Olav V of Norway", "King of Valois", "Alemannic", "Larry Alphonso Johnson Jr.", "Filipino-Swiss driver", "Taylor Swift", "October 27, 2016", "tenderness of meat", "Tigris and Euphrates", "Mozambique Channel", "Astor family", "Tashkent", "Pakistan's intelligence agency", "police", "grizzly bear", "Speed Racer", "Yogi bear", "sakura", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7058936403508772}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.21052631578947367, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-703", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5691", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-5298", "mrqa_hotpotqa-validation-3376", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1512", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-4696", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-9089", "mrqa_triviaqa-validation-5311", "mrqa_newsqa-validation-1216", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-5496"], "SR": 0.609375, "CSR": 0.5345982142857143, "EFR": 1.0, "Overall": 0.701997767857143}, {"timecode": 84, "before_eval_results": {"predictions": ["As of January 17, 2018, 201 episodes", "Atlanta", "around 1872", "1978", "Hodel", "Central Germany", "Tate", "the Outfield", "William Shakespeare's As You Like It", "a central place in Christian eschatology", "Robin Williams", "artes liberales", "the sinoatrial node", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "December 11, 2014", "A vanishing point", "Janie Crawford", "UTC \u2212 09 : 00", "mining", "Orange Juice", "interstitial fluid in the `` interstitial compartment ''", "nearly 92 %", "Missouri River", "macOS High Sierra", "Jason Marsden", "The Chipettes", "Linda Davis", "Fats Waller", "Thespis", "Neal Dahlen", "May 19, 2017", "season two", "11 : 40 p.m. ship's time", "the status line", "Samantha Jo `` Mandy '' Moore", "Dr. Lexie Grey", "Nodar Kumaritashvili", "the TLC - All That Theme Song", "Welch, West Virginia", "Nicki Minaj", "Steve Russell", "Julia Roberts", "the church at Philippi", "providing telecommunication services to enterprises and offices", "a valuable way to feed the poor, and would relieve some pressure of the land redistribution process", "Bed and breakfast", "The Lykan Hypersport", "five", "Geothermal gradient", "soft contact lenses", "Zak Starkey", "fire insurance", "Seoul, South Korea", "Friday", "Al Capone", "14 years", "ambassadors", "Russia", "a bin", "the Equator", "Caesar", "no"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7512620192307693}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.7692307692307692, 0.25, 1.0, 1.0, 1.0, 0.7000000000000001, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-2896", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-1767", "mrqa_triviaqa-validation-6303", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-5444", "mrqa_searchqa-validation-10351", "mrqa_searchqa-validation-16205", "mrqa_newsqa-validation-2213"], "SR": 0.65625, "CSR": 0.5360294117647059, "EFR": 0.9545454545454546, "Overall": 0.6931930982620321}, {"timecode": 85, "before_eval_results": {"predictions": ["the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "The uvea", "25 June 1932", "Felicity Huffman", "a site for genetic transcription that is segregated from the location of translation in the cytoplasm, allowing levels of gene regulation that are not available to prokaryotes", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag", "Roger Nichols and Paul Williams", "peninsular", "at least 28 vowel forms", "1957", "eleven separate regions of the Old and New World", "As of January 17, 2018, 201 episodes", "the Tennessee Titans", "Monk's", "B.R. Ambedkar, the chairman of the Drafting Committee", "septum", "Cecil Lockhart", "Executive Residence of the White House Complex", "David Motl", "cephalopods", "Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "PC2, a type II endoprotease, cleaves the C peptide - A chain bond", "New Croton Reservoir in Westchester and Putnam counties", "Professor Eobard Thawne", "Freddie Highmore", "USA Today", "1939", "74 languages", "during prenatal development in the central part of each developing bone", "Nebuchadnezzar", "A footling breech", "Robert Gillespie Adamson IV", "Waylon Jennings", "A rear - view mirror", "the fourth ventricle", "Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Doug Diemoz", "$5.4 trillion", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae", "Frank Zappa's 1979 album Joe's Garage Act I", "1956", "Camping World Stadium", "Louis XV's reign", "the final episode of the series", "Jonathan Breck", "Ludacris", "British Kennel Club", "126", "James Chadwick", "Olympia", "George Strait", "Swiss", "trade mark number 1", "Las Vegas Boulevard, commonly referred to as the Las Vegas strip, or the strip,", "Hindi cinema", "Brookhaven", "January 2004", "space shuttle Discovery", "dismissed all charges", "\"Empire of the Sun,\"", "frittata", "a crabitat", "a dime", "Matlock"], "metric_results": {"EM": 0.625, "QA-F1": 0.7338006207851158}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.9189189189189189, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6086956521739131, 0.0, 0.36363636363636365, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.2, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6485", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-5476", "mrqa_naturalquestions-validation-527", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2863", "mrqa_naturalquestions-validation-6116", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-10583", "mrqa_triviaqa-validation-2264", "mrqa_triviaqa-validation-1422", "mrqa_hotpotqa-validation-690", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-10904"], "SR": 0.625, "CSR": 0.5370639534883721, "EFR": 0.9166666666666666, "Overall": 0.6858242490310078}, {"timecode": 86, "before_eval_results": {"predictions": ["San Francisco", "a dancing twins", "( Rudolf) Nureyev", "flibutor", "Christo", "a song written by Giorgio Moroder and Tom Whitlock", "(Cyrus) McCormick", "the Ziegfeld Follies", "ballpoint pen", "a tabernacle", "a butterfly", "carbonara", "the Chowan Indians", "Einstein", "a Mafia Slang", "a roller coaster", "George H.W. Bush", "Elvis Presley", "a flushing toilet", "herbicides", "( Nikolaj) Gogol", "the Hudson River", "Banquo", "Virgo", "Texas", "a pardon", "an alligator", "General Mills", "a comb", "( Ferdinand Magellan)", "( Joey Beachum)", "the left side of the heart pumps oxygenated blood to the body", "Rabbit Hole", "The Secret Life of the American Teenager", "Greyhounds", "a circadian rhythm", "John Molson", "a lottery", "Alston", "Tanzania", "Colorado", "Christopher Columbus", "the Caribbean Sea", "Slovakia", "triangles", "Frederic Remington", "\"St. Patrick's... -and that he had been married one year.", "Evan Almighty", "the tooth Fairy", "third son", "a pillar", "Johannes Gutenberg", "203", "4 January 2011", "Neighbours", "a curb roof", "a Lion", "Norway", "Taylor Swift", "The Campbell Soup Company", "Stoke City.", "Victor Mejia Munera was a drug lord with ties to paramilitary groups,", "Los Angeles", "Venezuela"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5720486111111112}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, false, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1701", "mrqa_searchqa-validation-16088", "mrqa_searchqa-validation-10854", "mrqa_searchqa-validation-4009", "mrqa_searchqa-validation-7883", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-6274", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-10044", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-2269", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-15657", "mrqa_searchqa-validation-4790", "mrqa_searchqa-validation-632", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-12363", "mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-2353", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-14720", "mrqa_searchqa-validation-5092", "mrqa_searchqa-validation-971", "mrqa_triviaqa-validation-6287", "mrqa_triviaqa-validation-6649", "mrqa_hotpotqa-validation-5209", "mrqa_newsqa-validation-877", "mrqa_newsqa-validation-1516"], "SR": 0.46875, "CSR": 0.5362787356321839, "EFR": 1.0, "Overall": 0.7023338721264368}, {"timecode": 87, "before_eval_results": {"predictions": ["Shopgirl", "Easter Island", "True", "The Bridge on the River Kwai", "Motown Legends", "Virgin Atlantic", "dill", "Indira Gandhi", "pew", "Hitchcock", "circulatory system", "London Bridge", "Ho Chi Minh", "Vesuvius", "Himalaya", "Kodachrome", "House", "Paul Simon", "Sri Lanka", "A Prairie Home Companion", "Concord", "a dog", "the Thames", "Taipei", "Melba", "the Caspian Sea", "Babe Ruth", "the Edo era", "Stromboli", "apples", "chocolates", "bay", "the Canterbury Tales", "Japan", "Ben & Jerry", "a sweater", "Krakauer", "Saladin", "Sweden", "John Glenn", "the Cordillera Real mountain range", "ink", "Mitt Romney", "Goofy", "Peter Jackson", "an inch", "Neptune", "Earhart", "a whale shark", "Simon Cowell", "the Northern Mockingbird", "origins of replication, in the genome", "27 January -- 16 April 1898", "57 days", "Mr. Humphries", "Lucia Bozzola", "Tinie Tempah", "Richard Arthur", "Ghanaian national team", "Labour", "citizenship", "1,500", "South African President Thabo Mbeki,", "Louis XV"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7745535714285714}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10848", "mrqa_searchqa-validation-3533", "mrqa_searchqa-validation-10264", "mrqa_searchqa-validation-9510", "mrqa_searchqa-validation-13168", "mrqa_searchqa-validation-2075", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13900", "mrqa_searchqa-validation-9828", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-6808", "mrqa_searchqa-validation-4108", "mrqa_searchqa-validation-13821", "mrqa_searchqa-validation-13347", "mrqa_searchqa-validation-6778", "mrqa_searchqa-validation-6947", "mrqa_naturalquestions-validation-1277", "mrqa_triviaqa-validation-1234", "mrqa_newsqa-validation-3391"], "SR": 0.6875, "CSR": 0.5379971590909092, "EFR": 1.0, "Overall": 0.7026775568181819}, {"timecode": 88, "before_eval_results": {"predictions": ["Charles Dickens", "a fish", "Sherlock Holmes", "Grant Wood", "(Jeff) Probst", "\"Twelfth Night\"", "the Black Sea", "Eggs Benedict", "lovebird", "Agatha Christie", "a church", "a book of the Year", "the Mossad", "a backstroke", "a sitting place", "a defibrillator", "a tropical depression", "an airbags", "a proscenium arch", "a tuna", "(Thomas) Smith", "the sinuses", "Africa", "Charlotte Gainsbourg", "Kandahr", "(Nolan) Ryan", "a miso", "(William) Harrison", "clay", "the Jutland", "(Calvin) Hamilton", "the Fourteen Points", "(Kathy) Bates", "the Mohicans", "a crossword clue", "the Osmonds", "a guitar", "(Roberta) Flack", "Belgium", "(John) Miller", "Chicago", "an actuary", "a latke", "Montana", "a dulcimer", "(Candice) Bergen", "lead", "apocrypha", "a discus", "a fiesta", "Top Gun", "a chimera", "a membrane", "Nick Sager", "horse racing", "Uruguay", "the Solar System", "Deftones", "The Design Inference", "Dutch", "(Norra Grangesbergsgatan 4)", "3-0", "a bill", "Ronald Lyle \" Ron\" Goldman"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6192708333333334}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-16778", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-13978", "mrqa_searchqa-validation-8426", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-543", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-7047", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-16336", "mrqa_searchqa-validation-8423", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-8643", "mrqa_searchqa-validation-12604", "mrqa_searchqa-validation-2486", "mrqa_searchqa-validation-196", "mrqa_searchqa-validation-382", "mrqa_searchqa-validation-872", "mrqa_searchqa-validation-13957", "mrqa_naturalquestions-validation-181", "mrqa_triviaqa-validation-5284", "mrqa_triviaqa-validation-2631", "mrqa_triviaqa-validation-2402", "mrqa_hotpotqa-validation-3258", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-692", "mrqa_hotpotqa-validation-2410"], "SR": 0.546875, "CSR": 0.5380969101123596, "EFR": 1.0, "Overall": 0.7026975070224719}, {"timecode": 89, "before_eval_results": {"predictions": ["Four bodies", "The son of Gabon's former president", "African National Congress Deputy President Kgalema Motlanthe,", "Both men were hospitalized and expected to survive,", "that the six imprisoned leaders of the religious minority were held for security reasons and not because of their faith.", "18th", "the monarchy's", "that he acted in self defense in punching businessman Marcus McGhee.", "Dave Bego,", "near the equator,", "U.S. senators", "\"we have more work to do,\"", "one-shot victory", "that she was lured to a dorm and assaulted in a bathroom stall.", "Citizens", "death", "businessman", "his business dealings", "plastic surgery", "Meira Kumar", "The exact cause of IBS remains unknown,", "The Valley Swim Club", "Enrique Torres", "The FBI's Baltimore field office", "the UK", "the Delta Queen steamboat,", "Adidas,", "\"I think it is the cumulative effect of deployments from 12 to 15 months,\"", "Zac Efron", "he wants a \"happy ending\" to the case.", "1 percent", "4.6 million", "Miss USA Rima Fakih", "the Airbus A330-200", "the Internet", "228", "London's O2 arena,", "The Georgia Aquarium", "London Heathrow's Terminal 5.", "4.5 pounds of heroin,", "Bob Dole", "Arlington National Cemetery's", "Muqtada al-Sadr,", "Leaders of more than 30 Latin American and Caribbean nations", "At least 38", "two years,", "heavy turbulence", "alongside Deepwater Horizon", "Florida", "tax", "Jose Miguel Vivanco,", "genome", "2010", "Manchuria", "the Sea of Azov", "William WymarkJacobs", "serbia", "wine", "Tsavo East National Park", "\"Lucky\"", "bananas", "Ron Markstein", "Midas", "Home Alone"], "metric_results": {"EM": 0.5, "QA-F1": 0.5956113840115198}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6153846153846153, 0.0, 0.7499999999999999, 1.0, 0.125, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-3739", "mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-496", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-1418", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-236", "mrqa_newsqa-validation-4015", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2208", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-1036", "mrqa_naturalquestions-validation-5449", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-1968", "mrqa_hotpotqa-validation-3474", "mrqa_searchqa-validation-15665"], "SR": 0.5, "CSR": 0.5376736111111111, "EFR": 1.0, "Overall": 0.7026128472222222}, {"timecode": 90, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2785", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3258", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10369", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2495", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3173", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-3269", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6560", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11378", "mrqa_searchqa-validation-1144", "mrqa_searchqa-validation-11449", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-11678", "mrqa_searchqa-validation-11984", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-12581", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13540", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13572", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14006", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16523", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1707", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4313", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6491", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3822", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-4848", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6223", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2157", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-2771", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-3838", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5564", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5694", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5901", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6419", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.841796875, "KG": 0.5359375, "before_eval_results": {"predictions": ["1974", "bat", "\"Antz\"", "\"Switzerland of England\"", "Ford Island", "Food and Agriculture Organization", "Beno\u00eet Jacquot", "Lincoln,", "World War II", "Les Temps modernes", "Agra", "1986", "\"King of Cool\"", "841", "Missouri River", "1.23 million", "32", "Clarence Nash", "2015", "James Franco", "1970", "Sam Kinison", "1918", "Robert Gibson", "3730 km", "Washington State Cougars", "1939", "Forbes", "Univision", "biographer", "1877", "Dallas", "237 square miles", "Tennessee", "Orlando", "Oklahoma City, Oklahoma", "Dakota Johnson", "The School Boys", "Amble", "2007", "Japan", "indoor", "Sir Konstantin Sergeevich Novoselov", "Balloon Street, Manchester", "Ted", "Vanarama National League", "Anne Erin \"Annie\" Clark", "the Atlantic Ocean", "Samantha Spiro", "Fort Hood, Texas", "Hong Kong", "Clarence Darrow", "Ren\u00e9 Verdon", "Instagram", "1961", "j. M. W. Turner", "geese", "Bill Klein,", "last summer.", "speed attempts", "Andrew Wyeth", "a calico cat", "biddy", "to the U.S. Holocaust Memorial Museum"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6763888888888888}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2683", "mrqa_hotpotqa-validation-611", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4539", "mrqa_hotpotqa-validation-1811", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-4371", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5341", "mrqa_hotpotqa-validation-4208", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-2970", "mrqa_naturalquestions-validation-930", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-7502", "mrqa_triviaqa-validation-6410", "mrqa_searchqa-validation-9898", "mrqa_newsqa-validation-2420"], "SR": 0.5625, "CSR": 0.5379464285714286, "EFR": 0.9285714285714286, "Overall": 0.7133816964285714}, {"timecode": 91, "before_eval_results": {"predictions": ["Leo Tolstoy", "Montana", "Tigger", "Huguenots", "Mario", "Lexington and Concord", "a ray", "mint", "Roxanne", "salutatorian", "Roald Dahl", "ER", "Buffalo Bill", "a pager", "Hawaii", "Sir Isaac Newton", "Radiohead", "Cain", "a Lignite", "the Vietnam War", "Algebra", "Catherine of Aragon", "paul mcc McCartney", "Daisy", "Drumline", "Donnie Wahlberg", "cytokinesis", "the Unabomber", "Tom Petty", "Harry Potter", "The Sixth Sense", "New Wave", "Gin", "Santa", "fashion", "Billy the Kid", "The Stone Age", "(Cecil) Rhodes", "James Garner", "an inn", "Michelle Pfeiffer", "a double take", "Michael Lewis Webster", "Michael Phelps", "Paraguay", "the Chinook", "a spectroscope", "sesame", "a quart", "hock", "Late Night", "UNICEF's global programing", "defense against rain rather than sun", "Vijaya Mulay", "andean", "Istanbul", "U.S.", "Dante", "George Raft", "Grace O'Malley", "Zed", "Communist", "an empty water bottle down the touchline", "new wave rock band The Fixx"], "metric_results": {"EM": 0.640625, "QA-F1": 0.751376488095238}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9909", "mrqa_searchqa-validation-4541", "mrqa_searchqa-validation-12917", "mrqa_searchqa-validation-5080", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-14637", "mrqa_searchqa-validation-5939", "mrqa_searchqa-validation-12260", "mrqa_searchqa-validation-9516", "mrqa_searchqa-validation-12193", "mrqa_searchqa-validation-2465", "mrqa_searchqa-validation-15001", "mrqa_searchqa-validation-95", "mrqa_searchqa-validation-11934", "mrqa_searchqa-validation-3638", "mrqa_searchqa-validation-5686", "mrqa_searchqa-validation-11404", "mrqa_naturalquestions-validation-4641", "mrqa_naturalquestions-validation-7425", "mrqa_triviaqa-validation-2238", "mrqa_triviaqa-validation-3479", "mrqa_hotpotqa-validation-2012", "mrqa_newsqa-validation-317"], "SR": 0.640625, "CSR": 0.5390625, "EFR": 0.9565217391304348, "Overall": 0.7191949728260869}, {"timecode": 92, "before_eval_results": {"predictions": ["John Foster Dulles", "aluminum", "\" Look, Momno cavities\"", "The New York Times", "the PATRIOT Act", "a cleaver", "John Madden", "Ernest Hemingway", "a rubaiyat", "Judge Roy Bean", "Malaysia", "a yam", "Mariner 2", "the Chunnel", "President Lincoln", "Smithfield", "Edith Wharton", "Poland", "The Mausoleum at Halicarnassus", "a shepherd's pie", "the Tabernacle", "The Indianapolis 500", "the Galapagos", "Boston", "Nautilus", "the Phoenician", "parez", "Italy's second-oldest university", "Athens", "Eternity", "Michael Plummer", "Pennsylvania", "Cotton Mather", "the Berlin Wall", "Suzanne Valadon", "Sexuality", "elephants", "an axe", "Erwin Rommel", "France", "wheat", "Vermont", "Mending Wall", "Thomas Jefferson", "copper", "Wrigley", "rum", "a Towel", "Brian Curtis Wimes", "steel", "Cormac McCarthy", "Eydie Gorm\u00e9", "A turlough", "8,850", "Tutankhamun", "Funchal", "Ruth Rendell", "Anthony Lynn", "The 2016 United States Senate election", "Martin Scorsese", "African National Congress Deputy President Kgalema Motlanthe,", "Angela Merkel", "in September,", "Somali-based"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6163194444444444}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true], "QA-F1": [0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14994", "mrqa_searchqa-validation-2213", "mrqa_searchqa-validation-13482", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-11", "mrqa_searchqa-validation-5947", "mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-14152", "mrqa_searchqa-validation-4040", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-15747", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4976", "mrqa_searchqa-validation-4760", "mrqa_searchqa-validation-14570", "mrqa_searchqa-validation-1937", "mrqa_searchqa-validation-10353", "mrqa_searchqa-validation-11188", "mrqa_searchqa-validation-1261", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-15838", "mrqa_searchqa-validation-12309", "mrqa_searchqa-validation-2767", "mrqa_searchqa-validation-13222", "mrqa_naturalquestions-validation-335", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-4189"], "SR": 0.546875, "CSR": 0.539146505376344, "EFR": 1.0, "Overall": 0.7279074260752688}, {"timecode": 93, "before_eval_results": {"predictions": ["Twister", "Romeo & Juliet", "doughboy", "Georgetown", "the Dalmatian", "Cricket", "Duke", "Judy", "a mummies", "Lot", "a hull", "aluminum", "Kevin Smith", "Jabez Stone", "Major", "the Monitor", "Louis Pasteur", "vingtaines", "Lake Superior", "wheat", "Edgar Allan Poe", "Thomas Edison", "Manhattan", "(Curly) Lambeau", "T.E. Lawrence", "Blake Lively", "ape", "Eliot Spitzer", "licorice stick", "the union", "a fox", "Edgar Allan Poe", "a star", "impeachment", "Santo Domingo", "a Dagger", "Polk", "Nightingale", "\"Big Bun\"", "Washington", "butterflies", "a computer", "the devil's", "Goodyear", "corpulent", "Edinburgh", "a crumpet", "trailgator bars", "Great Smoky Mountains", "Sir Walter Scott", "Punjabi", "1983", "in the duodenum by enterocytes", "Anthony Hopkins", "six", "Dubai", "racehorse", "Abdul Razzak Yaqoob", "England", "Colonel Gaddafi", "first", "Haiti,", "if Gadhafi suffered the wound in crossfire or at close-range", "Roberto Micheletti,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6246448863636364}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13621", "mrqa_searchqa-validation-1145", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-15297", "mrqa_searchqa-validation-8433", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-879", "mrqa_searchqa-validation-3704", "mrqa_searchqa-validation-3150", "mrqa_searchqa-validation-14177", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-9835", "mrqa_searchqa-validation-7436", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-6686", "mrqa_searchqa-validation-11704", "mrqa_searchqa-validation-12720", "mrqa_searchqa-validation-6366", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-5257", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-7503", "mrqa_hotpotqa-validation-3442", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-2385"], "SR": 0.5625, "CSR": 0.5393949468085106, "EFR": 0.9642857142857143, "Overall": 0.720814257218845}, {"timecode": 94, "before_eval_results": {"predictions": ["John Mayer", "Benjamin Franklin", "the Amstel", "Constantinople", "Georgie Porgie", "Chad", "Adam", "Spain", "hearth", "Quebec", "Belshazzar", "the Cincinnati Reds", "Once", "Shelley", "China", "Frederick Douglass", "Sitka", "the Amazons", "Debussy", "a prince", "Bojangles", "Aunt Jemima", "Frank Sinatra", "a flying saucer", "Hawaii", "KLM", "(Frederick Victor) Zeller", "a carriage", "a vest", "The Adventures of Sherlock Holmes", "Ned Kelly", "a prostitution scandal", "World of Warcraft", "Shakespeare", "the Inca", "Alaska", "Sam Kinison", "Wii", "the high jump", "Champagne", "a new Broom", "Danica Patrick", "glands", "Midway", "stars", "Henry Cisneros", "sacristy", "the Great Seal", "Rihanna", "24", "Tom Brady", "a star", "the final scene of the fourth season", "Billy Hill", "iron", "Henry Ford", "Saint Bartholomew", "1943", "Battle of Britain and the Battle of Malta", "Kaep", "not", "David Beckham", "the vicious brutality which accompanied the murders of his father and brother.\"", "Lana Clarkson"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7722251400560224}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-10187", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-13912", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-15733", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-615", "mrqa_searchqa-validation-4359", "mrqa_searchqa-validation-16221", "mrqa_searchqa-validation-1365", "mrqa_searchqa-validation-13716", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-5383", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1915"], "SR": 0.734375, "CSR": 0.5414473684210526, "EFR": 0.9411764705882353, "Overall": 0.7166028928018576}, {"timecode": 95, "before_eval_results": {"predictions": ["The Buckwheat Boyz", "1998", "between the Eastern Ghats and the Bay of Bengal", "warming is the largest among the tropical oceans, and about 3 times faster than the warming observed in the Pacific", "160km / hour", "Dr. Rajendra Prasad", "it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "no more than 4.25 inches ( 108 mm )", "the Jews", "art of the Persian Safavid dynasty", "a neurosurgical or orthopedic surgical technique that joins two or more vertebrae together", "typically the player to the dealer's right", "his guilt in killing the bird", "February 6, 2005", "a global cultural icon of France", "on the basestar, she encounters numerous other Number Eight copies identical to herself", "Philippe Petit", "Terrell Suggs", "anion", "four", "1952", "The User State Migration Tool ( USMT )", "2017", "restoring someone's faith in love and family relationships", "an illustration by Everest creative Maganlal Daiya back in the 1960s", "the concentration of a compound exceeds its solubility ( such as when mixing solvents or changing their temperature )", "solemniser", "1961", "30 months", "an integral membrane protein that builds up a proton gradient across a biological membrane", "during the period of rest ( day )", "Ethel `` Edy '' Proctor", "Randy VanWarmer", "Isle Vierge ( 48 \u00b0 38 \u2032 23 '' N 4 \u00b0 34 \u2032 13 '' W ) to The southwestern limit of the North Sea", "Kevin Spacey", "Miller Lite", "the body - centered cubic ( BCC ) lattice", "1994", "their son Jack ( short for Jack - o - Lantern )", "Spektor", "in the evening, after 9pm ET ( UTC - 5 )", "February 2017 in Japan and in March 2018 in North America and Europe", "July 21, 1861", "Holden Nowell", "Muhammad Yunus", "The Bangles", "Juliet", "Texas, Oklahoma, and the surrounding Great Plains to adjacent regions", "Ethel Merman", "The Delaware Aqueduct", "Judith Cynthia Aline Keppel", "Kenny Everett", "Trinidad", "Ghost", "Haleiwa Ali'i Beach Park", "William Bradford", "Chuck Noll", "Chievo", "three", "suicides", "John Henry", "manure", "Two and a Half Men", "the 2004 Paris Motor Show"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6405224108973276}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.16, 1.0, 1.0, 0.9428571428571428, 1.0, 0.0, 0.0, 0.9565217391304348, 0.7272727272727272, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 1.0, 0.5714285714285715, 0.2857142857142857, 1.0, 0.1818181818181818, 1.0, 0.0, 1.0, 1.0, 0.4109589041095891, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6, 0.2666666666666667, 1.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9683", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-10172", "mrqa_naturalquestions-validation-384", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-636", "mrqa_naturalquestions-validation-2965", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-10294", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-9240", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-6035", "mrqa_hotpotqa-validation-4553", "mrqa_newsqa-validation-2754", "mrqa_searchqa-validation-5787"], "SR": 0.53125, "CSR": 0.5413411458333333, "EFR": 0.9, "Overall": 0.7083463541666667}, {"timecode": 96, "before_eval_results": {"predictions": ["Carlsberg", "painting", "simple", "Pisces", "The Law Society of Scotland", "Russell Crowe", "two", "Khomeini", "Clint Eastwood", "1921", "france", "David Bowie", "The Ten Commandments", "North Rhine-Westphalia", "volcanoes", "Porridge", "south africa", "New Orleans", "the eye", "pooh", "Ringo Starr", "John Mortimer", "bushfires", "Idaho", "Danny DeVito", "Sweden", "four", "the AllStars", "Rastafarians", "Sydney", "the 800m", "Leo Tolstoy", "Stirling Moss", "the Mayflower", "Yes", "Benghazi", "Brazil", "The Mary Tyler Moore Show", "Gordon Jackson", "scotlands", "Beyonce", "West Sussex", "Laputa", "Colombia", "the Addams", "a storm", "Tanzania", "Darby and Joan", "Robert Boyle", "Thailand", "Hugh Laurie", "Los Angeles", "sometime between 124 and 800 CE", "since been adopted by five other countries", "1 January 1788", "guitar feedback", "Stephen Lee", "Arroyo and her husband", "onstage demos.", "North Korea", "the Vietnam War", "white granite", "the ball of the upper arm bone (humerus)", "pinch"], "metric_results": {"EM": 0.59375, "QA-F1": 0.654672619047619}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4799999999999999, 0.0, 0.8, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-1554", "mrqa_triviaqa-validation-6629", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-3612", "mrqa_triviaqa-validation-3638", "mrqa_triviaqa-validation-3836", "mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-6139", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-2617", "mrqa_triviaqa-validation-50", "mrqa_triviaqa-validation-2263", "mrqa_triviaqa-validation-3232", "mrqa_triviaqa-validation-1866", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-1433", "mrqa_hotpotqa-validation-3234", "mrqa_hotpotqa-validation-1941", "mrqa_searchqa-validation-15627"], "SR": 0.59375, "CSR": 0.5418814432989691, "EFR": 1.0, "Overall": 0.7284544136597939}, {"timecode": 97, "before_eval_results": {"predictions": ["40 militants and six Pakistan soldiers dead,", "recall", "burgess meredith", "two United Arab Emirates based companies", "Madrid's Barajas International Airport", "The monarchy's end after 239 years of rule", "Kerstin and two of her brothers, ages 18 and 5,", "56,", "video cameras", "more than 200.", "Princess Diana", "1825", "maintain an \"aesthetic environment\" and ensure public safety,", "Caylee,", "The elections", "Alexandre Caizergues, of France,", "Arroyo and her husband", "\"It was terrible,", "hackers", "India", "vacant slot alongside Cameroon international Samuel Eto'o and Ivory Coast midfielder Yaya Toure in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "Haleigh Cummings,", "Republican senators", "Toffelmakaren.", "ended his playing career", "researchers", "between 1917 and 1924", "will be serving its fast burgers in the Carrousel du Louvre,", "3,000", "10 percent", "$250,000", "April 22.", "at a depth of about 1,300 meters in the Mediterranean Sea.", "Yusuf Saad Kamel", "Obama", "flights affected", "to try to make life a little easier for these families", "Jeffrey Jamaleldine", "more than two years,", "it was unjustifiable", "up to $50,000", "Citizens", "41,280 pounds", "Adam Lambert", "President Clinton.", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "Iran's nuclear program.", "\"The train ride up there is spectacular.", "133", "\"He tried to suppress the memories and to live as normal a life as possible;", "a deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "Richard Parker", "reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "electron pairs", "cricket", "groszy", "roman numbers", "Hawaii", "stolperstein", "Paper", "Las Vegas", "the nervous system", "the orangutan", "noddy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6005743739121399}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false], "QA-F1": [0.923076923076923, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.4615384615384615, 0.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.2666666666666667, 0.0, 1.0, 0.2553191489361702, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2471", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1451", "mrqa_newsqa-validation-3214", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-4055", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2375", "mrqa_newsqa-validation-2828", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1443", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-7701", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-4975", "mrqa_searchqa-validation-10850", "mrqa_triviaqa-validation-1532"], "SR": 0.484375, "CSR": 0.5412946428571428, "EFR": 0.9696969696969697, "Overall": 0.7222764475108224}, {"timecode": 98, "before_eval_results": {"predictions": ["it does not", "Swedish Foreign Ministry", "Denver", "Friday,", "Heshmat Tehran Attarzadeh", "Daniel Radcliffe", "South African teenager Caster Semenya", "the hunt for Nazi Gold and possibly the legendary Amber Room", "Susan Atkins", "Pakistan's High Commission in India", "\"Wicked,\"", "Rolling Stone", "Tsvangirai", "The oceans", "Asashoryu", "an Airbus A320-214,", "autonomy from China,", "South Africa's", "\"A good vegan cupcake has the power to transform everything for the better,\"", "[vehicle identification numbers], and they could contact the insured drivers who have failed to comply,\"", "between 1917 and 1924", "Les Bleus", "Too many glass shards left by beer drinkers in the city center,", "collapsed ConAgra Foods plant", "40-year-old", "Karthik Rajaram,", "the island's dining scene", "in the U.S.", "clogs", "Kurt Cobain's", "Turkey,", "Moody and sinister,", "United States, NATO member states, Russia and India.", "Ryder Russell,", "$60 billion", "Palestinian Islamic Army, which has links to al Qaeda,", "Haeftling,", "off the coast of Dubai", "change course", "Sri Lanka,", "eight-week", "drug cartels", "head for Italy.", "Anil Kapoor.", "\"The Closer.\"", "38 feet", "St. Louis.", "64,", "\"totaled,\"", "fifth", "order", "1977", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Jack Cutmore - Scott portrays Rufus Saville, and Lily Travers portrays Lady Sophie", "Nasdaq", "Van Rijn", "gander [male goose]", "Minnesota Twins baseball team", "hulder", "Lucille Ball", "Ziploc", "South Africa", "Ivory", "Kraftwerk"], "metric_results": {"EM": 0.625, "QA-F1": 0.7054487179487179}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1430", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-1485", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-2089", "mrqa_newsqa-validation-479", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-1956", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-3636", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-1683", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-2399"], "SR": 0.625, "CSR": 0.5421401515151515, "EFR": 1.0, "Overall": 0.7285061553030303}, {"timecode": 99, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1026", "mrqa_hotpotqa-validation-1038", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-1204", "mrqa_hotpotqa-validation-1232", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1474", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1814", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2106", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2194", "mrqa_hotpotqa-validation-2253", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-3077", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-3195", "mrqa_hotpotqa-validation-3217", "mrqa_hotpotqa-validation-327", "mrqa_hotpotqa-validation-3284", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3913", "mrqa_hotpotqa-validation-4013", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4073", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4091", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4179", "mrqa_hotpotqa-validation-4267", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4693", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-4831", "mrqa_hotpotqa-validation-4867", "mrqa_hotpotqa-validation-493", "mrqa_hotpotqa-validation-5013", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-538", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5554", "mrqa_hotpotqa-validation-5813", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5867", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-602", "mrqa_hotpotqa-validation-716", "mrqa_hotpotqa-validation-738", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-852", "mrqa_hotpotqa-validation-943", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-10031", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10319", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10632", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1213", "mrqa_naturalquestions-validation-1360", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1562", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-173", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-2012", "mrqa_naturalquestions-validation-2044", "mrqa_naturalquestions-validation-2133", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-2241", "mrqa_naturalquestions-validation-2332", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-2385", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3076", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-3241", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-3768", "mrqa_naturalquestions-validation-3899", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4165", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-4517", "mrqa_naturalquestions-validation-4569", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-4981", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-502", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-582", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-6058", "mrqa_naturalquestions-validation-6078", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-6304", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-7293", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7581", "mrqa_naturalquestions-validation-7794", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-8245", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-8355", "mrqa_naturalquestions-validation-8411", "mrqa_naturalquestions-validation-8531", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-8719", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8757", "mrqa_naturalquestions-validation-8794", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9789", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-9887", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-9992", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1036", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-1143", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-122", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1318", "mrqa_newsqa-validation-1366", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1457", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-16", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1778", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-2014", "mrqa_newsqa-validation-2021", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2106", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2179", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2310", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-2420", "mrqa_newsqa-validation-2468", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-2542", "mrqa_newsqa-validation-2566", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2707", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3000", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-33", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3338", "mrqa_newsqa-validation-3369", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3508", "mrqa_newsqa-validation-3539", "mrqa_newsqa-validation-3583", "mrqa_newsqa-validation-3601", "mrqa_newsqa-validation-3668", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-4046", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-616", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-785", "mrqa_newsqa-validation-796", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-88", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10054", "mrqa_searchqa-validation-10791", "mrqa_searchqa-validation-10886", "mrqa_searchqa-validation-10991", "mrqa_searchqa-validation-11081", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-11155", "mrqa_searchqa-validation-11170", "mrqa_searchqa-validation-1133", "mrqa_searchqa-validation-11540", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12414", "mrqa_searchqa-validation-12577", "mrqa_searchqa-validation-1276", "mrqa_searchqa-validation-12819", "mrqa_searchqa-validation-12831", "mrqa_searchqa-validation-12974", "mrqa_searchqa-validation-13030", "mrqa_searchqa-validation-13097", "mrqa_searchqa-validation-13264", "mrqa_searchqa-validation-13366", "mrqa_searchqa-validation-13436", "mrqa_searchqa-validation-13568", "mrqa_searchqa-validation-13601", "mrqa_searchqa-validation-13773", "mrqa_searchqa-validation-13838", "mrqa_searchqa-validation-1386", "mrqa_searchqa-validation-13865", "mrqa_searchqa-validation-13948", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14121", "mrqa_searchqa-validation-14143", "mrqa_searchqa-validation-14287", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14527", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-14580", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-14638", "mrqa_searchqa-validation-14663", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-15", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-1521", "mrqa_searchqa-validation-15251", "mrqa_searchqa-validation-15398", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15692", "mrqa_searchqa-validation-15711", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-15882", "mrqa_searchqa-validation-15922", "mrqa_searchqa-validation-15983", "mrqa_searchqa-validation-16013", "mrqa_searchqa-validation-16036", "mrqa_searchqa-validation-16127", "mrqa_searchqa-validation-16362", "mrqa_searchqa-validation-16574", "mrqa_searchqa-validation-16652", "mrqa_searchqa-validation-1776", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2196", "mrqa_searchqa-validation-2217", "mrqa_searchqa-validation-2249", "mrqa_searchqa-validation-2284", "mrqa_searchqa-validation-238", "mrqa_searchqa-validation-2661", "mrqa_searchqa-validation-2763", "mrqa_searchqa-validation-296", "mrqa_searchqa-validation-302", "mrqa_searchqa-validation-3187", "mrqa_searchqa-validation-3193", "mrqa_searchqa-validation-3505", "mrqa_searchqa-validation-3617", "mrqa_searchqa-validation-4013", "mrqa_searchqa-validation-4095", "mrqa_searchqa-validation-4269", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-4806", "mrqa_searchqa-validation-4826", "mrqa_searchqa-validation-5041", "mrqa_searchqa-validation-5547", "mrqa_searchqa-validation-5723", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6002", "mrqa_searchqa-validation-6149", "mrqa_searchqa-validation-622", "mrqa_searchqa-validation-6368", "mrqa_searchqa-validation-6467", "mrqa_searchqa-validation-6605", "mrqa_searchqa-validation-6799", "mrqa_searchqa-validation-6850", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7006", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-722", "mrqa_searchqa-validation-7237", "mrqa_searchqa-validation-7262", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7916", "mrqa_searchqa-validation-7923", "mrqa_searchqa-validation-7987", "mrqa_searchqa-validation-8000", "mrqa_searchqa-validation-8065", "mrqa_searchqa-validation-8108", "mrqa_searchqa-validation-8114", "mrqa_searchqa-validation-8519", "mrqa_searchqa-validation-8540", "mrqa_searchqa-validation-8885", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-9209", "mrqa_searchqa-validation-9443", "mrqa_searchqa-validation-9502", "mrqa_searchqa-validation-9554", "mrqa_searchqa-validation-9608", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-9663", "mrqa_squad-validation-10088", "mrqa_squad-validation-1248", "mrqa_squad-validation-1348", "mrqa_squad-validation-1488", "mrqa_squad-validation-1530", "mrqa_squad-validation-1645", "mrqa_squad-validation-2082", "mrqa_squad-validation-221", "mrqa_squad-validation-2274", "mrqa_squad-validation-2416", "mrqa_squad-validation-3179", "mrqa_squad-validation-3796", "mrqa_squad-validation-3906", "mrqa_squad-validation-4322", "mrqa_squad-validation-4739", "mrqa_squad-validation-5023", "mrqa_squad-validation-5189", "mrqa_squad-validation-521", "mrqa_squad-validation-5687", "mrqa_squad-validation-5750", "mrqa_squad-validation-5849", "mrqa_squad-validation-5951", "mrqa_squad-validation-6024", "mrqa_squad-validation-6402", "mrqa_squad-validation-6449", "mrqa_squad-validation-6471", "mrqa_squad-validation-7040", "mrqa_squad-validation-7211", "mrqa_squad-validation-7704", "mrqa_squad-validation-7775", "mrqa_squad-validation-7832", "mrqa_squad-validation-795", "mrqa_squad-validation-801", "mrqa_squad-validation-8065", "mrqa_squad-validation-8144", "mrqa_squad-validation-8257", "mrqa_squad-validation-8279", "mrqa_squad-validation-8332", "mrqa_squad-validation-8910", "mrqa_squad-validation-9017", "mrqa_squad-validation-9116", "mrqa_squad-validation-9443", "mrqa_squad-validation-9484", "mrqa_squad-validation-9813", "mrqa_triviaqa-validation-102", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1092", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-1371", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1713", "mrqa_triviaqa-validation-1939", "mrqa_triviaqa-validation-1970", "mrqa_triviaqa-validation-2024", "mrqa_triviaqa-validation-2059", "mrqa_triviaqa-validation-2282", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-2626", "mrqa_triviaqa-validation-271", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-3284", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3402", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3528", "mrqa_triviaqa-validation-3532", "mrqa_triviaqa-validation-3542", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3633", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-386", "mrqa_triviaqa-validation-3883", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-3995", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4130", "mrqa_triviaqa-validation-4141", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4478", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4893", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5494", "mrqa_triviaqa-validation-551", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5738", "mrqa_triviaqa-validation-5769", "mrqa_triviaqa-validation-5817", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6104", "mrqa_triviaqa-validation-6198", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-662", "mrqa_triviaqa-validation-6891", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-709", "mrqa_triviaqa-validation-7376", "mrqa_triviaqa-validation-7379", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-769", "mrqa_triviaqa-validation-7694"], "OKR": 0.849609375, "KG": 0.54296875, "before_eval_results": {"predictions": ["\" Number Ones\"", "\"She wasn't the best \"coach,\"", "$40 billion", "organizing the distribution of wheelchairs,", "10 municipal police officers", "Lifeway's 100-plus stores nationwide", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "2010", "two years,", "a group of college students of Pakistani background", "gasoline", "James Whitehouse,", "North Korea,", "four Impressionist paintings", "Omar bin Laden,", "nude beaches.", "1-1", "Jason Chaffetz", "Martin \"Al\" Culhane,", "President Obama", "A member of the group dubbed the \"Jena 6\"", "Animal Planet", "Minerals Management Service Director Elizabeth Birnbaum", "Mandi Hamlin", "41,280", "Arlington National Cemetery's Section 60,", "Matt Kuchar and Bubba Watson", "Caster Semenya", "his health", "Elisabeth captive", "Djibouti,", "123 pounds of cocaine and 4.5 pounds of heroin,", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "Brazil, Argentina, Mexico, Colombia", "second-degree aggravated battery.", "two Emmys", "an American pop star's connection to the country", "the forward's lawyer", "Turkish President Abdullah Gul.", "the children of street cleaners and firefighters.", "to secure more funds from the region.", "five female pastors", "14", "South Carolina Republican Party Chairwoman Karen Floyd", "African National Congress Deputy President", "1913,", "second", "Hu Jintao", "Michael Arrington,", "asylum in Britain.", "eight Indian army troopers, including one officer, and 17 militants,", "New York City", "2009", "Massachusetts", "Phil Mickelson", "the Quran", "Shanghai", "University of the District of Columbia", "an advertisement figure", "Aldosterone", "freezing", "power", "Maine", "November 17, 2017"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7154277408637874}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.9302325581395349, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-453", "mrqa_newsqa-validation-1139", "mrqa_newsqa-validation-2238", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-3431", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-2222", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-614", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-1310", "mrqa_newsqa-validation-3895", "mrqa_naturalquestions-validation-644", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-397"], "SR": 0.59375, "CSR": 0.5426562500000001, "EFR": 1.0, "Overall": 0.7331406250000001}]}