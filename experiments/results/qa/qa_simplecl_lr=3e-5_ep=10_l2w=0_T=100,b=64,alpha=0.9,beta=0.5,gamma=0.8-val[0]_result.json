{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=50, kg_eval_mode='metric', kr_eval_freq=50, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=100, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-val.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4230, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Fresno", "Truth, Justice and Reconciliation Commission", "Pittsburgh Steelers", "mid-18th century", "his sons and grandsons", "1875", "be reborn", "1971", "placing them on prophetic faith", "Cestum veneris", "the arts capital of the UK", "an idealized and systematized version of conservative tribal village customs", "conflict", "cytotoxic natural killer cells and Ctls (cytotoxic T lymphocytes)", "every four years", "three", "live", "Tugh Temur", "teach by rote", "excommunication", "Church of St Thomas the Martyr", "the move from the manufacturing sector to the service sector", "article 49", "Thailand", "immunomodulators", "hotel room", "they owned the Ohio Country", "10 million", "Pictish tribes", "oxides", "Economist Branko Milanovic", "Emergency Highway Energy Conservation Act", "Hurricane Beryl", "a better understanding of the Mau Mau command structure", "Satyagraha", "Jim Gray", "San Francisco Bay Area's Levi's Stadium", "1080i HD", "\"Blue Harvest\" and \"420\"", "Maria Sk\u0142odowska-Curie", "human", "water", "1201", "The Presiding Officer", "mesoglea", "redistributive", "$2 million", "Liao, Jin, and Song", "1313", "small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "visor helmet", "Mike Tolbert", "semi-arid savanna to the north and east", "Percy Shelley", "Arizona Cardinals", "a lute", "More than 1 million", "Manuel Blum", "unidirectional force", "Central Bridge", "was a major source of water pollution", "graduate and undergraduate students elected to represent members from their respective academic unit", "Dragon's Den", "24 March 1879"], "metric_results": {"EM": 0.828125, "QA-F1": 0.859375}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6804", "mrqa_squad-validation-8347", "mrqa_squad-validation-7382", "mrqa_squad-validation-7432", "mrqa_squad-validation-7364", "mrqa_squad-validation-133", "mrqa_squad-validation-652", "mrqa_squad-validation-7719", "mrqa_squad-validation-7324", "mrqa_squad-validation-75", "mrqa_squad-validation-9343"], "SR": 0.828125, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 1, "before_eval_results": {"predictions": ["Oahu", "its central location between the Commonwealth's capitals of Krak\u00f3w and Vilnius", "one (or more)", "linebacker", "the set of triples", "most of the items in the collection, unless those were newly accessioned into the collection", "the Los Angeles Times", "the Broncos", "anticlines and synclines", "Bells Beach SurfClassic", "Paleoproterozoic", "the end", "1894", "French Rhin", "Atlantic", "quotient", "less than a year", "The Scottish Parliament", "artisans and farmers", "Shia terrorist groups", "Royal Ujazd\u00f3w Castle", "hard-to-fill", "the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "\u00a315\u2013100,000", "mid-Eocene", "the infected corpses", "the United Kingdom, Australia, Canada and the United States", "11", "forces", "2005", "chief electrician", "lower incomes", "Luther states that everything that is used to work sorrow over sin is called the law", "phagocytes", "the center of the curving path", "a shortage of male teachers", "Masovian Primeval Forest", "days, weeks and months", "biodiversity", "two", "Nairobi, Mombasa and Kisumu", "an algorithm for multiplying two integers can be used to square an integer", "Qutb", "Stanford Stadium", "the chosen machine model", "s = \u22122, \u22124,...", "human", "Killer T cells", "British Gas plc", "More than 1 million", "2011", "by the market", "27-30%", "New Orleans", "Jamukha", "Gymnosperms", "Taoism", "Matthew 16:18", "the U.S. ship that was hijacked off Somalia's coast", "Rwanda", "revelry", "his health", "The Pilgrims", "the South"], "metric_results": {"EM": 0.734375, "QA-F1": 0.8186112498612499}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-885", "mrqa_squad-validation-5505", "mrqa_squad-validation-2969", "mrqa_squad-validation-7566", "mrqa_squad-validation-9243", "mrqa_squad-validation-4289", "mrqa_squad-validation-7763", "mrqa_squad-validation-7728", "mrqa_squad-validation-2520", "mrqa_squad-validation-6933", "mrqa_squad-validation-1763", "mrqa_squad-validation-366", "mrqa_squad-validation-7527", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-471", "mrqa_searchqa-validation-724"], "SR": 0.734375, "CSR": 0.78125, "EFR": 1.0, "Overall": 0.890625}, {"timecode": 2, "before_eval_results": {"predictions": ["negative", "1 July 1851", "Zhu Yuanzhang", "the greatest good", "50%", "mountainous areas", "Denmark", "quantum mechanics", "75th birthday", "Distinguished Service Medal", "30", "Virgin Media", "the destruction of Israel", "locomotion", "each six months", "Japanese", "the Electorate of Saxony", "Mark Twain", "the Commission", "1085", "shortening the cutoff", "Hastings", "1000 CE", "T. T. Tsui Gallery", "a multi-party system", "allows those tainted by sin to nevertheless make a truly free choice to accept or reject God's salvation in Christ", "Monopoly", "Evita and The Wiz", "The Master", "cholera", "Jingshi Dadian", "purposely damaging their photosynthetic system", "1991", "two", "the Arizona Cardinals", "1991", "Chaffee", "Isiah Bowman", "the poor", "100\u2013150", "John Elway", "Wijk bij Duurstede", "non-peer-reviewed sources", "Economist", "pathogens", "more integral", "declare martial law", "a customs union", "the Roman Catholic Church", "1050s", "political support", "the death of Elisabeth Sladen", "Robbie Williams and Liam Gallagher", "Johnsonkip", "the company's factory in Waterford City, Ireland", "nitrogen", "Annemarie Moody", "Har Har", "six", "it sticks with you long after the song is over; the sort of tune that makes it almost impossible to sit still.", "music director", "Illinois", "Rafael Palmeiro", "Wal-Mart Canada Corp."], "metric_results": {"EM": 0.734375, "QA-F1": 0.7751798876798877}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.16216216216216214, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-803", "mrqa_squad-validation-1174", "mrqa_squad-validation-1123", "mrqa_squad-validation-9896", "mrqa_squad-validation-8316", "mrqa_squad-validation-9805", "mrqa_squad-validation-235", "mrqa_squad-validation-6403", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2735", "mrqa_triviaqa-validation-3701", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-5936", "mrqa_hotpotqa-validation-3629"], "SR": 0.734375, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 3, "before_eval_results": {"predictions": ["the 1994 Works Council Directive", "42%", "21-minute", "The individual is the final judge of right and wrong", "an Eastern Bloc city", "Sakya", "christopher saints", "Britain", "23", "Fears of being labelled a pedophile or hebephile", "the best-known legend", "near the surface", "northern China", "giving her brother Polynices a proper burial", "political figures", "The Commission's President (currently an ex-Luxembourg Prime Minister, Jean-Claude Juncker)", "2000", "oxygen", "increase local producer prices by 20\u201325%", "Apollo 1 backup crew", "a body of treaties and legislation", "it initially used ARPANET technology but changed the host interface to X.25", "39", "the King", "four", "Guinness World Records", "issues under their jurisdiction", "women", "the Edict of Nantes", "national security, drug policy, employment, foreign policy and relations with Europe", "multiple revisions", "The Super Bowl 50 Host Committee", "the integer factorization problem", "economic inequality", "Isel", "adapted quickly and often married outside their immediate French communities", "Benazir Bhutto", "Charles-Fer Ferdinand University", "drowned in the Mur River", "yellow fever", "Tracy Wolfson and Evan Washburn", "lysozyme and phospholipase A2", "Brazil", "energy stored in an H+ or hydrogen ion gradient", "the late 19th century", "the Channel Islands", "in no way", "Alberich", "charleston", "Emeril Lagasse", "Churchill Downs", "charleston", "charleston", "Sunday 2nd January 2011", "christopher", "study the insect's nutrition, ecology, morphology or behaviour", "the limbic system", "trahan Mubarak", "George Fox", "Maryland", "charleston", "24 hours a day and 7 days a week", "Sponsorgate", "\"Krabby Road\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.6509358951914099}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39999999999999997, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6975", "mrqa_squad-validation-2597", "mrqa_squad-validation-9286", "mrqa_squad-validation-4293", "mrqa_squad-validation-4834", "mrqa_squad-validation-639", "mrqa_squad-validation-7083", "mrqa_squad-validation-9489", "mrqa_squad-validation-392", "mrqa_squad-validation-7321", "mrqa_squad-validation-3069", "mrqa_squad-validation-1189", "mrqa_squad-validation-7230", "mrqa_squad-validation-8906", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-5065", "mrqa_triviaqa-validation-6229", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-3361", "mrqa_triviaqa-validation-1581", "mrqa_hotpotqa-validation-437", "mrqa_hotpotqa-validation-3821"], "SR": 0.59375, "CSR": 0.72265625, "EFR": 0.9230769230769231, "Overall": 0.8228665865384616}, {"timecode": 4, "before_eval_results": {"predictions": ["in higher plants", "the Parliament of Victoria", "Zaha Hadid", "Fort Edward and Fort William Henry", "Science and Discovery", "the Army", "pedagogy", "red algal endosymbiont's original cell membrane", "Grand Canal d'Alsace", "a number of stages", "The Skirmish of the Brick Church", "port city of Kaffa in the Crimea", "Henry of Navarre", "reduced moist tropical vegetation cover", "wage or salary", "the Roman Catholic Church", "miners", "John Fox", "Royal Institute of British Architects", "March 1896", "disturbed", "Oireachtas funds", "Ogedei", "Brooklyn", "their cleats", "1700", "apicomplexan-related", "Academy of the Pavilion of the Star of Literature", "passenger space", "1639", "biostratigraphers", "the web", "the Song dynasty", "1985", "1606", "The Earth's mantle", "1991", "Ticonderoga", "Laszlo Babai and Eugene Luks", "October 2007", "LoyalKaspar", "other ctenophores", "the Italian government", "22", "at least 13 suspects were arrested Sunday and Monday, including three people carrying suicide jackets and explosives inside a bus station", "it was a right thing to say, something that we both acknowledge", "Brian Smith", "a bit more disposable income", "a Muslim", "this will be the first time any version of the Magna Carta has ever gone up for auction", "a unit of Time Warner", "15", "militants from Afghanistan", "Chesley \"Sully\" Sullenberger", "backbreaking", "CNN's Campbell Brown", "a woman who may have been contacted through a Craigslist ad", "one", "mike", "$1,500", "National Industrial Recovery Act", "Travis", "Humberside", "mike"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6671286069360655}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.08695652173913045, 0.3157894736842105, 1.0, 0.0, 0.0, 0.32, 0.0, 1.0, 0.0, 1.0, 0.125, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8825", "mrqa_squad-validation-10247", "mrqa_squad-validation-4773", "mrqa_squad-validation-2961", "mrqa_squad-validation-4510", "mrqa_squad-validation-3195", "mrqa_squad-validation-3733", "mrqa_squad-validation-166", "mrqa_newsqa-validation-628", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-840", "mrqa_newsqa-validation-1855", "mrqa_triviaqa-validation-6944", "mrqa_hotpotqa-validation-4853", "mrqa_searchqa-validation-574"], "SR": 0.640625, "CSR": 0.70625, "EFR": 1.0, "Overall": 0.853125}, {"timecode": 5, "before_eval_results": {"predictions": ["Danny Lane", "the United States", "New York City", "Larry Ellison", "Book of Common Prayer", "WLS", "Pi\u0142sudski", "10th century", "shaping ideas about the free market", "The United Methodist Church", "the Connectional Table", "Deformational", "a data network based on this voice-phone network", "500,000", "Ofcom", "Scottish independence", "lectured on the Psalms, the books of Hebrews, Romans, and Galatians", "3.55 inches (90.2 mm)", "2011", "algae", "part of a rule connected with civil disobedience", "June 1978", "Milton Latham", "1914", "Philippines", "Denver's Executive Vice President of Football Operations and General Manager", "the 1970s", "the characteristics of the conquering peoples", "German Te Deum", "1795", "Bermuda 419", "air could be liquefied, and its components isolated, by compressing and cooling it", "Infinity Broadcasting Corporation", "\"semi-legal\"", "1972", "a rudimentary immune system, in the form of enzymes that protect against bacteriophage infections", "1957", "mother-of-pearl made between 500 AD and 2000", "Gene Barry", "negotiates treaties with foreign nations", "It is mainly for the purpose of changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "from an Ohio newspaper on 8 February 1925", "Herbert Hoover", "radius R of the turntable", "Panning", "Justin Timberlake", "the following 15 countries or regions have reached an economy of at least US $2 trillion by GDP in nominal or PPP terms", "troops are contributed by member states on a voluntary basis", "unknown origin", "speed limit '' omitted and an additional panel stating the type of hazard ahead", "three", "the speech, once given during the day, is now typically given in the evening, after 9pm ET ( UTC - 5 )", "Jesse Frederick James Conaway", "the seven ages of man : infant, schoolboy, lover, soldier, justice, Pantalone and old age", "the present ( 2016 -- 2018, contemporaneous with airing ) and a storyline taking place at a set time in the past ; but some episodes are set in one time period or use multiple flashback time periods", "Morgan Freeman", "David Gahan", "The Stanley Hotel", "a long sustained period of inflation is caused by money supply growing faster than the rate of economic growth", "the day before the long fast for the Lenten fast", "Jaipur", "Jonas Olsson,", "\"torpedo boat destroyers\"", "Newport"], "metric_results": {"EM": 0.625, "QA-F1": 0.7341701558514155}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.56, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8333333333333333, 0.8750000000000001, 1.0, 0.09090909090909091, 0.0, 0.3157894736842105, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5833333333333334, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4836", "mrqa_squad-validation-2254", "mrqa_squad-validation-6719", "mrqa_squad-validation-376", "mrqa_squad-validation-3473", "mrqa_squad-validation-6450", "mrqa_squad-validation-5451", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-7297", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10138", "mrqa_triviaqa-validation-910", "mrqa_newsqa-validation-2048", "mrqa_searchqa-validation-2792", "mrqa_triviaqa-validation-4272"], "SR": 0.625, "CSR": 0.6927083333333333, "EFR": 0.9583333333333334, "Overall": 0.8255208333333333}, {"timecode": 6, "before_eval_results": {"predictions": ["William Hartnell and Patrick Troughton", "more expensive", "an antigen from a pathogen", "their disastrous financial situation", "a Serbian Orthodox priest", "receptions, gatherings or exhibition purposes", "the Pittsburgh Steelers", "Charly", "Henry Cole", "steam turbines", "\"social and political action,\"", "1936", "the New Birth", "gold", "a 3\u20130 lead", "Vivienne Westwood", "reduction", "disease", "\"TFIF\"", "Confucian propriety and ancestor veneration", "Luther's rediscovery of \"Christ and His salvation\"", "five", "European Court of Justice and the highest national courts", "1888", "business districts of Downtown San Bernardino", "BBC Radio 5 Live", "1876", "a chain or screw stoking mechanism", "#P", "George Westinghouse", "British failures in North America, combined with other failures in the European theater", "1,548", "Joy", "teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college peoples", "end of the season", "10", "Jonas", "African-Americans", "\"creates the precedent and possibility for undue regulation, censorship and legal abuse.\"", "David Duchovny, playing what the beetles would have you believe is an autobiographical role, has managed to hang onto his Bukowski-phase well into his forties", "always hot and humid and it rains almost every day of the year", "an animal tranquilizer, can put users in a dazed stupor for about two hours, doctors said.", "in an interview Tuesday on CNN's \"Larry King Live.\"", "Stuttgart on Sunday", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "more than 170", "North Korea", "first five Potter films", "get a list of cars before automotive industry experts to capture their take on several popular cars.", "3 to 17", "two suicide bombers, \"feigning a desire to conduct reconciliation talks, detonated themselves.\"", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Her husband and attorney, James Whitehouse, has been quoted as saying she has terminal brain cancer, according to a blog called Manson Family Today.", "we want to ensure we have all the capacity that may be needed over the course of the coming days", "a series of monthly meals for people with food allergies", "Zimbabwe", "2004", "Mohamed Alanssi", "Ludacris", "John Emburey, who was so poor as a captain that he was replaced after two Tests. Chris Cowdrey came into the team as captain for the fourth Test and was then injured.", "Colgate University", "Church of Christ, Scientist", "fats are comprised of lipids that contain? A fatty acid in which there is at least one double bond within the fatty acid chain", "Luke 6 : 67 -- 71"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6672250534065262}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 0.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.25, 0.2, 0.5, 1.0, 0.5, 0.25, 1.0, 0.11764705882352941, 1.0, 0.16666666666666669, 0.2666666666666667, 0.41379310344827586, 0.16666666666666666, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.37037037037037035, 0.75]}}, "before_error_ids": ["mrqa_squad-validation-266", "mrqa_squad-validation-800", "mrqa_squad-validation-6001", "mrqa_squad-validation-2133", "mrqa_squad-validation-2643", "mrqa_squad-validation-486", "mrqa_squad-validation-1906", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-265", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-1171", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-2395", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2425", "mrqa_newsqa-validation-3730", "mrqa_newsqa-validation-814", "mrqa_triviaqa-validation-2684", "mrqa_hotpotqa-validation-501", "mrqa_searchqa-validation-1275", "mrqa_naturalquestions-validation-1442", "mrqa_naturalquestions-validation-3770"], "SR": 0.5625, "CSR": 0.6741071428571428, "EFR": 1.0, "Overall": 0.8370535714285714}, {"timecode": 7, "before_eval_results": {"predictions": ["1970s", "his friendship", "increased trade with poor countries", "187 feet", "pH or available iron", "90\u00b0", "materials melted near an impact crater", "$100,000", "Stanford Stadium", "baptism in the Small Catechism", "Jim Gray", "P = PSPACE", "July 1969", "secret police demanded to know if they were hiding a Jew in their house.", "prolamellar body", "spontaneous", "the courts of member states", "gold", "TARDIS", "Buckland Valley", "Scottish rivers", "ricks for Warsaw", "1978", "1598", "Sheldon Ungar", "86", "tentacles and tentacle sheaths", "in 80 trunks marked N.T.", "up to \u00a339,942", "21 October 1512", "James O. McKinsey,", "a weight-loss show", "videos and commentaries", "India", "Zulfikar Ali Bhutto,", "at the Lindsey oil refinery in eastern England", "April 24", "Krishna Rajaram,", "early detection", "250,000", "Timothy Masters,", "homicide", "in the non-EU berths permitted under Spanish Football Federation (RFEF) rules.", "12 hours", "from the capital, Dhaka, to their homes in Bhola", "3rd District of Utah", "United States Holocaust Memorial Museum, The American Academy of Diplomacy and the United States Institute of Peace.", "\"Dancing With The Stars\"", "leniency", "Matthew Fisher", "Cain", "9 a.m.", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "seeking help", "Japanese officials", "patrolling", "\"Empire of the Sun,\"", "the Norman given name Robert", "len hrig,", "Matt Winer", "Doc Holliday", "opposite R\u00fcgen island", "Mustique", "green"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6344226953601954}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.4444444444444445, 1.0, 0.8666666666666666, 0.0, 1.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 0.5, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7533", "mrqa_squad-validation-2448", "mrqa_squad-validation-1796", "mrqa_squad-validation-6998", "mrqa_squad-validation-8883", "mrqa_squad-validation-3938", "mrqa_squad-validation-872", "mrqa_squad-validation-1556", "mrqa_squad-validation-2091", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3176", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-167", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2721", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-2589", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-417", "mrqa_naturalquestions-validation-6514", "mrqa_triviaqa-validation-991", "mrqa_hotpotqa-validation-4367", "mrqa_triviaqa-validation-2858"], "SR": 0.53125, "CSR": 0.65625, "EFR": 0.9666666666666667, "Overall": 0.8114583333333334}, {"timecode": 8, "before_eval_results": {"predictions": ["7 February 2009", "The British provided medical treatment for the sick and wounded French soldiers and French regular troops were returned to France aboard British ships with an agreement that they were not to serve again in the present war.", "Roman Catholic", "The Master is the Doctor's archenemy, a renegade Time Lord who desires to rule the universe.", "Enric Miralles", "25-foot (7.6 m)", "eight", "Tuesday", "\"Journey's End\"", "immediate", "Levi's Stadium", "decidedly Wesleyan", "art posters", "Tsakhiagiin Elbegdorj", "chinggis Khaan, English Chinghiz, Chinghis, and Chingiz", "Einstein", "fast forwarding of accessed content", "CALIPSO", "30 \u00b0C", "primary law, secondary law and supplementary law", "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers, Sir Henry Cheere,", "2,869", "Leonard Bernstein", "Commission v Austria", "9th", "random access machines", "ensure that the prescription is valid", "Stockton and Darlington Railway", "autonomy", "\"the largest center for breeding and exporting terrorism.\"", "$12.9 million", "Fernando Gonzalez", "Graeme Smith", "$15 billion in 2008 and is projected to grow by 10 percent, according to PricewaterhouseCoopers.", "finance", "terminal brain cancer.", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "the Employee Free Choice act", "The two were separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro, told Goldman -- to whom she was then married", "Animal Planet", "fake his own death", "there were no radar outages and said it had not lost contact with any planes", "54 bodies", "helping other women cope with the disease.", "Diversity", "$250,000", "make sure water continues flow through the river channel and not spread out over land.", "Nazi Germany", "March 27", "The Kirchners", "involved in an Internet broadband deal with a Chinese firm.", "The son of Gabon's former president was declared the winner of the country's presidential elections on Thursday,", "2050", "Alfredo Astiz,", "Abdullah Gul,", "Mikkel Kessler", "The Everglades,", "when the cell is undergoing the metaphase of cell division", "Gibraltar", "New Orleans Pelicans", "Tulip mania", "bistro", "it may mean if you miss a period when you're not expecting.", "get up-to-date St. Louis Blues  Players will be selected by coaches for their play both on and off the ice and will be highlighted on FOX Sports Midwest"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6885816559529796}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.13333333333333333, 1.0, 0.2857142857142857, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.23529411764705882, 1.0, 1.0, 0.4615384615384615, 1.0, 0.21428571428571427, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 0.07407407407407408, 1.0, 0.5, 1.0, 0.9411764705882353, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10258", "mrqa_squad-validation-7698", "mrqa_squad-validation-5100", "mrqa_squad-validation-455", "mrqa_squad-validation-9903", "mrqa_squad-validation-6300", "mrqa_squad-validation-5586", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-4086", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-2681", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-3456", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-302", "mrqa_naturalquestions-validation-8159", "mrqa_hotpotqa-validation-1123", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-9839", "mrqa_searchqa-validation-9016"], "SR": 0.5625, "CSR": 0.6458333333333333, "EFR": 1.0, "Overall": 0.8229166666666666}, {"timecode": 9, "before_eval_results": {"predictions": ["EastEnders", "1983", "The Book of Discipline", "Katharina", "theology and philosophy", "Pannerdens Kanaal", "487", "Jonathan Stewart", "O(n2)", "Levi's Stadium", "the General Sejm", "Derek Jacobi", "net force", "\"coo\", \"hoos\"", "30%\u201350% O2 by volume", "very badly disposed towards the French", "United States", "CRISPR sequences", "six years", "300 km long", "1962", "free radical production", "Video On Demand", "the substance of the statement", "Edict of Fontainebleau", "15", "\"Well, about time.\"", "Ronaldinho", "providing the basic securities that Turkey can be a great partner.", "25", "a trainer", "the couple's surrogate lost the pregnancy.", "environmental and political events", "he fears a desperate country with a potential power vacuum that could lash out.", "at least two and a half hours.", "Elin Nordegren", "Europe, Asia, Africa and the Middle East.", "6,000", "cortisone", "President Clinton.", "purchasing the machine guns and silencers from an undercover Bureau of Alcohol, Tobacco, Firearms and Kanye West.", "MDC head Morgan Tsvangirai.", "expulsion companies under British rule.", "a difference a credit crunch makes. Sovereign Wealth Funds control up to $3 trillion in assets,", "canyon", "Zuma", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.", "posting a $1,725 bail", "school", "strife", "Tom Hanks, Ayelet Zurer and Ewan McGregor", "Columbia, Illinois,", "a violation of a law that makes it illegal to defame, insult or threaten the crown.", "North Korea", "2005", "they did not know how many people were onboard.", "London", "after Shawn's kidnapping", "a person's", "William Tell", "OutKast", "Groundhog Day", "\" Cleopatra, Queen of Denial\"", "baltic"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6290737336601308}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.08, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.35294117647058826, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5278", "mrqa_squad-validation-3687", "mrqa_squad-validation-8471", "mrqa_squad-validation-9484", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-1242", "mrqa_newsqa-validation-3391", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1380", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-1713", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-624", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1778", "mrqa_naturalquestions-validation-5093", "mrqa_triviaqa-validation-2315", "mrqa_hotpotqa-validation-2679", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-11812"], "SR": 0.5625, "CSR": 0.6375, "EFR": 0.9642857142857143, "Overall": 0.8008928571428571}, {"timecode": 10, "before_eval_results": {"predictions": ["Paramount Pictures", "Ferncliff Cemetery", "pseudorandom", "John Wesley", "Genghis Khan", "water", "internal strife", "yellow fever outbreaks", "DC traction", "Prince of P\u0142ock", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "Lothar de Maizi\u00e8re", "premises of the hospital", "journalist", "Cam Newton", "over $40 million", "Super Bowl XXXIII", "endosymbiont", "Beyonc\u00e9 and Bruno Mars", "Theodor Fontane", "33", "chairman and CEO", "Brazil", "Friday", "broken pelvis", "issued his first military orders as leader of North Korea", "heavy snow and ice was heading from Texas and Oklahoma to points east,", "Gainsbourg", "\"Maude\"", "Phillip A. Myers", "Korean military", "two weeks after Black History Month was mocked in an off-campus party that was placed on the bookcase at the school.", "58", "two Metro transit trains that crashed the day before, killing nine,", "last summer.", "Christopher Savoie", "Lance Cpl. Maria Lauterbach", "Dangjin", "reading a novel", "Hu Jintao", "magazine", "The teen faces a lifelong recovery from his injuries,", "October 3,", "Adriano", "Larry Zeiger", "shock", "President Bush", "Jeffrey Jamaleldine", "35,000", "South Africa", "Tim Clark, Matt Kuchar and Bubba Watson", "Haiti", "Sunday", "lightning strikes", "Steve Young", "American Airlines", "16 August 1975", "Bonnie Aarons", "one", "Kabinett", "Lionsgate", "James Lofton", "Sohang", "hair-like structures"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6842397186147186}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.18181818181818182, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.14285714285714285, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1572", "mrqa_squad-validation-1299", "mrqa_squad-validation-8655", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-2524", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-4182", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-4180", "mrqa_newsqa-validation-1947", "mrqa_triviaqa-validation-1100", "mrqa_hotpotqa-validation-3949", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-9132"], "SR": 0.609375, "CSR": 0.6349431818181819, "EFR": 1.0, "Overall": 0.8174715909090909}, {"timecode": 11, "before_eval_results": {"predictions": ["Central Banking economist", "The combination of hermaphroditism and early reproduction", "Victoria Department of Education", "transported to the Manhattan Storage and Warehouse Company under the Office of Alien Property", "Manned Spacecraft Center", "economic inequality", "refusing to make a commitment", "use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks,", "Elway", "Philo of Byzantium", "36", "Louis Agassiz", "Melbourne", "Jawaharlal Nehru", "Austrian Polytechnic", "Lorelei", "Euler's totient function", "better relevant income", "Redwood City, California", "400 m", "Netherlands", "Agnes Wickfield", "The Soup Dragon", "antelope", "nipples", "the Precambrian period", "'helpful' businesses", "Anastasia Dobromyslova", "Gagapedia", "9", "The Female Brain", "radishes", "Robert Ludlum", "giant grubs", "Shuttle Launch", "the largest showcase of Grand Prix racing cars in the world", "Saturday Night Live", "Hebrew alephbet", "The London Underground Piccadilly Line", "Canada", "orangutan", "Manet", "Charlie and the Great Glass Elevator", "Wyoming", "2005", "1969", "minivans", "dolt", "Rome", "petticoat", "Enrico Caruso", "Elizabeth Arden", "collapsible support assembly", "Sir Hardy Amies", "Liechtenstein", "the 14th most common surname in Wales", "`` Can't Get You Out of My Head ''", "Cody Miller", "Bloomingdale Firehouse", "Israeli's vice prime minister compared Iran to Nazi Germany", "Golden Gate Yacht Club of San Francisco", "Roger Vivier", "Ocho Rios", "Buddhism"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6115747100122101}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true], "QA-F1": [0.5, 1.0, 1.0, 0.28571428571428575, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6153846153846153, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7383", "mrqa_squad-validation-1596", "mrqa_squad-validation-7320", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-6795", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-2034", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-6010", "mrqa_triviaqa-validation-1018", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-4860", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-2331", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-1934", "mrqa_triviaqa-validation-2416", "mrqa_triviaqa-validation-5216", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1138", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-2291", "mrqa_hotpotqa-validation-4834", "mrqa_newsqa-validation-3753", "mrqa_searchqa-validation-14983", "mrqa_searchqa-validation-13120"], "SR": 0.53125, "CSR": 0.6263020833333333, "EFR": 0.9333333333333333, "Overall": 0.7798177083333333}, {"timecode": 12, "before_eval_results": {"predictions": ["the Southern Border Region", "90-60's", "Panini", "Bills", "anti-colonial movements", "glacial alpine valley", "G", "be suspicious of even the greatest thinkers and to test everything himself by experience", "Zhongshu Sheng", "legitimate medical purpose", "in the case of an express wish of the people to withdraw from the EU", "1788", "2006", "Roman Catholic archdiocese", "the pattern of warfare", "John Wesley", "the nationalisation law was from 1962, and the treaty was in force from 1958,", "Eternal Heaven", "london", "Jessica Simpson", "Sue Ryder", "Val Doonican", "Virgil", "France", "T.S. Eliot", "he", "game of bridge", "Vladivostok", "Sheryl Crow", "TESLAR", "sinensis", "AFC Wimbledon", "Bob Monkhouse and Kenneth Connor", "Malaysia", "cosmology", "gin", "George Clooney", "Eric Coates", "James Chadwick", "\"No one was saved\"", "Monopoly", "champagne", "an extended period of abundant rainfall lasting many thousands of years", "the United States", "Brigit Forsyth", "london", "london", "problem play", "Thomas Edward Lawrence,", "Kent", "Paul Lhote", "london", "white", "Switzerland", "gin", "the people of France", "79", "ITV", "Scottish national team", "the death of a pregnant soldier", "Derek Mears", "heide", "verrocchio", "\"The Screening Room\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6079807194616977}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.2666666666666667, 1.0, 0.2857142857142857, 0.08695652173913043, 1.0, 1.0, 0.8, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2659", "mrqa_squad-validation-9126", "mrqa_squad-validation-2078", "mrqa_squad-validation-6426", "mrqa_squad-validation-4116", "mrqa_squad-validation-917", "mrqa_squad-validation-3161", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-3847", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-946", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-302", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-3128", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-5192", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-6384", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-1141", "mrqa_triviaqa-validation-1423", "mrqa_triviaqa-validation-5933", "mrqa_naturalquestions-validation-594", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-5428", "mrqa_searchqa-validation-8450", "mrqa_searchqa-validation-9647", "mrqa_newsqa-validation-3860"], "SR": 0.546875, "CSR": 0.6201923076923077, "EFR": 1.0, "Overall": 0.8100961538461539}, {"timecode": 13, "before_eval_results": {"predictions": ["168,637", "the Barnett Center", "entertainment", "Muhammad ibn Zakar\u012bya R\u0101zi", "Georgia", "articles 1 to 7", "it would appear to be some form of the ordinary Eastern or bubonic plague", "had their own militia", "until after the end of the Mexican War", "Over 61", "the quality of a country's institutions", "cilia", "gravity", "Sky Digital", "2005", "force", "mustelids", "John Connally", "saffron", "HYmenaeus", "god Zeus", "albinism", "the Straits of Tiran", "Brigit Forsyth", "Call My Bluff", "March 10, 1997", "go on a roller-coaster ride of frights and laughter", "the Battle of the Three Emperors", "Velazquez", "althea Gibson", "lizards", "strong cold southwest wind", "table tennis", "medical journal", "penhaligon", "Gandalf", "auguste elean Doyle", "Jinnah", "Monday", "Caracas", "renoir", "soap", "highball", "Avro", "Genesis", "Charlie Brooker", "chamomile", "Harrods", "2007", "renoir", "Scarface", "pale yellow", "renoir", "bubba", "June 12, 2018", "Filipino", "London", "Lambic", "Nook", "Steven Green", "commas", "auguste", "a sovereign principality located along the Mediterranean Sea", "Synchronicity"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6428323412698412}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.28571428571428575, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6029", "mrqa_squad-validation-4908", "mrqa_squad-validation-2875", "mrqa_squad-validation-2920", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2334", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-264", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-3807", "mrqa_triviaqa-validation-5254", "mrqa_triviaqa-validation-4070", "mrqa_triviaqa-validation-5060", "mrqa_triviaqa-validation-385", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-372", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-6994", "mrqa_naturalquestions-validation-3162", "mrqa_newsqa-validation-3314", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-8598", "mrqa_searchqa-validation-6628"], "SR": 0.5625, "CSR": 0.6160714285714286, "EFR": 1.0, "Overall": 0.8080357142857143}, {"timecode": 14, "before_eval_results": {"predictions": ["seven", "woodblocks", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium", "the Teaching Council", "ABC Entertainment Group", "Doctor in Bible", "mountainous areas", "it is separated from the body", "1960", "John Mayow", "3.62", "the Treaties establishing the European Union", "a declining state of mind", "1898", "The serials The Deadly Assassin and Mawdryn undead", "scientific observation", "Cody Fern", "Nicklaus", "Jim Gaffigan", "cat in the hat", "2020", "1974", "332", "1997", "Authority", "junior enlisted sailor", "Spanish moss", "Chinese cooking", "Vienna", "between 2 World Trade Center and 3 World Trade center", "Kevin Spacey", "eve", "78", "white blood cell", "Bangladesh", "the President", "G minor", "Coppolas", "Chandan Shetty", "Sedimentary rock", "October 1, 2014", "United States", "Claims adjuster", "the female uterine tubes", "Darlene Cates", "Atlanta, Georgia", "homicidal thoughts of a troubled youth", "infection", "Garfield Sobers", "12 November 2010", "pneumonoultramicroscopicsilicovolcanoconiosis", "Palm Sunday celebrations", "vertebral column", "six", "annual plants", "sausages", "Kew Gardens", "Nikita Khrushchev", "$500,000", "young self-styled anarchists", "reaper", "robbie coltrane", "the BBC's central London offices", "\"Larry King Live\""], "metric_results": {"EM": 0.578125, "QA-F1": 0.6789374348477609}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false], "QA-F1": [0.5, 1.0, 0.6956521739130436, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9523809523809523, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-653", "mrqa_squad-validation-125", "mrqa_squad-validation-2339", "mrqa_squad-validation-7670", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-342", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-303", "mrqa_triviaqa-validation-6328", "mrqa_triviaqa-validation-3542", "mrqa_newsqa-validation-3571", "mrqa_searchqa-validation-726", "mrqa_searchqa-validation-196", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1279"], "SR": 0.578125, "CSR": 0.6135416666666667, "EFR": 0.9629629629629629, "Overall": 0.7882523148148148}, {"timecode": 15, "before_eval_results": {"predictions": ["T\u00f6regene Khatun", "rising inequality", "CBS also aired a special episode of The Late Late Show with James Corden.", "small renovations, such as addition of a room, or renovation of a bathroom", "John Madejski Garden", "declare martial law and sent the state militia to maintain order", "Famous musicians", "ESPN Deportes", "Jean Ribault", "Tetzel", "the Electorate of Saxony", "88%", "Necessity-based", "950 pesos ( approximately $ 18 )", "60", "Seattle, Washington", "Battle of Antietam", "Dimitar Berbatov and Carlos Tevez", "In Time", "early 3rd century", "Glenn Close", "twelve different countries", "Agostino Bassi", "five", "One Direction spending time on a beach in Malibu, California", "the church at Philippi", "Jurriaen Aernoutsz", "September 2017", "Professor Kantorek", "1546", "arthur as Robert Hanson", "Bhupendranath Dutt", "Grey Wardens", "Dr. Lexie Grey ( Chyler Leigh )", "Majandra Delfino", "September 1972", "Uruguay", "Alex Skuby", "Matt Jones", "National Legal Aid & Defender Association ( NLADA )", "Monk's Caf\u00e9", "domesticated sheep goes back to between 11000 and 9000 BC", "1970s", "Director of National Intelligence", "Remus Lupin", "Isaiah Amir Mustafa", "John Marlborough Churchill Blenheim Charlton", "Saphira", "5.7 million", "Woody Harrelson, Juliette Lewis, Robert Downey Jr.", "Thespis", "Portugal. The Man", "John Coffey", "Rachel Kelly Tucker", "Bohemia, now Czech Republic", "boisea trivittata", "Code 02PrettyPretty", "Joe Dever", "The opposition group,", "the abduction of minors", "Nevada", "Chile", "Stage Stores", "1881"], "metric_results": {"EM": 0.5, "QA-F1": 0.612748276029526}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 0.0, 0.9090909090909091, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-6739", "mrqa_squad-validation-7141", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7390", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-4609", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-2232", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-1766", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-2756", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3253", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2806", "mrqa_triviaqa-validation-4262", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-3870", "mrqa_newsqa-validation-2674"], "SR": 0.5, "CSR": 0.6064453125, "EFR": 0.9375, "Overall": 0.77197265625}, {"timecode": 16, "before_eval_results": {"predictions": ["BBC 1", "the Arizona Cardinals", "Bert Bolin", "390 billion individual trees divided into 16,000 species", "igneous, sedimentary, and metamorphic", "The high school student follows an education specialty track, obtain the prerequisite \"student-teaching\" time, and receive a special diploma to begin teaching after graduation.", "six", "11", "hydrogen and helium", "Khitan", "November 1979", "Robert Lane and Benjamin Vail", "Germany", "Francis the Talking Mule", "Helsinki, Finland.", "Microsoft Office", "SAVE", "Scandinavian Airlines", "1993 to 2001", "1951", "NCAA Division I Football Bowl Subdivision", "Martin Lee Truex Jr.", "Easter Rising of 1916", "45%", "two decades", "BAFTA TV Award Best Actor winner", "rash", "the 1745 rebellion of Charles Edward Stuart", "Burny Mattinson", "Sir William McMahon", "Inverbervie", "7.63\u00d725mm Mauser", "Howl's Moving Castle", "Pakistan Aeronautical Complex (PAC)", "Delacorte Press.", "Neighbourhood", "Secretariat", "Wake Island", "hydrogen vehicle", "Fort Valley, Georgia.", "King of France", "\"Southern Living\" Reader's Choice Awards", "Thomas Harold Amer", "Johnson & Johnson", "ZZ Top", "Mahoning County", "Alticor", "Parlophone Records", "Durban", "Surrey", "My Sassy Girl", "Charles Russell", "Boyd Gaming", "Stephen Curry of the Golden State Warriors", "1991", "Glenn Close", "Myra Zamparelli", "Neighbours", "Ewan McGregor", "2011", "Robert Browning", "an enslaved African American", "new government \"that reflects the will of the Zimbabwean people,\"", "Republican Rep. Shelley Moore Capito"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5848417207792208}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.9090909090909091, 0.0, 0.3636363636363636, 0.2857142857142857, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4415", "mrqa_squad-validation-2191", "mrqa_squad-validation-3667", "mrqa_squad-validation-8087", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-1092", "mrqa_hotpotqa-validation-1133", "mrqa_hotpotqa-validation-4689", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2396", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-789", "mrqa_hotpotqa-validation-1661", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-1428", "mrqa_hotpotqa-validation-2409", "mrqa_hotpotqa-validation-2771", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-1436", "mrqa_hotpotqa-validation-4859", "mrqa_naturalquestions-validation-2650", "mrqa_triviaqa-validation-2052", "mrqa_newsqa-validation-174", "mrqa_searchqa-validation-4338", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-3521"], "SR": 0.484375, "CSR": 0.5992647058823529, "EFR": 1.0, "Overall": 0.7996323529411764}, {"timecode": 17, "before_eval_results": {"predictions": ["the force of gravity acting on the object balanced by a force applied by the \"spring reaction force\"", "theology and philosophy", "ITV", "University of Chicago College Bowl Team", "Philip Webb and William Morris", "7:00 to 9:00 a.m. weekdays", "Japanese", "charter status", "1830", "nonfunctional pseudogenes", "the inner chloroplast membrane", "Charlie welch", "\"Steveland Hardaway Morris\"", "beaver", "La Boh\u00e8me Giacomo Puccini", "formic acid", "Talavera de la Reina", "Zimbabwe", "\"Pressure of Speech\"", "\"I'll miss this place but it's time to move on,\"", "Richard Walter Jenkins", "Japan", "Lewis Carroll", "\"multi-user dungeon\"", "Mercury", "hound", "Xenophon", "London Pride", "Plimsoll line", "Nick Hornby", "\"The Two Gentlemen\"", "Charles V", "England", "welch", "weight plates", "\"big house\"", "Hadrian", "US.", "human flea", "Melbourne, Victoria.", "Hamburg", "mulberry tree", "Tangled", "\"The French Connection\"", "CBS", "In 2014/15, only six have won the title", "Prokofiev", "Jessica Simpson", "British public", "England", "3000m", "Scotland", "Japan", "Travis Tritt and Marty Stuart", "The Union's forces", "New Jewel Movement", "in Africa", "U.S. 93", "Anjuna beach in Goa", "\"father of classical ballet,\"", "Oshkosh", "\"Papa's Got a Brand New Bag\"", "jeopardy/1870_Qs.txt", "\"The Sunday Thing\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.5755667892156863}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10351", "mrqa_squad-validation-7089", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-7521", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-5963", "mrqa_triviaqa-validation-1343", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-2813", "mrqa_triviaqa-validation-1391", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-4440", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6151", "mrqa_naturalquestions-validation-767", "mrqa_hotpotqa-validation-1658", "mrqa_newsqa-validation-2981", "mrqa_searchqa-validation-5843", "mrqa_searchqa-validation-9843", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-9467"], "SR": 0.484375, "CSR": 0.5928819444444444, "EFR": 0.9696969696969697, "Overall": 0.7812894570707071}, {"timecode": 18, "before_eval_results": {"predictions": ["low latitude", "1622", "high", "Manakintown", "northwest", "fewer than 10 employees", "Middle Miocene", "magma", "salt and iron", "Grundschule", "September 29, 2017", "James Martin Lafferty", "balance sheet", "July 2, 1776", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another", "2010", "Coppolas and, technically, the Farrow / Previn / Allens", "Allison Janney", "the Isthmus of Corinth", "ability to comprehend and formulate language", "Splodgenessabounds", "Tyrion", "electron donors", "Alison", "( 1985 -- 1993 )", "775 rooms", "Solange Knowles & Destiny's Child", "Gupta Empire", "December 2, 1942", "Lewis Carroll", "20 November 1989", "Coton in the Elms", "pass grades 1 ( threshold 85 %, a distinction )", "Ella Eyre", "1995", "defining goals", "16 August 1975", "December 1974", "`` Killer Within ''", "Western Australia", "aorta", "July 21, 1861", "Dr. Addison Montgomery", "state or other organizational body", "empty line", "on the lateral side of the tibia", "Toto", "Thomas Mundy Peterson", "the poem explores a violation of nature and the resulting psychological effects on the mariner", "September 2017", "moral", "Rising Sun Blues", "Part 2", "dumbo", "the \u201cBloody Assizes\u201d of Monmouth", "Christian", "Robert L. Stone", "2008", "Jeddah", "julsiz", "Robert Langdon", "ABC1 and ABC2", "NBA 2K16", "mistress of the Robes"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6477157801526408}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7843137254901961, 0.0, 0.2857142857142857, 1.0, 0.0, 0.8, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.08695652173913042, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.42857142857142855, 0.4444444444444445, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3193", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-31", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-1161", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-3164", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-4227", "mrqa_newsqa-validation-1493", "mrqa_searchqa-validation-7111", "mrqa_hotpotqa-validation-4735"], "SR": 0.5625, "CSR": 0.591282894736842, "EFR": 0.9642857142857143, "Overall": 0.7777843045112782}, {"timecode": 19, "before_eval_results": {"predictions": ["everything that is used to work sorrow over sin is called the law, even if it is Christ's life, Christ's death for sin, or God's goodness experienced in creation.", "black", "Illinois Country", "Jaime Weston", "1978", "high art and folk music", "warming", "the mid-sixties through to the present day", "270,000", "Long troop deployments", "Barack Obama", "the girl who disappeared in February, plans to file for divorce from the girl's stepmother, a key witness in the case, his attorneys told HLN's \"Nancy Grace.\"", "innovative, exciting skyscrapers", "Rawalpindi", "Michael Jackson", "nearly three out of four", "the Falklands, known as Las Malvinas in Argentina, lie in the South Atlantic Ocean off the Argentinean coast and have been under British rule since 1833.", "Tuesday in Los Angeles.", "forgery and flying without a valid license", "Anil Kapoor", "19", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "unwanted baggage from the 80s", "ancient Egyptian antiquities", "snowstorm", "Ferraris, a Lamborghini and an Acura NSX", "a cold shower in his home in New Zealand.", "Saadi", "two Manchester, England shows have been moved from Thursday and Friday to the end of her tour on June 17 and 18,", "\"Steamboat Bill, Jr.\"", "Russia", "alcohol", "Atlantic Ocean", "the president, speaking at his palace in the capital, Mogadishu,", "a reaction, an anaphylactic shock", "\u00a320 million ($41.1 million) fortune", "Kingman Regional Medical Center,", "Laura Ling and Euna Lee", "Manmohan Singh", "Michael Jackson", "be silent", "40 militants and six Pakistan soldiers dead, said military spokesman Gen. Athar Abbas.", "Spaniard Carlos Moya", "Stratfor subscriber data, including information on 4,000 credit cards and the company's \"private client\" list,", "lousiana", "Southeast", "The father of Haleigh Cummings, a Florida girl who disappeared in February, plans to file for divorce from the girl's stepmother, a key witness in the case, his attorneys told HLN's \"Nancy Grace.\"", "Carol Browner", "\"A Mother For All Seasons.\"", "The Maraachlis' daughter, Zeina, had died at home in 2002 after a tracheotomy", "back at work", "the initial necropsy or animal autopsy.", "teenager", "Amber Riley and her partner Derek Hough", "John Adams", "beets", "Zager and Evans", "Robert Matthew Hurley", "fourth term", "obscenity", "Cromwell", "Lapland", "2000", "Emad Hashim"], "metric_results": {"EM": 0.390625, "QA-F1": 0.49038429406850464}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true], "QA-F1": [0.6, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.08, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.4, 1.0, 1.0, 0.0, 0.1904761904761905, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.6666666666666666, 1.0, 0.631578947368421, 0.0, 0.8333333333333333, 1.0, 1.0, 0.06666666666666667, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2520", "mrqa_squad-validation-5702", "mrqa_squad-validation-10180", "mrqa_squad-validation-7831", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-3473", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3018", "mrqa_newsqa-validation-3775", "mrqa_newsqa-validation-1892", "mrqa_newsqa-validation-355", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-1958", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-6786", "mrqa_searchqa-validation-8011", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-5120"], "SR": 0.390625, "CSR": 0.58125, "EFR": 1.0, "Overall": 0.790625}, {"timecode": 20, "before_eval_results": {"predictions": ["the late 19th century", "1550 to 1900", "torque variability", "115 \u00b0F (46.1 \u00b0C)", "Rhenus", "1331", "Death wish Coffee", "L", "Cameroon", "just after midday on a cold December Monday", "ballots", "fabric", "three empty vodka bottles,", "strategy, plans and policy", "Bobby Darin,", "Nico Rosberg", "16", "his former Boca Juniors teammate and national coach Diego Maradona", "\"The woman involved -- Mandi Hamlin -- told reporters earlier Friday she was humiliated by last month's incident,", "composer of \"Phantom of the Opera\" and \"Cats\"", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "Caylee Anthony,", "Amanda Knox's aunt", "well over 1,000 pounds", "Iran's development of a nuclear weapon", "a welcoming, bright blue-purple", "allegedly faking a doctor's note and was restricted from leaving his house in Tokyo,", "ceo Herbert Hainer", "his client, Brett Cummins,", "children are predominantly African-American.", "inmates", "Col. Elspeth Cameron-Ritchie,", "on the set at \"E! News\"", "six members of Zoe's Ark", "jobs", "saying Tuesday the reality he has seen is \"terrifying.\"", "waterboarding at least 266 times on two top al Qaeda suspects,", "it would investigate the video and any group that tries to take justice into its own hands.", "Republicans", "a business principles book called \"Get in the Game: 8 Elements of Perseverance That Make the difference,\"", "An undated photo of Alexandros Grigoropoulos,", "a power-sharing deal with the opposition party's breakaway faction,", "a 57-year old male", "Kim Jong Il seems to be \"testing the new administration.\"", "Angola", "Gary Brooker", "Jund Ansar Allah,", "Jason Voorhees", "determining which Guant detainees should be tried by a U.S. military commision,", "San Antonio,", "guard in the jails of Washington, D.C., and on the streets of post- Katrina New Orleans,", "about 50", "the Ku Klux Klan", "The Wizard of Oz ( 1939 )", "Branford College", "Bury", "husbands", "Malayalam", "August 17, 2017", "a jacket, gloves or a briefcase", "clone. right: Dave.", "Hodel", "access to US courts", "British rock group Coldplay"], "metric_results": {"EM": 0.34375, "QA-F1": 0.46161211321573165}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2, 0.0, 0.0, 1.0, 0.9523809523809523, 0.33333333333333337, 0.25, 0.23076923076923078, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.8571428571428571, 0.0, 0.2222222222222222, 1.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.10526315789473685, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.19047619047619047, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.22222222222222224, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9248", "mrqa_squad-validation-543", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-43", "mrqa_newsqa-validation-609", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3056", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-72", "mrqa_newsqa-validation-4041", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2873", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-212", "mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-1449", "mrqa_naturalquestions-validation-10284", "mrqa_naturalquestions-validation-3788", "mrqa_triviaqa-validation-6406", "mrqa_triviaqa-validation-1427", "mrqa_hotpotqa-validation-5345", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-13277", "mrqa_naturalquestions-validation-7987"], "SR": 0.34375, "CSR": 0.5699404761904762, "EFR": 0.9523809523809523, "Overall": 0.7611607142857142}, {"timecode": 21, "before_eval_results": {"predictions": ["the whole curriculum", "Eliot Ness", "the poor", "oxygen-16", "middle eastern scientists", "Amazoneregenwoud", "regulations and directives", "\u201cUnder The Sea\u201d", "Nicola Adams", "copper and zinc", "eagle", "Peter Nichols", "Gulf of Aden", "Carlo Collodi", "Tony Blair,", "Illinois", "shoulders", "Madonna's", "Glasgow", "satellite-based navigational system that can tell users exactly where they are on Earth.", "Australia", "roch", "Pearson PLC.", "Irish Setter", "American Civil War,", "Loch Ness", "Roman Catholic Church,", "New South Wales", "a gentle cat with a somewhat shy nature around strangers.", "China", "Harrisburg", "Mustela erminea,", "percussion", "Dr John Sentamu", "roch\u00f5es", "Pongo", "Anne Boleyn", "EMI", "Holly Johnson", "Emma Chambers", "charlemagne", "the community", "Russell Crowe", "Theodore Roosevelt", "ACC", "Robin Goodfellow", "Samuel Butler", "chamomile", "Ireland", "tarn", "Michel", "Albert Square", "Newbury", "the Old Testament", "70 million people,", "Target Corporation", "\"The Omega Man\"", "Michelle Rounds", "doctors assured him using the surgical anesthetic propofol at home to induce sleep was safe as long as he was monitored.", "international NGO", "Francis Marion", "the proceeds from sales go to organizations that support prisoners' rights and better conditions for inmates, like Amnesty International.", "Oprah Winfrey.", "Mom."], "metric_results": {"EM": 0.546875, "QA-F1": 0.5729166666666667}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7382", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-7121", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-3498", "mrqa_triviaqa-validation-6330", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3513", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-6408", "mrqa_triviaqa-validation-3166", "mrqa_triviaqa-validation-6307", "mrqa_triviaqa-validation-6055", "mrqa_triviaqa-validation-93", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-4303", "mrqa_triviaqa-validation-4805", "mrqa_triviaqa-validation-1328", "mrqa_triviaqa-validation-2040", "mrqa_triviaqa-validation-1664", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-6287", "mrqa_hotpotqa-validation-1217", "mrqa_searchqa-validation-11802", "mrqa_searchqa-validation-1273", "mrqa_newsqa-validation-2256", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-3088"], "SR": 0.546875, "CSR": 0.5688920454545454, "EFR": 1.0, "Overall": 0.7844460227272727}, {"timecode": 22, "before_eval_results": {"predictions": ["The flushing action of tears and urine", "1765", "the frontiers between New France and the British colonies", "standardized", "when the present amount of funding cannot cover the current costs for labour and materials,", "Vicodin,", "Christianity", "michael henson", "pearl", "Utah Territory", "Carrie Underwood", "Drambuie", "he made his horse a consul, his palace a brothel, and his", "Google", "Langston Hughes", "Pain tolerance", "black", "Tito Puente", "lariat", "philosophical", "USS LST 325", "prey drive", "David Beckham", "Arturo Toscanini", "economics", "Miracle", "the triumphal arch", "Montenegro", "discus", "SLAB", "basidiomycota", "james", "Courtney Thorne-Smith", "Idi Amin", "deere", "a body, body part, or personal object", "terracotta", "Plutarch", "Rudy Giuliani", "masa", "near the end of each half", "the Vikings", "fairfield", "Bastille Day", "typhoid fever", "river valley", "the capital of Bavaria", "Williamsburg", "\"Wire Rope Express\"", "University of Missouri-St. Louis", "hydrogen peroxide", "jesuit", "the uterus", "more than $1 billion worldwide", "the distribution and determinants of health and disease conditions in defined populations", "the Big Bopper", "Tesco", "michaelard", "Graham Hill", "the Battelle Energy Alliance", "IT products and services,", "debris", "$10 billion", "Trenton, Florida"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5567646329365079}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true, true, false], "QA-F1": [0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.625, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6437", "mrqa_searchqa-validation-1891", "mrqa_searchqa-validation-5055", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-9187", "mrqa_searchqa-validation-15814", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-6193", "mrqa_searchqa-validation-11922", "mrqa_searchqa-validation-15426", "mrqa_searchqa-validation-10720", "mrqa_searchqa-validation-7416", "mrqa_searchqa-validation-2843", "mrqa_searchqa-validation-5373", "mrqa_searchqa-validation-5223", "mrqa_searchqa-validation-4344", "mrqa_searchqa-validation-9424", "mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-12592", "mrqa_searchqa-validation-8447", "mrqa_searchqa-validation-2327", "mrqa_searchqa-validation-5331", "mrqa_searchqa-validation-16870", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-12608", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-16854", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-4036", "mrqa_triviaqa-validation-1118", "mrqa_newsqa-validation-1997"], "SR": 0.484375, "CSR": 0.5652173913043479, "EFR": 1.0, "Overall": 0.782608695652174}, {"timecode": 23, "before_eval_results": {"predictions": ["2010", "1493\u20131500", "Pittsburgh Steelers", "an Australian public X.25 network operated by Telstra.", "Hamas", "Nintendo", "Atlantic", "cat", "the daughter of Tony Richardson and Vanessa Redgrave", "Switzerland", "the Argo", "prometheus", "the Altamont Speedway Free Festival", "John F Kennedy", "Tim Gudgin", "Rosslyn Chapel", "conducting", "RuneScape", "Italy", "Khaki", "a volcano", "Miguel Indurain", "Velazquez", "British Arts and Crafts", "Apollo", "African violet", "Pete Best", "Mendip", "Barack Obama", "the Earth", "Nafea Faa Ipoipo", "phosphorus", "Mumbai", "Joan Rivers", "Moses Sithole", "New Netherland", "Justin Trudeau", "aircraft, ships, spacecraft, guided missiles, motor vehicles, weather formations", "Denis Law", "Love Is All Around", "William Golding", "Sally Ride", "a cyclone", "Fife", "economics", "Adidas", "the Hunting of the Snark", "Elizabeth Arden", "Buxton", "woe", "James Bond", "\"White\" and \"Black\"", "flour and water", "Lee Baldwin", "Frankie Valli", "Scotland", "Beauty and the Beast", "Alex Song", "86", "Musharraf", "The Wall Street Journal Europe", "fox", "60 Minutes", "Jupiter"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7385876225490196}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3049", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-3693", "mrqa_triviaqa-validation-6205", "mrqa_triviaqa-validation-5686", "mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-3467", "mrqa_triviaqa-validation-7765", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-1491", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-3359", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-6140", "mrqa_hotpotqa-validation-5087", "mrqa_newsqa-validation-848"], "SR": 0.6875, "CSR": 0.5703125, "EFR": 1.0, "Overall": 0.78515625}, {"timecode": 24, "before_eval_results": {"predictions": ["coercive", "the chosen machine model", "Paramount Pictures", "1997", "a suite of network protocols created by Digital Equipment Corporation,", "Noriko Savoie", "Anne Frank,", "nine-wicket win over the world's number one ranked Test nation in Melbourne on Tuesday.", "Pyongyang and Seoul", "killed a man,", "11", "change course", "Damon Bankston", "Jason Chaffetz", "money or other discreet aid for the effort if it could be made available,", "Sarah,", "illegal", "environmental", "Costa Rica", "Afghan police", "Saturday", "38,", "70,000 or so", "Climatecare,", "\"E! News\"", "former Boca Juniors teammate and national coach Diego Maradona", "Steve Williams", "McDonald's", "poetry", "Pastor Paula White", "2008", "Diego Maradona", "Dog patch Labs Europe", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "two", "Itawamba County School District", "Mitt Romney", "EU naval force", "Plymouth Rock", "Liza Murphy,", "the nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul Stevens", "police", "former U.S. secretary of state", "33", "Samir Kuntar,", "improve health and beauty.", "black market of prison life", "campus patrols are in reducing campus violence, the most powerful form of prevention is believing that students can help stop crime from happening.", "Damon Bankston", "Krishna Rajaram,", "Sunday", "death and destruction,", "an Irish feminine name", "southwestern Colorado and northwestern New Mexico", "March 31 to April 8, 2018", "northern irish", "radar", "art", "point guard", "the NFL single-season touchdown reception record", "South America", "freestyle", "the Nightingale", "Belief"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5517895894058553}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.5555555555555556, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2631578947368421, 0.0, 1.0, 0.25, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6846", "mrqa_squad-validation-610", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2207", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-3660", "mrqa_naturalquestions-validation-2293", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-3290", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-4806", "mrqa_searchqa-validation-1545", "mrqa_searchqa-validation-3826"], "SR": 0.4375, "CSR": 0.565, "EFR": 0.9722222222222222, "Overall": 0.7686111111111111}, {"timecode": 25, "before_eval_results": {"predictions": ["its 50th anniversary special.", "Thomas Savery", "Vicodin,", "Eastern crops such as carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "22,000 years ago", "violent separatist campaign", "Eleven", "269,000", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "38 feet", "Eintracht Frankfurt", "150", "an angry mob.", "Russian bombers", "41,", "Los Alamitos Joint Forces Training Base", "Wally is still in the design stage as the company has not yet managed to sell the concept to a buyer.", "137", "the Kurdish militant group in Turkey", "3-2", "autonomy", "the area where the single-engine Cessna 206 went down,", "the Russian air force", "34", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "around 3.5 percent of global greenhouse emissions.", "Amanda Knox's aunt", "a lightning strike was a possibility,", "ensuring that all prescription drugs on the market are FDA approved", "think that more is not necessarily better...some vitamins and minerals can be toxic in high doses,\"", "Larry Zeiger", "Pakistan", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "Bikini Atoll,", "Brian Mabry", "iTunes,", "May 2000", "60 euros -- $89 --", "Former detainees", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "The tower will be built in the Saudi town of Jeddah and will be part of a larger project that will cost $26.7 billion,", "a passenger's name.", "he was one of 10 gunmen who attacked several targets in Mumbai on November 26,", "2006", "San Diego,", "five", "Bergdahl, 23, was captured June 30 from Paktika province in southeastern Afghanistan,", "#JustSayin'", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Henry Ford", "The Port-au-Prince force of 4,000 has dropped to about 1,500,", "heart", "Hyderabad", "Mediterranean Sea", "to stay, abide", "is one of the most-visited cities in the world.", "Jackson Pollock", "lyrical", "Mississippi", "Janis Lyn Joplin", "King Duncan", "Georgia", "monopoly", "timeshare resale"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5355054633488916}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, true, true, true, false], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.8750000000000001, 1.0, 0.8, 0.0, 0.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1, 1.0, 0.0, 0.0, 1.0, 0.6, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.07407407407407408, 0.0, 0.5, 1.0, 0.2666666666666667, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.1111111111111111, 0.0, 0.4, 0.888888888888889, 1.0, 1.0, 0.0, 0.2608695652173913, 0.0, 0.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7809", "mrqa_squad-validation-8068", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1506", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-3964", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-667", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-415", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-3819", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-826", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1695", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2757", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-9767", "mrqa_triviaqa-validation-1422", "mrqa_triviaqa-validation-1677", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5837", "mrqa_searchqa-validation-9476"], "SR": 0.40625, "CSR": 0.5588942307692308, "EFR": 0.9736842105263158, "Overall": 0.7662892206477734}, {"timecode": 26, "before_eval_results": {"predictions": ["gaseous oxygen", "chlorophyll b", "Off-Off Campus", "reckless", "pro-democracy activists clashed Friday with Egyptian security forces", "Karthik Rajaram", "at least 25", "Booches Billiard Hall,", "finance", "Ross Perot", "Hong Kong's Victoria Harbor", "2002", "seven", "the legitimacy of that race.", "could be more powerful than the tears of a Native American Indian", "three", "Monday", "Scarlett Keeling", "two years", "84-year", "regulators in the agency's Colorado office", "give detainees greater latitude in selecting legal representation and afford basic protections to those who refuse to testify. Military commission judges also will be able to establish the jurisdiction of their own courts.", "July", "Akshay Kumar", "farmer Alan Graham", "the guerrillas detained and \"executed\" eight people on February 6 in the town of Rio Bravo", "\"disagreements\" with the Port Authority of New York and New Jersey,", "June 2004", "Michelle Rounds", "James Newell Osterberg", "strangulation and asphyxiation and had two broken bones in his neck,", "Phil Spector", "Kim Il Sung", "1994", "numerous suicide attacks,", "Friday", "the death of a pregnant soldier", "Aryan Airlines Flight 1625", "Republicans", "Afghanistan's restive provinces", "Izzat Ibrahim al-Douri,", "people have chosen their rides based on what their cars say about them.", "raping her in a Milledgeville, Georgia, bar", "Pop star Michael Jackson", "Hoover Dam.", "a Yemeni cleric and his personal assistant,", "overthrow the socialist government of Salvador Allende in Chile,", "Miguel Cotto", "by 9 a.m.", "same-sex civil unions,", "military veterans", "bartering -- trading goods and services without exchanging money", "semi-autonomous organisational units", "6 - 6", "Bongos", "Jack Frost", "the innermost digit of the forelimb", "1988", "20 million", "Peoria, Illinois", "Hawaii", "\"The eyes of these croaking critters usually bulge,", "son", "Ottoman Empire"], "metric_results": {"EM": 0.5, "QA-F1": 0.5982503607503606}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8181818181818181, 0.2857142857142857, 1.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.2222222222222222, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8646", "mrqa_squad-validation-2754", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-270", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2399", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3032", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-714", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-9563", "mrqa_triviaqa-validation-4992", "mrqa_triviaqa-validation-5602", "mrqa_triviaqa-validation-4905", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-4159", "mrqa_searchqa-validation-11586", "mrqa_searchqa-validation-6839"], "SR": 0.5, "CSR": 0.556712962962963, "EFR": 1.0, "Overall": 0.7783564814814815}, {"timecode": 27, "before_eval_results": {"predictions": ["in the early 1990s", "leaf-shaped", "silver", "1755", "AbdulMutallab", "trading goods and services without exchanging money", "Kenner, Louisiana", "John Dillinger,", "The remains of Cologne's archive building following the collapse on Tuesday afternoon.", "Seasons of My Heart", "Haleigh Cummings,", "Whitney Houston", "Kris Allen", "a government-run health facility that provides her with free drug treatment.", "Lashkar-e-Tayyiba", "$1.5 million", "2006", "Rev. Alberto Cutie", "Angels", "Indian army troopers,", "\"There's no chance of it being open on time.", "South Carolina Republican Party Chairwoman Karen Floyd", "14", "a Starbucks", "\"BADBUL,\"", "98 people,", "2008", "on extreme caution because of the recent pirate attacks in the region.", "Paul Ryan", "state senators", "Dr. Jennifer Arnold and husband Bill Klein,", "Swat Valley,", "South Dakota State Penitentiary", "Iran", "November 26,", "people have chosen their rides based on what their", "in July", "a French charity", "Four Americans", "Josef Fritzl,", "Glasgow, Scotland", "38", "near the George Washington Bridge,", "President Bush", "fake his own death by crashing his private plane into a Florida swamp.", "in neighboring Copenhagen,", "fractured pelvis and sacrum", "Wednesday", "abduction of minors", "gun", "Jennifer Aniston, Marta Kauffman, co-creator of the series \"Friends\" and Kristin Hahn,", "U.S. Vice President Dick Cheney", "19 June 2018", "1954", "11 p.m.", "Charlotte Corday", "Thailand", "barley", "Norwood, Massachusetts", "Manchester, England", "Drowning Pool", "Missouri", "Vermont's largest city", "a beta blocker"], "metric_results": {"EM": 0.625, "QA-F1": 0.7491152332144979}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false], "QA-F1": [0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4285714285714285, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5, 0.3636363636363636, 0.4444444444444445, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705885, 1.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4094", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3895", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-933", "mrqa_newsqa-validation-938", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-436", "mrqa_naturalquestions-validation-6383", "mrqa_naturalquestions-validation-4505", "mrqa_hotpotqa-validation-4117", "mrqa_searchqa-validation-14535", "mrqa_searchqa-validation-12398"], "SR": 0.625, "CSR": 0.5591517857142857, "EFR": 1.0, "Overall": 0.7795758928571428}, {"timecode": 28, "before_eval_results": {"predictions": ["700,000", "coordinating lead author", "the control is spread more subtly through technological superiority,", "1981", "forgery and flying without a valid license,", "a racially-tinged remark made by his former caddy,", "Daniel Radcliffe", "nomination of Elena Kagan to fill the seat of retiring Supreme Court Justice John Paul", "Genocide Prevention Task Force", "announced what is thought to be a long-range missile on its launch pad,", "European Commission", "Whitney Houston", "firefighter", "a president who understands the world today, the future we seek and the change we need.", "Kurt Cobain", "seven", "the \"face of the peace initiative has been attacked,\"", "misdemeanor assault charges", "the shipping industry -- responsible for 5% of global greenhouse gas emissions, according to the United Nations -- embraces this technology the same way the public has.", "Anil Kapoor.", "to kill members of the Zetas cartel from the state of Veracruz,", "\"The Rosie Show,\"", "Form Design Center.", "collaborating with the Colombian government,", "Christianity and Judaism,", "the Dalai Lama's", "Russia", "8 p.m. local time Thursday", "Passers-by", "\"My gut started feeling like something just wasn't right,\"", "executive director of the Americas Division of Human Rights Watch,", "750", "at least 300", "Matthew Fisher", "The Ski Train", "oys And Girls Alone", "Ozzy Osbourne", "AbdulMutallab,", "some U.S. senators", "inconclusive", "5:20 p.m. at Terminal C", "some 700 vessels were illegally operating in the region and fishing the local stock to near depletion.", "$250,000", "byproducts emitted during the process of burning and melting raw materials.", "School-age girls", "5,600 people", "a million", "Sen. Arlen Specter", "Deutschneudorf,", "legislation that would let prisons jam cell-phone signals within their walls.", "a deceased organ donor,", "bragging about his sex life on television", "a vertebral column ( spine )", "December 11, 2014", "Michael Madhusudan Dutta", "Goldtrail", "Spain", "the Royal Festival Hall", "Douglas Hofstadter", "\"The Dark Tower\"", "American", "Little Women", "Castle Rock", "anchovies"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7276203312959606}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.6, 0.4, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.9565217391304348, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.625, 1.0, 1.0, 0.125, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 1.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8616", "mrqa_squad-validation-9810", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-4057", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-3413", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-1465", "mrqa_newsqa-validation-692", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-4028", "mrqa_hotpotqa-validation-4809"], "SR": 0.609375, "CSR": 0.5608836206896552, "EFR": 0.96, "Overall": 0.7604418103448276}, {"timecode": 29, "before_eval_results": {"predictions": ["stagnant wages for the working class", "poison", "438,000", "Erick Avari, Michael McKean, Amy D. Jacobson, Marty Ingels, Earl Boen, Jordana Capra, Dirk Benedict, and Rip Taylor.", "coaxial", "Pakistan A", "Ever Bank Field.", "14 directly elected members, 12 indirectly elected members representing functional constituencies and 7 members appointed by the chief executive.", "the German Campaign of 1813", "John Churchill,", "1965", "Paris", "fifth", "Culiac\u00e1n, Sinaloa", "seven", "Syracuse", "1963", "coca wine", "puzzle video game", "Knoxville, Tennessee", "Washington, D.C.", "Gal\u00e1pagos giant rat", "Tom Kartsotis", "2017", "Wayman Tisdale", "Mexico,", "Kolkata", "British", "the late 19th and early 20th centuries", "political thriller", "22,500", "the Harpe brothers", "Eric Liddell", "2002", "Gregg Harper", "Adventures of Huckleberry Finn", "small forward", "ARY Films", "Erinsborough", "Marine Corps", "Robert A. Iger", "Major Charles White Whittlesey.", "Floridians", "Virginia", "NBA Slam Dunk Contest", "$10\u201320 million", "January 28, 2016", "Kennedy Road", "Somerset County, Pennsylvania,", "Dallas, Texas", "Nigel Patrick, Margaret Whiting, Harry Andrews, Derren Nesbitt and Colin Blakely", "two Nobel Peace Prizes", "an international educational foundation headquartered in Geneva, Switzerland and founded in 1968", "Richard Parker", "the southernmost tip of the South American mainland", "Cecil B. De Mille", "allergic reaction", "King Edward VIII,", "3,000 kilometers", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Swiss art heist", "Russia", "peel and devein shrimp", "Australia"], "metric_results": {"EM": 0.484375, "QA-F1": 0.596792963980464}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true], "QA-F1": [0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.28571428571428575, 0.5, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.5, 0.0, 0.3076923076923077, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615383, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7189", "mrqa_squad-validation-8164", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-1936", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-1213", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-1101", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-1817", "mrqa_hotpotqa-validation-2220", "mrqa_hotpotqa-validation-1421", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-4074", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-3533", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-1144", "mrqa_hotpotqa-validation-1944", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-2774", "mrqa_triviaqa-validation-5424", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3888", "mrqa_searchqa-validation-2585", "mrqa_searchqa-validation-6793"], "SR": 0.484375, "CSR": 0.5583333333333333, "EFR": 1.0, "Overall": 0.7791666666666667}, {"timecode": 30, "before_eval_results": {"predictions": ["British", "October 16, 2012", "deforestation", "Prussian army general, adjutant to Frederick William IV of Prussia", "London", "Dave Thomas", "a cooperative where farmers pool their resources in certain areas of activity", "Danish", "1903", "the attack on Pearl Harbor", "other individuals, teams, or entire organizations", "ten years of probation", "In Pursuit", "Bolton", "Monty Python's Flying Circus", "Kansas City crime family", "Dirk Nowitzki", "lifetime achievements", "Alexandre Dimitri Song Billong", "Doc Hollywood", "1999", "200", "Theme Park World", "Formula E.", "New Jersey", "various deities, beings, and heroes", "86,112", "Celtic", "Ouse and Foss", "Springfield, Massachusetts", "comedian", "Apatosaurus", "1885", "American", "Frank Thomas' Big Hurt", "\"Gliding Dance of the Maidens\"", "Margarine Unie", "Winecoff Hotel fire", "New York City", "The Seduction of Hillary Rodham", "2005", "Lambic", "Massive Entertainment", "Argentina", "Larry Alphonso Johnson Jr.", "Mike Mills", "the veto power", "Joseph E. Grosberg", "\"Chelsea Lately\"", "276,170", "Turkmenistan", "Wembley Stadium, London", "Sally Field", "Tatsumi", "the Californian coast at The Inn at Newport Ranch,", "New York", "U.S Olympic Trials", "Aston, Birmingham, England", "2005", "228", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "Post-Traumatic Stress disorder", "Copenhagen", "Nez Perce"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7020487325174825}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.8, 1.0, 1.0, 0.5833333333333334, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5454545454545454, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3341", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-1467", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-2230", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-55", "mrqa_hotpotqa-validation-3090", "mrqa_hotpotqa-validation-4633", "mrqa_naturalquestions-validation-2250", "mrqa_triviaqa-validation-3906", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-5351", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3905", "mrqa_searchqa-validation-6975"], "SR": 0.59375, "CSR": 0.5594758064516129, "EFR": 1.0, "Overall": 0.7797379032258065}, {"timecode": 31, "before_eval_results": {"predictions": ["Fresno", "79", "Iceland", "Wyoming", "Silent Snow, Secret Snow", "Knott's Berry Farm", "Senator Tom Harkin", "a Van Morrison song", "Nassau", "Mother of Pearl", "AIDS", "Martin Van Buren", "Grande Vitesse", "Rigoletto", "aardwolf", "Beijing", "georgia", "Inuk", "Death Valley hottest spot", "Yves Saint Laurent", "reindeer", "Fortinbras", "the War of 1812", "Grandma Moses", "a Sailor Moon", "georgia", "New York Times Fiction Best Sellers", "a bear", "a whirlwind", "George Harrison", "Monty Python and the Holy Grail", "a negative electrode", "Milton Berle", "George Herbert Walker Bush", "Congolese", "lunar module", "vote of Chile", "Dan Marino", "Mars", "a clownfish", "E = mc2", "The Love Guru", "Las Vegas", "millet", "a butterfly", "Camelot", "orangutan", "Baja California", "Soothsayer", "Yitzhak Rabin", "Saul", "Gettysburg National Military Park", "Jack Gleeson", "Plank", "Buddhism", "Carl John", "Portugal", "Graham Bond", "Johnson &amp", "acidic bogs", "20 March to 1 May 2003", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "knocking the World Cup off the front pages for the first time in days.", "12.3 million people worldwide"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5732142857142857}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-10705", "mrqa_searchqa-validation-15396", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-8253", "mrqa_searchqa-validation-1768", "mrqa_searchqa-validation-5343", "mrqa_searchqa-validation-6655", "mrqa_searchqa-validation-15130", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-3343", "mrqa_searchqa-validation-14888", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-6838", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-13071", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-8189", "mrqa_searchqa-validation-6612", "mrqa_searchqa-validation-8550", "mrqa_searchqa-validation-10037", "mrqa_searchqa-validation-12761", "mrqa_searchqa-validation-13033", "mrqa_triviaqa-validation-3265", "mrqa_triviaqa-validation-4765", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-187", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3574"], "SR": 0.515625, "CSR": 0.55810546875, "EFR": 1.0, "Overall": 0.779052734375}, {"timecode": 32, "before_eval_results": {"predictions": ["During the Second World War", "62 acres", "jean peterington", "40", "Libya", "Shania Twain", "Sheffield Wednesday", "insulin and glucagon", "The New York Yankees", "rapid eye movement", "green", "Ann Dunham", "al-Bakr", "French", "peter pizarro", "Ohio", "Francis Matthews", "photographic", "iron", "Noah", "London", "New", "Sarah Ferguson", "Mercury", "watt", "Jennifer Richard", "Subway", "Madagascar", "Swansea City", "Gatcombe Park", "S\u00e3o Paulo", "optimism", "75", "Jennifer Lopez", "1664", "Eurythmics", "Fred Perry", "Downton Abbey", "Martina Hingis", "painter", "cyclops", "The Woodentops", "Michael Miles", "Sheryl Crow", "gulliver's Travels", "peterona", "Italy", "The Streets", "Appalachian Mountains", "a black Ferrari", "algebra", "grizzly bear", "Michael Moriarty", "June 1992", "24", "1952", "The Campbell Soup Company", "Kirkcudbright", "the soldiers", "cortisone.", "the United States can learn much from Turkey's expertise on Afghanistan and Pakistan.", "Jennifer Aronofsky", "Helvetica", "a lung"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6586538461538461}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.5, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.33333333333333337, 1.0, 0.15384615384615383, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-163", "mrqa_triviaqa-validation-3073", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-1023", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-6228", "mrqa_triviaqa-validation-7198", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-1545", "mrqa_triviaqa-validation-6378", "mrqa_triviaqa-validation-3570", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-4092", "mrqa_triviaqa-validation-5967", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-3742", "mrqa_hotpotqa-validation-3001", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-4171", "mrqa_searchqa-validation-14318", "mrqa_searchqa-validation-16567"], "SR": 0.609375, "CSR": 0.5596590909090908, "EFR": 1.0, "Overall": 0.7798295454545454}, {"timecode": 33, "before_eval_results": {"predictions": ["hymn-writer", "deadly explosives", "Knutsford", "insulin", "a Caesar salad", "Hudson Bay", "florida", "hay fever", "st. James Palace, London", "Getafix", "Brighton", "Belfast", "wind", "fire", "Robin Hood Men in Tights", "West Point", "Andy Warhol", "Spain", "clare", "rio de janeiro", "solar system", "potatoes", "Moldova", "Mitsubishi A6M Zero Fighter", "Warblers", "georgia", "Estimate", "baroudeur", "clon", "Pet Sounds", "Madness", "Buxton", "discretion", "Christian Dior", "Rudyard Kipling", "Leeds", "Philippines", "beaver", "Mel Blanc", "a dog", "Moffitt", "stania DeGeneres", "Phil Woolas", "5000 meters", "racing", "rennet", "Newfoundland and Labrador", "crow", "Yellowstone", "St Francis Xavier", "Philippines", "David Shore", "Buddhism", "Guy Berryman", "Ohio", "Port Melbourne", "Osbald", "Scarface", "forgery and flying without a valid license,", "English Premier League Fulham produced a superb performance in Switzerland on Wednesday to eliminate opponents Basel from the Europa League with a 3-2 victory.", "Liza Murphy,", "Spock", "Kazakhstan", "Andorra, Belgium, Germany, Italy, Luxembourg, Monaco, Spain"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6710261093073593}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-7206", "mrqa_triviaqa-validation-2126", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-6877", "mrqa_triviaqa-validation-6154", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-3424", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-4909", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-6068", "mrqa_triviaqa-validation-2642", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-1976", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-5602", "mrqa_newsqa-validation-2281", "mrqa_searchqa-validation-11382"], "SR": 0.609375, "CSR": 0.5611213235294117, "EFR": 0.92, "Overall": 0.7405606617647058}, {"timecode": 34, "before_eval_results": {"predictions": ["Battle of Fort Bull", "business districts", "tundras tundra", "bologna, Italy", "george Santayana", "marsupials", "Alice Cooper", "Beta-Blockers", "trumpet", "peter Kay", "The Scream", "shildon", "Appalachia", "mania", "ballet", "storm", "madonna", "reptiles", "Blackburn Lancashire", "Frankie Laine", "The Mystery of Edwin Drood", "pommel horse", "bird", "Dick Van Dyke", "Egremont", "manhunt", "miro", "prixus", "manhattan tallmouth", "Canada", "ink sac", "pears soap", "Some Like It Hot", "manhattan", "Ireland", "Mike Meyers", "sea horse", "a Volkswagen", "magma", "Passepartout", "Omniglot", "Spain", "Spain", "shrek", "26.22", "Cleveland Brown", "heston Blumenthal", "One Direction", "Flint", "Uranus", "manhattan Murders", "Charles Lindbergh", "September 2001", "Baaghi", "Lead and lead dioxide", "boxer", "East Kn Boyle", "beer", "University of Pittsburgh", "Pakistan's High Commission in India", "astonishment", "Hunter S. Thompson", "Tchaikovsky", "Howard Carter"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5463541666666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.0, 0.5, 1.0, 0.5, 0.4, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-439", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-918", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2786", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1963", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-766", "mrqa_triviaqa-validation-3204", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-3837", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-2212", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-4931", "mrqa_triviaqa-validation-4012", "mrqa_triviaqa-validation-4715", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-6918", "mrqa_naturalquestions-validation-4096", "mrqa_naturalquestions-validation-3623", "mrqa_hotpotqa-validation-2388", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-78"], "SR": 0.46875, "CSR": 0.5584821428571429, "EFR": 1.0, "Overall": 0.7792410714285715}, {"timecode": 35, "before_eval_results": {"predictions": ["alcohol", "John Forster", "Matlock", "the American Civil War", "shoa", "cetaceans", "the Arafura Sea", "daedalus", "the Tigris", "b Bavarian", "to make wrinkles in one's face,", "Spain", "carousel", "bullfight", "Mike Brady,", "octave above middle C3", "satya", "lemore", "Guys and Dolls", "jean Fellowes", "Denmark", "Another Day in Paradise", "The Last King of Scotland", "iceland", "eddisbury", "G. Ramon", "jean feldsworth", "marty rockner", "Finland", "black holes", "Mille Miglia", "saturn", "Bill Haley", "50p", "Muriel Spark", "happy birthday", "seven", "opossum", "pickwick", "presliced bread", "The Bridge", "raven", "Jordan", "new zealand", "armageddon", "Etruscan", "Ken Burns", "jean park", "hugh storney", "jean offenbach", "mujiburRahman", "saturn", "Donna", "season four", "the atrioventricular node", "Lee Sun-mi", "tomato", "2002", "problems with the way Britain implements European Union employment directives.", "the L'Aquila earthquake", "March 24,", "Prince Philip, Duke of Edinburgh", "equinox", "Pocahontas"], "metric_results": {"EM": 0.421875, "QA-F1": 0.4906994047619047}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5537", "mrqa_triviaqa-validation-3430", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-4295", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-400", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3517", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-5879", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7156", "mrqa_triviaqa-validation-6047", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4758", "mrqa_naturalquestions-validation-1091", "mrqa_hotpotqa-validation-5135", "mrqa_newsqa-validation-2799", "mrqa_newsqa-validation-629", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-14619"], "SR": 0.421875, "CSR": 0.5546875, "EFR": 1.0, "Overall": 0.77734375}, {"timecode": 36, "before_eval_results": {"predictions": ["Americans to explain the Iranian Islamic Revolution and apolitical Islam", "kim", "city of acacias", "Branson", "Gordon Ramsay", "Manchester City", "Robert Kennedy", "sulfur dioxide", "Annelies", "Ringway Airport", "Portuguese", "travelocity", "The Avengers", "Richmond", "comets", "comets", "Emmaus", "rapeseed", "Tina Turner", "Sweeney Todd", "p Preston North End", "Bolivia", "John Donne", "Uranus", "Rio Grande", "clowns", "The Graduate", "Greek", "comets of Wrath", "king jim", "One Foot in the Grave", "Bronx", "comets", "George Santayana", "Shooting Glove", "Borrowdale", "Wee Jimmy Krankie", "Tomas De torquemada", "christopher farenboim", "Canada", "rum", "Lake Union", "ghee", "king george", "Justin Bieber", "hyperbole", "oldpatrick", "June", "David Graham", "Ceylon", "screwdrivers", "the Kansas City Chiefs", "Symphony No. 40 in G minor", "A Christmas Story", "1974", "The Outsiders", "Amberley Village", "lack of a cause of death", "President Obama", "Elisabeth,", "A Revisionist Study of the Empress", "Brigham Young", "pearl", "chalk quarry"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5406249999999999}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, true], "QA-F1": [0.19999999999999998, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.4, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9574", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-334", "mrqa_triviaqa-validation-1471", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1971", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-3877", "mrqa_triviaqa-validation-4188", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-4661", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-6678", "mrqa_triviaqa-validation-5196", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-2834", "mrqa_triviaqa-validation-1270", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-4966", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-3564", "mrqa_triviaqa-validation-132", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-712", "mrqa_triviaqa-validation-3013", "mrqa_triviaqa-validation-3756", "mrqa_triviaqa-validation-7258", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4108", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-2908", "mrqa_searchqa-validation-7120"], "SR": 0.453125, "CSR": 0.5519425675675675, "EFR": 0.9714285714285714, "Overall": 0.7616855694980695}, {"timecode": 37, "before_eval_results": {"predictions": ["a not-for-profit United States computer networking consortium", "pH ( / pi\u02d0\u02c8 ( h ) e\u026a t\u0283 / ) ( potential of hydrogen )", "Durham", "Alex Ryan", "Sakshi Malik", "Columbia River Gorge", "adrenal medulla produces a hormonal cascade that results in the secretion of catecholamines, especially norepinephrine and epinephrine", "49 cents", "1876", "geologist Charles Lyell", "14 \u00b0 41 \u2032 34", "joy of living", "420", "John Prine and Roger Cook", "sovereignty over some or all of the current territory of the U.S. state of Texas", "1989", "Shawn", "Kiss", "London, England", "Los Angeles", "February 10, 2017", "Kelly Reno", "provides the public with financial information about a nonprofit organization", "1770 BC", "Niveditha, Diwakar, Shruti", "two", "John C. Reilly", "DNA nucleus", "Anakin", "Travis Tritt and Marty Stuart", "1976", "Bee Gees", "Matt Czuchry", "Pradyumna", "1902", "( 48 \u00b0 38 \u2032 23 '' N 4 \u00b0 34 \u2032 13 '' W \ufeff 48.63972 \u00b0 N 4.57028 \u00b0 W", "Psychomachia", "New Jersey Devils", "two", "4 in ( 10 cm )", "Cress", "Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "January 2018", "Sir Donald Bradman", "Tokyo", "1978", "Nicki Minaj", "alcohol or smoking, biological agents, stress, or chemicals to mortality or morbidity", "Gloria ( Lisa Stelly )", "the Canadian Rockies", "The Maginot Line", "pussia", "dumbo", "purple", "Charles Guiteau", "Gettysburg Address", "iTunes, iTunes Radio, and iTunes Music", "eight years", "India", "Gulf of Aden", "Desperate Housewives", "Cannonball Run", "Morelos", "Tuesday"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6392281706819749}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.787878787878788, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.8, 0.5, 0.6666666666666666, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5277777777777778, 0.25, 0.5454545454545454, 1.0, 0.6666666666666666, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-307", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3916", "mrqa_naturalquestions-validation-6583", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-233", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-2502", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-5308", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1415", "mrqa_naturalquestions-validation-8025", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8514", "mrqa_triviaqa-validation-6008", "mrqa_triviaqa-validation-7642", "mrqa_hotpotqa-validation-5119", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-2554", "mrqa_searchqa-validation-2335"], "SR": 0.484375, "CSR": 0.5501644736842105, "EFR": 0.9696969696969697, "Overall": 0.7599307216905902}, {"timecode": 38, "before_eval_results": {"predictions": ["Michelangelo", "roughly five hundred experts across the world", "United States", "Kim Basinger", "fall of 2015", "secretion of catecholamines, especially norepinephrine and epinephrine", "Kusha", "positions Arg15 - Ile16", "Joseph M. Scriven", "Lady Gaga", "Chicago metropolitan area", "President of the United States", "Domhnall Gleeson", "eusebeia", "horticulture", "Notts County", "a nobiliary particle indicating a noble patrilineality", "Stephen A. Douglas", "1984", "related to the Common Germanic word guma ( Old English guma `` man '', Middle English gome", "Pakistan", "21 February", "Tagalog or English", "Bryan Cranston", "the thylakoid membranes", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Felix Baumgartner", "Wake County, it lies just north of the state capital, Raleigh", "January 1923", "2017 / 18 Divisional Round game", "520", "average energy of 251 keV", "between $10,000 and $30,000", "the Sicob show in Paris", "1931", "University of Oxford", "11 : 40 p.m. ship's time", "Gladys Knight & the Pips", "1959", "The statesmen who led the secession movement", "Randy", "that country's surprise attack on Pearl Harbor the prior day", "Joseph Stalin", "into the intermembrane space", "divergent tectonic", "Idaho", "Sara Gilbert", "13", "First Lieutenant Israel Greene", "Gunpei Yokoi", "Lizzy Greene", "yellow", "Margaret Thatcher", "Roddy doddy Doyle", "Daniil Shafran", "TD Garden", "Venus", "should have met with the Dalai Lama.", "10 below", "General Motors", "David McCullough", "Rendezvous with Rama", "CERN", "l Lisbon"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6144020482927133}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 0.5, 0.6363636363636364, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8695652173913044, 1.0, 0.5714285714285715, 0.0, 0.0, 0.32, 0.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6153846153846153, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5569", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-1181", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-4929", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-2876", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-3724", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-10724", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-5292", "mrqa_triviaqa-validation-6088", "mrqa_triviaqa-validation-5582", "mrqa_newsqa-validation-3486", "mrqa_searchqa-validation-3219", "mrqa_triviaqa-validation-2762"], "SR": 0.515625, "CSR": 0.5492788461538461, "EFR": 0.967741935483871, "Overall": 0.7585103908188586}, {"timecode": 39, "before_eval_results": {"predictions": ["comb-rows", "Diary of a Wimpy Kid : The Long Haul", "Jenny Slate", "ATP", "Philippe Petit ( French pronunciation : \u200b ( filip p\u0259ti )", "R2E Micral CCMC", "January 2004", "southwest and along the Yangtze ; it is planted in March to June and harvested in October and November and also contributed about 34 percent to total rice output in the 1980s", "Toby Keith", "development of electronic computers in the 1950s", "17 - year - old", "alternative rock", "Set six months after Kratos killed his wife and child, he has been imprisoned by the three Furies for breaking his blood oath to Ares", "Teri Hatcher", "the spacecraft to become unstable and break apart", "XXXX", "experimental psychology", "53", "between the Eastern Ghats and the Bay of Bengal", "Julie Adams", "Rachael Harris", "Richard Crispin Armitage", "Don Cook", "Dirk Benedict ( born Dirk Niewoehner on March 1, 1945 )", "Bonnie Aarons", "expected in either late 2018 or early 2019", "generally composed of roughly 70 % hydrogen by mass, with most of the remaining gas consisting of helium", "the decision effectively overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "McKim Marriott", "john F. Kelly", "Charles Sherrington", "1890", "either small fission systems or radioactive decay for electricity or heat", "Joseph Stalin", "man", "As of September, 2016", "is a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "1978", "defense against rain rather than sun", "1940", "Ariel Winter", "Mark Jackson", "Michael Buffer ( born November 2, 1944 )", "God and Father of all, who is over all and through all and in all", "because we travelled quite far, we built sets, and they spend a lot of time in a forest,", "federal government", "American author Elizabeth George Speare", "Cody Fern", "questions about the name of the war, the tariff, states'rights and the nature of Abraham Lincoln's war goals", "prophets and beloved religious leaders", "about 0.04 mg / L several times during a day", "Juan Manuel de Ayala", "john smith, Jr.", "funny Folks (1874 - 1894)", "1909", "John Duigan", "179", "Princess Diana", "Mikkel Kessler", "curfew", "me and Bobby McGee", "shark", "Fast Food Nation", "ABBA"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5638375785434608}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.1212121212121212, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.6153846153846153, 0.07407407407407407, 0.16666666666666666, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.5, 0.20000000000000004, 0.0, 0.0, 0.0, 1.0, 0.11764705882352941, 0.7499999999999999, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2542", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-5070", "mrqa_naturalquestions-validation-10199", "mrqa_naturalquestions-validation-2842", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-753", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4366", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-9093", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-3353", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-6119", "mrqa_triviaqa-validation-765", "mrqa_hotpotqa-validation-2429", "mrqa_hotpotqa-validation-4917", "mrqa_newsqa-validation-302", "mrqa_searchqa-validation-10341"], "SR": 0.453125, "CSR": 0.546875, "EFR": 1.0, "Overall": 0.7734375}, {"timecode": 40, "before_eval_results": {"predictions": ["monophyletic", "\"We tortured (Mohammed al-) Qahtani,\"", "the Indian army and separatist militants", "Joan Rivers", "\"You're The One That I Want\"", "glamour and hedonism", "2-0", "15,000", "58 people", "Michael Schumacher", "\"Neural devices are innovating at an extremely rapid rate and hold tremendous promise for the future,\"", "numerous suicide attacks,", "Zimbabwe", "since 2004", "NATO", "Switzerland", "Monday", "second", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "\"To improve America's competitiveness, the senator from Illinois said he wants to spend $10 billion on childhood education, $150 billion over 10 years on developing alternative energy", "T.I.", "about 112 miles northeast of Eureka", "Robert Barnett", "$627", "41", "Los Angeles Angels", "a strict interpretation of the law,", "Derek Mears", "Sylt", "rural Tennessee", "Joseph Maraachli,", "the southern city of Naples", "fake his own death by crashing his private plane into a Florida swamp.", "11", "don't have to visit laundromats because they enjoy the luxury of a free laundry service.", "dual nationality", "on the headstones to show that a visitor had been to the grave.", "Ali Bongo", "Allred", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "Two pages", "A Brazilian supreme court judge", "Derek Mears", "Operation Crank Call", "help rebuild the nation's highways, bridges and other public-use facilities.", "East Java", "St. Louis, Missouri.", "NATO fighters", "High Court Judge Justice Davis", "Adam Lambert and Kris Allen", "some of the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "released in 2007", "P.V. Sindhu", "on location in Mexico", "snickers", "monoceros", "capone", "Anaheim, California", "uncle Juan Nepomuceno Guerra", "Bergen", "embalming", "Cartagena", "a graphical user interface", "the American Kennel Club"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6660828268876477}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.8333333333333333, 1.0, 0.0, 0.375, 1.0, 0.5, 0.6666666666666666, 1.0, 0.9333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.3870967741935484, 0.08888888888888889, 0.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 0.29629629629629634, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9166666666666666, 1.0, 0.0, 1.0, 0.0, 0.9523809523809523, 1.0, 0.4, 1.0, 1.0, 0.9411764705882353, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5384615384615384, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-2439", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3794", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-3448", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1761", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8460", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-2685", "mrqa_hotpotqa-validation-4241", "mrqa_hotpotqa-validation-877", "mrqa_searchqa-validation-7810", "mrqa_searchqa-validation-1293", "mrqa_naturalquestions-validation-10583"], "SR": 0.484375, "CSR": 0.5453506097560976, "EFR": 1.0, "Overall": 0.7726753048780488}, {"timecode": 41, "before_eval_results": {"predictions": ["Battle of Sainte-Foy", "during the 1890s Klondike Gold Rush, when strong sled dogs were in high demand", "Stephen A. Douglas", "1998", "Directed distance is a positive, zero, or negative scalar quantity", "based on sovereign states is often traced back to the Peace of Westphalia of 1648, a stepping stone in the development of the modern state system", "Megan Park", "euro", "Kate Walsh", "September 14, 2008", "American country music artist Trace Adkins", "Mars Hill, 150 miles ( 240 km ) to the northeast", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "2002", "they find cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "pour point of a liquid", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "A remittance", "Akshay Kumar", "Shirley Mae Jones", "15 February 1998", "5.7 million", "estimated in 2009 to be less than $10,000 per year", "mining", "Cedric Alexander", "interspecific hybridization and parthenogenesis", "David Joseph Madden", "1902", "the Dutch figure of Sinterklaas", "Yuzuru Hanyu", "provide jobs for young men and to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Phillipa Soo", "A firm, flexible cup - shaped device worn inside the vagina to collect menstrual flow", "pigs", "General George Washington", "Spanish", "Howard Ellsworth Rollins Jr", "an integral membrane protein that builds up a proton gradient across a biological membrane", "usually by the sinoatrial node to cause contraction of the heart muscle", "four", "Jack Nicklaus", "Norman Greenbaum", "Dan Stevens", "16", "to solve South Africa's `` ethnic problems '' by creating complementary economic and political units for different ethnic groups", "the Intertropical Convergence Zone ( ITCZ )", "Missouri River", "the right to be served in facilities which are open to the public -- hotels, restaurants, theaters, retail stores, and similar establishments ''", "frontal lobe", "10 June 1940", "at Tandi, in Lahaul", "Alberich", "ear", "Brazil", "The Dressmaker", "$10.5 million", "Tim Whelan", "September 21.", "Denver, Colorado.", "the Sadr City and Adhamiya districts of Baghdad City,\"", "September 19, 2007", "King Arthur", "Howie Mandel", "Virgin America"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7259402655245872}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.5555555555555556, 1.0, 0.0, 0.0, 0.045454545454545456, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 0.8, 0.6666666666666666, 0.7000000000000001, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6451612903225806, 1.0, 1.0, 0.4, 1.0, 1.0, 0.4, 1.0, 0.8235294117647058, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5940", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-4768", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-7049", "mrqa_naturalquestions-validation-9856", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-7807", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6887", "mrqa_newsqa-validation-3002", "mrqa_newsqa-validation-939", "mrqa_searchqa-validation-16518"], "SR": 0.59375, "CSR": 0.5465029761904762, "EFR": 0.9230769230769231, "Overall": 0.7347899496336996}, {"timecode": 42, "before_eval_results": {"predictions": ["Ancient Egypt", "vaporization of water", "Middlesex County, Province of Massachusetts Bay, within the towns of Lexington, Concord, Lincoln, Menotomy ( present - day Arlington ), and Cambridge", "caused by chlorine and bromine from manmade organohalogens", "Michael Buffer", "Thomas Edison", "the Seventeenth Amendment in 1913", "Zeus", "During Hanna's recovery masquerade celebration, she suddenly regains her memory, revealing that Mona is A.", "Abid Ali Neemuchwala", "between the Mediterranean Sea to the north and the Red Sea in the south", "to bring, and \u03bd\u03af\u03ba\u03b7, n\u00edk\u00ea, `` victory '', i.e. `` she who brings victory ''", "Field Marshal Paul von Hindenburg", "Ceramic art", "the Soviet Union", "Covington, Kentucky", "New Mexico", "to condense the steam coming out of the cylinders or turbines", "December 15, 2017", "between Polaris Avenue and Dean Martin Drive, just west of Interstate 15", "L.K. Advani", "differential erosion", "American video game designer Roger Dearly ( Jeff Daniels )", "the middle of the Sermon on the Mount", "about 375 miles ( 600 km ) south of Newfoundland", "Andy Serkis", "West Ham United ( 1980 )", "2018", "electricity generation, power distribution, and power transmission on the island", "fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "Norman Greenbaum", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "electron shells", "ratio of the length s of the arc by the radius r of the circle", "Charlotte Thornton", "the Northeast Monsoon or Retreating Monsoon", "March 16, 2018", "President Lyndon Johnson", "approximately 1945", "Ariana Clarice Richards", "Jonathan Breck", "Husrev Pasha", "Daya Jethalal Gada", "880,000 square kilometres ( 340,000 sq mi )", "parthenogenesis", "1926", "Durban, South Africa", "starting in 1560s", "Frankie Muniz", "Lou Rawls", "between 1765 and 1783", "Loki", "Illinois", "Alice in Wonderland", "Los Angeles", "Elijah Wood", "96,867", "recall notices", "won two awards.", "prostate cancer,", "wyvern", "Lord Fauntleroy", "a key ring or a decorative key fob", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6546173100860602}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.972972972972973, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.9, 0.5333333333333333, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6857142857142856, 0.14814814814814814, 0.6666666666666666, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10512", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-81", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-5566", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-4983", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-2833", "mrqa_hotpotqa-validation-1134", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1248", "mrqa_searchqa-validation-5636", "mrqa_searchqa-validation-11152"], "SR": 0.515625, "CSR": 0.5457848837209303, "EFR": 0.8387096774193549, "Overall": 0.6922472805701425}, {"timecode": 43, "before_eval_results": {"predictions": ["2013", "February 27, 2007", "a'pick yourself up and dust yourself off and keep going ', female - empowerment song", "to relieve families who had difficulty finding jobs during the Great Depression in the United States", "Lynne", "2013", "as the arms of the king of Ireland", "Miami Heat", "1981", "After World War I", "in the mid - to late 1920s", "Napoleon Bonaparte", "Juan Francisco Ochoa", "Augustus Waters", "Pierre Mallet", "Virgil Ogletree, a numbers operator who was wanted for questioning in the bombing of rival numbers racketeer and future boxing promoter Don King's home three days earlier", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Haliaeetus", "a thick bunch of rootlets ( branch roots )", "Alex Ryan", "a habitat", "2018", "WMA", "100 members, two from each of the 50 states", "Toledo", "Transvaginal ultrasonography", "During the last Ice Age", "Haikou on the Hainan Island", "Robert Irsay", "Paradise, Nevada", "Alicia Vikander as Lara Croft, who embarks on a perilous journey to her father's last - known destination, hoping to solve the mystery of his disappearance", "in late January or early February", "Ashoka", "a roof collapsing under the weight of stone above the Chamber", "Robert Andrews Millikan, who had previously determined the charge of the electron, and thus increasing the energy of each photon remains low", "Puerto Rico Electric Power Authority", "Bumblebee", "into the Christian biblical canon", "Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont", "AMX - 50", "honey bees", "Mary Chapin Carpenter", "Louvre Museum in Paris", "over two days in July 2011", "during the winter of the 2017 -- 18 network television season on CBS", "Florida", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "in the southwestern part of the island", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "winter", "Pangaea", "Newcastle Brown Ale", "Western Australia", "Vaclav Havel", "Mary Bon auto, Susan Murray, and Beth Robinson", "Chelsea", "North America", "\"has been plagued by massive cost and schedule problems - and almost no progress,\"", "\" resetting\"", "$60 billion on America's infrastructure.", "Dean Acheson", "Bob Kerrey", "Jane Goodall", "Forrest Gump"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6212018530596567}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.7741935483870968, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.4, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.3571428571428571, 0.4705882352941177, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 0.3636363636363636, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 1.0, 0.9189189189189189, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-151", "mrqa_naturalquestions-validation-8594", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5052", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-4351", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-4675", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-5474", "mrqa_triviaqa-validation-2697", "mrqa_hotpotqa-validation-1693", "mrqa_newsqa-validation-2449", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1977"], "SR": 0.484375, "CSR": 0.5443892045454546, "EFR": 1.0, "Overall": 0.7721946022727273}, {"timecode": 44, "before_eval_results": {"predictions": ["\u00a330m", "lightweight aluminum foil", "Laurel, Mississippi", "about the outdoors", "Indianola", "Escorts Limited", "Jean Baptiste Point DuSable", "1964", "the Goddess of Pop", "eastern", "James Harrison", "Toronto", "Tomorrowland", "fennec fox", "United States Army", "stop motion animation", "Jean Acker", "4,530", "Democritus", "Caesars Entertainment Corporation", "Mary Ellen Mark", "Reinhard Heydrich", "Karl Kraus", "Christopher Rich Wilson", "Maria Brink", "Manitowoc County, Wisconsin", "the Northrop P-61 Black Widow", "Adelaide", "World Famous Gold & Silver Pawn Shop", "Sri Lanka Freedom Party", "Bishop's Stortford F.C.", "United States ambassador to Ghana", "Emmy", "1991", "Leatherheads", "September 25, 2017", "John Delaney", "Tampa", "OutKast", "Richard Street", "Zaire", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Pakistan", "Shohola Falls", "American chef, author and television personality", "South America", "2006", "perjury and obstruction of justice", "Operation Neptune", "Mary Elizabeth Hartman", "over 9,000", "Susan Jameson", "potential of hydrogen", "the Alamodome in San Antonio, Texas", "Stephen King Biography", "The Finger Tab", "Kent", "almost 9 million", "Kenya", "2008", "small", "Moses", "Chapter 5", "Wilson Pickett"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6254407051282052}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3253", "mrqa_hotpotqa-validation-5461", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-2340", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-3470", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-1069", "mrqa_hotpotqa-validation-2679", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-5620", "mrqa_naturalquestions-validation-9081", "mrqa_triviaqa-validation-1184", "mrqa_triviaqa-validation-6953", "mrqa_newsqa-validation-1932", "mrqa_newsqa-validation-4197", "mrqa_searchqa-validation-516", "mrqa_searchqa-validation-13590", "mrqa_naturalquestions-validation-9677"], "SR": 0.515625, "CSR": 0.54375, "EFR": 1.0, "Overall": 0.771875}, {"timecode": 45, "before_eval_results": {"predictions": ["calcitriol", "Mazda", "1937", "Australian", "1903", "interstate commerce", "Naomi Wallace", "McLaren-Honda", "Tufts College", "People's Republic of China", "Azeroth", "Squam Lake", "Philip Livingston", "Tayeb Salih", "King James II", "God Save the Queen", "526", "Scotland", "\"rock and roll\"", "GmbH", "Mick Jackson", "Lalit", "her performances", "Tampa Bay Lightning", "Steven Selling", "Sully", "the Manhattan Project", "Pacific War", "Romantic", "Air Chief Marshal Hugh Caswall Tremenheere Dowding, 1st Baron Dowding", "AMC Entertainment Inc.", "New York Islanders", "fennec", "1978", "the Surtees Racing Organisation team that competed as a constructor in Formula One, Formula 2 and Formula 5000", "French", "Pacific Place", "the Matildas", "Humble", "Rudebox", "about 5320 km", "Andrea Maffei", "politician", "Sacramento Kings", "Walldorf", "Fife", "Fyvie Castle", "Faisal Qureshi", "the British Army", "\"rule of the people\"", "Boletus edulis", "Robert Remak", "JackScanlon", "Steve Hale", "Judy Garland", "Switzerland", "Model A", "NATO's International Security Assistance Force", "2,000", "Cyprus", "\"This Love\"", "Saudi Arabia", "Geraldine A. Ferraro", "two"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6494791666666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.4, 1.0, 0.6666666666666666, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1718", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-4880", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-1909", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-5589", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-3592", "mrqa_newsqa-validation-321", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-8327"], "SR": 0.546875, "CSR": 0.5438179347826086, "EFR": 1.0, "Overall": 0.7719089673913043}, {"timecode": 46, "before_eval_results": {"predictions": ["less than a year", "tepuis", "is a song from The King and I", "Republican", "1996", "5 hearts", "Greenland shark", "The Word", "President Abraham Lincoln's portrait", "St Jude Thaddeus", "Anthoonij van Diemenslandt", "the Death Penalty", "xerophyte", "joseph Robinson", "Staten Island", "Dian Fossey", "MI5", "harrow", "cr\u00e8me anglaise", "onions", "chicken", "curling", "Victoria Coren Mitchell", "carrie", "Chile\u2019s best wine producing regions", "Majorca (Mallorca)", "is mainly based on a character named Pip", "Laputa", "Lee Harvey Oswald", "joseph wieck", "Mercury", "Venus", "President Obama", "Canada", "Most of Americans", "Cuban republic", "is known as \"The Idiot and Lust for Life at Hansa, widely seen as Iggy's two finest albums", "Stephen King", "Hinduism", "caryatid", "feet", "Spain's busiest port", "Mary Poppins", "glyn Jones", "Port Moresby Harbour", "Connecticut", "Quentin Blake", "whooping cough", "the Daily Herald", "numerous", "kosher", "beginning in 2016", "the courts", "2017", "Chief of Protocol", "Diamond White", "1944", "Vera Zvonareva of Russia and Austria's Daniel Koellerer", "Jeddah, Saudi Arabia", "death", "Beatrix Potter", "Dan Rather", "Reader's Digest Association", "\"Divide the living child in two"], "metric_results": {"EM": 0.453125, "QA-F1": 0.55}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-7469", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-762", "mrqa_triviaqa-validation-1742", "mrqa_triviaqa-validation-5056", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-5642", "mrqa_triviaqa-validation-6276", "mrqa_triviaqa-validation-1581", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-5266", "mrqa_triviaqa-validation-4157", "mrqa_triviaqa-validation-4519", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-4384", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-9246", "mrqa_newsqa-validation-1157", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2520", "mrqa_searchqa-validation-3262", "mrqa_searchqa-validation-7827", "mrqa_searchqa-validation-6488"], "SR": 0.453125, "CSR": 0.5418882978723405, "EFR": 1.0, "Overall": 0.7709441489361702}, {"timecode": 47, "before_eval_results": {"predictions": ["zebra", "allergic reaction", "david Beckham", "getty-five", "Runic", "Spain", "cricketer", "Max Planck", "rotherham United", "heat transfer", "misery", "Styal", "olek", "Blind Beggar", "Brainwash", "Leroy Burrell", "parlophone", "Wild Atlantic Way", "john Denver", "unseen", "noddy", "lackawanna six", "chile", "oscar", "muezzin", "window", "keel", "flaubert", "Apollo 11", "flit", "Nikola Tesla", "hugh", "evita", "albino sperm whale", "roosevelt", "east fife", "st Pancras International Station", "immediate physical and social setting in which people live or in which something happens or develops", "sliced bread", "Dilbert", "mayor of casterbridge", "simeon", "French", "medea", "b Burgundy", "cribbage", "Beatles", "Johannesburg", "France", "muffin man", "South Korea", "Prince James, Duke of York and of Albany", "considered to be unfair", "Mike Nesmith", "Pansexuality", "Tony Ducks", "1754", "drugs", "Galveston, Texas,", "filing for bankruptcy protection", "Robert Frost", "King Henry VIII", "hk", "Turbo Charged Prelude"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6207986111111111}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.32, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6521", "mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-739", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5895", "mrqa_triviaqa-validation-5433", "mrqa_triviaqa-validation-3862", "mrqa_triviaqa-validation-2627", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-5130", "mrqa_triviaqa-validation-6813", "mrqa_triviaqa-validation-4781", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-5014", "mrqa_hotpotqa-validation-3408", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1947", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-12618", "mrqa_naturalquestions-validation-9564"], "SR": 0.5625, "CSR": 0.5423177083333333, "EFR": 0.9642857142857143, "Overall": 0.7533017113095237}, {"timecode": 48, "before_eval_results": {"predictions": ["joe henderson", "sesame street", "tomatoes", "cabbage", "crickets", "jimmy henderson", "fleece", "Ash tree", "opossum", "New Zealand", "julian henderson", "60", "goldfinger", "1984", "frog", "mongols", "1875", "tax collector", "pennies", "robin stanford", "Wars of the Roses", "the Bagram Collection Point", "jimmy henderson", "Chrysler", "ushanka", "dandy", "education", "United States", "Brazil", "pei Tang", "biathlon", "lodiston", "Charlie Chan", "Vienna", "white", "jaws", "jimmy henderson", "rabbit", "scotland", "jimmy", "orson Welles", "hindu", "menorah", "Dutch", "texas", "Super Bowl Sunday", "A long pole", "jimmy stout", "mozarts", "Rhododendron", "Irish", "Chuck Noland", "Virginia", "in Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Clive Staples Lewis", "Tsavo East National Park, Kenya", "2010", "the domed BC Place Stadium.", "10 below", "100 to 150", "coins", "the American Kennel Club", "Omaha", "Dick & Jane"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5533137077294685}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.9565217391304348, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3951", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-1990", "mrqa_triviaqa-validation-7027", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-3262", "mrqa_triviaqa-validation-6865", "mrqa_triviaqa-validation-2687", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-2957", "mrqa_triviaqa-validation-2352", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-3019", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-5229", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-726", "mrqa_triviaqa-validation-6306", "mrqa_triviaqa-validation-2158", "mrqa_naturalquestions-validation-6564", "mrqa_hotpotqa-validation-2352", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-11366"], "SR": 0.46875, "CSR": 0.5408163265306123, "EFR": 1.0, "Overall": 0.7704081632653061}, {"timecode": 49, "before_eval_results": {"predictions": ["shanghai, Alabama", "Iran", "tobacco", "francis", "jennifer", "little Richard", "Thames Street", "Theodore Roosevelt", "satyrs", "tartar", "la boheme", "IBM", "a wishbone", "theatre antiquarian", "Lackawanna 6", "master Humphrey", "of ireland", "the American Civil War", "detective stories", "tommy lee", "jimmy Robertson", "Florence", "saint Basil", "veruca salt", "shrewsbury", "australia", "South Africa", "droughts", "Nicaraguan", "tartan", "(1455\u201385)", "leipzig and Dresden", "bagram", "trout", "ap\u00e9ro", "jennifer adan", "central America", "shanghai", "bagram", "sprint", "Charlie Drake", "robin hood", "clangers", "Flintstones", "lead detective sergeant (Lee Ingleby)", "rugby", "honda", "shanghai", "11", "tobacco", "(A.I.)", "a value to it", "Tom Selleck", "New Orleans", "pinball", "Texas Tech University", "loughborough, Leicestershire, in the East Midlands of England", "Herman Cain", "Afghanistan", "\"border states\"", "Dan Burton", "Oklahoma", "(1970)", "four"], "metric_results": {"EM": 0.28125, "QA-F1": 0.32638888888888884}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-2933", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-7701", "mrqa_triviaqa-validation-7063", "mrqa_triviaqa-validation-1188", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-3165", "mrqa_triviaqa-validation-3180", "mrqa_triviaqa-validation-3429", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-51", "mrqa_triviaqa-validation-1836", "mrqa_triviaqa-validation-2917", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-3092", "mrqa_triviaqa-validation-1810", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-1300", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-6968", "mrqa_triviaqa-validation-1960", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-3257", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-1370", "mrqa_triviaqa-validation-523", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-3984", "mrqa_triviaqa-validation-7243", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-8560", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2146", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1442", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-3615"], "SR": 0.28125, "CSR": 0.535625, "EFR": 0.9782608695652174, "Overall": 0.7569429347826087}, {"timecode": 50, "UKR": 0.689453125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-117", "mrqa_hotpotqa-validation-1195", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-1598", "mrqa_hotpotqa-validation-1715", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1889", "mrqa_hotpotqa-validation-1943", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-2082", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2772", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3225", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-3704", "mrqa_hotpotqa-validation-3705", "mrqa_hotpotqa-validation-3810", "mrqa_hotpotqa-validation-3839", "mrqa_hotpotqa-validation-3854", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-4191", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4834", "mrqa_hotpotqa-validation-4876", "mrqa_hotpotqa-validation-4917", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-5135", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-5600", "mrqa_hotpotqa-validation-5643", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-681", "mrqa_hotpotqa-validation-841", "mrqa_hotpotqa-validation-877", "mrqa_hotpotqa-validation-947", "mrqa_hotpotqa-validation-993", "mrqa_naturalquestions-validation-10054", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-10659", "mrqa_naturalquestions-validation-1187", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-1736", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-2159", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-2472", "mrqa_naturalquestions-validation-2482", "mrqa_naturalquestions-validation-2580", "mrqa_naturalquestions-validation-2692", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2803", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-39", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3985", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4348", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5631", "mrqa_naturalquestions-validation-5724", "mrqa_naturalquestions-validation-5802", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-6620", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-6692", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6764", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-7164", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7382", "mrqa_naturalquestions-validation-7408", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8298", "mrqa_naturalquestions-validation-8346", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8503", "mrqa_naturalquestions-validation-854", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8814", "mrqa_naturalquestions-validation-8910", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9188", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-928", "mrqa_naturalquestions-validation-9341", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-9422", "mrqa_naturalquestions-validation-9444", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9574", "mrqa_naturalquestions-validation-9752", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-1130", "mrqa_newsqa-validation-115", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1544", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1720", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-184", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2101", "mrqa_newsqa-validation-224", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2331", "mrqa_newsqa-validation-2397", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-2676", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-27", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2725", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-3078", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3143", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-3487", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3513", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3543", "mrqa_newsqa-validation-3606", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3658", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3840", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-3893", "mrqa_newsqa-validation-390", "mrqa_newsqa-validation-3915", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-4074", "mrqa_newsqa-validation-4096", "mrqa_newsqa-validation-4132", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4184", "mrqa_newsqa-validation-458", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-576", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10384", "mrqa_searchqa-validation-10782", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-11820", "mrqa_searchqa-validation-12398", "mrqa_searchqa-validation-12828", "mrqa_searchqa-validation-13033", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13941", "mrqa_searchqa-validation-13982", "mrqa_searchqa-validation-14619", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-15040", "mrqa_searchqa-validation-15484", "mrqa_searchqa-validation-15660", "mrqa_searchqa-validation-16041", "mrqa_searchqa-validation-16840", "mrqa_searchqa-validation-16966", "mrqa_searchqa-validation-2009", "mrqa_searchqa-validation-2043", "mrqa_searchqa-validation-2051", "mrqa_searchqa-validation-2973", "mrqa_searchqa-validation-3113", "mrqa_searchqa-validation-3232", "mrqa_searchqa-validation-3818", "mrqa_searchqa-validation-4136", "mrqa_searchqa-validation-5881", "mrqa_searchqa-validation-620", "mrqa_searchqa-validation-631", "mrqa_searchqa-validation-6482", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7120", "mrqa_searchqa-validation-7443", "mrqa_searchqa-validation-8165", "mrqa_searchqa-validation-8323", "mrqa_searchqa-validation-9476", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9648", "mrqa_searchqa-validation-9840", "mrqa_searchqa-validation-9931", "mrqa_squad-validation-10062", "mrqa_squad-validation-1016", "mrqa_squad-validation-1189", "mrqa_squad-validation-1201", "mrqa_squad-validation-1291", "mrqa_squad-validation-1412", "mrqa_squad-validation-1454", "mrqa_squad-validation-163", "mrqa_squad-validation-1776", "mrqa_squad-validation-178", "mrqa_squad-validation-1893", "mrqa_squad-validation-2052", "mrqa_squad-validation-2087", "mrqa_squad-validation-2137", "mrqa_squad-validation-2144", "mrqa_squad-validation-2168", "mrqa_squad-validation-2429", "mrqa_squad-validation-2622", "mrqa_squad-validation-2780", "mrqa_squad-validation-2875", "mrqa_squad-validation-2903", "mrqa_squad-validation-2969", "mrqa_squad-validation-2972", "mrqa_squad-validation-3037", "mrqa_squad-validation-3043", "mrqa_squad-validation-3069", "mrqa_squad-validation-3162", "mrqa_squad-validation-3237", "mrqa_squad-validation-3390", "mrqa_squad-validation-3473", "mrqa_squad-validation-3687", "mrqa_squad-validation-3957", "mrqa_squad-validation-4044", "mrqa_squad-validation-4158", "mrqa_squad-validation-4178", "mrqa_squad-validation-4328", "mrqa_squad-validation-4437", "mrqa_squad-validation-446", "mrqa_squad-validation-4580", "mrqa_squad-validation-4590", "mrqa_squad-validation-4613", "mrqa_squad-validation-4708", "mrqa_squad-validation-4764", "mrqa_squad-validation-4773", "mrqa_squad-validation-479", "mrqa_squad-validation-4836", "mrqa_squad-validation-4890", "mrqa_squad-validation-4908", "mrqa_squad-validation-4927", "mrqa_squad-validation-5034", "mrqa_squad-validation-5067", "mrqa_squad-validation-5082", "mrqa_squad-validation-516", "mrqa_squad-validation-5437", "mrqa_squad-validation-5481", "mrqa_squad-validation-5498", "mrqa_squad-validation-55", "mrqa_squad-validation-5611", "mrqa_squad-validation-5725", "mrqa_squad-validation-5905", "mrqa_squad-validation-597", "mrqa_squad-validation-639", "mrqa_squad-validation-6403", "mrqa_squad-validation-6530", "mrqa_squad-validation-6655", "mrqa_squad-validation-6933", "mrqa_squad-validation-7141", "mrqa_squad-validation-7230", "mrqa_squad-validation-7230", "mrqa_squad-validation-7264", "mrqa_squad-validation-7284", "mrqa_squad-validation-7451", "mrqa_squad-validation-749", "mrqa_squad-validation-7872", "mrqa_squad-validation-7897", "mrqa_squad-validation-7949", "mrqa_squad-validation-8068", "mrqa_squad-validation-811", "mrqa_squad-validation-8136", "mrqa_squad-validation-8159", "mrqa_squad-validation-8182", "mrqa_squad-validation-8316", "mrqa_squad-validation-8435", "mrqa_squad-validation-8440", "mrqa_squad-validation-8447", "mrqa_squad-validation-8471", "mrqa_squad-validation-9162", "mrqa_squad-validation-9307", "mrqa_squad-validation-9653", "mrqa_squad-validation-9655", "mrqa_squad-validation-9703", "mrqa_squad-validation-9740", "mrqa_squad-validation-998", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1276", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-1442", "mrqa_triviaqa-validation-1463", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-1677", "mrqa_triviaqa-validation-1700", "mrqa_triviaqa-validation-1752", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1856", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2025", "mrqa_triviaqa-validation-2158", "mrqa_triviaqa-validation-2274", "mrqa_triviaqa-validation-2364", "mrqa_triviaqa-validation-2473", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2622", "mrqa_triviaqa-validation-2812", "mrqa_triviaqa-validation-2877", "mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-2977", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-324", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3420", "mrqa_triviaqa-validation-3479", "mrqa_triviaqa-validation-3500", "mrqa_triviaqa-validation-3592", "mrqa_triviaqa-validation-3597", "mrqa_triviaqa-validation-3600", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-3859", "mrqa_triviaqa-validation-3889", "mrqa_triviaqa-validation-3930", "mrqa_triviaqa-validation-4007", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-4080", "mrqa_triviaqa-validation-4100", "mrqa_triviaqa-validation-411", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4576", "mrqa_triviaqa-validation-4606", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-464", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5139", "mrqa_triviaqa-validation-516", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5277", "mrqa_triviaqa-validation-5299", "mrqa_triviaqa-validation-5326", "mrqa_triviaqa-validation-5343", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-5556", "mrqa_triviaqa-validation-5588", "mrqa_triviaqa-validation-5645", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-5677", "mrqa_triviaqa-validation-5678", "mrqa_triviaqa-validation-5711", "mrqa_triviaqa-validation-5730", "mrqa_triviaqa-validation-5836", "mrqa_triviaqa-validation-5865", "mrqa_triviaqa-validation-5866", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6252", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6392", "mrqa_triviaqa-validation-6423", "mrqa_triviaqa-validation-660", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-6881", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6917", "mrqa_triviaqa-validation-6953", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-6994", "mrqa_triviaqa-validation-7113", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7279", "mrqa_triviaqa-validation-7297", "mrqa_triviaqa-validation-7314", "mrqa_triviaqa-validation-735", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7447", "mrqa_triviaqa-validation-7552", "mrqa_triviaqa-validation-7554", "mrqa_triviaqa-validation-7638", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-7698", "mrqa_triviaqa-validation-7736", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-7778", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-838", "mrqa_triviaqa-validation-851", "mrqa_triviaqa-validation-879", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-989", "mrqa_triviaqa-validation-991"], "OKR": 0.81640625, "KG": 0.42890625, "before_eval_results": {"predictions": ["RAF Mount Pleasant", "University of Kansas", "the Gentle Don", "Walcha", "Michelle Anne Sinclair", "Adam Levine", "Tim Allen", "The Dressmaker", "Oakland, California", "37", "Tufts University", "Owsley Stanley", "The Late Late Show", "Kongo", "Harold Lipshitz", "Spanish", "\"Ted\" (2012), its sequel \"Ted 2\" (2015), and \"A Million Ways to Die in the West\" (2014)", "1945", "69.7 million", "Neneh Mariann Karlsson", "Sunyani West District in the Brong-Ahafo Region of Ghana", "artiodactyl mammal", "Comodoro Arturo Merino Ben\u00edtez International Airport, Santiago, Chile", "Scotty Grainger", "9", "a mid-door higher equipment content version of the Civic", "8,648", "Alfonso Cuar\u00f3n", "1858", "September 30, 2017", "1898", "Nicolas Winding Refn", "devotional literature", "Fortunino Francesco Verdi", "Laban Movement Analysis", "Cecily Strong", "J. Robert Oppenheimer", "invoicing the employees' work based on an hourly rate", "seasonal television specials, particularly its work in stop motion animation", "nearly 8 km", "1853", "Love", "The Supremes", "48,982", "Vincent Landay", "Lieutenant Martin \"Marty\" Castillo", "Pamela Chopra", "1901", "Pope John X", "Excellence in Production Design Award", "VAQ-135", "Doug Pruzan", "American country music group The Nitty Gritty Dirt Band", "English", "'Q'", "U.S. marshals", "a large jug or pitcher", "not for sale, the music label that owns them said Sunday,", "UNICEF", "9 a.m.,", "George Noel Gordon", "Van Helsing", "Aramaic", "a long-range missile"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5970965232683982}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false], "QA-F1": [0.8, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.125, 1.0, 0.8, 0.4, 0.5454545454545454, 0.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.1818181818181818, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-3514", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-5030", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-1652", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-400", "mrqa_hotpotqa-validation-4792", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-2957", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-2480", "mrqa_hotpotqa-validation-2058", "mrqa_naturalquestions-validation-34", "mrqa_triviaqa-validation-7487", "mrqa_triviaqa-validation-2358", "mrqa_newsqa-validation-2096", "mrqa_searchqa-validation-4962", "mrqa_searchqa-validation-14277", "mrqa_newsqa-validation-1661"], "SR": 0.4375, "CSR": 0.5337009803921569, "EFR": 1.0, "Overall": 0.6936933210784313}, {"timecode": 51, "before_eval_results": {"predictions": ["Ford Field in Detroit, Michigan", "comedy", "Mickey's Christmas Carol", "143,007", "\u00c7ankaya Campus", "\"Realty Bites\"", "24 NCAA sports", "Razor Ramon", "Morita therapy", "Forbes", "St. George, Maine", "Kramer Guitars", "Lithuanian", "International Boxing Hall of Fame", "35", "Conservatorio Verdi", "Benny", "Smoothie King Center", "World Outgames", "Homebrewing", "Umberto II", "Presbyterian Church", "neuro-orthopaedic Irish veterinary surgeon", "Hookend Recording Studios", "North Sea", "2006", "67,575", "England", "OSRIC", "Emad Hashim", "5320 km", "Heinkel Flugzeugwerke", "English", "Eric Edward Whitacre", "the largest Mission Revival Style building in the United States", "180", "George Adamski", "\"Hand of Thrawn\"", "Switzerland", "an ancient Celtic ringfort", "Scunthorpe", "Canadian comedian", "Captain Cook's Landing Place", "Jeux olympiques d'\u00e9t\u00e9", "1862", "1970", "Royal Albert Hall", "Budget Rent a Car", "Japan", "lion", "1959", "Donna Mills", "a pop and R&B ballad, with Latin pop influences", "735 feet", "Maine", "Crawford's and Bette Davis's Hudson sisters Blanche and Baby Jane", "maxilla", "Intel", "4.6 million", "Iran,", "tea rose", "five", "Kahlua", "Rear Window"], "metric_results": {"EM": 0.5, "QA-F1": 0.6023634785353535}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.18181818181818182, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.125, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.22222222222222224, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-1260", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1207", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-1576", "mrqa_hotpotqa-validation-2018", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-1219", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-4460", "mrqa_hotpotqa-validation-4820", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-580", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-4448", "mrqa_hotpotqa-validation-3576", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-1476", "mrqa_naturalquestions-validation-7250", "mrqa_triviaqa-validation-84", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-4083", "mrqa_searchqa-validation-10653", "mrqa_searchqa-validation-3934"], "SR": 0.5, "CSR": 0.5330528846153846, "EFR": 1.0, "Overall": 0.6935637019230769}, {"timecode": 52, "before_eval_results": {"predictions": ["September 19, 2017", "Billy Idol", "influencing a federal election, and registers with the Federal Election Commission", "the Romans kept track of who was eligible to vote", "3 lines of reflection and rotational symmetry of order 3 about its center", "up to 100,000", "Sachin Tendulkar and Kumar Sangakkara", "Judiththia Aline Keppel", "Camping World Stadium in Orlando", "Windows Media Video ( WMV )", "the Ramones", "Albert Finney", "a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Gospel of Matthew in the middle of the Sermon on the Mount, and the short form in the Gospel of Luke", "30 years after Return of the Kenobi", "a leonine contract, a take - it - or - leave - it contract, or a boilerplate contract", "Diego Tinoco", "sport utility vehicles and off - road vehicles", "Melbourne was the initial capital following the 1901 Federation of Australia", "The Drew Las Vegas", "October 2008", "John Hancock", "1960", "the British military launched a campaign to capture the Colony of Canada ( part of New France )", "The following year", "in the U.S. states of Oregon and Washington", "Magnetically soft ( low coercivity ) iron is used for the cores in electromagnets", "2013", "malicious software", "Cyndi Grecco", "differs in ingredients", "1900", "dorsally on the forearm", "carnivore", "Terry Kath", "roofing", "one person", "The Parlement de Bretagne", "Microsoft Windows", "declared state laws establishing separate public schools for black and white students to be unconstitutional", "1986", "late - September through early January", "currency option", "first published in the First Folio in 1623", "a normally inaccessible mini-game in the 2004 video game Grand Theft Auto : San Andreas, developed by Rockstar North", "the medulla oblongata that, together with the cardiovascular center and respiratory center, regulates blood pressure and other homeostatic processes", "2018", "Gibraltar, a British Overseas Territory, located at the southern tip of the Iberian Peninsula, is the subject of an irredentist territorial claim by Spain", "Howard Caine", "May 3, 2005", "Andy Cole", "the first world war", "Steptoe and Son (1962\u201374), about two rag and bone men, father and son, who live together in a squalid house in West London", "Paul Maskey", "child actor or child actress", "Saoirse Ronan", "Revolution Studios and Happy Madison Productions", "The Kirchners", "north London rivals Tottenham have suffered a setback with a serious injury to Croatia playmaker", "Gordon Brown", "veterans", "yellow fever", "winter solstice", "Netflix"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5602536211853162}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.07142857142857142, 0.0, 0.14285714285714288, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3157894736842105, 0.8333333333333334, 0.3137254901960785, 1.0, 0.6, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.8571428571428571, 0.21052631578947367, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.24000000000000002, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-961", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-49", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-6621", "mrqa_naturalquestions-validation-802", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-2282", "mrqa_naturalquestions-validation-6805", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-10706", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-7336", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-4572", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-8414", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-3959", "mrqa_triviaqa-validation-1896", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-4852", "mrqa_hotpotqa-validation-3162", "mrqa_hotpotqa-validation-2688", "mrqa_newsqa-validation-318", "mrqa_searchqa-validation-4527"], "SR": 0.453125, "CSR": 0.5315448113207547, "EFR": 0.9142857142857143, "Overall": 0.6761192301212937}, {"timecode": 53, "before_eval_results": {"predictions": ["Al Lenhardt", "cosmology", "Handel", "Black Acres", "12", "Snickers", "the Doppler shift", "Lobster Newberg", "Shel Silverstein", "American Airlines", "James Russell Lowell", "Vermont", "Calvin Coolidge", "the Institute of Cybernetics", "salmon", "pudd'nhead Wilson", "Osaka", "tapir", "France", "canned precooked meat", "Stephen Dedalus", "early", "Camembert", "Friday", "the Union Flag", "a centaur", "Mentor", "Hariri tribunal", "Manifest Destiny", "Frank Cardelle", "the Americans with Disabilities Act", "China", "Bruce Springsteen", "Korea", "Glucosamine", "Madagascar", "Google scholar", "celebration", "astrachan", "Susan Faludi", "Dr. Dre", "Al Lang Stadium", "Fidel Castro", "fudge", "heffalump rabbit", "Laborers' International Union", "goldfish", "hormones", "a dive in which the diver bends in midair to touch the toe, keeping the legs straight, and then straightens out", "yellowtail", "watermelon", "between the Mediterranean Sea to the north and the Red Sea in the south", "London ( summer ) and Cortina d'Ampezzo, Italy ( winter ) in 1944", "Zeus", "van Morrison", "antelopes", "manufacturer, distributor, and marketer of non-alcoholic beverage concentrates", "Rocky Mountain Institute", "21", "a cobblestone-size (10x10 cm ) concrete cube bearing a brass plate inscribed with the name and life dates of victims of Nazi extermination or persecution", "Three thousand", "insurgent small arms fire.", "3,000 kilometers (1,900 miles), possibly putting U.S. military bases in the Pacific Ocean territory of Guam within striking distance,", "Lambic"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4812026515151515}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.9, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-3854", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-7512", "mrqa_searchqa-validation-15120", "mrqa_searchqa-validation-9715", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-15543", "mrqa_searchqa-validation-7691", "mrqa_searchqa-validation-9341", "mrqa_searchqa-validation-1130", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-7145", "mrqa_searchqa-validation-14860", "mrqa_searchqa-validation-7425", "mrqa_searchqa-validation-14168", "mrqa_searchqa-validation-1451", "mrqa_searchqa-validation-12711", "mrqa_searchqa-validation-1007", "mrqa_searchqa-validation-5107", "mrqa_searchqa-validation-14085", "mrqa_searchqa-validation-1108", "mrqa_searchqa-validation-8821", "mrqa_searchqa-validation-5611", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-11345", "mrqa_searchqa-validation-13258", "mrqa_searchqa-validation-2314", "mrqa_searchqa-validation-10587", "mrqa_searchqa-validation-4481", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-12755", "mrqa_searchqa-validation-6093", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-4242", "mrqa_triviaqa-validation-7747", "mrqa_hotpotqa-validation-3325", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3349"], "SR": 0.40625, "CSR": 0.529224537037037, "EFR": 0.9736842105263158, "Overall": 0.6875348745126705}, {"timecode": 54, "before_eval_results": {"predictions": ["Keeley Clare Julia Hawes", "1837", "Trace Adkins", "Dan Stevens", "the state legislators of Assam", "a travelling circus", "the stems and roots of certain vascular plants", "Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "1987", "John F. Kennedy", "in teaching elocution", "on the Isle of FERNANDO 'S!, a fictional location based in Puerto de la Cruz, Tenerife", "seven", "1999", "Rashida Jones", "Hook", "Bush", "tissues in the vicinity of the nose", "December 1, 2009", "the eighth episode in the ninth season of the American animated television series South Park", "70 million", "a brownstone in Brooklyn Heights, New York, at 10 Stigwood Avenue", "Haji Sahib of Turangzai", "45 %", "handheld subscriber equipment", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "1998", "Will Champion", "the Ming dynasty", "the red - bed country of its watershed", "Thomas Jefferson", "The Intolerable Acts", "National Industrial Recovery Act", "semi-autonomous organisational units within the National Health Service in England", "Peter Billingsley", "Cyanea capillata", "1996", "Hyderabad", "Johannes Gutenberg", "1885", "1964 Republican National Convention in San Francisco, California", "Blackstone's Commentaries, a book given to her", "Somatic", "Pyeongchang County, Gangwon Province, South Korea", "New York City", "two - year terms", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "invoices", "Tenochtitlan", "Ravi River", "O'Meara", "the Philippines", "julian tandy", "the Greek Goddess of Revenge", "Marine Corps Air Station Kaneohe Bay", "PlayStation 4", "2015 Orange Bowl", "Seminole Tribe", "the Defense of Marriage Act", "Alinghi", "Eiffel Tower", "barnacles", "Lobo", "gerry adaption of Angels & Demons"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6449770554308093}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.4615384615384615, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5882352941176471, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 1.0, 0.8, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-556", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-5469", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-3132", "mrqa_naturalquestions-validation-9002", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-451", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5925", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-892", "mrqa_hotpotqa-validation-4645", "mrqa_newsqa-validation-3493", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-2466", "mrqa_searchqa-validation-11506", "mrqa_triviaqa-validation-7361"], "SR": 0.515625, "CSR": 0.5289772727272728, "EFR": 1.0, "Overall": 0.6927485795454545}, {"timecode": 55, "before_eval_results": {"predictions": ["king joust", "golf", "purple", "aeoline", "ascot", "Lithuanian Litas", "Loretta Lynn", "November 1985's The Wrestling Classic", "steppenwolf", "chop suey", "Ross mcManus", "Coronation Street", "sierra one", "evevraj smashes six sixes in one over", "mad h Saddam", "New Zealand", "capri", "bobby sand", "mauritania ( ';  or ; ; soninke: Murutaane; Pulaar: Moritani; ), officially the Islamic republic of Mauritania", "han lippershey", "Bolivia", "bob Giraldi", "Mozambique Channel", "ash", "king Edward VII", "Thomas Cranmer", "testicles", "trogon", "bacall", "caroline aherne", "winton", "s\u00e8vres", "simi knife", "kipps", "myrrh", "Serena Williams", "Lome", "Pegida", "nibelung", "Utrecht", "1709", "Mitford sisters", "kansas", "capone", "vineyards", "baltimore", "tswalu Kalahari", "hugh quarshie", "a stern tube", "batgirl", "Beijing", "Kimberlin Brown", "seven", "Kid Creole and the Coconuts", "Jack Ridley", "Linux Format", "Stage Stores", "26", "immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanamo Bay, Cuba.", "Beijing", "Kool- aid", "Treaty of Versailles", "Ken Russell", "Russia and China"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5084821428571429}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true], "QA-F1": [0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.5, 0.14285714285714288, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1825", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-4775", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3596", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5479", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-3454", "mrqa_triviaqa-validation-597", "mrqa_triviaqa-validation-2148", "mrqa_triviaqa-validation-5428", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6323", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-5930", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-4642", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-2488", "mrqa_searchqa-validation-4261"], "SR": 0.46875, "CSR": 0.5279017857142857, "EFR": 0.9411764705882353, "Overall": 0.6807687762605041}, {"timecode": 56, "before_eval_results": {"predictions": ["ireland", "Bolivia", "The Telegraph", "liver", "portugal", "Drunk Crosswords", "Galway", "Aldo Moro", "calcium carbonate", "Salman Rushdie", "george elosevelt", "some 1500 km east of Australia across the Tasman Sea", "fast food", "benazir butto", "bowler", "Sam Mendes", "john steed", "way back Attack", "1994", "business", "henry byron", "david byron", "guatamala", "Towy", "flat", "1984", "Swansea", "three", "shinto", "Sussex", "king elvis iv", "Mickey Mouse", "oxygen", "son", "tokyo", "come quietly", "Dodoma", "radiohead", "Wilson", "Loch lomond", "pyrenees", "south Korea", "gelatine", "guatamala", "gulf of Aden", "Yorkshire", "a\u00e9roport de Gaulle", "sankt Moritz", "the French Revolution", "the Old Kent Road", "on your way in one of two ways, cremation or inhumation", "An acetate / \u02c8\u00e6s\u026ate\u026at / is a salt formed by the combination of acetic acid with an alkaline, earthy, or metallic base", "iron", "President since creation of the office in 1789", "Christopher Whitelaw Pine", "Yorgos Lanthimos", "Tottenham ( ) or Spurs", "Unseeded", "off Somalia's coast.", "Shanghai", "capone", "Pershing", "governess", "a Maine politician"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5098958333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_triviaqa-validation-3680", "mrqa_triviaqa-validation-3016", "mrqa_triviaqa-validation-4296", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-3412", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-5432", "mrqa_triviaqa-validation-3116", "mrqa_triviaqa-validation-1645", "mrqa_triviaqa-validation-745", "mrqa_triviaqa-validation-3487", "mrqa_triviaqa-validation-4256", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-1466", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-5679", "mrqa_triviaqa-validation-3148", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-2943", "mrqa_triviaqa-validation-5888", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-6420", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-6236", "mrqa_triviaqa-validation-6376", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-3964", "mrqa_triviaqa-validation-361", "mrqa_triviaqa-validation-3828", "mrqa_naturalquestions-validation-1202", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-4370", "mrqa_hotpotqa-validation-3603", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-1022", "mrqa_searchqa-validation-10590", "mrqa_hotpotqa-validation-4052"], "SR": 0.421875, "CSR": 0.5260416666666667, "EFR": 1.0, "Overall": 0.6921614583333333}, {"timecode": 57, "before_eval_results": {"predictions": ["William Shakespeare", "jimmy smith", "micelles", "river Lee", "Rudolf nureyev", "Jessica", "placebo", "weather", "Lake Placid", "papal state", "contractions", "William Boyd", "Saint Cecilia", "Madison Keys", "morecambe & Wise", "tommy lee jones", "actor", "cowpox", "deer hunting", "Stockholm", "France", "so far Away", "sense of smell", "remains into space using conventional rockets", "chemnitz", "rue", "yellow", "sea raven", "Caracas", "ennio morricone", "British", "mexico", "time team", "Turandot", "mexico", "mountain peak", "darthur", "Howard Keel", "marriage", "boutros Ghali", "mexico", "Sinclair Lewis", "mexico", "garden of gethsemane", "decision tree", "2", "Sunday Times", "mexico", "Kristiania", "keirin", "selene", "vehicles designed for off - road use are known as `` four - wheel drives '', `` 4WDs '', or `` 4 \u00d7 4s ''", "Noel Kahn", "Tbilisi", "Las Vegas", "2006", "number five", "natural gas", "Michael Jackson was in good health, contrary to media reports he was diagnosed with skin cancer.", "Shiza Shahid", "Perseid", "accordion", "bones", "Marky Mark"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6159722222222223}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.1, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-404", "mrqa_triviaqa-validation-5768", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-5969", "mrqa_triviaqa-validation-7431", "mrqa_triviaqa-validation-1020", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-2804", "mrqa_triviaqa-validation-7268", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-3054", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-2091", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-5363", "mrqa_triviaqa-validation-7300", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-812", "mrqa_triviaqa-validation-4499", "mrqa_triviaqa-validation-2116", "mrqa_triviaqa-validation-4425", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-6078", "mrqa_naturalquestions-validation-10066", "mrqa_hotpotqa-validation-5219", "mrqa_hotpotqa-validation-794", "mrqa_newsqa-validation-3655", "mrqa_searchqa-validation-16209"], "SR": 0.53125, "CSR": 0.5261314655172413, "EFR": 0.9666666666666667, "Overall": 0.6855127514367816}, {"timecode": 58, "before_eval_results": {"predictions": ["Frottage", "Jonah", "The Color Purple", "Constantinople", "Jacqueline Susann", "Brazil", "Hudson", "hematopoietic", "New Years Day (Jan 1st)", "Sons of Liberty", "Napoleon Bonaparte", "Cecil Rhodes", "Hindenburg", "atrium", "Valley Forge", "gold", "get absorbed into the woods,", "Siberia", "William Pitt the Younger", "five", "Friday the 13th", "Manchester", "Godfather", "wrinkles", "Nostradamus", "jihad", "harpoons", "Mandy", "financial services", "Conrad Hilton Jr.", "Jasper Johns", "plutonium", "cyanotype", "Zimbabwe", "Battle of Trafalgar", "bald eagle", "meatball", "Panax", "hurricanes", "Home Improvement", "Kashmir", "Airport", "Nu", "new orleans", "a tragedy", "Little Mermaid", "Grant", "lethal", "Pell grants", "emerald", "an dome", "19 July 1990", "anvil", "Louis XV", "germany", "Mansion House", "spainal", "London", "Comme des Gar\u00e7ons", "College Green", "Dutch", "Nasser Medical Institute in Cairo", "Auckland", "getting bald"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5934895833333333}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.7499999999999999, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1815", "mrqa_searchqa-validation-4370", "mrqa_searchqa-validation-13207", "mrqa_searchqa-validation-6293", "mrqa_searchqa-validation-11218", "mrqa_searchqa-validation-12288", "mrqa_searchqa-validation-15988", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-4925", "mrqa_searchqa-validation-14549", "mrqa_searchqa-validation-9097", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-16444", "mrqa_searchqa-validation-12173", "mrqa_searchqa-validation-6389", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-1195", "mrqa_searchqa-validation-4488", "mrqa_searchqa-validation-7736", "mrqa_searchqa-validation-1938", "mrqa_searchqa-validation-6110", "mrqa_naturalquestions-validation-1731", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-737", "mrqa_hotpotqa-validation-3894", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-1225", "mrqa_triviaqa-validation-2596"], "SR": 0.53125, "CSR": 0.5262182203389831, "EFR": 0.9666666666666667, "Overall": 0.68553010240113}, {"timecode": 59, "before_eval_results": {"predictions": ["Alabama", "TriviaBistro.com", "the distillation process", "Leonard Bernstein", "magnesium", "Scarpanto", "the Danube", "the albatross", "Seinfeld", "The Smashing Pumpkins", "the context of the sentence", "Ohio State", "William Tecumseh Sherman", "Pakistan", "Theology of God", "Ireland", "Sally Field", "Barbara Cartland", "Rum", "a Pringles can", "Paul Hamm", "a cantante", "East Siberia", "the candle season", "Tom Hanks", "Clue", "House Busters", "(Bron)", "alternating current", "Walter Cronkite", "Robert Burns", "the Bicentennial Man", "Marshall", "General Motors", "the trade winds", "the United Nations", "silk", "W", "a Unicorn", "Scrabble", "humerus", "The Bodyguard", "animal", "the Philippines", "a fungus", "Che Guevara", "Yale", "Oscar Wilde", "(Briseis)", "Dian Fossey", "a map", "an iron -- nickel alloy and some other elements", "ABC", "a legal case in certain legal systems", "Pear", "Melbourne", "jockey", "Ringo Starr", "Do Kyung-soo", "Hanna", "an acid attack by a spurned suitor.", "North Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "\"all the world's largest producers of greenhouse gas emissions, including developed and developing nations,\" to come together and \"set a long-term goal for reducing\" greenhouse emissions.", "Priam"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6406994047619048}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6548", "mrqa_searchqa-validation-14429", "mrqa_searchqa-validation-2000", "mrqa_searchqa-validation-4274", "mrqa_searchqa-validation-8167", "mrqa_searchqa-validation-2384", "mrqa_searchqa-validation-15083", "mrqa_searchqa-validation-3853", "mrqa_searchqa-validation-10144", "mrqa_searchqa-validation-2557", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-15665", "mrqa_searchqa-validation-9632", "mrqa_searchqa-validation-10544", "mrqa_searchqa-validation-2948", "mrqa_searchqa-validation-16962", "mrqa_searchqa-validation-14151", "mrqa_searchqa-validation-13514", "mrqa_searchqa-validation-5964", "mrqa_searchqa-validation-10610", "mrqa_naturalquestions-validation-5251", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-186", "mrqa_triviaqa-validation-2582", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2745"], "SR": 0.578125, "CSR": 0.5270833333333333, "EFR": 1.0, "Overall": 0.6923697916666667}, {"timecode": 60, "before_eval_results": {"predictions": ["26 November", "50 million", "Pacific Place", "1002", "Anandji Virji Shah", "Lu\u00eds Carlos Almeida da Cunha", "four", "Sippin' on Some Syrup", "Cote", "Maersk Mc- Kinney", "Cressida Chrishell Stause", "the University of Kentucky", "five", "Charlie Wilson", "Sim Theme Park", "the first front-wheel drive vehicle from Volvo", "its riverside location", "1859", "Julie Taymor", "actor", "James Edward Kelly", "Spanish", "Indiana University", "lower Manhattan", "Homeland", "2016", "Virgin Atlantic", "green and yellow", "Champion Jockey", "March 2012", "Tom Courtenay", "Erinsborough", "2015", "Vladimir Menshov", "Cleopatra", "Londonderry", "York County", "National Basketball Development League", "Father Dougal McGuire", "Bill Curry", "UFC Fight Pass", "1949", "Savannah River Plant", "\u00c6thelwald Moll", "God and the just cause", "Swiss", "Tiberius", "World War I", "at age 27", "Clayton Mark's planned worker community in Northwest Indiana", "five", "Rodney Crowell", "Mendel", "near major hotels and in the parking areas of major Chinese supermarkets", "a major scale", "Frankfurt", "Apollo", "Anil Kapoor", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "Arthur E. Morgan III,", "quarantina", "Silk Road", "ABBA", "Spain"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6943309294871796}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, false, false, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.15384615384615383, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-1788", "mrqa_hotpotqa-validation-5741", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-4817", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-3100", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-2022", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-871", "mrqa_hotpotqa-validation-1812", "mrqa_naturalquestions-validation-6949", "mrqa_triviaqa-validation-2692", "mrqa_triviaqa-validation-6034", "mrqa_newsqa-validation-867", "mrqa_searchqa-validation-14894", "mrqa_naturalquestions-validation-6011"], "SR": 0.609375, "CSR": 0.5284323770491803, "EFR": 0.96, "Overall": 0.6846396004098361}, {"timecode": 61, "before_eval_results": {"predictions": ["Gaels", "Chicago Bears", "girls", "Taylor Swift", "Adolfo Rodr\u00edguez Sa\u00e1", "Freeform", "Adult Swim", "2003\u20132008", "Rio Ferdinand", "247,597", "2,664", "841", "Cher", "Australian Broadcasting Corporation (ABC)", "MG Cars", "Walt Disney", "1979", "15", "January 23, 1898", "John W. Henry", "Bolton", "Argentinian", "Them", "575 acres (2.08 km\u00b2)", "John Snow", "The New York and New Jersey campaign", "2013\u201314", "Melbourne Storm", "UNLV", "21", "Paradzhanov", "Friday", "Oklahoma Sooners", "2002\u201303", "7pm", "1866", "Gaahl", "Lega Serie B", "1887", "Sojourner Truth", "RAF Tangmere, West Sussex", "North Holland", "Don Bluth", "Golden Calf", "The Lykan Hypersport", "Khalifa International Stadium", "Agatha Christie", "Mercer", "1951", "35,124", "154 days", "September 30", "James P. Flynn", "the Soviet Union", "his finger", "Ronald Wilson Reagan", "Bacofoil", "Long troop deployments in Iraq, above, and Afghanistan", "fuel economy and safety while boosts the economy.", "forcibly drugging", "James Watt", "Michelangelo", "Anastasia", "Games"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5780877976190476}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.5714285714285715, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.1, 0.25, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5336", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-1187", "mrqa_hotpotqa-validation-3573", "mrqa_hotpotqa-validation-4916", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-5797", "mrqa_hotpotqa-validation-3044", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-5250", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-541", "mrqa_hotpotqa-validation-3306", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1453", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-5832", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-826", "mrqa_naturalquestions-validation-9687", "mrqa_triviaqa-validation-7611", "mrqa_triviaqa-validation-6998", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-129", "mrqa_searchqa-validation-11699"], "SR": 0.453125, "CSR": 0.5272177419354839, "EFR": 1.0, "Overall": 0.6923966733870968}, {"timecode": 62, "before_eval_results": {"predictions": ["Paris", "13 October 1958", "Walt Disney and Ub Iwerks", "pressure-sensitive film products", "Babylon", "a card (or cards) during a card game", "water sprite", "Sean Yseult", "law firm", "1937", "Hillsborough County", "Charles Nungesser", "Burning Man", "Love Streams", "King George VI", "August 10, 1933", "Dallas", "Black Panthers", "globetrotters", "Francis Schaeffer", "Washington, DC", "Simon Bolivar Buckner", "German", "Gareth Jones", "consulting", "April", "1978", "actor, producer, and director", "Melanie Owen", "1983", "Indian state of Gujarat", "143,007", "May 4, 1924", "American jewelry designer", "Axl Rose", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "The Big Bang Theory", "The Mountbatten family", "dice", "Kal Ho Naa Ho", "Dungeness", "Pendlebury, Lancashire", "25 October 1921", "Canadian comedian", "David O'Leary", "HackThis Site", "Reginald Engelbach", "American", "Black Friday", "Minneapolis", "Jean Erdman", "5.7 million customer accounts", "During Hanna's recovery masquerade celebration", "Anakin Skywalker", "Richard Seddon", "the heart", "Sitka, Alaska", "to take the Rio Group to a new level by creating the organization.", "\"Now that we know Muhammad is an Ennis man, and yet today it was beautiful. The rain held off wherever Muhammad Ali went,\"", "Bob Bogle", "circumference", "The Hague", "a stationwagon", "2001"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6462594696969697}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1897", "mrqa_hotpotqa-validation-2286", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-2419", "mrqa_hotpotqa-validation-4162", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-2535", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-1374", "mrqa_hotpotqa-validation-197", "mrqa_naturalquestions-validation-9222", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-7459", "mrqa_newsqa-validation-3319", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-12404"], "SR": 0.578125, "CSR": 0.5280257936507937, "EFR": 0.9629629629629629, "Overall": 0.6851508763227513}, {"timecode": 63, "before_eval_results": {"predictions": ["a torpedo", "two weevils", "The pound of flesh", "the data set", "the beluga", "Nicholas II", "tuna", "Shalom", "Russia", "a chimp", "The Larry Sanders Show", "The Buenos Aires Herald", "Thor", "Saint Albans", "astride", "Borneo", "Versailles", "a cereal", "Raleigh", "whipped cream", "The Atlantic bluefin tuna", "Macbeth", "Jean-Michel Basquiat", "The 40 Greatest Led Zeppelin", "War and Peace", "the largest city in South Korea", "Bruce Willis", "the outskirts of a small Southern town", "Columbo", "John Tyler", "Milwaukee", "sin", "The People v.", "sake", "Notre Dame", "Portland", "Charles-Franois de Broglie", "The Indianapolis 500", "Toy Story", "improv", "Charles Askegardshe", "Latvian", "Nikolai Gogol", "David Hare", "Fletcher Christian", "weaving", "John Paul II", "Greenland", "in sanctification", "The Marx Brothers", "watermelon", "Phillip Schofield and Christine Bleakley", "the duodenum", "Reverend J. Long", "violin", "to the sexual imagination", "the second peak", "Garrett Morris", "third", "12 mi east-southeast of Bridgeport", "Susan Atkins,", "almost 9 million", "nearly 2,000", "Iraqi Prime Minister Nouri al-Maliki"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6151413690476191}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-513", "mrqa_searchqa-validation-4522", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-1267", "mrqa_searchqa-validation-10557", "mrqa_searchqa-validation-5508", "mrqa_searchqa-validation-8531", "mrqa_searchqa-validation-5195", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-6528", "mrqa_searchqa-validation-4458", "mrqa_searchqa-validation-14769", "mrqa_searchqa-validation-724", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-11061", "mrqa_searchqa-validation-6694", "mrqa_searchqa-validation-2412", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-15871", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-5497", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-4356", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-667", "mrqa_newsqa-validation-955"], "SR": 0.5625, "CSR": 0.528564453125, "EFR": 1.0, "Overall": 0.692666015625}, {"timecode": 64, "before_eval_results": {"predictions": ["some of the most gigantic pumpkins in the world,", "Seminole", "billions of dollars", "\"green-card warriors\"", "228", "a traditional form of lounge music", "2005", "contaminated groundwater,", "consumer confidence", "Second seed Fernando Gonzalez", "in the southern port city of Karachi,", "The pilot,", "Jason Chaffetz", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "the Somali coast", "Obama", "Sunday,", "the Islamic militant group Abu Sayyaf,", "France", "380,000 pounds", "be silent.", "iTunes", "Spanish workers from the medical charity Doctors Without Borders", "\"gotten the balance right\"", "a dozen", "10-person", "\"Quiet Nights,\"", "his death cast a shadow over festivities", "social media networks", "123 pounds of cocaine and 4.5 pounds of heroin", "engineering and construction", "killed somebody.", "fractured pelvis and sacrum", "five", "to step up.", "murder", "Russian", "Mashhad", "summer", "two", "Bryant Purvis", "Tripplehorn", "al Qaeda,", "Garth Brooks", "in Oxbow,", "an Iranian court", "different women coping with breast cancer", "Schumacher", "Lula da Silva", "release of the four men", "2006", "12.9 - kilometre", "Tim Passmore", "Theodosius I", "BHS Philip Green", "Estonia", "is our children learning", "Princess Anne", "Sergeant First Class", "Champion Jockey", "Frederic Remington", "Ptolemy", "Woodrow Wilson", "the middle"], "metric_results": {"EM": 0.5, "QA-F1": 0.632140429015429}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.9090909090909091, 1.0, 0.0, 0.2222222222222222, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1764", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-1367", "mrqa_newsqa-validation-1101", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-2081", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-4082", "mrqa_newsqa-validation-2197", "mrqa_newsqa-validation-429", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-1389", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-2545", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-1147", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-616", "mrqa_hotpotqa-validation-1077", "mrqa_searchqa-validation-3653", "mrqa_searchqa-validation-709"], "SR": 0.5, "CSR": 0.528125, "EFR": 0.96875, "Overall": 0.686328125}, {"timecode": 65, "before_eval_results": {"predictions": ["304,000", "he never robbed the average guy,", "North Korea", "February 12", "Mandi Hamlin", "United Nations", "\"falling space debris,\"", "located outside the military recruiting center.", "voluntary manslaughter", "immediately put Morgan on a helicopter to Rainbow Babies and Children's Hospital in Cleveland,", "Kate Hudson's ex, Black Crowes rocker Chris Robinson,", "Grease", "\"black box\"", "34", "E. coli bacteria", "More than 15,000", "against meat consumption by covering themselves in fake blood and lying in human-sized meat packages.", "\"The Sopranos,\"", "government", "September,", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "South Africa", "Dick Cheney,", "The apartment building collapsed together with two other buildings on March 3.", "Mary Procidano,", "against using injectable vitamin supplements because the quantities are not regulated.", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "around 1610,", "Gulf of Aden,", "President Obama", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "Caylee Anthony,", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "in five days.", "managing his time.", "it does not grant full health-care coverage,", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.\"", "us to step up.\"", "he acted in self defense in punching businessman Marcus McGhee.", "education about rainforests.", "Three", "drug cartels", "state", "Trevor Rees-Jones,", "at least 28 passengers,", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "Ed McMahon,", "London Heathrow's Terminal 5.", "creation of an Islamic emirate in Gaza,", "Prince George's County Correctional Center,", "Genocide Prevention Task Force.", "the longest rotation period ( 243 days ) of any planet in the Solar System and rotates in the opposite direction to most other planets ( meaning the Sun would rise in the west and set in the east )", "Kirstjen Nielsen", "August 5, 1937", "20", "Wisconsin", "Mr Loophole", "Arlo Looking Cloud", "Queenston", "1694", "the Golden Fleece", "Gustav", "Amish", "6teen"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6084290983139666}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.28571428571428575, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2727272727272727, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.18181818181818182, 1.0, 0.10526315789473685, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0606060606060606, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2056", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-1748", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-2262", "mrqa_newsqa-validation-545", "mrqa_newsqa-validation-1801", "mrqa_newsqa-validation-1822", "mrqa_newsqa-validation-3127", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-1340", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-34", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-242", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-681", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-2735", "mrqa_naturalquestions-validation-9728", "mrqa_triviaqa-validation-1217", "mrqa_hotpotqa-validation-4692"], "SR": 0.53125, "CSR": 0.5281723484848485, "EFR": 1.0, "Overall": 0.6925875946969697}, {"timecode": 66, "before_eval_results": {"predictions": ["Jeffery Deaver", "sonar", "Pete Best", "Robert Taylor", "vincent van Gogh", "Spain", "about a mile north of the village of Dunvegan", "ArcelorMittal Orbit", "\"lodges\")", "Stilwell", "Tallinn", "the solar system", "coelacanth", "Belgium", "Dennis Potter", "Calcium", "Eric Coates", "Geoffrey Cox", "Mel Brooks", "the California condor", "york", "a turbomachine", "Sid James", "The Bill", "0", "Hamlet", "Johannesburg", "crackerjack pencil", "Bleak House", "carousel", "Spain", "minder", "mustard", "Les Dennis", "kansas city", "book 1: Sowing", "Tuscany", "18 meters", "Singapore", "Archie", "Pakistan International Airlines", "gold, red, blue, black and white", "France", "Tomorrow Never Dies", "jaundice", "hk", "Chuck Yeager", "violet- Elizabeth Bott", "Canada", "stamp collecting", "moby Dick", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "12.9 - kilometre ( 8 mi )", "a sociological perspective which developed around the middle of the twentieth century and that continues to be influential in some areas of the discipline", "21 July 2015", "Bern", "28 June 1945", "\"foreign Terrorist Organization\"", "25 years", "Pakistan's", "Yves Saint Laurent", "Rush", "Yogi Berra", "his business dealings for possible securities violations"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6563988095238096}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.09523809523809525, 1.0, 0.09523809523809523, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6]}}, "before_error_ids": ["mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-89", "mrqa_triviaqa-validation-1954", "mrqa_triviaqa-validation-6641", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-7211", "mrqa_triviaqa-validation-4722", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-7335", "mrqa_triviaqa-validation-3366", "mrqa_triviaqa-validation-1294", "mrqa_triviaqa-validation-3663", "mrqa_triviaqa-validation-7066", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-4003", "mrqa_triviaqa-validation-613", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-1450", "mrqa_hotpotqa-validation-4788", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-2374", "mrqa_searchqa-validation-7017", "mrqa_newsqa-validation-2682"], "SR": 0.609375, "CSR": 0.5293843283582089, "EFR": 1.0, "Overall": 0.6928299906716417}, {"timecode": 67, "before_eval_results": {"predictions": ["to convert single - stranded genomic RNA into double - stranded cDNA", "United States Department of the Interior", "Peter Brooks", "August 17, 1945", "after obtaining the consent of the United Kingdom", "a cake", "Olivia Olson", "Beijing", "Pyeongchang County, South Korea", "480", "April 7, 2016", "5.7 million customer accounts", "Wembley Stadium", "the President", "David Joseph Madden", "The Fixx", "at night", "Jack Nicklaus", "Plank", "Executive Residence of the White House Complex", "United Kingdom", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "the benefits of the US Privacy Act", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "originally known as Misi\u00f3n San Antonio de Valero", "Mariah Carey", "Spektor", "H CO", "seven", "September of that year", "American composer Jerome Kern and lyricist Otto Harbach", "October 2017", "Krypton", "November 25, 2002", "IBM", "Chernobyl Nuclear Power Plant", "435", "sport utility vehicles", "Kanawha Rivers", "The Bellamy Brothers", "John Wesley ( 1703 -- 1791 ) and his younger brother Charles ( 1707 -- 1788 )", "Massachusetts", "the plane crash", "Sir Mix - a-Lot", "June 1991", "July 8, 1997", "New Guinea", "Frank Oz", "Flag Day in 1954", "2010", "Missouri River", "\u00c6thelstan", "lute", "Bulgaria", "John Churchill,", "Gregg Popovich", "Asiana Town", "Araceli Valencia,", "people", "16 Indiana National Guard soldiers", "bees", "Jefferson Davis", "Farsi (Persian)", "synecdoche"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6451075131009341}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false], "QA-F1": [0.5263157894736842, 0.0, 0.0, 0.3333333333333333, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 0.7368421052631579, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 0.08, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1974", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-4455", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-9220", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-7483", "mrqa_naturalquestions-validation-5511", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-5995", "mrqa_triviaqa-validation-5828", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1702", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-335", "mrqa_triviaqa-validation-4040"], "SR": 0.53125, "CSR": 0.5294117647058824, "EFR": 0.9666666666666667, "Overall": 0.6861688112745098}, {"timecode": 68, "before_eval_results": {"predictions": ["German state of North Rhine-Westphalia", "Roberta Flack", "sesame seed", "Infante", "skye", "barnaby rudge", "Buddha", "ethiopia", "1963", "discus", "tabloid", "the Royal Festival Hall", "york racecourse", "ohio", "Jews of Iberia (in Hebrew, Sepharad) and the Spanish diaspora", "skye", "saint Basil", "Peru", "the keel", "Evander Holyfield", "the Creator's Game", "Buddhism", "new Orleans", "sprite Zero", "fat like oil or lard", "Graham Henry", "brash", "Ken Burns", "paddy dooley", "Barry Howard and yvonne", "phi", "Hungary", "So Solid Crew", "the Yardbirds", "Pennsylvania", "the main Caucasus range", "australian", "morningtown ride", "Jupiter", "watch with mother", "gosse mf", "18", "Queens Park Rangers", "wake", "they were giants", "flannel", "B\u00e9la Bart\u00f3k", "Hugh Dowding", "Montpelier", "October", "Arthur, Prince of Wales", "every year", "17", "Chris Rea", "Tomasz Adamek", "a scholar during the Joseon Dynasty who begins to write erotic novels, and becomes the lover of the King's favorite concubine", "March 30, 2025", "Amanda Knox's aunt Janet Huff", "Dubai", "rabbit hole", "deep-rooted", "Russia Time Zone", "Crackle", "skye"], "metric_results": {"EM": 0.5, "QA-F1": 0.568030753968254}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.1111111111111111, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2948", "mrqa_triviaqa-validation-7661", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-1786", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-1649", "mrqa_triviaqa-validation-5202", "mrqa_triviaqa-validation-2190", "mrqa_triviaqa-validation-7645", "mrqa_triviaqa-validation-7499", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-4876", "mrqa_triviaqa-validation-826", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-4131", "mrqa_triviaqa-validation-214", "mrqa_triviaqa-validation-5258", "mrqa_triviaqa-validation-3236", "mrqa_triviaqa-validation-1193", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-4652", "mrqa_triviaqa-validation-5860", "mrqa_triviaqa-validation-3362", "mrqa_triviaqa-validation-4490", "mrqa_naturalquestions-validation-5317", "mrqa_naturalquestions-validation-7919", "mrqa_hotpotqa-validation-412", "mrqa_newsqa-validation-3210", "mrqa_searchqa-validation-6026", "mrqa_searchqa-validation-12670", "mrqa_searchqa-validation-10485", "mrqa_searchqa-validation-7826"], "SR": 0.5, "CSR": 0.5289855072463768, "EFR": 0.96875, "Overall": 0.6865002264492753}, {"timecode": 69, "before_eval_results": {"predictions": ["oregon", "Jeffrey Archer", "Chicago", "filly/mare", "Dar es Salaam", "Sarah Keays", "miss marple", "elkie Brooks", "UPS", "romanian Nadal", "piano", "c Cambridge", "bennet", "spice girls", "syrupy", "addams", "doubting castle", "beetles", "european nation", "england", "harry shearer", "9", "pirate day", "penny", "spice girls", "48 Hours", "AFC Wimbledon", "Dutch", "Tombstone", "Friedrich Nietzsche", "Cambridge", "South Africa", "bagram", "pygmalion", "bajan", "cassis", "Dieppe Raid", "dengue fever", "left-wing political books aimed at the political education of the mass public", "triathlon", "Gabriel Byrne, Kevin Spacey, Benicio Del Toro, Kevin Pollak, and Stephen Baldwin", "dividing of cells into additional cell bodies", "strictly Come Dancing", "sound and light", "Par-4", "jack Russell Terrier", "arthur", "raclette", "kilimanjaro", "magic", "The Potsdam Conference", "to start fires, hunt, and bury their dead", "Wimbledon, London", "Cordelia", "London Luton Airport", "Sarah Winnemucca Hopkins", "Barbuda, Argentina, South Africa, Fernando P\u00f3, S\u00e3o Tom\u00e9, Madagascar, Mauritius, Mayotte, R\u00e9union, Seychelles, Comoro Islands, Cape Verde, Lebanon", "saying Chaudhary's death was warning to management.", "\"This is something that he andMisty Cummings have been discussing for the last few days and they're contemplating it because they ultimately feel that it is in the best interest for both of them.\"", "\"The deceased appeared to have been there for some time.\"", "Vanilla Ice", "Wordsworth", "Voltaire", "the summit of Cadillac Mountain"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5731473965848966}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5454545454545454, 0.6666666666666666, 0.0, 0.5, 0.0, 0.07407407407407407, 0.0, 0.14285714285714285, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4264", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-4988", "mrqa_triviaqa-validation-2520", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-2639", "mrqa_triviaqa-validation-2494", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-854", "mrqa_triviaqa-validation-5185", "mrqa_triviaqa-validation-2018", "mrqa_triviaqa-validation-6478", "mrqa_triviaqa-validation-2422", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-7513", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-1346", "mrqa_triviaqa-validation-5756", "mrqa_naturalquestions-validation-6736", "mrqa_naturalquestions-validation-8147", "mrqa_naturalquestions-validation-9011", "mrqa_hotpotqa-validation-1017", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1716", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3627", "mrqa_searchqa-validation-3002"], "SR": 0.484375, "CSR": 0.5283482142857143, "EFR": 1.0, "Overall": 0.6926227678571428}, {"timecode": 70, "before_eval_results": {"predictions": ["Geraldine Margaret Agnew", "A complex sentence", "Australia", "Ashrita Furman", "Lana Del Rey", "1994", "2010", "12 November 2010", "1 October 2006", "1977", "2018", "Missouri", "Vincent Price", "1902", "Frederick Chiluba", "4 January 2011", "Dougie MacLean", "the band's logo in gold lettering over black sleeve", "1999", "the Maryland Senate's actions, the Phoenix group withdrew its offer. That afternoon, Irsay paid a call to Mayor Hudnut and the city of Indianapolis", "altitude", "anembryonic gestation", "Michael Rosen", "The economy could no longer sustain itself with the shift and changes ; therefore, many wealthy Cubans lost their property, and joined the urban middle class", "103", "Eddie Van Halen", "$100", "the team that lost the pre-game coin toss", "Bonhomme", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "smacking a fly on her mirror and removes its corpse", "around 2011", "New Jersey Devils", "ulnar nerve", "2011", "1851", "many forested parts of the world", "the majority coming from Western Australia", "Carol Worthington", "1830", "alcohol", "a thanksgiving for a good harvest", "( born November 28, 1973 )", "a rural setting", "Justice Harlan", "Bart Howard", "Triple threat", "Bangalore", "Anthony Hopkins", "Jesus", "1996", "holography", "spain", "kevin vinterberg", "Stella Gibson", "direct scattering and inverse scattering", "the 45th Infantry Division", "it should stay that way.", "2009", "open window", "(Charles) Dickens", "Coleridge", "Pygmalion", "yen"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7261894814744079}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.823529411764706, 1.0, 0.09523809523809523, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 0.7142857142857143, 0.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8333333333333333, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8314", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-10040", "mrqa_naturalquestions-validation-3556", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-2402", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-2577", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-1049", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-1705", "mrqa_naturalquestions-validation-6113", "mrqa_naturalquestions-validation-2556", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-7692", "mrqa_naturalquestions-validation-5562", "mrqa_triviaqa-validation-6296", "mrqa_triviaqa-validation-5434", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-706", "mrqa_newsqa-validation-2658", "mrqa_searchqa-validation-4464", "mrqa_searchqa-validation-2541"], "SR": 0.5625, "CSR": 0.5288292253521127, "EFR": 0.8928571428571429, "Overall": 0.6712903986418511}, {"timecode": 71, "before_eval_results": {"predictions": ["John Goodman", "Brooke Wexler", "Lou Rawls", "inner core and growing bud of certain palm trees ( notably the coconut ( Cocos nucifera ), palmito ju\u00e7ara ( Euterpe edulis )", "elected to their positions in the Senate by their respective party caucuses, the Senate Democratic Caucus and the Senate Republican Conference", "David Gahan", "the ball is fed into the gap between the two forward packs and they both compete for the ball to win possession", "Total Drama Action", "forks, plate, butter knife, and napkin generally are placed to the left of the dinner plate", "red lead primer and a lead - based topcoat", "where Junior finds himself caught between home on the reservation and pursuing his dreams in the outside world", "either in front or on top of the brainstem", "On March 14, 1942", "Agamemnon and his brother, Menelaus, with his brother's assistance, drove out Aegisthus and Thyestes to recover his father's kingdom", "Epithelium", "Erika Mitchell Leonard", "American singer Daya", "Vincent Price", "India", "Tessa Peake - Jones", "It is an expected or free or continuously changing behaviour and may have a given individual social status or social position", "United Nations Peacekeeping Operations", "part of the normal flora of the human colon and is generally commensal, but can cause infection if displaced into the bloodstream or surrounding tissue following surgery, disease, or trauma", "Noahic Covenant", "Shirley Mae Jones", "Heat transfer by thermal radiation may be minimized by silvering flask surfaces facing the gap", "Luke 6 : 12 -- 16", "on August 19, 2016", "Prevents excessive dilation of the heart in cases of acute volume overload", "Meiji Japanese culture and art", "Terrell Suggs", "Sam", "August 22, 1980", "retinal ganglion cell axons and glial cells", "a large, high - performance luxury coupe sold in very limited numbers", "September 25", "part of Virginia State Route 48, which also includes the Virginia portion of the Blue Ridge Parkway, but this designation is not signed", "the Confederacy", "1955", "from electron donors to electron acceptors via redox ( both reduction and oxidation occurring simultaneously ) reactions, and couples this electron transfer with the transfer of protons ( H ions ) across a membrane", "2", "Montreal Canadiens", "2008", "On 1 September 1939", "The tower has three levels for visitors, with restaurants on the first and second levels", "product that can satisfy that market", "Wyatt `` Dusty '' Chandler ( George Strait )", "last book", "In the 1920s", "ice giants", "on September 19, 1977", "Camellia", "European Economic Community", "Mumbai", "Frank Fertitta, Jr.", "Coronation Street", "Michael A. Cremo", "backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry", "complicated and deeply flawed", "Crandon, Wisconsin,", "Butterflies", "Lyme disease", "at home", "Afghan forces"], "metric_results": {"EM": 0.359375, "QA-F1": 0.519483191764476}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 0.5, 0.0, 0.5714285714285715, 0.5263157894736842, 0.25, 0.0, 1.0, 0.4, 0.1, 1.0, 0.35294117647058826, 0.7499999999999999, 1.0, 0.0, 1.0, 0.11428571428571428, 0.0, 0.13793103448275862, 1.0, 1.0, 0.7272727272727273, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0625, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.4, 0.14285714285714288, 0.0, 0.5714285714285715, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5242", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1617", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-6305", "mrqa_naturalquestions-validation-9451", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-7012", "mrqa_naturalquestions-validation-5114", "mrqa_naturalquestions-validation-10501", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-9154", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-6049", "mrqa_naturalquestions-validation-1813", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2969", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-387", "mrqa_naturalquestions-validation-7912", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-9774", "mrqa_triviaqa-validation-302", "mrqa_triviaqa-validation-3027", "mrqa_hotpotqa-validation-1747", "mrqa_newsqa-validation-843", "mrqa_newsqa-validation-2315", "mrqa_searchqa-validation-2419", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-2088", "mrqa_newsqa-validation-2177"], "SR": 0.359375, "CSR": 0.5264756944444444, "EFR": 1.0, "Overall": 0.6922482638888889}, {"timecode": 72, "before_eval_results": {"predictions": ["Earth Day", "Afghanistan", "(Vinegar) sauce", "suffrage", "(Aragorn) Gamgee", "Christopher Darden", "a jelly bean", "a cloudy day", "(Philip) Berrigan", "a sheaf of wheat", "Carole King", "Spain", "The Pro-Jig Clamp Set", "Christo and his wife", "Wichita", "U.S. Department of Agriculture", "Gilligan's Island", "Penelope", "(Tom) Harkin", "the Channel Islands", "Krackel", "Penelope", "me", "a chimpanzee", "(E Einstein) Buffer", "Veep", "alex", "lullaby", "(Pigeon) blood", "Pan's Labyrinth", "(Hans) Christian Andersen", "(John) Irving", "a demonstrative pronoun", "The Who", "Turkey", "Xerox", "(Oscar) oil", "Pierre Trudeau", "earned run average", "anxiety disorder", "hemp", "Beijing", "(Lee) Harvey Oswald", "(George) Armstrong Custer", "Newton's Second Law", "(Marilyn) Monroe's", "Stockholm", "Alaska", "a puff", "The Mausoleum at Halicarnassus", "Qi", "Number 4, Privet Drive, Little Whinging in Surrey, England", "John Smith", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "Matthew 2:11", "proverbs", "Saint Cecilia the Patron Saint of Musicians", "Germany", "1989", "Suzuki YZF-R6", "80 percent of the woman's face", "Donald Trump and Joan Rivers", "three", "Kim Bauer"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5250744047619047}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.4, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.4, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-383", "mrqa_searchqa-validation-14432", "mrqa_searchqa-validation-4715", "mrqa_searchqa-validation-10065", "mrqa_searchqa-validation-9796", "mrqa_searchqa-validation-12366", "mrqa_searchqa-validation-5219", "mrqa_searchqa-validation-5330", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10504", "mrqa_searchqa-validation-2375", "mrqa_searchqa-validation-11098", "mrqa_searchqa-validation-3351", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-2136", "mrqa_searchqa-validation-2799", "mrqa_searchqa-validation-15033", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-2902", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-16212", "mrqa_searchqa-validation-14147", "mrqa_searchqa-validation-10453", "mrqa_searchqa-validation-6714", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-9881", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-6286", "mrqa_triviaqa-validation-4653", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-1741", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1587"], "SR": 0.4375, "CSR": 0.5252568493150684, "EFR": 1.0, "Overall": 0.6920044948630137}, {"timecode": 73, "before_eval_results": {"predictions": ["a symphony", "Happy Days", "Rita Mae Brown", "Bolivia and Paraguay", "Kansas", "a grasshopper", "an executive officer", "Sure", "1876", "to protect babies", "an onlooker", "The Big Sleep", "Maryland", "a Lion's beer", "a pen", "Herod", "the Lone Ranger", "Malaysia", "Georgetown University", "Bruce Rauner", "Goofy", "Walter Payton", "Mount Everest", "Winston Rodney", "a pindar poem", "a bird", "the Tom Thumb", "Prince Edward Island", "the Mad Hatter", "the sleep inducing brain chemical, serotonin", "Cincinnati", "to aid the athlete", "a concert grand piano", "ketchup", "banana", "soccer", "Tom Petty", "Tuscany", "Tunisia", "Claudette Colvin", "an inch", "Paris", "William Henry Harrison", "Corinthian", "a gram", "Bern", "Prada", "Chicago", "the umbilical cord", "the Pinta", "possible", "October 22, 2017", "Terrell Owens", "April 21, 2015", "tipping point", "skara", "the Dogger Bank", "James Harden", "eworldly wave", "Ron Goldman", "\"Sesame Street\"", "\"fusion teams,\" as they're being called,", "Rolling Stone", "Ren\u00e9 Descartes"], "metric_results": {"EM": 0.625, "QA-F1": 0.68125}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11731", "mrqa_searchqa-validation-14786", "mrqa_searchqa-validation-16533", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-12562", "mrqa_searchqa-validation-16552", "mrqa_searchqa-validation-12916", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-4479", "mrqa_searchqa-validation-8753", "mrqa_searchqa-validation-14945", "mrqa_searchqa-validation-13871", "mrqa_searchqa-validation-10415", "mrqa_searchqa-validation-403", "mrqa_searchqa-validation-11680", "mrqa_searchqa-validation-10595", "mrqa_searchqa-validation-16369", "mrqa_searchqa-validation-15975", "mrqa_searchqa-validation-15724", "mrqa_naturalquestions-validation-7366", "mrqa_triviaqa-validation-2254", "mrqa_hotpotqa-validation-5148", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-510"], "SR": 0.625, "CSR": 0.5266047297297297, "EFR": 1.0, "Overall": 0.692274070945946}, {"timecode": 74, "before_eval_results": {"predictions": ["Urban Outfitters", "The Tiger", "The River", "The Last Supper", "Baccarat", "a pawn", "Harlem", "Bosch", "hull", "a rehab center", "a cricket", "India", "Children of Men", "Juneau", "a petition", "Hippolyta", "a species", "John Galt", "spinach", "milk", "(kWh)", "reproduce", "World War I", "a student loan", "the Gateway Arch", "Kobi Malkin", "Wolfgang Puck", "a Schlitz", "the Monitor", "Cyprus", "Milwaukee", "Coffee Syrup", "a baseball movie", "Hot Lips", "Isadora Duncan", "Pig Latin", "Little Debbie", "Richard Cheney", "Speed Racer", "(John) Mellencamp", "Aristotle", "Eriq La Salle", "the Eagles", "An American Tail", "a bus tour", "an argyle", "Honda", "a wallaby", "a leather feather", "Mark Twain", "Greg", "30 October 1918", "Mel Tillis", "Michael Moriarty", "james christopher Bolam", "pawn", "australian nation", "House of Habsburg-Lorraine", "highest commissioned SS rank", "The Kansas\u2013Nebraska Act of 1854", "Orbiting Carbon Observatory,", "South Africa", "Tuesday.", "two"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6895833333333333}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9795", "mrqa_searchqa-validation-2629", "mrqa_searchqa-validation-11529", "mrqa_searchqa-validation-1283", "mrqa_searchqa-validation-8228", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-16232", "mrqa_searchqa-validation-3118", "mrqa_searchqa-validation-13579", "mrqa_searchqa-validation-16121", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-2238", "mrqa_searchqa-validation-5816", "mrqa_searchqa-validation-10468", "mrqa_searchqa-validation-16046", "mrqa_searchqa-validation-11419", "mrqa_searchqa-validation-4652", "mrqa_searchqa-validation-1047", "mrqa_searchqa-validation-7628", "mrqa_searchqa-validation-3857", "mrqa_naturalquestions-validation-4288", "mrqa_triviaqa-validation-1400", "mrqa_triviaqa-validation-3779", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-837"], "SR": 0.609375, "CSR": 0.5277083333333333, "EFR": 1.0, "Overall": 0.6924947916666666}, {"timecode": 75, "before_eval_results": {"predictions": ["Muhammad", "following the 2017 season", "state", "1908", "at the origin", "Yuzuru Hanyu", "Sarah Brightman", "silk floss", "Hold On", "Allies", "November 2016", "Empiricism", "Identification of alternative plans / policies", "Underlings", "North America", "Johnson", "Song of Songs", "Taron Egerton", "its vast territory was divided into several successor polities", "acronym", "Sheev Palpatine", "Divyanka Tripathi", "September 24, 2012", "Lex Luger and Rick Rude", "Michael Christopher McDowell", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "American indie pop band Foster the People", "John C. Reilly", "Daniel A. Dailey", "Mickey Mantle", "+, -, *, and / keys", "February 3, 2017", "Kid Creole and the Coconuts", "525", "2010", "a microfilament", "1983", "Edward Douglass White, Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist", "President of the United States", "1978", "Ravi River", "May 19, 2017", "47 cents", "Sri Lanka Podujana Peramuna", "1773", "Buddhism", "By functions", "March 16, 2018", "Joseph M. Scriven", "Heat transfer by thermal radiation may be minimized", "Frederik Barth", "peacock", "st Pancras", "t.S. Eliot", "Kent Hovind", "aviation pioneer Lieutenant Colonel Horace Meek Hickam", "Rihanna", "The minister later apologized, telling CNN his comments had been taken out of context.", "fractured pelvis and sacrum", "2001", "to recoup any of his principal", "Blackbird", "Sara Ramirez", "September 25, 2017"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6072746971595655}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.10526315789473684, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.8333333333333333, 0.0, 0.2666666666666667, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-9440", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-8155", "mrqa_naturalquestions-validation-10460", "mrqa_naturalquestions-validation-9766", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-5096", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-72", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-9609", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-813", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-5781", "mrqa_hotpotqa-validation-1047", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1576", "mrqa_searchqa-validation-5337", "mrqa_searchqa-validation-14889"], "SR": 0.546875, "CSR": 0.5279605263157895, "EFR": 0.9655172413793104, "Overall": 0.6856486785390199}, {"timecode": 76, "before_eval_results": {"predictions": ["works in a bridal shop with Anita, the girlfriend of her brother, Bernardo", "Walter Mondale", "a system of state ownership of the means of production, collective farming, industrial manufacturing and centralized administrative planning", "1928", "awarded to the team that lost the pre-game coin toss", "21 June 2007", "28", "Theodore Roosevelt", "23 hours, 56 minutes, and 4 seconds", "Universal Pictures and Focus Features", "multiple", "restarting play after a minor infringement", "A footling breech", "The Hunger Games : Mockingjay -- Part 2 ( 2015 )", "the President of India", "fingers on either side of the mouth ( usually with the knuckles facing the observer ) and to stick the tongue out", "28 %", "Elvis Presley", "a Native American nation from the Great Plains whose historic territory, known as Comancheria, consisted of present - day eastern New Mexico, southeastern Colorado, southwestern Kansas, western Oklahoma, northern Chihuahua", "Jack Scanlon", "the eighth episode of Arrow's second season", "Sean Astin", "head - up", "Doug Pruzan", "by October 1986", "The Paris Sisters", "inside the cell nucleus", "pathology", "until the age of about 14", "Thomas Hobbes in his Leviathan", "William the Conqueror", "Shawn Wayans", "Wisconsin", "slavery, especially Southern political leaders'resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "ingredients", "Bali, Indonesia", "Panzerkampfwagen VIII Maus ( `` Mouse '' )", "a set of exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time in exchange for detailed public disclosure of an invention", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "New England Patriots ( 5 - 4 )", "40 %", "Janie Crawford's `` ripening from a vibrant, but voiceless, teenage girl into a woman with her finger on the trigger of her own destiny", "in the west by the east coast of Queensland, in the east by Vanuatu ( formerly the New Hebrides ) and by New Caledonia", "April 2, 2018", "Lana Del Rey", "Jerry Leiber and Mike Stoller", "Sonu Nigam", "Arnold Schoenberg's twelve - tone technique", "Boston Bruins ( 13 )", "around 1872", "coonhound", "cellulose", "carbon", "on the first Monday of September", "Prudential Center", "Lisburn Distillery Football Club", "Ken Howard", "one", "Government Accountability Office report", "journalists who were found guilty in Ethiopia of supporting terrorism were sentenced to 11 years in jail Tuesday,", "a suit jacket", "Heroes Reborn", "the first known occurrence of the spells included in the Book of Reeds of Osiris", "since 1983."], "metric_results": {"EM": 0.375, "QA-F1": 0.5258738774363775}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666665, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.7692307692307692, 0.5, 0.8, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809523, 1.0, 0.0, 0.8571428571428571, 0.4864864864864865, 0.4615384615384615, 0.0, 0.0, 0.09090909090909091, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.26666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-952", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-9467", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-2014", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-8452", "mrqa_naturalquestions-validation-750", "mrqa_naturalquestions-validation-5007", "mrqa_naturalquestions-validation-3707", "mrqa_naturalquestions-validation-9487", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-8823", "mrqa_naturalquestions-validation-7246", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-1459", "mrqa_naturalquestions-validation-8301", "mrqa_naturalquestions-validation-10195", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-10519", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-2782", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-3040", "mrqa_hotpotqa-validation-218", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2043", "mrqa_searchqa-validation-3373", "mrqa_searchqa-validation-1664", "mrqa_searchqa-validation-15577", "mrqa_newsqa-validation-377"], "SR": 0.375, "CSR": 0.525974025974026, "EFR": 0.95, "Overall": 0.6821479301948051}, {"timecode": 77, "before_eval_results": {"predictions": ["a painful rash that develops on one side of the face or body.", "zork", "roddy doyle", "Mr Jaggers' function in the novel is to link between the two plots of Magwitch and Miss Havisham", "Prussia", "julyard Kipling", "Spongebob", "Exile", "an enclave nation which is entirely enclosed by another nation.", "South Dakota", "Brian Close", "an aphid", "Leeds", "Edinburgh", "meter maid", "cricketer", "france", "Neptune", "Vimto", "phobia", "alberta", "carry On quip", "afro-Asiatic", "sense of taste", "snare drum beat", "phiescent", "sesame seed", "hurdles", "The Centaurs", "tallest building", "American Football League (AFC) North division", "MGM", "kitty in Boots", "Giglio Island", "cestrian", "The Haight-Ashbury district", "tom hanks", "Harry patch", "funny Folks", "Sight & Sound", "inigo Jones", "sonar", "Nelson Mandela", "Today newspaper", "trousseau", "bootleggers", "Mark Darcy", "reptilian", "a supercontinent", "Salyut 1", "india", "DJ Khaled's music producer", "62", "Matthew Gregory Wise", "1861", "Karen Rose", "Limbo", "12.3 million", "July", "U.S. Vice President Dick Cheney", "New Hampshire", "a miniskirt", "a fathom", "$163 million (180 million Swiss francs)"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5403799019607843}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_triviaqa-validation-2781", "mrqa_triviaqa-validation-3244", "mrqa_triviaqa-validation-5462", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-6945", "mrqa_triviaqa-validation-2907", "mrqa_triviaqa-validation-1171", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-1204", "mrqa_triviaqa-validation-1663", "mrqa_triviaqa-validation-1855", "mrqa_triviaqa-validation-2359", "mrqa_triviaqa-validation-7059", "mrqa_triviaqa-validation-479", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-2432", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-765", "mrqa_triviaqa-validation-5151", "mrqa_triviaqa-validation-720", "mrqa_triviaqa-validation-57", "mrqa_triviaqa-validation-520", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-4918", "mrqa_triviaqa-validation-2317", "mrqa_naturalquestions-validation-3208", "mrqa_hotpotqa-validation-5278", "mrqa_newsqa-validation-3866", "mrqa_newsqa-validation-4029"], "SR": 0.46875, "CSR": 0.5252403846153846, "EFR": 1.0, "Overall": 0.6920012019230769}, {"timecode": 78, "before_eval_results": {"predictions": ["Three", "he believed he was about to be attacked himself.", "1964", "\"the voice of change,\"", "\"momentous discovery\"", "Sheikh Abu al-Nour al-Maqdessi,", "Tillakaratne Dilshan scored his sixth Test century", "as soon as 2050,", "eel.", "Middle East and North Africa,", "Sadr City", "Barack Obama", "Arnoldo Rueda Medina.", "left his indelible fingerprints on the entertainment industry.", "ketamine.", "Brian David Mitchell", "Defense of Marriage", "Jacob", "Ronaldinho", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "\"brain hacking\"", "\"Swingin\" Down the Lane.\"", "end her trip in Crawford", "al Qaeda,", "Manmohan Singh", "help the convicts find calmness in a prison", "J. Crew", "noticed a UPS delivery box where it shouldn't be.", "10,000", "Meira Kumar", "antihistamine", "CNN", "allegations that a dorm parent mistreated students at the school.", "Virgin America", "1940's Japan.", "Arthur", "million", "bribing other wrestlers to lose bouts,", "Eleven", "not feel Misty Cummings has told them everything she knows.", "(3 degrees Fahrenheit),", "Steven Gerrard", "$8.8 million", "Karl Eikenberry", "\"The Closer.\"", "Steven Chu", "conviction of Peru's ex-president is a warning to those who deny human rights.", "Brazil", "more than 125 million", "the return of a fallen U.S. service member", "model", "Doug Douglas", "Christians of Mesopotamia", "16 seasons", "motto", "the well", "egypt", "Capture of the Five Boroughs", "I Should Have Known Better", "pornographicstar", "the burning bush", "water", "Fannie Merritt Farmer", "liberty as its main idea, promoting free expression, freedom of choice, other social freedoms, and \"laissez-faire\" capitalism"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5784970238095237}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.5, 0.14285714285714288, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.19047619047619047]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-3433", "mrqa_newsqa-validation-4080", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-3463", "mrqa_newsqa-validation-3695", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-3572", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-679", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-1390", "mrqa_naturalquestions-validation-4008", "mrqa_naturalquestions-validation-7235", "mrqa_triviaqa-validation-7665", "mrqa_triviaqa-validation-744", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3392", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-7153", "mrqa_hotpotqa-validation-3140"], "SR": 0.46875, "CSR": 0.5245253164556962, "EFR": 0.9705882352941176, "Overall": 0.6859758353499628}, {"timecode": 79, "before_eval_results": {"predictions": ["Togus", "Ulysses S. Grant", "Yangtze River", "Genesis", "Queen Anne", "New York Times", "Scotland", "Oklahoma", "Communist Party", "the Nuclear Age", "Humphry Davy", "Asian countries", "24 hours", "smallpox", "the Dead Sea", "the fairway", "Hill Street Blues", "pain and swelling", "Mao Zedong", "Harriet M. Welsch", "Mickey Mouse", "Xerox", "blitz", "Jamaica", "gossip", "an exothermic reaction", "the Chocolate Factory", "Morocco", "Surf's Up", "Yao Ming", "U.S. government", "children's charities", "Andrew Marvell", "vegetables", "Bollywood", "Titanic", "Take Me Out to the Ballgame", "parapet", "(Joseph) Lieberman", "New Testament", "coffee", "Nike", "Margaret Thatcher", "gas masks", "Suriname", "Pearl", "Pirates of the Burning Sea", "Switzerland", "Vestal Virgins", "The Lord of the Rings: The Return of the King", "Fidel Castro", "H CO ( equivalently OC ( OH ) ) )", "in a thousand years", "W. Edwards Deming", "clara wieck", "Douglas Trendle", "Warwick Davis", "Ricky Marco", "edith Cavell", "Forbes", "20 minutes of cardio five days a week.", "22", "John Demjanjuk,", "1,776"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6790719696969697}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727272, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13653", "mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-2045", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-8082", "mrqa_searchqa-validation-5897", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-4390", "mrqa_searchqa-validation-3519", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-14601", "mrqa_searchqa-validation-5551", "mrqa_searchqa-validation-16286", "mrqa_searchqa-validation-4161", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16591", "mrqa_searchqa-validation-16014", "mrqa_searchqa-validation-5810", "mrqa_naturalquestions-validation-1834", "mrqa_triviaqa-validation-5346", "mrqa_hotpotqa-validation-3653", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-2118", "mrqa_naturalquestions-validation-3561"], "SR": 0.578125, "CSR": 0.5251953125, "EFR": 0.9629629629629629, "Overall": 0.6845847800925926}, {"timecode": 80, "before_eval_results": {"predictions": ["homebrewer", "the German Empire", "Tim Whelan", "Waimea Bay", "the Virgin label", "The Boeing EA-18G Growler", "George Harrison", "The bald eagle", "1923", "7pm", "Francis Schaeffer", "26,788", "10", "Greek gods and goddesses", "Azeroth", "1,467", "Marco Fu", "Jean- Marc Vall\u00e9e", "Adelaide", "Strange Interlude", "2004", "Hall & Oates", "\"From Here to Eternity\"", "The More", "England, Scotland, and Ireland", "the Workers' Party", "the line between using animals for entertainment purposes and abusing them", "his exploration and settlement of what is now Kentucky,", "six", "Mauthausen-Gusen", "Adrian Peter McLaren", "Distillery", "Ted Nugent", "New York", "Viscount Cranborne", "The Princess and the Frog", "the Surtees Racing Organisation team", "Them", "Levi Weeks", "Bruce R. Cook", "Mandarin", "Obafemi Martins", "Boulder High School in Boulder, Colorado", "Dutch", "November of that year", "Boston, Massachusetts", "\"The Big Bang Theory\"", "Brendan O'Brien", "Delphine Software International", "Sullivan", "October 21, 2016", "various locations in Redford's adopted home state of Utah", "31 - member Senate", "September 15, 2012", "Billie Jean King", "Vienna", "Malaysia", "Akio Toyoda", "Iran's nuclear program.", "\"The Energy and Environmental Showcase,\"", "\"Bix\"", "Seoul", "the Fool", "soybeans"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7098958333333334}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.19999999999999998, 0.5, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-5694", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-3650", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1210", "mrqa_hotpotqa-validation-2379", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-2743", "mrqa_naturalquestions-validation-123", "mrqa_naturalquestions-validation-1533", "mrqa_naturalquestions-validation-2411", "mrqa_triviaqa-validation-5406", "mrqa_newsqa-validation-43", "mrqa_searchqa-validation-870", "mrqa_searchqa-validation-15999", "mrqa_searchqa-validation-10352"], "SR": 0.609375, "CSR": 0.5262345679012346, "EFR": 0.96, "Overall": 0.6842000385802469}, {"timecode": 81, "before_eval_results": {"predictions": ["Bhaktivedanta Manor", "Ariel Ram\u00edrez", "Potomac River", "four", "1853", "Allies of World War I", "the mid-1980s", "Esteban Ocon", "Sophie Lara Winkleman", "Perfume: The Story of a Murderer", "Barbara Feldon", "Razor Ramon", "Birmingham, Alabama", "half of the Nobel Prize in Physics", "Adam Karpel, Alex Baskin, Douglas Ross, Gregory Stewart, Scott Dunlop, Stephanie Boyriven and Andy Cohen", "rock and roll", "1957", "Windermere Hotel", "Sir Frank P. Lowy,", "Hermione Baddeley", "MediaCityUK tram stop", "South Australia", "1698", "Tulsa's white residents", "Cartoon Network Studios", "July 25 to August 4", "Restoration Hardware", "John William Henry II", "2009", "McLaren-Honda", "Alexandre Dumas, p\u00e8re, and Paul Meurice", "The Books", "evangelical Christian periodical", "Annales de chimie et de physique", "Dar es Salaam", "The English Electric Canberra", "1864", "Washington, D.C.", "Chechen Republic", "H. R. Haldeman", "Cartoon Cartoons", "Roman \u00e0 clef", "5", "Duchess Eleanor", "Latium", "April 1, 1949", "English", "Ericsson", "interstate commerce", "1935", "Michael Redgrave", "Tyrann Devine Mathieu", "1933", "British Columbia, Canada", "1925 novel", "willow", "Cliff Willie", "Kim Clijsters", "Africa.", "hanged in 1979 for the murder of a political opponent two years after he was ousted as prime minister in a military coup.", "basic", "New York City Ballet", "voltage", "Willa Cather"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5447317242114237}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5, 0.23529411764705882, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 0.5, 0.2608695652173913, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-3028", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4891", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-130", "mrqa_hotpotqa-validation-2579", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-1381", "mrqa_hotpotqa-validation-2893", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-473", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3696", "mrqa_hotpotqa-validation-3513", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-1244", "mrqa_hotpotqa-validation-2137", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4314", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-873", "mrqa_triviaqa-validation-388", "mrqa_newsqa-validation-808", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-847", "mrqa_searchqa-validation-9177", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-5952"], "SR": 0.421875, "CSR": 0.5249618902439024, "EFR": 1.0, "Overall": 0.6919455030487804}, {"timecode": 82, "before_eval_results": {"predictions": ["1988", "Dame Helen Mirren and Dame Judi Dench", "Algernod Lanier Washington", "Conservative Party", "four", "October 23, 1971", "Ashgabat, Turkmenistan", "the Earth", "Standard Oil", "2007", "Norwegian", "The Late Late Show", "The Ryukyuan people (\u7409\u7403\u6c11\u65cf, Ry\u016bky\u016b minzoku", "Commanding General", "\"50 best cities to live in.\"", "Mike Mills", "Parlophone", "January 15, 2016", "Vilnius Airport (IATA: VNO, ICAO: EYVI) (Lithuanian: \"Vilniaus oro uostas\"", "George Clooney, Thekla Reuten", "The Worm", "Herman's Hermits", "Nikhil Banerjee", "810", "German", "the Vietnam War", "Anatoly Vasilyevich Lunacharsky", "1902", "Daniel Espinosa", "a novel", "Gabriel Iglesias", "3,384,569", "Gambaga", "2 March 1972", "Shenandoah National Park", "La Scala, Milan", "public and private life", "Gary Ross", "Hanford Site", "Commissioner", "Sam Tick", "Estelle Sylvia Pankhurst", "Aaliyah Dana Haughton", "Spain", "The Royal Albert Hall", "Leatherheads", "2 May 2015", "England", "King Duncan", "Bourbon County", "Serial (Bad) Weddings", "Epithelium", "Amybeth McNulty", "the Super Bowl", "Wichita", "american rock band", "1882", "off the coast of Dubai", "The incident Sunday evening", "not doing more since taking office.", "the AT bus", "polio", "the treble", "from an undercover Bureau of Alcohol, Tobacco, Firearms and Explosives agent Wednesday,"], "metric_results": {"EM": 0.625, "QA-F1": 0.7358180014430016}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-3919", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-976", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-904", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-1844", "mrqa_naturalquestions-validation-571", "mrqa_triviaqa-validation-6402", "mrqa_triviaqa-validation-3181", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1427", "mrqa_searchqa-validation-882", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-5174", "mrqa_newsqa-validation-1245"], "SR": 0.625, "CSR": 0.5261671686746988, "EFR": 1.0, "Overall": 0.6921865587349397}, {"timecode": 83, "before_eval_results": {"predictions": ["in a bed by a window", "Metacomet", "Chicago", "Leon Trotsky", "to idle away time", "a family tree", "The New York Times", "Martin Van Buren", "Ugly Betty", "the Pooh", "Charles Gounod", "Alexander Graham Bell", "Vijay Singh", "a fog", "a modem", "China", "the Boston Red Sox", "Season One", "Hitler", "a poem", "Jane's Electro-Optic Systems 2006-2007", "Christo", "a psychiatrist", "Ichiro Suzuki", "Frank Sinatra", "New Zealand's South", "the banjo", "Grant", "Belle Watling", "Mozart", "American alternative rock band from Chicago, Illinois", "Nellie Bly", "Lord Byron", "meningitis", "Douglas MacArthur", "3M", "The Rolling Stones", "Edie Falco", "U.S.A.", "Oneonta College", "1936", "the CN Tower", "the King of Siam", "inheritance", "Annapolis", "the cardinal", "Japan", "a mad cow", "New Brunswick", "Hindu", "the pronghorn", "January 2, 1971", "in San Francisco", "in Moscazzano", "cliff thorburn", "china", "as a way of housing a fierce half-man, half-bull creature known as the Minotaur", "Conservatorio Verdi", "close range combat", "Paul Mueller Jr.", "1959.", "The son of Gabon's former president", "United States", "in mid November"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6367931547619048}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.25]}}, "before_error_ids": ["mrqa_searchqa-validation-7412", "mrqa_searchqa-validation-11803", "mrqa_searchqa-validation-16524", "mrqa_searchqa-validation-11538", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-3392", "mrqa_searchqa-validation-1302", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-7948", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-8259", "mrqa_searchqa-validation-2208", "mrqa_searchqa-validation-9445", "mrqa_searchqa-validation-15675", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-9378", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-3977", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-13757", "mrqa_searchqa-validation-4122", "mrqa_searchqa-validation-3473", "mrqa_searchqa-validation-12578", "mrqa_naturalquestions-validation-10030", "mrqa_naturalquestions-validation-9311", "mrqa_triviaqa-validation-4560", "mrqa_hotpotqa-validation-3714", "mrqa_newsqa-validation-3923", "mrqa_naturalquestions-validation-8884"], "SR": 0.515625, "CSR": 0.5260416666666667, "EFR": 1.0, "Overall": 0.6921614583333333}, {"timecode": 84, "before_eval_results": {"predictions": ["Shinto", "Captain Hook", "Jabez Stone", "Taft", "olives", "pemmican", "Chloe Lattanzi", "Oahu", "Joseph Smith", "arthropods", "Roosevelt", "Capricorn", "Diane Arbus", "the chili relleno", "Thomas Jefferson", "the legislature", "soy miso", "Old School", "the Distant Early Warning Line", "Henry VIII", "Bonn", "mathematical research", "Pope John Paul II", "the arts journal", "Robert the Bruce", "zirconium", "oxys and genes", "gargantuan", "Elke Sommer", "an unguis", "Robin Williams", "Philadelphia", "Ivory soap", "Giuseppe Garibaldi", "An Old Man, a Young Man", "the anglerfish", "the Big Cat", "Jefferson Family Cemetery", "Gandhi", "Brazil", "Jim Thorpe", "9-to-5", "Jack Crabb", "King Lear", "the parachute jump", "the Bicentennial Symphony", "the Haunted Mansion Holiday", "Rembrandt", "Gilligan's Island", "a stride", "the Cowboy Artists", "Stanwyck's bedroom window", "Atticus Finch", "Andy Serkis", "america", "a horizontal desire", "charlie darin", "Esp\u00edrito Santo Financial Group", "Herman's Hermits", "Punjabi/Pashtun", "Asashoryu,", "\"political and religious\"", "secretary of state.", "Newcastle Falcons"], "metric_results": {"EM": 0.46875, "QA-F1": 0.52421875}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-15165", "mrqa_searchqa-validation-10629", "mrqa_searchqa-validation-6884", "mrqa_searchqa-validation-16513", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-6258", "mrqa_searchqa-validation-16336", "mrqa_searchqa-validation-6978", "mrqa_searchqa-validation-12706", "mrqa_searchqa-validation-3190", "mrqa_searchqa-validation-2225", "mrqa_searchqa-validation-14016", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-4945", "mrqa_searchqa-validation-7856", "mrqa_searchqa-validation-10624", "mrqa_searchqa-validation-9210", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-16785", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5050", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-7461", "mrqa_triviaqa-validation-5201", "mrqa_triviaqa-validation-4084", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-2724"], "SR": 0.46875, "CSR": 0.5253676470588236, "EFR": 0.9705882352941176, "Overall": 0.6861443014705882}, {"timecode": 85, "before_eval_results": {"predictions": ["Crime and Punishment", "Postcards from the Edge", "birds", "Virginia", "chocolate", "\"Elementary, My Dear Watson\"", "Ramadan", "Hamlet", "Green Acres", "the breastie", "Gertrude Stein", "Pope John XXIII", "love", "Inigo Jones", "Charles Ponzi", "elia Earhart", "Tippi", "Adrian", "Nova Scotia", "cocoa", "cocoa", "the Absinthe", "Miles Klee", "a kangaroo", "a quid", "Abraham Lincoln", "Anthony Newley", "a swimmer's ear", "Henry", "2.4", "Greek", "Jeff Probst", "\"Grease\"", "Nasser", "The Moment of Truth", "Laura", "a constellation", "Charles Manson", "Jerusalem", "Xerox", "Billy Crystal", "thyroid", "Hephaestus", "Hurricane Katrina", "pineapple", "Bill & George Clinton", "the Black Sea", "May 12, 1907", "a penny", "Frankenstein", "ShoutWipe", "to form a higher alkane", "comprehend and formulate language", "Hellenic Polytheism", "south america", "The Shootist", "Sega dreamcast", "National Society of Daughters of the American Revolution", "Johnny Galecki", "44,300", "it has not", "At least 88", "1981 drowning death,", "\"Four days before Brisman's killing, Markoff allegedly robbed a 29-year-old woman at gunpoint"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6338541666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.5, 0.5, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-590", "mrqa_searchqa-validation-247", "mrqa_searchqa-validation-1059", "mrqa_searchqa-validation-1681", "mrqa_searchqa-validation-9410", "mrqa_searchqa-validation-7724", "mrqa_searchqa-validation-738", "mrqa_searchqa-validation-10836", "mrqa_searchqa-validation-15430", "mrqa_searchqa-validation-10463", "mrqa_searchqa-validation-7355", "mrqa_searchqa-validation-7969", "mrqa_searchqa-validation-7460", "mrqa_searchqa-validation-15985", "mrqa_searchqa-validation-2305", "mrqa_searchqa-validation-11701", "mrqa_searchqa-validation-863", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-13742", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-13877", "mrqa_searchqa-validation-10146", "mrqa_triviaqa-validation-2690", "mrqa_hotpotqa-validation-4024", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-3967", "mrqa_newsqa-validation-839"], "SR": 0.5625, "CSR": 0.5257994186046512, "EFR": 1.0, "Overall": 0.6921130087209302}, {"timecode": 86, "before_eval_results": {"predictions": ["Rosemary DeCamp", "at a given temperature", "season ten", "October 28, 2007", "seven units of measure", "absorbed the superhuman powers and the psyche of Carol Danvers", "the northern terminus of Port Said", "privatized", "Stephen A. Douglas", "lost the support of the army", "the First Epistle of John", "between the stomach and the large intestine", "Gupta", "Vicente Fox", "Atelier de Construction d'Issy - Les - Moulineaux", "Egypt", "the base of the right ventricle", "Lager", "Pepsi", "Destiny's Child", "statistical", "Russell Huxtable", "Husrev Pasha", "Anna Maria Demara", "The Osmonds", "The Drew Las Vegas", "as a primer", "the use of cumin", "September 8, 2017", "SURFACE AREA OF ROOTS", "Iowa", "Matt Flinders", "1 October 2006", "T\u0101\u1e47\u1e0dava dance ( Shiva ), the theory of rasa, of bh\u0101va, expression, gestures, acting techniques, basic steps, standing postures", "Nucleotides", "generally believed to be in the Superstition Mountains, near Apache Junction, east of Phoenix, Arizona", "Egypt", "the nasal septum", "Session Initiation Protocol", "January 1, 1976", "parthenogenesis", "Implementation of plans / policies", "Ludacris", "Jack Scanlon", "fruit", "Welch, West Virginia", "Andy Cole", "the medial epicondyle of the humerus", "the retina", "Donna Mills", "Donna", "annette Crosbie", "bobby joseph jennifer", "minder", "The leopard", "Patricia Arquette", "University Grants Commission", "Symbionese Liberation Army", "101", "two young sons", "\"Like a Rock\"", "Cats", "King George III", "Norwegian"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6385662309182045}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.5454545454545454, 0.0, 1.0, 0.4, 0.761904761904762, 0.14285714285714285, 0.0, 1.0, 0.4210526315789474, 0.0, 0.20000000000000004, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10613", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-10526", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-953", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-6431", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-7398", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-6340", "mrqa_triviaqa-validation-4028", "mrqa_hotpotqa-validation-1504", "mrqa_hotpotqa-validation-5241", "mrqa_newsqa-validation-3257", "mrqa_searchqa-validation-1808", "mrqa_searchqa-validation-4175"], "SR": 0.546875, "CSR": 0.5260416666666667, "EFR": 0.9310344827586207, "Overall": 0.6783683548850574}, {"timecode": 87, "before_eval_results": {"predictions": ["The Province of Syracuse", "Guardians of the Galaxy Vol. 2", "Arlo Looking Cloud", "Jyothika", "The Franco-Prussian War", "Hirsch index rating", "Cody Miller", "1951", "Teen Titan Go!", "The Ramna Stacks", "Book of Judges", "torpedo boats", "9 February 1971", "San Francisco, California", "Three's Company", "9,984", "Diondre Cole", "Marktown", "the Rose Theatre", "1 million", "Trey Parker and Matt Stone", "La Scala, Milan", "Fidenza", "615 square kilometers or 237 square miles", "timeline of Shakespeare criticism", "Balloon Street, Manchester", "University of Southern California", "6teen", "the port city of Aden", "Noel Gallagher", "Michael Rispoli", "U2 360\u00b0 Tour", "Danny Green", "Scarface", "the Austro-Hungarian Army", "St. George", "Ericsson (\"Telefonaktiebolaget L. M. Ericsson\")", "Nova Planta Decree", "Vancouver", "Urijah Faber", "four", "1958", "The Thomas Crown Affair", "Bharat Ratna", "October \u2013 December 31, 1985", "Taoiseach of Ireland", "Unbreakable", "The Spiderwick Chronicles", "Sacramento Kings", "Sam Kinison", "Ferdinand Magellan", "Franklin Roosevelt", "the 1920s", "Blue laws", "james hargreaves", "Peter Paul Mary Song Meanings", "vaticanan", "Samoa", "flooding was so fast that the thing flipped over,\"", "composer", "FontSpace", "Bath", "Atlanta", "a greeting which is used by some on birthdays, and by others in response to `` Merry Christmas '' and `` Happy New Year ''"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6639599116161616}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.8, 1.0, 1.0, 0.0, 0.25, 0.22222222222222224, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.12121212121212123]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-4156", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-2145", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-5220", "mrqa_hotpotqa-validation-5398", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2807", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-1302", "mrqa_hotpotqa-validation-2792", "mrqa_naturalquestions-validation-5215", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-2044", "mrqa_naturalquestions-validation-9361"], "SR": 0.5625, "CSR": 0.5264559659090908, "EFR": 1.0, "Overall": 0.6922443181818181}, {"timecode": 88, "before_eval_results": {"predictions": ["four", "12-hour-plus", "as", "Ameneh Bahrami", "40", "state senators", "2005", "by text messaging", "Hawaii.", "about the devastation from Tuesday's earthquake", "Brazil's", "her most important work is her charity, the Happy Hearts Fund.", "ways to speed up screening of service members and, to the extent possible, their families, when the service members are in uniform and traveling on orders.", "Her husband and attorney, James Whitehouse,", "helping to plan the September 11, 2001, terror attacks,", "\" Crash,\"", "to convey the importance of national security in this election without going to extremes, and we encourage a constructive dialogue,\"", "because the Indians were gathering information about the rebels to give to the Colombian military.", "Washington Redskins fan and loved to travel,", "time", "people left without loved ones, without homes, without life's belongings.", "Jason Chaffetz", "summer", "southern port city of Karachi,", "hired translators to eavesdrop on a series of conversations in Arabic, Russian and Mandarin", "if a security officer were to pull a gun on an armed individual in a mall, it could result in \"airport style\" security measures,", "Ricardo Valles de la Rosa,", "Islamabad", "Toffelmakaren.", "at 3 p.m. Wednesday", "Microsoft.", "1995", "Jaime Andrade", "Casalesi Camorra clan", "Nigeria", "201-262-2800.", "South Africa", "could sustain future exploration of the moon and beyond.", "France,", "President Obama", "late Tuesday night,", "to contact the insured drivers who have failed to comply,\"", "Mashhad, Iran.", "Plymouth Rock", "Alina Cho", "Federer", "October 29 and November 5.", "treadmill", "Anthony Chambers", "10", "This will be the second", "Central Germany ( German : Mitteldeutschland )", "Iron ( III ) oxide - hydroxide", "gastrocnemius muscle", "Granada", "axe handle", "portugal", "June 17, 2007", "New Zealand", "Black Elk Speaks", "spinach", "Kwanzaa", "\"Sorry, boss\"", "Panthera pardus melas"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6134289149806884}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.10526315789473685, 0.7368421052631579, 1.0, 0.8571428571428571, 0.0, 0.07407407407407407, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 0.0, 0.3333333333333333, 0.6956521739130435, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-368", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-706", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-2877", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-728", "mrqa_newsqa-validation-1906", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-982", "mrqa_newsqa-validation-1919", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3906", "mrqa_newsqa-validation-929", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-2362", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1361", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-1201", "mrqa_newsqa-validation-2642", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-663", "mrqa_triviaqa-validation-6987", "mrqa_hotpotqa-validation-528", "mrqa_hotpotqa-validation-4378", "mrqa_searchqa-validation-1182", "mrqa_searchqa-validation-2749", "mrqa_hotpotqa-validation-855"], "SR": 0.453125, "CSR": 0.5256320224719101, "EFR": 1.0, "Overall": 0.692079529494382}, {"timecode": 89, "before_eval_results": {"predictions": ["Salt Lake City, Utah,", "10 below", "the two \"have not been able to prove that they did not support terrorism.\"", "killing rampage.", "Eintracht Frankfurt", "they did not receive a fair trial.", "The federal officers' bodies", "Bill Haas", "Larry Ellison,", "without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago.", "development of two courses on the Black Sea coast in Bulgaria.", "Joan Rivers", "The public endorsement", "Phoenix, Arizona,", "KBR", "Copts", "Alicia Keys", "two years", "the body of the aircraft", "United States, Japan, Russia, South Korea", "chairman of the House Budget Committee,", "pattern matching.\"", "Teen Patti", "almost 9 million", "U.S. senators", "the situation of America", "London and Buenos Aires", "hanged in 1979", "Nazi Germany", "Nafees A. Syed,", "a bank", "two", "illegal immigrants", "At least 38", "Sri Lanka", "The BBC", "\"wipe out\" the United States if provoked.", "after they ambushed a convoy carrying supplies for NATO forces in southern Afghanistan,", "is a city of romance, of incredible architecture and history.", "top designers, such as Stella McCartney,", "clogs", "debris late Sunday night in the area where the single-engine Cessna 206 went down,", "Alicia Keys", "ALS6", "The EU naval force", "well over 1,000 pounds", "Iran's Green Movement of protesters", "help make the new truck safer, but also could make it more expensive to repair after a collision.", "Friday,", "provides nearly $162 billion in war funding", "Lindsey Vonn", "three", "the optic chiasm", "Hugo Weaving", "aragonite", "\u201creckless\u201d", "Olympia", "Bruce R. Cook", "Los Angeles", "86,112", "the Charles Carroll", "a soap opera", "the CPI", "penrhyn"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6440139721389722}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true], "QA-F1": [0.7272727272727273, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 0.2222222222222222, 1.0, 0.923076923076923, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-906", "mrqa_newsqa-validation-2045", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-436", "mrqa_newsqa-validation-1767", "mrqa_newsqa-validation-452", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-1732", "mrqa_naturalquestions-validation-3358", "mrqa_triviaqa-validation-4977", "mrqa_triviaqa-validation-7370", "mrqa_triviaqa-validation-3909", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-11100"], "SR": 0.546875, "CSR": 0.5258680555555555, "EFR": 1.0, "Overall": 0.6921267361111111}, {"timecode": 90, "before_eval_results": {"predictions": ["(William) Inge", "anthrax", "Pluto", "larynx", "the Surgeon General", "Ebony", "Cook County", "Sartre", "Wordsworth", "St. Louis", "James K. Polk", "Harpy", "Lacrosse", "Naples", "a Dormouse", "the Romans", "a cow pie", "Paradise Lost", "\"beautiful\"", "the White Sea", "Doctor Dolittle", "Graceland", "Mitch Albom", "the beaver", "earthquakes", "Donovan", "Best Supporting Actor", "The Bionic Woman", "the multitude", "a tan", "Narnia", "comet Tempel 1", "Mount Sinai", "Kamehameha", "(Elbert) Gary", "corporality", "crowded", "\"Duke\"", "Orleans", "\"Another Brick in the Wall\"", "Pulp Fiction", "Hester Prynne", "pajamas", "Continental", "a bagpipe", "a stork", "hieroglyphs", "Henry David Thoreau", "Encephalitis", "Iran", "Sydney", "central Saskatchewan", "When the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On ''", "The long - hair gene", "Islands", "\"Another Day in Paradise\"", "the Danelaw", "Donald Wayne Johnson", "Robert Allen Iger", "Manchester Airport", "Iowa,", "16", "North Korea", "financial gain,"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7749348958333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8125000000000001, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2401", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-4076", "mrqa_searchqa-validation-14750", "mrqa_searchqa-validation-11959", "mrqa_searchqa-validation-12389", "mrqa_searchqa-validation-15887", "mrqa_searchqa-validation-14528", "mrqa_searchqa-validation-10496", "mrqa_searchqa-validation-15600", "mrqa_searchqa-validation-7404", "mrqa_searchqa-validation-5813", "mrqa_searchqa-validation-11566", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-10583", "mrqa_triviaqa-validation-1702", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-1072"], "SR": 0.703125, "CSR": 0.5278159340659341, "EFR": 1.0, "Overall": 0.6925163118131868}, {"timecode": 91, "before_eval_results": {"predictions": ["Wales", "keirin", "welterweight", "christopher nolan", "Johann von Goethe", "a highball", "arthur conan doyle", "lady Godiva", "brain", "six", "Bashir", "dog sport", "The Double", "calcium", "omega", "Mickey Mouse", "can be 108 or 126 gallons", "The Welcome Stranger", "the recorder", "Oman", "can be able to remain faithful and please God even in the midst of a corrupt and sinful generation", "Ladysmith", "californium", "rafael can come get Guerrero\u2019s WBC interim title anytime he wants.", "the Arizona Diamondbacks", "george Orwell", "Golda Meir", "carriage", "can also refer to smoked meats and fish", "William Shakespeare", "the early 1960's", "some like it hot", "Beaujolais", "intravenously three times a day", "the National Council for the Unmarried Mother and her Child", "Sarajevo", "edward ii", "the canary", "bullfighting", "leicestershire", "cycling's most famous race following the disqualification of Alberto Contador", "Crimea", "pea", "Switzerland", "Shanghai", "duke orsino", "Girl Scout Day", "the Swordfish", "France", "Australia", "France", "17 - year - old Augustus Waters", "1898", "December 1922", "South Asian Games", "Rana Daggubati", "four", "1,500", "a group of teenagers.\"", "38 feet", "the Marquis de Lafayette", "turquoise", "birds", "2010"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5906994047619047}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7142857142857143, 0.5, 0.0, 0.8571428571428571, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5840", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-3369", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5590", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-307", "mrqa_triviaqa-validation-5014", "mrqa_triviaqa-validation-5418", "mrqa_triviaqa-validation-6545", "mrqa_triviaqa-validation-1439", "mrqa_triviaqa-validation-3375", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6589", "mrqa_triviaqa-validation-1599", "mrqa_triviaqa-validation-7051", "mrqa_triviaqa-validation-5484", "mrqa_naturalquestions-validation-3859", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-4071", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-3814", "mrqa_newsqa-validation-3440", "mrqa_searchqa-validation-6259", "mrqa_searchqa-validation-11134"], "SR": 0.515625, "CSR": 0.5276834239130435, "EFR": 0.967741935483871, "Overall": 0.6860381968793828}, {"timecode": 92, "before_eval_results": {"predictions": ["Tomasz Adamek", "51,271", "Roger Thomas Staubach", "from 1979 to 2013", "two", "1987", "Meghan Markle", "2006", "alcoholic drinks", "Seoul, South Korea", "Dutch", "41st President of the United States from 1989 to 1993", "political correctness", "Russell Humphreys", "Minnesota to the west, and Wisconsin and the Upper Peninsula of Michigan to the south", "November 23, 2011", "more than 40 million", "Mazda", "Jack St. Clair Kilby", "\"My Father\"", "water", "more than 70 countries", "Black pudding", "Animorphs", "Francis", "two Nobel Peace Prizes", "Emperor of Japan", "\"Apatosaurus\"", "TD Garden", "the controversial and explicit nature of many of their songs", "Sam Kinison", "Melbourne Storm", "Hawaii", "2007", "Texas", "Prudence Jane Goward", "Vincent Anthony Guaraldi", "\"What's My Line? \"", "Kim So-hyun", "seasonal television specials, particularly its work in stop motion animation", "Carol Ann Duffy", "Lauren Lane", "Joseph I", "17 October 2006", "\"When the Levee Breaks\"", "29,000", "Dire Straits", "Niger\u2013Congo", "Princess Jessica", "2018 Unibet Premier League Darts", "first freshman to finish as the runner-up", "Canada", "defense against rain rather than sun", "eurozone", "Jane Seymour", "Willie Nelson", "1994", "Argentine", "3.5 percent", "Ali Bongo", "Antietam", "your premium", "Princeton", "Miguel Cotto"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7387820512820513}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6, 0.4, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-3594", "mrqa_hotpotqa-validation-3037", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-5615", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-2029", "mrqa_hotpotqa-validation-4007", "mrqa_hotpotqa-validation-5319", "mrqa_naturalquestions-validation-7425", "mrqa_naturalquestions-validation-1162", "mrqa_triviaqa-validation-7271", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-2508", "mrqa_searchqa-validation-15208", "mrqa_searchqa-validation-4038"], "SR": 0.640625, "CSR": 0.5288978494623655, "EFR": 1.0, "Overall": 0.692732694892473}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "Bill Cosby Jr.", "Edward R. Murrow", "Liesl", "Stage Stores, Inc.", "\"Men\" (2014)", "1998", "the World Famous Gold & Silver Pawn Shop in Las Vegas", "1972", "Argentina", "the Crab Orchard Mountains", "Dayton's department store", "1 September 1864", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Iranian-American", "Buck Owens and the Buckaroos", "the IRA's South Armagh Brigade", "Tel Aviv", "Chevy", "the tissues of the outer third of the vagina", "Overijssel, Netherlands", "the great-grandfather of Miami Marlin Christian Yelich", "Yolande Cornelia", "\"Love Letter\"", "2013", "Jericho Union Free School District", "January 15, 1975", "John R. Dilworth", "actor and former fashion model", "18.7 miles", "Oracle", "Titus Lucretius Carus", "water sprite", "Hopeless Records", "August Heckscher", "Isabella Hedgeland", "Pennsylvania's 18th congressional district", "\"The Five\"", "anabolic\u2013androgenic steroids", "Dulwich", "Red Dead Redemption", "the district of Sierre", "Buffalo", "Heathrow", "George Martin", "Timo Hildebrand", "Adam Dawes", "the Maasai phrase \"Enkare Nairobi\"", "Rockland, Maine", "2009", "the Vietnam War", "Toto", "9 February 2018", "Todd Griffin", "Gabriel Byrne and Kevin Spacey", "Funchal", "the British", "Champions League final", "it would", "a new model is simply out of their reach.", "Caroline Ponsonby", "Florida", "Burkina Faso", "Agriculture"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6632575757575758}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.47619047619047616, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.1818181818181818, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-5743", "mrqa_hotpotqa-validation-2158", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-4194", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-2879", "mrqa_hotpotqa-validation-3449", "mrqa_hotpotqa-validation-5128", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3403", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-2849", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-960", "mrqa_hotpotqa-validation-961", "mrqa_triviaqa-validation-6478", "mrqa_newsqa-validation-2754", "mrqa_searchqa-validation-16694"], "SR": 0.53125, "CSR": 0.5289228723404256, "EFR": 1.0, "Overall": 0.692737699468085}, {"timecode": 94, "before_eval_results": {"predictions": ["drummer", "Hitler", "Mrs. Miniver", "Simon Cowell", "The Eagles", "neon", "lifejackets", "Ian Fleming", "The Taming of the Shrew", "Anne Frank", "Bora Bora", "Cops", "C-Bridge Capital", "a geisha", "France", "the Barbary pirates", "the C.I.A.", "antimicrobial", "Blackberry", "the Temptations", "Phonetics", "Crosby", "Cheers", "the great sleuth", "a projectile", "the Court of Cassation", "Chichen Itza", "a second cat love song", "Afghanistan", "Australia", "a mozzarella", "lice", "Surgeons", "pitch", "Pete Rose", "Esther", "South Africa", "Lauren Bacall", "GoldenEye", "anthropology", "Dumbo", "Wharton", "Aretha Franklin", "marsupials", "swedish", "The Crow", "baseball", "Orson Welles", "mongoose", "a gladiator", "Ecuador", "the first word of the text", "four", "elected or appointed by means of a commission ( letters patent )", "Friends", "frankincense", "the solar system", "Henry I of England", "the Runaways", "Musschenbroek", "Seoul", "\"I've been feeling better every single day since surgery and this weekend my doctors gave me the green light to get back to work.\"", "The Klan had become so powerful and intimidating that police were hesitant to build a case against them.", "The Krankies"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6274283008658009}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.5, 1.0, 0.13636363636363635, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9874", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-13645", "mrqa_searchqa-validation-2987", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-14916", "mrqa_searchqa-validation-6734", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-11623", "mrqa_searchqa-validation-15760", "mrqa_searchqa-validation-15296", "mrqa_searchqa-validation-12196", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11108", "mrqa_searchqa-validation-5897", "mrqa_searchqa-validation-4014", "mrqa_searchqa-validation-10367", "mrqa_searchqa-validation-307", "mrqa_searchqa-validation-6577", "mrqa_searchqa-validation-3821", "mrqa_searchqa-validation-1101", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-2476", "mrqa_hotpotqa-validation-1508", "mrqa_hotpotqa-validation-2512", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-108"], "SR": 0.578125, "CSR": 0.5294407894736842, "EFR": 1.0, "Overall": 0.6928412828947368}, {"timecode": 95, "before_eval_results": {"predictions": ["Clarence Thomas", "Penguin Books", "Kokomo", "Profiles in Courage", "Berkeley", "the summer session", "Lady Godiva", "Beat The Clock", "Ninette de Valois", "Dag Hammarskjld", "Latin", "King Henry VIII", "San Francisco", "\"You can't stop time\"", "Neptune", "Harry Potter and the Chamber of Secrets", "Brutus", "the wild-goose", "Joseph Haydn", "Willa Cather", "Dow Jones Industrial Average", "Aunt Jemima", "the fowls", "dynasties", "Homer", "Amanda Bynes", "Ted Danson", "O. Henry", "middle-aged", "Memphis", "Kennedy Onassis", "Donovan", "plankton", "Candlestick Park", "spokeshaves", "just compensation", "vodka", "corned beef", "Adam", "Protestantism", "Celia Humphrey", "Seoul", "thunder", "Ham", "calamine", "Sicily", "Nelson's Column", "Luxor", "Drew Barrymore", "Philip Seymour Hoffman", "Sherlock Holmes", "Caleb", "31 December 1960", "Jurchen Aisin Gioro clan", "great men and women", "Little arrows", "Rotary", "Craig William Macneill", "William Adelin", "twenty", "Newark's Liberty International Airport,", "4,000", "Lifeway Christian Stores", "Pakistan"], "metric_results": {"EM": 0.625, "QA-F1": 0.664360119047619}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2232", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-13696", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-15065", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-12886", "mrqa_searchqa-validation-503", "mrqa_searchqa-validation-9992", "mrqa_searchqa-validation-12050", "mrqa_searchqa-validation-9043", "mrqa_searchqa-validation-15970", "mrqa_searchqa-validation-13657", "mrqa_searchqa-validation-4483", "mrqa_searchqa-validation-2081", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-1852", "mrqa_hotpotqa-validation-1300", "mrqa_newsqa-validation-3479", "mrqa_newsqa-validation-2278"], "SR": 0.625, "CSR": 0.5304361979166667, "EFR": 0.9166666666666666, "Overall": 0.6763736979166667}, {"timecode": 96, "before_eval_results": {"predictions": ["swans", "\"Ozymandias\" poet", "Unbreakable", "Holy Week", "Tijuana", "a Wizard", "a byte", "Planned Parenthood", "Jamie Lee Curtis", "King of the Hill", "an Abduction", "Alexander Graham Bell", "the Northern Mountain", "a baffle plate", "corpulent", "Herman", "Erin Go Bragh", "Queen Victoria", "a giant slalom", "Medusa", "zoology", "\"Lucia di Lammermoor\"", "a globe", "cricket", "Stephen Hawking", "St. Francis of Assisi", "luminous intensity", "The Scarlet Letter", "2016", "a rehab facility", "pastries", "the Hundred Years' War", "the Met's Greek and Roman galleries", "milk and honey", "Three", "a lung liver", "The Beatles", "the \"Bronx\" in New York City", "a saccharide", "King Kong", "Cubism", "Umbria", "popcorn", "M. C. Escher", "Oahu", "the kidney", "F. Scott Fitzgerald", "an aria", "Ghostbusters II", "Marquette University", "the monk", "Fall 1998", "infection, irritation, or allergies", "Bart Howard", "the Netherlands", "marillion", "Usain Bolt", "Keeper of the Great Seal of Scotland", "J. K. Rowling", "Victorian England", "we seek a new way forward, based on mutual interest and mutual respect.\"", "in early 2008,", "acid attack", "number five"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7534226190476191}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16082", "mrqa_searchqa-validation-572", "mrqa_searchqa-validation-16101", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-9048", "mrqa_searchqa-validation-12137", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14498", "mrqa_searchqa-validation-13015", "mrqa_searchqa-validation-15106", "mrqa_searchqa-validation-4441", "mrqa_searchqa-validation-1885", "mrqa_searchqa-validation-3270", "mrqa_searchqa-validation-143", "mrqa_searchqa-validation-2023", "mrqa_searchqa-validation-10547", "mrqa_naturalquestions-validation-2666", "mrqa_hotpotqa-validation-694", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-2741"], "SR": 0.671875, "CSR": 0.5318943298969072, "EFR": 0.9523809523809523, "Overall": 0.6838081814555719}, {"timecode": 97, "before_eval_results": {"predictions": ["12", "gold rings", "Gaston Leroux", "Concorde", "gold", "eec", "Canterbury and Lancaster", "vietnam", "Florentius", "Wanderers", "emilia fox", "WWF", "San Marino", "Shaft", "gal", "Ramadan", "bizet", "the Count Basie Orchestra", "Pegida", "plutonium", "kevin Thatcher", "edward hopper", "Einstein", "faversham", "Justin Trudeau", "Michael Jackson", "time team", "Thom Yorke", "OKLAHOMA!", "UNESCO", "Bolivia", "Christian Wulff", "lively", "usk", "spider", "Malcolm Turnbull", "the Daily Herald newspaper", "nairobi", "alonzo Church", "tendon", "the heart's conduction system", "a \"puck\"", "ring", "dubonnet", "Emma", "Rocky Graziano", "cashmere", "Today newspaper", "dayrington Messenger", "Gene Vincent", "Midgard", "reaction of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus", "USS Chesapeake", "Ben Faulks", "a Taylor series", "second war with India", "Art of Dying", "John Auer,", "iPod Nano", "his son, Isaac, and daughter, Rebecca.", "Zeus", "an elegant girl", "World War I", "Joseph"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5848958333333334}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1962", "mrqa_triviaqa-validation-3617", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-4006", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-4277", "mrqa_triviaqa-validation-1804", "mrqa_triviaqa-validation-7423", "mrqa_triviaqa-validation-6370", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-5350", "mrqa_triviaqa-validation-3391", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-3074", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-5209", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-4083", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-1861", "mrqa_hotpotqa-validation-1240", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2807", "mrqa_searchqa-validation-1947", "mrqa_searchqa-validation-6024"], "SR": 0.5625, "CSR": 0.5322066326530612, "EFR": 1.0, "Overall": 0.6933944515306122}, {"timecode": 98, "before_eval_results": {"predictions": ["Queen Victoria", "belfast", "Jerry Mouse", "cirrus", "procol Harum", "the river Alt", "adare", "newquay", "Uganda", "yorkshire", "lactic acid", "marseille", "Robinson Crusoe", "once a week", "\u201cMy Favorite Martian\u201d", "whist", "claustrophobia", "Madagascar", "Wyatt", "March", "one Direction", "The West Wing", "prince Harry", "1987", "titanium", "car park", "Pegasus", "alaskan", "carol", "Brazil", "the rhizome", "wood", "eyes", "oregon", "bowie knife", "c Congo", "a rat", "Independence Day", "Tinie Tempah", "portugal", "cracow", "pullchair", "Beard", "angel", "oldham", "The Sunday Post", "bobby", "shariqah", "Nick Griffin", "mansfield park", "south africa", "Cam Clarke", "only drivers who were Daytona Pole Award winners", "Florida", "twenty-three", "Mexican War on Drugs", "relationship with Apple co-founder Steve Jobs", "opium", "Basilan", "Tutsi and Hutu rivalry", "The Tempest", "Naples", "Solomon", "heavier than a feather"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5412202380952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.5, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.28571428571428575]}}, "before_error_ids": ["mrqa_triviaqa-validation-2746", "mrqa_triviaqa-validation-5217", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-4266", "mrqa_triviaqa-validation-6929", "mrqa_triviaqa-validation-5353", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-3760", "mrqa_triviaqa-validation-3452", "mrqa_triviaqa-validation-829", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-5529", "mrqa_triviaqa-validation-1337", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-487", "mrqa_triviaqa-validation-2295", "mrqa_triviaqa-validation-2257", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-3198", "mrqa_triviaqa-validation-3550", "mrqa_triviaqa-validation-4677", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-2492", "mrqa_triviaqa-validation-4987", "mrqa_naturalquestions-validation-9588", "mrqa_naturalquestions-validation-9149", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-260", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3659", "mrqa_hotpotqa-validation-3713"], "SR": 0.453125, "CSR": 0.5314078282828283, "EFR": 0.9428571428571428, "Overall": 0.6818061192279942}, {"timecode": 99, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1090", "mrqa_hotpotqa-validation-1203", "mrqa_hotpotqa-validation-1295", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1647", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-1702", "mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1829", "mrqa_hotpotqa-validation-1875", "mrqa_hotpotqa-validation-1915", "mrqa_hotpotqa-validation-2013", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-2070", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-2332", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2832", "mrqa_hotpotqa-validation-2909", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-324", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3487", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-364", "mrqa_hotpotqa-validation-3814", "mrqa_hotpotqa-validation-3911", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4219", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-4367", "mrqa_hotpotqa-validation-4380", "mrqa_hotpotqa-validation-4403", "mrqa_hotpotqa-validation-4407", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-457", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4735", "mrqa_hotpotqa-validation-4750", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4788", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-4878", "mrqa_hotpotqa-validation-4891", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5138", "mrqa_hotpotqa-validation-5148", "mrqa_hotpotqa-validation-5152", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5414", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-689", "mrqa_hotpotqa-validation-79", "mrqa_hotpotqa-validation-851", "mrqa_naturalquestions-validation-10691", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-1092", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1193", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1756", "mrqa_naturalquestions-validation-2006", "mrqa_naturalquestions-validation-2144", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2264", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-2542", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-2631", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-2782", "mrqa_naturalquestions-validation-2806", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-3124", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-338", "mrqa_naturalquestions-validation-3474", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3510", "mrqa_naturalquestions-validation-3561", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3851", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4242", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4846", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-495", "mrqa_naturalquestions-validation-4966", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-5613", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6027", "mrqa_naturalquestions-validation-6046", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6193", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7029", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-715", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-72", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7814", "mrqa_naturalquestions-validation-8115", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8397", "mrqa_naturalquestions-validation-852", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-8870", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-9059", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9488", "mrqa_naturalquestions-validation-9506", "mrqa_naturalquestions-validation-9716", "mrqa_naturalquestions-validation-9722", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-9802", "mrqa_newsqa-validation-1084", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1412", "mrqa_newsqa-validation-1426", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1732", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-1762", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-1922", "mrqa_newsqa-validation-1944", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-2068", "mrqa_newsqa-validation-2079", "mrqa_newsqa-validation-2087", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2155", "mrqa_newsqa-validation-2178", "mrqa_newsqa-validation-2206", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2299", "mrqa_newsqa-validation-2312", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-2371", "mrqa_newsqa-validation-2378", "mrqa_newsqa-validation-2419", "mrqa_newsqa-validation-2483", "mrqa_newsqa-validation-2579", "mrqa_newsqa-validation-2614", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-281", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-2971", "mrqa_newsqa-validation-3", "mrqa_newsqa-validation-3099", "mrqa_newsqa-validation-3109", "mrqa_newsqa-validation-3122", "mrqa_newsqa-validation-3129", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3198", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-322", "mrqa_newsqa-validation-323", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3278", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-3353", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-3466", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3511", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-361", "mrqa_newsqa-validation-3620", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3793", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-4103", "mrqa_newsqa-validation-4120", "mrqa_newsqa-validation-4155", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4175", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-447", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-524", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-665", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-701", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-78", "mrqa_newsqa-validation-846", "mrqa_newsqa-validation-850", "mrqa_newsqa-validation-884", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-997", "mrqa_searchqa-validation-10025", "mrqa_searchqa-validation-10101", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10547", "mrqa_searchqa-validation-10578", "mrqa_searchqa-validation-10588", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-11141", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11699", "mrqa_searchqa-validation-11813", "mrqa_searchqa-validation-11835", "mrqa_searchqa-validation-11866", "mrqa_searchqa-validation-1195", "mrqa_searchqa-validation-12095", "mrqa_searchqa-validation-12886", "mrqa_searchqa-validation-13371", "mrqa_searchqa-validation-13411", "mrqa_searchqa-validation-1373", "mrqa_searchqa-validation-13757", "mrqa_searchqa-validation-13800", "mrqa_searchqa-validation-14133", "mrqa_searchqa-validation-14277", "mrqa_searchqa-validation-14360", "mrqa_searchqa-validation-14442", "mrqa_searchqa-validation-14523", "mrqa_searchqa-validation-14604", "mrqa_searchqa-validation-14731", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14916", "mrqa_searchqa-validation-14939", "mrqa_searchqa-validation-15296", "mrqa_searchqa-validation-15850", "mrqa_searchqa-validation-15886", "mrqa_searchqa-validation-15999", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16014", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16530", "mrqa_searchqa-validation-16533", "mrqa_searchqa-validation-1980", "mrqa_searchqa-validation-2000", "mrqa_searchqa-validation-2021", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2604", "mrqa_searchqa-validation-2720", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-3052", "mrqa_searchqa-validation-3342", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-3721", "mrqa_searchqa-validation-3934", "mrqa_searchqa-validation-4019", "mrqa_searchqa-validation-4161", "mrqa_searchqa-validation-426", "mrqa_searchqa-validation-4613", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-4921", "mrqa_searchqa-validation-50", "mrqa_searchqa-validation-5032", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5525", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5704", "mrqa_searchqa-validation-5813", "mrqa_searchqa-validation-5873", "mrqa_searchqa-validation-6296", "mrqa_searchqa-validation-6684", "mrqa_searchqa-validation-6863", "mrqa_searchqa-validation-6874", "mrqa_searchqa-validation-7035", "mrqa_searchqa-validation-7469", "mrqa_searchqa-validation-7512", "mrqa_searchqa-validation-7664", "mrqa_searchqa-validation-7675", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-7821", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-834", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8821", "mrqa_searchqa-validation-8894", "mrqa_searchqa-validation-9132", "mrqa_searchqa-validation-9228", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9614", "mrqa_searchqa-validation-9881", "mrqa_squad-validation-10180", "mrqa_squad-validation-10185", "mrqa_squad-validation-10337", "mrqa_squad-validation-10399", "mrqa_squad-validation-111", "mrqa_squad-validation-1311", "mrqa_squad-validation-1555", "mrqa_squad-validation-1572", "mrqa_squad-validation-1763", "mrqa_squad-validation-1897", "mrqa_squad-validation-1974", "mrqa_squad-validation-2032", "mrqa_squad-validation-2731", "mrqa_squad-validation-2875", "mrqa_squad-validation-2985", "mrqa_squad-validation-3423", "mrqa_squad-validation-3441", "mrqa_squad-validation-3473", "mrqa_squad-validation-392", "mrqa_squad-validation-4114", "mrqa_squad-validation-4128", "mrqa_squad-validation-4178", "mrqa_squad-validation-4436", "mrqa_squad-validation-4546", "mrqa_squad-validation-4708", "mrqa_squad-validation-479", "mrqa_squad-validation-4927", "mrqa_squad-validation-558", "mrqa_squad-validation-6637", "mrqa_squad-validation-7079", "mrqa_squad-validation-7141", "mrqa_squad-validation-7333", "mrqa_squad-validation-7488", "mrqa_squad-validation-7599", "mrqa_squad-validation-7698", "mrqa_squad-validation-801", "mrqa_squad-validation-8030", "mrqa_squad-validation-8308", "mrqa_squad-validation-8513", "mrqa_squad-validation-9162", "mrqa_squad-validation-9184", "mrqa_squad-validation-9254", "mrqa_squad-validation-9600", "mrqa_squad-validation-9921", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-1392", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1645", "mrqa_triviaqa-validation-1663", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-1915", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-2151", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-235", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2614", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-2874", "mrqa_triviaqa-validation-2891", "mrqa_triviaqa-validation-2907", "mrqa_triviaqa-validation-3037", "mrqa_triviaqa-validation-3074", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3210", "mrqa_triviaqa-validation-3241", "mrqa_triviaqa-validation-3290", "mrqa_triviaqa-validation-3369", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-3503", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-3771", "mrqa_triviaqa-validation-3779", "mrqa_triviaqa-validation-3868", "mrqa_triviaqa-validation-388", "mrqa_triviaqa-validation-3973", "mrqa_triviaqa-validation-3989", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4065", "mrqa_triviaqa-validation-4098", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4117", "mrqa_triviaqa-validation-4139", "mrqa_triviaqa-validation-4153", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4588", "mrqa_triviaqa-validation-4594", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-4884", "mrqa_triviaqa-validation-4947", "mrqa_triviaqa-validation-5003", "mrqa_triviaqa-validation-5038", "mrqa_triviaqa-validation-5063", "mrqa_triviaqa-validation-5142", "mrqa_triviaqa-validation-5151", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5618", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-5652", "mrqa_triviaqa-validation-5732", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-582", "mrqa_triviaqa-validation-5904", "mrqa_triviaqa-validation-5991", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6260", "mrqa_triviaqa-validation-6291", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6510", "mrqa_triviaqa-validation-6669", "mrqa_triviaqa-validation-6747", "mrqa_triviaqa-validation-688", "mrqa_triviaqa-validation-690", "mrqa_triviaqa-validation-692", "mrqa_triviaqa-validation-6945", "mrqa_triviaqa-validation-700", "mrqa_triviaqa-validation-7034", "mrqa_triviaqa-validation-7159", "mrqa_triviaqa-validation-7286", "mrqa_triviaqa-validation-7298", "mrqa_triviaqa-validation-7361", "mrqa_triviaqa-validation-7639", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-772", "mrqa_triviaqa-validation-802", "mrqa_triviaqa-validation-858", "mrqa_triviaqa-validation-972", "mrqa_triviaqa-validation-989"], "OKR": 0.845703125, "KG": 0.4953125, "before_eval_results": {"predictions": ["stoned to death by an angry mob.", "37", "dismissed all charges", "U.S. Defense Department", "11,", "inmates", "Kenyan and Somali", "prostate cancer,", "Philip Markoff", "crocodile eggs", "Jacob,", "crafts poems telling of the pain and suffering of children just like her;", "Red Lines", "Kirchners", "an African-American woman", "Arsene Wenger", "Arnold Drummond", "Carrousel du Louvre,", "left his indelible fingerprints on the entertainment industry.", "Revolutionary Armed Forces of Colombia,", "his business dealings", "toxic smoke", "found in a hotel near Fort Bragg.", "June 25.", "\"El Senor de los Cielos,\"", "Kerstin Fritzl,", "Amnesty International", "The Tinkler", "\"Here Comes the Sun.\"", "\"to do the dirty work,\"", "lump in Henry's nether regions", "reached an agreement late Thursday", "snow,", "\"Steamboat Bill, Jr.\"", "dogs who walk on ice in Alaska.", "45 minutes, five days a week.", "students at the school.", "Harrison Ford", "Aniston, Demi Moore and Alicia Keys", "as he tried to throw a petrol bomb", "al-Moayad", "racially-tinged", "Three thousand", "Palestinian Islamic Army,", "cause of the child's death will be listed as homicide by undetermined means,", "separately on multiple corruption charges.", "2,000", "park bench", "Cirque du Soleil", "9 percent", "10 percent", "hydrogen", "Kevin Spacey", "foreign investors", "back of the neck", "wrigley", "CBS", "25 April", "North Dakota", "Phil Spector", "Charlie and the Chocolate Factory", "Death of a Salesman", "Sports Illustrated", "Colonel"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6146453373015872}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.25, 0.5714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-530", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-723", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-2952", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-3869", "mrqa_newsqa-validation-4022", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3798", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-1992", "mrqa_newsqa-validation-321", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-3450", "mrqa_naturalquestions-validation-4326", "mrqa_triviaqa-validation-7478"], "SR": 0.53125, "CSR": 0.5314062500000001, "EFR": 1.0, "Overall": 0.7213593749999999}]}