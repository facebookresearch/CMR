{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8', diff_loss_weight=0, ewc_gamma=0.9, ewc_lambda=250.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=250_gm=9e-1_T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.95,beta=0.5,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 3990, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["UK", "cnidarians", "seven", "religious", "Mnemiopsis", "TEU articles 4 and 5", "Denver", "public-key encryption", "1939", "communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "Christian", "a painting", "2012", "Bruno Mars", "economic liberalism", "Einstein", "The Shah's army was split by diverse internecine feuds", "Lunar Roving Vehicle", "SoCal", "Block II spacesuit", "Dr. George E. Mueller", "a multi-party system", "attacks on teachers in Welsh schools which reached an all-time high between 2005 and 2010", "cartels", "Tony Hawk", "Broncos", "Stromatoveris", "Sonderungsverbot", "Vince Lombardi Trophy", "the need for alliances", "1525\u201332", "wealth", "a phylum of animals that live in marine waters worldwide", "The United Methodist Church", "A piece of paper", "concrete", "one", "opportunistic", "Duke Kent- Brown", "1910\u20131940", "Museum of Manufactures", "Downtown Los Angeles", "south", "1980s", "the Main Quadrangles", "the government and the National Assembly and the Senate", "48 hours", "Edgar Atheling", "Rhine knee", "materia medica", "three years", "An attorney", "11 million", "The Scotland Act 1998", "24 March 1879", "Albert Einstein", "an attack on New France's capital, Quebec", "the Common Core", "1851", "office of Patriarch", "life expectancy", "seven", "Jonathan Stewart", "Manakin Town"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8046130952380952}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.09523809523809525, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9086", "mrqa_squad-validation-8065", "mrqa_squad-validation-166", "mrqa_squad-validation-664", "mrqa_squad-validation-7400", "mrqa_squad-validation-6163", "mrqa_squad-validation-8316", "mrqa_squad-validation-1886", "mrqa_squad-validation-2513", "mrqa_squad-validation-4534", "mrqa_squad-validation-2476", "mrqa_squad-validation-6945", "mrqa_squad-validation-2634", "mrqa_squad-validation-2314"], "SR": 0.78125, "CSR": 0.78125, "EFR": 0.9285714285714286, "Overall": 0.8549107142857143}, {"timecode": 1, "before_eval_results": {"predictions": ["The necessity defense", "The BBC was given the highlights of most of the matches, while BSkyB paying \u00a3304m for the Premier League rights", "their main method of locomotion", "Albert of Mainz", "nine", "bachelor's degree", "Duisburg", "Latin Mass", "#P", "soap sponge", "semantical problems", "The worst-case is when the input is sorted or sorted in reverse order", "force", "Bermuda 419 turf", "1975", "ABC Sunday Night Movie", "Chicago Theological Seminary", "The Daleks (a.k.a. The Mutants)", "comedies and family-oriented series", "The label Huguenot was purportedly first applied in France to those conspirators (all of them aristocratic members of the Reformed Church) involved in the Amboise plot of 1560", "Chinatown", "almost half", "2,000", "$2 million", "the head of government of a country", "Gods of Egypt", "the next day", "six", "turbine type", "late 14th-century", "Wittenberg", "Stanford", "427,652", "four days", "Thomas Edison", "1979", "a tool of the devil", "high voltage", "the warmest months from May through September, while the driest months are from November through April", "Dublin", "Madison Square Garden", "indulgences for the living", "domestic Islamists", "1891", "God", "Henry Cole", "the Marburg Colloquy", "actions-oriented", "western", "The special was one of several special 3D programmes the BBC produced at the time, using a 3D system that made use of the Pulfrich effect requiring glasses with one darkened lens", "T. J. Ward", "George Stephenson", "wiring the walls of a schoolroom and, \"saturating [the schoolroom] with infinitesimal electric waves vibrating at high frequency.\"", "\"blurring of theological and confessional differences in the interests of unity.\"", "cantatas", "a Qutb", "John Fox", "feigned retreat to break enemy formations and to lure small enemy groups away from the larger group", "biologically contained", "case law by the Court of Justice", "296", "it was made by this man who died in 1933", ",, The prospect of successful fur trade prompted the States General, the governing body of the Dutch Republic, to issue a statement on March 27", "1960"], "metric_results": {"EM": 0.734375, "QA-F1": 0.7843769639265963}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.06666666666666667, 1.0, 1.0, 0.2, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2925", "mrqa_squad-validation-1719", "mrqa_squad-validation-6024", "mrqa_squad-validation-3048", "mrqa_squad-validation-6807", "mrqa_squad-validation-2165", "mrqa_squad-validation-7211", "mrqa_squad-validation-1456", "mrqa_squad-validation-2423", "mrqa_squad-validation-9744", "mrqa_squad-validation-7835", "mrqa_squad-validation-1531", "mrqa_squad-validation-2419", "mrqa_squad-validation-6254", "mrqa_squad-validation-8896", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-5692"], "SR": 0.734375, "CSR": 0.7578125, "EFR": 1.0, "Overall": 0.87890625}, {"timecode": 2, "before_eval_results": {"predictions": ["the mouth of the Monongahela River", "Rutherford Grammar School", "to recover market share", "Clair Cameron Patterson", "a tool of the devil", "Skyclad", "one MSP", "European Union", "700", "Peanuts", "Charles Darwin", "1906", "Protestantism", "Scottish rivers", "violence", "Rhenus", "Christ and His salvation", "form business partnerships with physicians or give them \"kickback\" payments", "Troupes de la Marine and Indians", "(leptin, pituitary growth hormone, and prolactin)", "The Tomorrow People", "2005", "Karakorum", "Paris", "Ukraine", "the policies of major powers, or simply, general-purpose aggressiveness", "Energiprojekt AB", "high inequality", "the Ikh Zasag (Great Administration)", "when the present amount of funding cannot cover the current costs for labour and materials", "Marconi successfully transmitted the letter S from England to Newfoundland", "Chuck Howley", "theatre", "the increase in tea drinking", "Istanbul", "Scotland Act 1998", "a stolen Mark I Type 40 TARDIS", "DC traction motor", "17 seconds", "General Electric", "Omnicare, Kindred Healthcare and PharMerica", "the 14th century", "Robert Koch and Emil von Behring", "Guglielmo Marconi", "the first Thursday in May", "article 49", "60 minutes", "the Cobham\u2013Edmonds thesis", "the nobles", "Mork & Mindy", "Los Angeles Kings", "Arabic", "the steam escapes", "Word and Image Department", "6 miles (9.7 km)", "Wisconsin v. Yoder", "Industrial", "three of his ribs were broken", "(IV, vii. 6^)", "(with Pictures)", "(especially Welsh) retain many traces of what appears to...  Romance languages", "(4) He was a son that David failed to discipline or reign in", "a Maraschino cherry", "step up"], "metric_results": {"EM": 0.65625, "QA-F1": 0.705079816017316}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5400", "mrqa_squad-validation-4291", "mrqa_squad-validation-7051", "mrqa_squad-validation-2283", "mrqa_squad-validation-9247", "mrqa_squad-validation-10203", "mrqa_squad-validation-6622", "mrqa_squad-validation-7637", "mrqa_squad-validation-9807", "mrqa_squad-validation-6333", "mrqa_squad-validation-635", "mrqa_squad-validation-7370", "mrqa_squad-validation-1394", "mrqa_squad-validation-9569", "mrqa_squad-validation-478", "mrqa_squad-validation-5473", "mrqa_squad-validation-7049", "mrqa_squad-validation-1625", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-10983", "mrqa_searchqa-validation-7494"], "SR": 0.65625, "CSR": 0.7239583333333333, "EFR": 0.9545454545454546, "Overall": 0.8392518939393939}, {"timecode": 3, "before_eval_results": {"predictions": ["Genghis Khan", "Rutherford Grammar School", "Thomas Edison", "tidal currents", "32", "Newcastle University's student's union building", "11 July 1934", "15,000", "The network's lead broadcast team", "4k + 3", "the aboral organ", "Wojciech Bogus\u0142awski Theatre", "an antigen from a pathogen", "Maling", "NBC", "the level of the top tax rate", "October 16, 2012", "the Metro Light Rail system", "staying home", "The European Commission", "the people themselves", "The time and space hierarchy theorems", "(static discs)", "A contract", "US$100,000", "Cretaceous\u2013Paleogene extinction", "the red algal endosymbiont's original cell membrane", "Saturn V", "60%", "Litton's Weekend Aventure", "Happy Days", "about seven-eighths", "1916", "Jonathan Stewart", "Ron Grainer", "Roone Arledge", "the Scottish Government", "The Huguenots adapted quickly and often married outside their immediate French communities", "8 mm cine film", "MODES", "a mainline Protestant Methodist denomination", "the Golden Gate Bridge", "a new magma", "31 October", "Necessity-based", "love implies wrong-doing", "Gallifrey", "twelve", "EU law", "ctenophores", "relationship contracting", "Mitochondria", "The Talons of Weng-Chiang", "discipline problems with the Flight Director's orders", "6.7+", "Paul Claudel", "Port Moresby", "one of these equaled 1,000 paces", "light as he was with gravity", "Wildlife Rescue", "Brussels", "Tristan da Cunha", "ccoli", "zoology"], "metric_results": {"EM": 0.78125, "QA-F1": 0.8177083333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-589", "mrqa_squad-validation-2337", "mrqa_squad-validation-9487", "mrqa_squad-validation-3069", "mrqa_squad-validation-6913", "mrqa_squad-validation-4060", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-13132", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-16512", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-5998"], "SR": 0.78125, "CSR": 0.73828125, "EFR": 0.8571428571428571, "Overall": 0.7977120535714286}, {"timecode": 4, "before_eval_results": {"predictions": ["\u00a334m per year", "11.5 inches", "patient care rounds drug product selection", "494,665", "1,160,000", "highest", "Frederick W. Mote", "(Plasmodium falciparum", "720p high definition", "many other herbs", "an orbital scientific instrument package", "Hasar, Hachiun, and Tem\u00fcge", "Hamburg merchants and traders", "Ikh Zasag", "remote sensing", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "Building construction", "rapid expansion in telecommunication and financial activity", "Vampire bats", "theNP-complete Boolean satisfiability problem", "Rose", "socialist realism", "270,000", "not in Wittenberg", "2005\u20132010", "environmental determinism", "Samuel Phillips", "pulmonary fibrosis", "45\u201360 nanometers", "Four thousand", "more than 70", "A fundamental error", "subtraction and multiplication", "Charly", "The Secret Life of Pets", "thousands", "1966", "13", "silicon dioxide", "2007", "160 kPa", "use the potential energy stored in an H+ or hydrogen ion gradient", "NFL owners", "Roger Goodell", "the packets may be delivered according to a multiple access scheme", "regeneration", "with the help of the military", "Genghis Khan", "national courts", "color confinement", "75,000 to 100,000", "males", "a B.S.", "William Claude Dukenfield", "Helen Hayes", "Tom Chaney", "a mathematician", "port", "FDR", "Edna Ferber", "a horse", "a raven", "Indianapolis", "an album"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8341145833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1862", "mrqa_squad-validation-8789", "mrqa_squad-validation-9047", "mrqa_squad-validation-600", "mrqa_squad-validation-3664", "mrqa_squad-validation-8902", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-14679"], "SR": 0.796875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 5, "before_eval_results": {"predictions": ["the Uighurs", "the depths of the oceans and seas", "1562", "San Jose State", "John Sutcliffe", "a military coup d'\u00e9tat", "the weekly screenings of all available classic episodes", "Manned Spacecraft Center", "dislodge the French", "July", "the Norman dynasty", "Yale", "middle of the 20th century", "1852", "the \"father of the Mongols\"", "private", "1815", "mid-Cambrian", "Bayeux Tapestry", "specialised education and training", "US$100,000", "Ollie Treiz", "1599", "John Fox", "a stronger, tech-oriented economy", "Venom", "eliminate the accusing law", "the Scotland Parliament", "Narrow alleys", "reciprocating steam engines", "Evaporative", "Japan", "NP-intermediate problems", "1542", "remote sensing", "Leverage", "in a number of stages", "new laws or amendments to existing laws", "suburban", "Higher Real Gymnasium", "the school's financial endowment", "Asia", "Battle of Olustee", "the Ten Commandments", "District Superintendents", "six", "Jurassic Period", "Lenin", "seven", "the 17th century", "Cliff Richard", "Earth", "fluoride", "18", "The peripheral nervous system ( PNS) is one of the two components of the nervous system", "Pyeongchang County, Gangwon Province, South Korea", "IBM", "SAMCRO", "the biblical Book of Exodus", "538 electors", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "the Marx Brothers", "TNT", "Coach Carter"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7216052827380952}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0625, 1.0, 0.6666666666666666, 0.125, 0.8571428571428571, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3860", "mrqa_squad-validation-1075", "mrqa_squad-validation-6227", "mrqa_squad-validation-4802", "mrqa_squad-validation-2835", "mrqa_squad-validation-9492", "mrqa_squad-validation-3369", "mrqa_squad-validation-3403", "mrqa_squad-validation-7936", "mrqa_squad-validation-9452", "mrqa_squad-validation-6919", "mrqa_squad-validation-7094", "mrqa_squad-validation-2457", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5396", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-14925"], "SR": 0.640625, "CSR": 0.7317708333333333, "EFR": 1.0, "Overall": 0.8658854166666666}, {"timecode": 6, "before_eval_results": {"predictions": ["net force", "October 12, 1943", "by absolute and qualified majority", "non-self molecules", "1893", "the Channel Islands", "Doctor Who \u2013 The Ultimate Adventure", "preventable diseases like malaria, HIV/AIDS, pneumonia, diarrhoea and malnutrition", "US$10 a week", "Einstein", "1988", "pyrenoid and thylakoids", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "GM", "Inalchuq", "51.6%", "NBC Red Network", "quantitative statements", "John 8:7", "rain", "Christ Church Hall", "short-tempered and even harsher", "theology and philosophy", "KRFX", "the Bay Area's technology, culinary creations, and cultural diversity", "Metro: All Change", "General Samuel C. Phillips", "Brad Nortman", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "Anheuser-Busch InBev", "Inflammation", "1313", "Rice University", "Dignity Health", "six", "December 1878", "force", "a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "Start Here", "Karluk Kara-Khanid ruler", "send aid and sometimes to go themselves to fight for their faith", "judicial branch", "the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "inequitable taxes", "as dangerous enemies", "peer tuitions and the school's financial endowment", "Albert Einstein", "(wasp)", "Al Yeganeh", "South Africa", "pitched", "Antony", "George Cornell", "84", "Francis Matthews Dies", "197 million", "Lord Marmaduke", "Leon Baptiste", "Star Wars: The Force Unleashed", "The o'jays", "the UK", "Derry City F.C.", "H. R. Haldeman", "business process outsourcing (BPO)"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6502232142857143}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.47619047619047616, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5697", "mrqa_squad-validation-4074", "mrqa_squad-validation-1272", "mrqa_squad-validation-1550", "mrqa_squad-validation-8690", "mrqa_squad-validation-3751", "mrqa_squad-validation-6161", "mrqa_squad-validation-2486", "mrqa_squad-validation-519", "mrqa_squad-validation-2315", "mrqa_squad-validation-5054", "mrqa_squad-validation-9643", "mrqa_triviaqa-validation-1772", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-1164", "mrqa_triviaqa-validation-3388", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-632"], "SR": 0.578125, "CSR": 0.7098214285714286, "EFR": 1.0, "Overall": 0.8549107142857143}, {"timecode": 7, "before_eval_results": {"predictions": ["confirmation and sometimes the profession of faith", "latent heat", "3,837", "Canada", "UK", "487", "Westinghouse Electric", "1957", "soap sponge", "Dynasty", "torn down", "by store and forward switching", "a university or college", "Charles River", "October 2006", "arrows", "The Victorian Alps", "Louis Comfort Tiffany and \u00c9mile Gall\u00e9", "evolution of the German language and literature", "silent", "The Greens", "weak labor movements", "gas supplied through oxygen masks", "one way streets", "oppidum Ubiorum", "middle", "private finance initiatives", "only people established in the Netherlands could give legal advice", "left foot", "1901", "poor application of well-established IP procedures", "Industrial", "100% oxygen", "Madame de Pompadour", "Disney Media Networks", "Saxon Garden", "political figures", "modern art by Polish and international artists", "Neoclassical economics", "Starch", "immune surveillance", "pharma", "commerce, schooling and government", "commensal flora", "more (or no less) meaning than the individual orator intends it to have", "Boston", "Odisha", "Geoffrey Hutchings", "Dublin", "Jesper Myrfors", "from 1986 to 2013", "Vince Guaraldi", "M Rookie Blaylock", "Martha Coolidge", "Rose Garden", "Royal Navy", "Viacom Media Networks", "trans-Pacific flight", "Oklahoma City", "Lions for Lambs", "1958", "Britain", "poems", "jerry"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8511656746031746}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.8571428571428571, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4891", "mrqa_squad-validation-5168", "mrqa_squad-validation-3686", "mrqa_squad-validation-9298", "mrqa_squad-validation-4287", "mrqa_squad-validation-6779", "mrqa_squad-validation-4460", "mrqa_squad-validation-8547", "mrqa_squad-validation-6814", "mrqa_hotpotqa-validation-1931", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-5745", "mrqa_triviaqa-validation-101", "mrqa_searchqa-validation-10900"], "SR": 0.765625, "CSR": 0.716796875, "EFR": 1.0, "Overall": 0.8583984375}, {"timecode": 8, "before_eval_results": {"predictions": ["extremely high", "Elton Rule", "those who already hold wealth", "Protestant clergy to marry", "between 1859 and 1865", "Robert of Jumi\u00e8ges", "two Japanese research teams", "1905", "gambling", "Mount Bogong", "Mark Twain", "1573", "17", "enthusiasm", "Roman law meaning 'empty land'", "a double displacement loop", "attempting to reconcile electromagnetic theory with two observations", "WBT-FM (99.3 FM)", "post-secondary (non-tertiary) schools", "antiforms", "justifying grace", "the Compromise of 1850", "Keraite", "the end of the Pleistocene (~11,600 BP)", "draw British resources away from North America and the European mainland", "1724", "Hendrix v Employee Insurance Institute", "500", "heat and pressure", "guerrilla warfare campaign", "temperance", "standard of pastoral care and Christian education", "Oirads", "automatically assigned addresses, updated the distributed namespace, and configured any required inter-network routing", "Golden Gate Bridge", "Larger Catechism", "a Saturn V", "2014", "Conan O'Brien", "The issues of conflicting territorial claims between British and French colonies", "around 1700", "its humoral system", "1853", "wrestler, mixed martial artist and a former amateur wrestler", "Spring Sonata", "Golden Gate National Recreation Area", "1938", "LA Galaxy", "Edward Robert Martin Jr.", "July 10, 2017", "the reigning monarch of the United Kingdom", "punk rock", "Konstant\u012bns Raudive", "Rob Reiner", "Alain Robbe-Grillet", "Craig William Macneill", "Der Oderturm", "during the Nixon administration and Dean of the University of Southern California's medical school", "2009", "Sri Lanka Freedom Party", "Penelope Garcia", "Carbon", "while the administration appeared to have \"gotten the balance right\" on Myanmar", "black-skinned wine grape"], "metric_results": {"EM": 0.703125, "QA-F1": 0.750015318627451}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.11764705882352941, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5620", "mrqa_squad-validation-9761", "mrqa_squad-validation-10489", "mrqa_squad-validation-6981", "mrqa_squad-validation-5111", "mrqa_squad-validation-10294", "mrqa_squad-validation-10027", "mrqa_squad-validation-4789", "mrqa_squad-validation-10168", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-5170", "mrqa_newsqa-validation-3485", "mrqa_searchqa-validation-7920"], "SR": 0.703125, "CSR": 0.7152777777777778, "EFR": 1.0, "Overall": 0.8576388888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["gilt bronze", "Charles I", "Christian Whiton", "39", "hymn-writer", "two", "MPEG-4", "1724", "Reformation doctrine", "NewcastleGateshead", "famous musicians include W\u0142adys\u0142aw Szpilman and Fr\u00e9d\u00e9ric Chopin", "Immunology", "Jesus Christ was born a Jew", "temperature-dependent", "his friendship", "shipping toxic waste", "118", "2:45 a.m.", "Charles L. Hutchinson", "NFL Mobile", "English and Swahili", "a bachelor's degree", "Alvaro Martin", "Belgrade", "no French regular army troops were stationed in North America", "Hadrian's Wall", "American Association of University Women", "uncertain", "Charlesfort", "1650", "William Hartnell's poor health", "Three's Company", "October 6, 2004", "Dutch East India Company", "1060s", "Richard E. Grant, Jim Broadbent, Hugh Grant and Joanna Lumley", "coastal beaches and the game reserves", "10 July 1856", "Leonardo da Vinci's", "in his lab and elsewhere", "Japan Airlines Flight 123", "13\u20133", "Sir Hiram Stevens Maxim", "riders are turned upside-down and then back upright", "47,818", "Humvee", "Schutzstaffel", "1984 South Asian Games", "2012", "Hoosick", "Shenae Grimes-Beech", "Chris Evans", "Seacoast Region", "McLean, Virginia", "Dame Harriet Walter", "Fainaru Fantaj\u012b Tuerubu", "Pan Am Railways", "Taoiseach", "paternalistic policies enacted upon Native American tribes", "Jason Marsden", "Arrecife", "Sudanese nor orphans", "Bananas are rich in this element that controls the body's fluid balance, providing more than almost any other fruit", "tsuzumi"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7460086137820513}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0625, 0.5, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2305", "mrqa_squad-validation-652", "mrqa_squad-validation-3554", "mrqa_squad-validation-7885", "mrqa_squad-validation-5537", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4655", "mrqa_triviaqa-validation-6110", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-8586"], "SR": 0.703125, "CSR": 0.7140625, "EFR": 1.0, "Overall": 0.85703125}, {"timecode": 10, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-10", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2417", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-369", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-384", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-407", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4890", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-13", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7845", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-424", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10267", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-13132", "mrqa_searchqa-validation-13360", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-16491", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2676", "mrqa_searchqa-validation-3109", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7154", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-8586", "mrqa_searchqa-validation-9529", "mrqa_squad-validation-10018", "mrqa_squad-validation-10022", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10062", "mrqa_squad-validation-10078", "mrqa_squad-validation-10136", "mrqa_squad-validation-10168", "mrqa_squad-validation-10191", "mrqa_squad-validation-10203", "mrqa_squad-validation-10294", "mrqa_squad-validation-10337", "mrqa_squad-validation-10338", "mrqa_squad-validation-1035", "mrqa_squad-validation-10370", "mrqa_squad-validation-10406", "mrqa_squad-validation-10474", "mrqa_squad-validation-1068", "mrqa_squad-validation-1075", "mrqa_squad-validation-1110", "mrqa_squad-validation-1113", "mrqa_squad-validation-1131", "mrqa_squad-validation-1148", "mrqa_squad-validation-1182", "mrqa_squad-validation-1243", "mrqa_squad-validation-1272", "mrqa_squad-validation-1301", "mrqa_squad-validation-1320", "mrqa_squad-validation-1343", "mrqa_squad-validation-1349", "mrqa_squad-validation-135", "mrqa_squad-validation-1355", "mrqa_squad-validation-1371", "mrqa_squad-validation-1404", "mrqa_squad-validation-1456", "mrqa_squad-validation-1467", "mrqa_squad-validation-1478", "mrqa_squad-validation-1531", "mrqa_squad-validation-1548", "mrqa_squad-validation-1556", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-1625", "mrqa_squad-validation-1639", "mrqa_squad-validation-1640", "mrqa_squad-validation-1656", "mrqa_squad-validation-166", "mrqa_squad-validation-1684", "mrqa_squad-validation-1719", "mrqa_squad-validation-1755", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1811", "mrqa_squad-validation-1862", "mrqa_squad-validation-1886", "mrqa_squad-validation-1889", "mrqa_squad-validation-195", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-1977", "mrqa_squad-validation-2073", "mrqa_squad-validation-2077", "mrqa_squad-validation-2079", "mrqa_squad-validation-2106", "mrqa_squad-validation-2112", "mrqa_squad-validation-2118", "mrqa_squad-validation-2122", "mrqa_squad-validation-2133", "mrqa_squad-validation-2165", "mrqa_squad-validation-2165", "mrqa_squad-validation-2169", "mrqa_squad-validation-218", "mrqa_squad-validation-2283", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2305", "mrqa_squad-validation-2315", "mrqa_squad-validation-2323", "mrqa_squad-validation-2326", "mrqa_squad-validation-2329", "mrqa_squad-validation-2337", "mrqa_squad-validation-237", "mrqa_squad-validation-2375", "mrqa_squad-validation-2390", "mrqa_squad-validation-2399", "mrqa_squad-validation-2423", "mrqa_squad-validation-2457", "mrqa_squad-validation-2476", "mrqa_squad-validation-2486", "mrqa_squad-validation-2489", "mrqa_squad-validation-2513", "mrqa_squad-validation-2540", "mrqa_squad-validation-2544", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2607", "mrqa_squad-validation-2616", "mrqa_squad-validation-2634", "mrqa_squad-validation-2648", "mrqa_squad-validation-2667", "mrqa_squad-validation-2709", "mrqa_squad-validation-271", "mrqa_squad-validation-2723", "mrqa_squad-validation-2736", "mrqa_squad-validation-2785", "mrqa_squad-validation-2798", "mrqa_squad-validation-2810", "mrqa_squad-validation-2835", "mrqa_squad-validation-2885", "mrqa_squad-validation-2911", "mrqa_squad-validation-2923", "mrqa_squad-validation-2925", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3010", "mrqa_squad-validation-3028", "mrqa_squad-validation-3040", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3081", "mrqa_squad-validation-3097", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-318", "mrqa_squad-validation-32", "mrqa_squad-validation-3207", "mrqa_squad-validation-3225", "mrqa_squad-validation-3233", "mrqa_squad-validation-3324", "mrqa_squad-validation-3345", "mrqa_squad-validation-3369", "mrqa_squad-validation-340", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3432", "mrqa_squad-validation-3554", "mrqa_squad-validation-3623", "mrqa_squad-validation-3636", "mrqa_squad-validation-3640", "mrqa_squad-validation-3641", "mrqa_squad-validation-3664", "mrqa_squad-validation-3751", "mrqa_squad-validation-381", "mrqa_squad-validation-3816", "mrqa_squad-validation-3836", "mrqa_squad-validation-3853", "mrqa_squad-validation-3860", "mrqa_squad-validation-3887", "mrqa_squad-validation-3921", "mrqa_squad-validation-3925", "mrqa_squad-validation-3937", "mrqa_squad-validation-3942", "mrqa_squad-validation-3965", "mrqa_squad-validation-3991", "mrqa_squad-validation-402", "mrqa_squad-validation-4023", "mrqa_squad-validation-4024", "mrqa_squad-validation-4057", "mrqa_squad-validation-4060", "mrqa_squad-validation-4074", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-415", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4187", "mrqa_squad-validation-419", "mrqa_squad-validation-4267", "mrqa_squad-validation-4287", "mrqa_squad-validation-4291", "mrqa_squad-validation-4307", "mrqa_squad-validation-4311", "mrqa_squad-validation-4348", "mrqa_squad-validation-4358", "mrqa_squad-validation-4383", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4437", "mrqa_squad-validation-4442", "mrqa_squad-validation-4477", "mrqa_squad-validation-4490", "mrqa_squad-validation-453", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4575", "mrqa_squad-validation-462", "mrqa_squad-validation-4694", "mrqa_squad-validation-4724", "mrqa_squad-validation-4755", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4891", "mrqa_squad-validation-4983", "mrqa_squad-validation-4986", "mrqa_squad-validation-5081", "mrqa_squad-validation-5134", "mrqa_squad-validation-5156", "mrqa_squad-validation-5162", "mrqa_squad-validation-5168", "mrqa_squad-validation-5179", "mrqa_squad-validation-519", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5257", "mrqa_squad-validation-5269", "mrqa_squad-validation-530", "mrqa_squad-validation-5322", "mrqa_squad-validation-5351", "mrqa_squad-validation-537", "mrqa_squad-validation-5373", "mrqa_squad-validation-5396", "mrqa_squad-validation-5400", "mrqa_squad-validation-5400", "mrqa_squad-validation-5412", "mrqa_squad-validation-5423", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-5489", "mrqa_squad-validation-5502", "mrqa_squad-validation-5533", "mrqa_squad-validation-5537", "mrqa_squad-validation-5555", "mrqa_squad-validation-5557", "mrqa_squad-validation-5593", "mrqa_squad-validation-5620", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5697", "mrqa_squad-validation-5754", "mrqa_squad-validation-5760", "mrqa_squad-validation-5846", "mrqa_squad-validation-5891", "mrqa_squad-validation-5939", "mrqa_squad-validation-5966", "mrqa_squad-validation-5967", "mrqa_squad-validation-6007", "mrqa_squad-validation-6024", "mrqa_squad-validation-6031", "mrqa_squad-validation-6034", "mrqa_squad-validation-604", "mrqa_squad-validation-6040", "mrqa_squad-validation-6086", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6142", "mrqa_squad-validation-6161", "mrqa_squad-validation-6163", "mrqa_squad-validation-619", "mrqa_squad-validation-6214", "mrqa_squad-validation-6221", "mrqa_squad-validation-6222", "mrqa_squad-validation-6227", "mrqa_squad-validation-6254", "mrqa_squad-validation-626", "mrqa_squad-validation-6284", "mrqa_squad-validation-629", "mrqa_squad-validation-6318", "mrqa_squad-validation-6330", "mrqa_squad-validation-6333", "mrqa_squad-validation-6349", "mrqa_squad-validation-635", "mrqa_squad-validation-6373", "mrqa_squad-validation-6412", "mrqa_squad-validation-6447", "mrqa_squad-validation-652", "mrqa_squad-validation-6541", "mrqa_squad-validation-6606", "mrqa_squad-validation-6628", "mrqa_squad-validation-664", "mrqa_squad-validation-6653", "mrqa_squad-validation-6694", "mrqa_squad-validation-6718", "mrqa_squad-validation-6718", "mrqa_squad-validation-6738", "mrqa_squad-validation-6777", "mrqa_squad-validation-6807", "mrqa_squad-validation-6812", "mrqa_squad-validation-6813", "mrqa_squad-validation-6814", "mrqa_squad-validation-6831", "mrqa_squad-validation-686", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6894", "mrqa_squad-validation-6913", "mrqa_squad-validation-6919", "mrqa_squad-validation-6919", "mrqa_squad-validation-6945", "mrqa_squad-validation-696", "mrqa_squad-validation-7046", "mrqa_squad-validation-7049", "mrqa_squad-validation-7094", "mrqa_squad-validation-7111", "mrqa_squad-validation-7135", "mrqa_squad-validation-7165", "mrqa_squad-validation-7198", "mrqa_squad-validation-7203", "mrqa_squad-validation-7204", "mrqa_squad-validation-721", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7263", "mrqa_squad-validation-7267", "mrqa_squad-validation-7312", "mrqa_squad-validation-7322", "mrqa_squad-validation-7327", "mrqa_squad-validation-7354", "mrqa_squad-validation-7369", "mrqa_squad-validation-7370", "mrqa_squad-validation-7400", "mrqa_squad-validation-7424", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-751", "mrqa_squad-validation-7547", "mrqa_squad-validation-7580", "mrqa_squad-validation-7637", "mrqa_squad-validation-7646", "mrqa_squad-validation-7649", "mrqa_squad-validation-7663", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7733", "mrqa_squad-validation-7752", "mrqa_squad-validation-7821", "mrqa_squad-validation-7835", "mrqa_squad-validation-7851", "mrqa_squad-validation-7885", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7975", "mrqa_squad-validation-7979", "mrqa_squad-validation-7991", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8073", "mrqa_squad-validation-8092", "mrqa_squad-validation-8163", "mrqa_squad-validation-820", "mrqa_squad-validation-823", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8301", "mrqa_squad-validation-831", "mrqa_squad-validation-8316", "mrqa_squad-validation-8319", "mrqa_squad-validation-8337", "mrqa_squad-validation-8341", "mrqa_squad-validation-8401", "mrqa_squad-validation-8418", "mrqa_squad-validation-8455", "mrqa_squad-validation-8465", "mrqa_squad-validation-8466", "mrqa_squad-validation-8527", "mrqa_squad-validation-8547", "mrqa_squad-validation-8599", "mrqa_squad-validation-8654", "mrqa_squad-validation-8664", "mrqa_squad-validation-8682", "mrqa_squad-validation-8690", "mrqa_squad-validation-8700", "mrqa_squad-validation-878", "mrqa_squad-validation-8837", "mrqa_squad-validation-8837", "mrqa_squad-validation-8902", "mrqa_squad-validation-9041", "mrqa_squad-validation-9047", "mrqa_squad-validation-9082", "mrqa_squad-validation-9086", "mrqa_squad-validation-9110", "mrqa_squad-validation-9192", "mrqa_squad-validation-9276", "mrqa_squad-validation-929", "mrqa_squad-validation-9298", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9427", "mrqa_squad-validation-9452", "mrqa_squad-validation-9456", "mrqa_squad-validation-9492", "mrqa_squad-validation-9543", "mrqa_squad-validation-9543", "mrqa_squad-validation-9569", "mrqa_squad-validation-9597", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-967", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9709", "mrqa_squad-validation-9719", "mrqa_squad-validation-9744", "mrqa_squad-validation-9770", "mrqa_squad-validation-9773", "mrqa_squad-validation-980", "mrqa_squad-validation-9807", "mrqa_squad-validation-9837", "mrqa_squad-validation-9844", "mrqa_squad-validation-9857", "mrqa_squad-validation-9889", "mrqa_squad-validation-9900", "mrqa_squad-validation-9940", "mrqa_squad-validation-9989", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1289", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6283", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7276"], "OKR": 0.919921875, "KG": 0.3765625, "before_eval_results": {"predictions": ["5 to 15 years", "Tolui", "a conjunction of three planets", "Gottfried Fritschel", "1960s", "a supervisory church body", "The Emperor presented the final draft of the Edict of Worms", "2000", "Budget cuts", "Sky Digital", "carbohydrates", "combination of what has been done with several hundred kilowatts of high frequency energy liberated", "light reactions", "between 1346 and 1671", "an epidemiological account of the plague", "Doritos", "Stanford Stadium", "Sava Kosanovi\u0107", "A diaspora of French Australians", "research universities and other public and private institutions", "ABC Inc.", "Super Bowl City", "ESPN Deportes", "Article 102", "1973", "Africa", "to elect and appoint bishops", "seven", "Beyonc\u00e9", "modular exponentiation", "Wade Phillips", "combination combined immunodeficiency, acquired conditions such as HIV/AIDS, or the use of immunosuppressive medication", "domestic legislation of the Scottish Parliament", "Aaron Spelling", "Emmerich Rhine Bridge", "music", "three", "unsustainable monetary stimulation", "music teacher", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "Them", "Armada", "Boyd Gaming", "Little Big League", "October 25, 1881", "Rudolph the Red-Nosed Reindeer", "Quasimodo", "Canada's first train robbery", "1933", "Patrick Dempsey", "2016", "Ian Rush", "The Number Twelve", "A. R. Rahman", "ice hockey", "Lincoln Memorial University", "Vincent Anthony Guaraldi", "Romas Kalanta", "French", "It is scheduled to be released on May 1, 2018", "Charles Dickens", "to put a lid on the marking of Ashura", "February", "his father's parenting skills"], "metric_results": {"EM": 0.65625, "QA-F1": 0.6871463112910481}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2105263157894737, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15999999999999998, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 0.923076923076923, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6166", "mrqa_squad-validation-4875", "mrqa_squad-validation-2209", "mrqa_squad-validation-1474", "mrqa_squad-validation-8873", "mrqa_squad-validation-4953", "mrqa_squad-validation-3063", "mrqa_squad-validation-2708", "mrqa_squad-validation-103", "mrqa_squad-validation-6442", "mrqa_squad-validation-7387", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-4338", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-4950", "mrqa_naturalquestions-validation-6391", "mrqa_newsqa-validation-1058", "mrqa_searchqa-validation-8755"], "SR": 0.65625, "CSR": 0.7088068181818181, "EFR": 0.9545454545454546, "Overall": 0.7489985795454545}, {"timecode": 11, "before_eval_results": {"predictions": ["the courts of member states", "on a phased basis", "Ukraine", "different viewpoints and political parties", "raised his arm", "1888", "Academy Awards", "August 1914", "1962", "the Chinese", "eastwards", "Zh\u00e8ng", "detention", "main porch", "Imperial", "Matt Smith", "Education in Wales", "New England Patriots", "San Jose Marriott", "2016", "AD 14", "encourage growth", "Buddhism", "c1600", "a supervisory church body", "The Snowmen", "Pole Mokotowskie", "to avoid trivialization", "1948", "article 49", "the butcher Market", "the highest terrace", "San Mateo", "Waterlogged roots", "workers engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project", "4 August 1915 until November 1918", "Queens, New York", "Maryland", "Francis Egerton", "1910", "eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Super Bowl XXIX", "Hugh Christopher Edmund Fearnley-Whittingstall", "Vancouver", "1978", "Martin Scorsese", "Groupe PSA", "850 m", "50JJB Sports Fitness Clubs", "Nia Kay", "Massachusetts", "125 lb (57 kg)", "16 January 1856", "Ericsson", "wrestler", "1949", "\"Heinrich Himmler\"", "Soha Ali Khan", "1985", "get the psychological basis for a woman\u2019s desire to smoke", "in the west African nation", "\"Millsaps College Purple and White\"", "Addis Ababa", "The Merry Wives of Windsor"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7213076636904762}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.0, 0.47619047619047616, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666665, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.25, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7818", "mrqa_squad-validation-2080", "mrqa_squad-validation-2468", "mrqa_squad-validation-7758", "mrqa_squad-validation-6766", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-2763", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-5386", "mrqa_hotpotqa-validation-686", "mrqa_naturalquestions-validation-8175", "mrqa_triviaqa-validation-828", "mrqa_newsqa-validation-3429", "mrqa_searchqa-validation-10721"], "SR": 0.671875, "CSR": 0.7057291666666667, "EFR": 1.0, "Overall": 0.7574739583333334}, {"timecode": 12, "before_eval_results": {"predictions": ["less than $1.25", "Brazil", "Oahu", "turbine type steam engines", "Pegasus satellites", "1708", "Zh\u00e8ng (Chinese: \u6b63)", "captured enemies", "US", "Paul Samuelson", "Construction", "baptism", "1227", "President", "the Stockton and Darlington Railway", "Venus", "Thomas Edison", "the Santa Clara Marriott", "Maria Sk\u0142odowska-Curie Institute of Oncology", "from January 1964", "Thomas Davis", "Blaydon Race", "45,000 pounds", "1972", "1994\u20131999", "Denver Broncos", "Jacksonville", "the Song dynasty", "10", "the United States", "HAMAS (\"zeal\")", "arranged marriages", "Algeria", "the innate immune system versus the adaptive immune system", "three", "stronger to 110 mph", "Vernon Forrest", "50,000", "\"persistent pain.\"", "22", "male veterans", "the Dutch patent office", "prevention our public-owned seas from turning into sprawling, watery versions of Houston, Texas, or Atlanta, Georgia", "$1.5 million", "Dennis Davern", "American", "the tune", "$250,000", "Kaka", "Laura Ling and Euna Lee", "1995", "\"disagreements\" with the Port Authority of New York and New Jersey", "the Indians were gathering information about the rebels", "30", "a process to ensure that auto owners comply with recalls", "a president who understands the world today", "Jan Brewer", "15,000", "Brazilian state of Mato Grosso", "Daniel Radcliffe", "Argand lamp", "British commander who led the 21st Army Group at Normandy", "Charles \"Lucky\" Luciano", "John Ford"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7447840073529413}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9411764705882353, 0.625, 0.8, 0.5, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3324", "mrqa_squad-validation-6279", "mrqa_squad-validation-3854", "mrqa_squad-validation-1050", "mrqa_squad-validation-1", "mrqa_squad-validation-9599", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-111", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-4145", "mrqa_naturalquestions-validation-3390", "mrqa_triviaqa-validation-1871", "mrqa_searchqa-validation-4954", "mrqa_hotpotqa-validation-3124"], "SR": 0.65625, "CSR": 0.7019230769230769, "EFR": 1.0, "Overall": 0.7567127403846153}, {"timecode": 13, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "gilt copper", "antithetical", "young and the elderly", "because it has survived many wars, conflicts and invasions throughout its long history.", "common flagellated protists that contain chloroplasts derived from a green alga.", "anti-colonial movements", "five million", "John Pell, Lord of Pelham Manor,", "expendable nature of the worker in relation to his or her particular job.", "five-year", "3%", "1080i HD", "their Annual Conference", "\"an important game for us as a league\"", "electric", "aspirational consumption", "Vistula River", "390 billion", "red algae red.", "other ctenophores.", "to protect the King's land in the Ohio Valley from the British.", "Department of Justice", "Barbara Walters", "Chicago Theological Seminary.", "May", "1885", "William S. Paley", "six", "Iran", "modern Eldon Garden", "an extensive, electrified, passenger system throughout Melbourne and suburbs", "southern California coast", "1989 for soft drinks and 1990 for alcoholic beverages", "Cathy Dennis and Rob Davis", "1971 album, What's Going On", "Louis Mountbatten", "in a geographical coordinate system at which longitude is defined to be 0 \u00b0", "Detective Abigail Baker", "159 beats per minute", "independence from the Kingdom of Great Britain, and became the first states in the U.S.", "The eighth and final season of the fantasy drama television series", "as the home", "Hugh S. Johnson", "$75,000", "Rigg", "NFL coaches, general managers, and scouts", "Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha - Raw, Ian McKellen, and Emma Thompson", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "March 11, 2018", "nucleus, where the DNA is held", "The Vamps, McGregor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "georgia", "which are independent of the quantity of a good produced and include inputs ( capital ) that can not be varied in the short term, such as buildings and machinery", "Michael Christopher McDowell", "China, and features Kung Fu instead of Okinawan Karate", "8 January 1999", "Magyarorsz\u00e1g z\u00e1szlaja", "Argentina", "professional footballer, and a current manager.", "safety issues in the company's cars", "waron (Gyps fulvus)", "d(AACCCC)", "warwick davis"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6425244445449187}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.19999999999999998, 1.0, 0.0, 0.4444444444444445, 0.0, 0.4, 0.0, 0.8275862068965517, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7692307692307693, 0.33333333333333337, 1.0, 0.25, 0.33333333333333337, 0.33333333333333337, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.33333333333333337, 0.33333333333333337, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_squad-validation-89", "mrqa_squad-validation-4415", "mrqa_squad-validation-8840", "mrqa_squad-validation-9568", "mrqa_squad-validation-5009", "mrqa_squad-validation-2900", "mrqa_squad-validation-2585", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-10347", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-6020", "mrqa_hotpotqa-validation-5578", "mrqa_newsqa-validation-247", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-5681"], "SR": 0.53125, "CSR": 0.6897321428571428, "EFR": 0.9, "Overall": 0.7342745535714286}, {"timecode": 14, "before_eval_results": {"predictions": ["Worldvision Enterprises", "specific details of the computational model", "16,000 rpm", "scholars", "it infringed on democratic freedoms.", "a \"racket\"", "The Muslims in the semu class", "The Five Doctors", "high supply", "Tyneside's shipbuilding heritage, and inventions which changed the world", "many events and festivals", "platyctenids", "increased blood flow into tissue", "Deformational", "phycobilin", "organic solvents", "three to five", "1996", "half", "increases or decreases in response to the applied force up to an upper limit", "Brad Nortman", "Super Bowl XLIV", "San Jose State", "12 to 15 million", "the Carmichael numbers", "BPP, ZPP and RP", "Genghis Khan", "Advanced Steam movement", "Dutch law said only people established in the Netherlands could give legal advice.", "Alsace", "Vistula River", "the Buonapartes", "George W. Bush", "a sweet soda", "Idaho State", "the heart", "the Navajo Code Talkers", "Tennessee", "the focal point", "President Theodore Roosevelt", "the United States Navy's flight demonstration squadron", "Jack Nicholson", "Giselle", "prostate", "the American South", "green", "Willie Mays", "\"The only thing we have to fear is Fear is Fear Itself\"", "Hitler", "Stripes", "Israel", "Boo Boo", "the Arctic", "Wyoming", "Crayola", "antique", "the pommel horse", "four", "Riemannian geometry and geometric topology", "Elizabeth River", "Stratfor", "Joseph Heller", "fealty and filial piety", "September 2000"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6362035533910535}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.9090909090909091, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.5, 1.0, 0.5, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1714", "mrqa_squad-validation-9865", "mrqa_squad-validation-7792", "mrqa_squad-validation-5226", "mrqa_squad-validation-680", "mrqa_squad-validation-8677", "mrqa_squad-validation-10316", "mrqa_squad-validation-6044", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15884", "mrqa_searchqa-validation-9543", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-9604", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-8967", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-12609", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-15249", "mrqa_searchqa-validation-6365", "mrqa_searchqa-validation-10053", "mrqa_searchqa-validation-10017", "mrqa_triviaqa-validation-6244", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-5897"], "SR": 0.546875, "CSR": 0.6802083333333333, "EFR": 0.9655172413793104, "Overall": 0.7454732399425288}, {"timecode": 15, "before_eval_results": {"predictions": ["7.8%", "Manakin Town", "three", "the city council", "the Monarch", "filaments", "rapid expansion in telecommunication and financial activity", "Accountants", "Jim Nantz and Phil Simms", "phagolysosome", "George Westinghouse", "October 16, 2012", "1832", "a universal Ku band LNB (9.75/10.600 GHz)", "June 4, 2014", "26", "$5 million", "trade unions", "Ronnie Hillman", "Chris Keates", "the murder of Christ", "to provide a standardized interface into and out of packet networks", "the days, weeks and months", "complexity", "CBS", "prices", "random noise", "sorcery or even poison", "an Islamic state", "Hendrix v Employee Insurance Institute", "the chords", "the Boston Marathon", "Homealone (1990)", "Tainted Love", "\"Suspicious Minds\"", "humans", "Saratoga Race Course", "Duranduran", "a fragmentation grenade", "cinnamon", "Bo Schembechler, Jr.", "PT 109", "Titanosaurus", "the endocrine pancreas", "Afrikaans", "the right whale", "Edgar Allan Poe", "(Case Stengel)", "the bus", "Robert Burns", "Cy Young", "a checker", "the parrot", "genes", "the Firth of Forth", "The Pirates of Penzance", "the Whig Party", "his frustration with the atmosphere in the group at that time", "(R&B)", "Australia", "the Internet", "86,112", "a pioneer", "Sweden, Norway and Denmark"], "metric_results": {"EM": 0.640625, "QA-F1": 0.709375}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-413", "mrqa_squad-validation-8899", "mrqa_squad-validation-2743", "mrqa_squad-validation-4968", "mrqa_squad-validation-6933", "mrqa_searchqa-validation-9550", "mrqa_searchqa-validation-4723", "mrqa_searchqa-validation-2002", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-1993", "mrqa_searchqa-validation-16236", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-13960", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-6370", "mrqa_searchqa-validation-8626", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-8628", "mrqa_searchqa-validation-16371", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-5415"], "SR": 0.640625, "CSR": 0.677734375, "EFR": 1.0, "Overall": 0.751875}, {"timecode": 16, "before_eval_results": {"predictions": ["middle eastern scientists", "an electrical generator", "force", "4,404.5", "time or space", "International Association of Methodist-related Schools, Colleges, and Universities", "commercial intentions", "Jason Bourne", "48.8 \u00b0C", "0.52/sq mi", "Jamukha", "journalist", "Doctor Who", "3 million", "eight", "2014", "Ugali with vegetables, sour milk, meat, fish or any other stew", "1285", "interleukins", "MSPs", "1978", "not designed to fly through the Earth's atmosphere", "Hughes Hotel", "into the North Sea in the Netherlands", "The Emperor", "Josh Norman", "government officials and climate change experts", "Thomas Commerford Martin", "the Russell Terrier", "Stephen King", "piano", "Tiberius", "Carly Simon", "a capella", "Ida Tacke Noddack", "Japan", "Charlotte Elizabeth Diana", "Uganda", "Stradivari", "Canada", "Taiwan", "Liverpool", "E pluribus unum", "Budapest", "parliaments", "parliaments", "parliaments", "Jupiter", "Chuck Hagel", "Victoria", "a period", "Tina Turner", "Margaret Thatcher", "the tapestry", "hypopituitarism", "Prime Minister John Prescott", "boudin", "parliaments", "Bardney", "4 meters (13 feet) high", "the Black Sea", "Todd Griffin", "1908", "a thick bunch of rootlets ( branch roots )"], "metric_results": {"EM": 0.625, "QA-F1": 0.6872395833333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7777777777777778, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-9800", "mrqa_squad-validation-601", "mrqa_squad-validation-7877", "mrqa_squad-validation-72", "mrqa_squad-validation-3848", "mrqa_squad-validation-9169", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-7612", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-2320", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-3542", "mrqa_naturalquestions-validation-2462", "mrqa_newsqa-validation-1214", "mrqa_naturalquestions-validation-1704"], "SR": 0.625, "CSR": 0.6746323529411764, "EFR": 1.0, "Overall": 0.7512545955882353}, {"timecode": 17, "before_eval_results": {"predictions": ["life expectancy", "eleven", "1937", "biomass", "to avoid being targeted by the boycott", "Walter Reed", "September 1944", "two", "new form", "an area of science where our scientific understanding is rapidly changing, this has been raised as a serious shortcoming in a body which is widely regarded as the ultimate authority on the science", "12%", "six to nine percent", "The European Court of Justice", "2010", "the law is no longer to be taught to Christians but belonged only to city hall", "Ardabil Carpet", "US", "the Fifth, Sixth and Seventh", "land and housing", "Pons Aelius", "which further highlight the difference between a problem and an instance,", "slash and burn", "he often deliberately used \"vulgarity and violence\" for effect, both in his writings condemning the Jews and in diatribes against \"Turks\" (Muslims) and Catholics", "131", "Hugh Downs", "Dillon, Read & Co.", "the sum of divisors", "arthur", "the otto", "four", "Adam Werritty", "four", "Ipswich Town", "Margaret Thatcher", "South Korea", "otto", "arthur", "Pakistan", "2014 Mille lance", "Papua New Guinea", "arraf al Omari", "her skills", "Ramadan", "arles", "arthur", "Creation", "the Mongols", "South africa", "Edward de Bono", "aron ralston (James Franco)", "kyu", "arctic", "Andes Mountains of Chile and Argentina", "arthur", "INTC", "Nelson Mandela", "A", "above the light source and under the sample in an upright microscope, and above the stage and below the light sources in an inverted microscope", "9", "Sunday", "arthur", "1960s", "Ahmad Givens ( Real )", "the volume"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5745802926433732}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.9473684210526315, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8470", "mrqa_squad-validation-2473", "mrqa_squad-validation-7859", "mrqa_squad-validation-1670", "mrqa_squad-validation-2523", "mrqa_squad-validation-9416", "mrqa_squad-validation-9063", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-5660", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-3203", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-1480", "mrqa_triviaqa-validation-2519", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-5219", "mrqa_triviaqa-validation-3748", "mrqa_triviaqa-validation-2246", "mrqa_naturalquestions-validation-4132", "mrqa_hotpotqa-validation-893", "mrqa_searchqa-validation-2851", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-5051"], "SR": 0.515625, "CSR": 0.6657986111111112, "EFR": 0.967741935483871, "Overall": 0.7430362343189965}, {"timecode": 18, "before_eval_results": {"predictions": ["violence", "the Swiss-Austrian border", "Michael P. Millardi", "In the autumn of 1991, talks were held for the broadcast rights for Premier League for a five-year period, from the 1992 season.", "MetroCentre", "force", "packet switching", "high wages", "Buena Vista International Television", "three hundred sixty", "his own projects with varying degrees of success", "near the Alps", "Marco Polo", "January 2003", "1972", "the 1965\u201366 season", "Luther", "Storybook", "1981", "1264", "monophyletic", "complete the modules to earn Chartered Teacher Status", "1981", "glowed even when turned off", "21 October 1512", "hard, opaque, shiny, and has good electrical and thermal conductivity", "canola", "the people", "the Nokia tune", "the clangers", "dans", "gobi", "gobs", "steppenwolf", "Pakistan", "the Tower of London", "the skull", "Devon", "dans", "shmoop", "dans", "Isle of Wight", "a murre", "tartary", "Bob Beamon", "a cathedral", "omid dalili", "the man in London", "Thor", "The Crab", "zulu", "the transfusion of blood from an animal to a human.", "domingo", "S\u00e8vres", "dans", "The World is Not Enough", "The Romantics", "2020", "The Pirate", "1968", "the lower house", "(l-r)", "tartare", "The University of Exeter"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5792613636363636}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.2727272727272727, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2921", "mrqa_squad-validation-1249", "mrqa_squad-validation-9071", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-6625", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-1410", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-751", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-3465", "mrqa_hotpotqa-validation-774", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2128", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-10897"], "SR": 0.546875, "CSR": 0.6595394736842105, "EFR": 1.0, "Overall": 0.7482360197368421}, {"timecode": 19, "before_eval_results": {"predictions": ["\u03b2-defensins", "10.0%", "Josh Norman", "the finite field with p elements", "Andrew Alper", "1990s", "CBS", "2003", "seven", "southern China to Daidu in the north", "3.6%", "the park", "Book of Discipline", "drug treatment", "Neil Shubin and Paul Sereno", "Colorado Springs", "more convenient and private method", "Several procedures", "on Fresno's far southeast side", "antigenic variation", "c1110", "glaucophyte", "700,000", "Cestida", "john moroli", "Copenhagen", "David Nixon", "cricket", "between Seventh Avenue and Broadway", "homogenocene", "john john morpster", "Coldplay", "fern", "Stanislas Wawrinka", "Western Australia", "Volkswagen", "Florence", "syutthaya Province", "Porridge", "Strangeways", "Uranus", "Phil Redmond", "Claretta Petacci", "sebastian moran", "takerx.comroman numerals", "Montmorency", "south africa", "Chillicothe and Zanesville", "Boyle", "Mozambique Channel", "Ytterby", "tinted fosdick", "four", "moran", "doe", "south africa", "3D modeling", "February 7, 2018", "Mary Bonauto, Susan Murray, and Beth Robinson", "872 to 930", "Hugo Chavez", "Michoacan Family", "Parkinson's disease", "crustacean"], "metric_results": {"EM": 0.65625, "QA-F1": 0.732717803030303}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.7272727272727273, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-225", "mrqa_squad-validation-9036", "mrqa_squad-validation-8103", "mrqa_squad-validation-6282", "mrqa_squad-validation-4662", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-5148", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-5181", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-5318", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-1884", "mrqa_naturalquestions-validation-4879", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-2897"], "SR": 0.65625, "CSR": 0.659375, "EFR": 1.0, "Overall": 0.748203125}, {"timecode": 20, "UKR": 0.78125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-3099", "mrqa_hotpotqa-validation-3115", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3461", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-384", "mrqa_hotpotqa-validation-3861", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4338", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5415", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-881", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-13", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-7447", "mrqa_naturalquestions-validation-8654", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9505", "mrqa_naturalquestions-validation-9684", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3356", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-12771", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15467", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-15884", "mrqa_searchqa-validation-16371", "mrqa_searchqa-validation-23", "mrqa_searchqa-validation-2676", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-4220", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-4997", "mrqa_searchqa-validation-5283", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7494", "mrqa_searchqa-validation-7935", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-9604", "mrqa_searchqa-validation-9885", "mrqa_squad-validation-10009", "mrqa_squad-validation-10018", "mrqa_squad-validation-10022", "mrqa_squad-validation-10027", "mrqa_squad-validation-10062", "mrqa_squad-validation-10087", "mrqa_squad-validation-101", "mrqa_squad-validation-10168", "mrqa_squad-validation-10191", "mrqa_squad-validation-10206", "mrqa_squad-validation-10252", "mrqa_squad-validation-10264", "mrqa_squad-validation-10294", "mrqa_squad-validation-10316", "mrqa_squad-validation-10338", "mrqa_squad-validation-10383", "mrqa_squad-validation-10406", "mrqa_squad-validation-1042", "mrqa_squad-validation-10474", "mrqa_squad-validation-1048", "mrqa_squad-validation-1068", "mrqa_squad-validation-1075", "mrqa_squad-validation-1081", "mrqa_squad-validation-1110", "mrqa_squad-validation-1148", "mrqa_squad-validation-1156", "mrqa_squad-validation-1243", "mrqa_squad-validation-1272", "mrqa_squad-validation-1371", "mrqa_squad-validation-1394", "mrqa_squad-validation-1404", "mrqa_squad-validation-1441", "mrqa_squad-validation-1547", "mrqa_squad-validation-1550", "mrqa_squad-validation-1556", "mrqa_squad-validation-1557", "mrqa_squad-validation-1577", "mrqa_squad-validation-159", "mrqa_squad-validation-1613", "mrqa_squad-validation-1631", "mrqa_squad-validation-1639", "mrqa_squad-validation-167", "mrqa_squad-validation-1719", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1862", "mrqa_squad-validation-1882", "mrqa_squad-validation-1889", "mrqa_squad-validation-192", "mrqa_squad-validation-195", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-1993", "mrqa_squad-validation-2033", "mrqa_squad-validation-2079", "mrqa_squad-validation-2112", "mrqa_squad-validation-2122", "mrqa_squad-validation-2140", "mrqa_squad-validation-2169", "mrqa_squad-validation-2191", "mrqa_squad-validation-225", "mrqa_squad-validation-2292", "mrqa_squad-validation-2329", "mrqa_squad-validation-2337", "mrqa_squad-validation-237", "mrqa_squad-validation-2375", "mrqa_squad-validation-2399", "mrqa_squad-validation-2411", "mrqa_squad-validation-2423", "mrqa_squad-validation-2453", "mrqa_squad-validation-2461", "mrqa_squad-validation-2467", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-2523", "mrqa_squad-validation-2540", "mrqa_squad-validation-256", "mrqa_squad-validation-2570", "mrqa_squad-validation-258", "mrqa_squad-validation-2585", "mrqa_squad-validation-2616", "mrqa_squad-validation-2625", "mrqa_squad-validation-2634", "mrqa_squad-validation-2648", "mrqa_squad-validation-2708", "mrqa_squad-validation-2736", "mrqa_squad-validation-2778", "mrqa_squad-validation-2834", "mrqa_squad-validation-2885", "mrqa_squad-validation-2900", "mrqa_squad-validation-2909", "mrqa_squad-validation-2923", "mrqa_squad-validation-296", "mrqa_squad-validation-2994", "mrqa_squad-validation-3028", "mrqa_squad-validation-305", "mrqa_squad-validation-3063", "mrqa_squad-validation-3081", "mrqa_squad-validation-3097", "mrqa_squad-validation-3125", "mrqa_squad-validation-3165", "mrqa_squad-validation-317", "mrqa_squad-validation-3193", "mrqa_squad-validation-32", "mrqa_squad-validation-3204", "mrqa_squad-validation-3225", "mrqa_squad-validation-3233", "mrqa_squad-validation-3233", "mrqa_squad-validation-3267", "mrqa_squad-validation-327", "mrqa_squad-validation-3324", "mrqa_squad-validation-3345", "mrqa_squad-validation-340", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3469", "mrqa_squad-validation-3661", "mrqa_squad-validation-3686", "mrqa_squad-validation-3740", "mrqa_squad-validation-3751", "mrqa_squad-validation-3759", "mrqa_squad-validation-3802", "mrqa_squad-validation-3806", "mrqa_squad-validation-381", "mrqa_squad-validation-3836", "mrqa_squad-validation-3860", "mrqa_squad-validation-3902", "mrqa_squad-validation-3925", "mrqa_squad-validation-3991", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-415", "mrqa_squad-validation-419", "mrqa_squad-validation-4267", "mrqa_squad-validation-4366", "mrqa_squad-validation-4383", "mrqa_squad-validation-4429", "mrqa_squad-validation-4429", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4575", "mrqa_squad-validation-4607", "mrqa_squad-validation-462", "mrqa_squad-validation-4662", "mrqa_squad-validation-4670", "mrqa_squad-validation-468", "mrqa_squad-validation-4694", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-5003", "mrqa_squad-validation-5010", "mrqa_squad-validation-5012", "mrqa_squad-validation-5054", "mrqa_squad-validation-5077", "mrqa_squad-validation-5134", "mrqa_squad-validation-5162", "mrqa_squad-validation-5179", "mrqa_squad-validation-5185", "mrqa_squad-validation-519", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5269", "mrqa_squad-validation-5282", "mrqa_squad-validation-5319", "mrqa_squad-validation-5351", "mrqa_squad-validation-5363", "mrqa_squad-validation-5382", "mrqa_squad-validation-5400", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-552", "mrqa_squad-validation-5537", "mrqa_squad-validation-5556", "mrqa_squad-validation-5557", "mrqa_squad-validation-5600", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5718", "mrqa_squad-validation-5754", "mrqa_squad-validation-5797", "mrqa_squad-validation-5840", "mrqa_squad-validation-5846", "mrqa_squad-validation-5853", "mrqa_squad-validation-5889", "mrqa_squad-validation-5903", "mrqa_squad-validation-5927", "mrqa_squad-validation-5939", "mrqa_squad-validation-5948", "mrqa_squad-validation-596", "mrqa_squad-validation-6024", "mrqa_squad-validation-6026", "mrqa_squad-validation-6034", "mrqa_squad-validation-604", "mrqa_squad-validation-6086", "mrqa_squad-validation-6142", "mrqa_squad-validation-6151", "mrqa_squad-validation-6161", "mrqa_squad-validation-6209", "mrqa_squad-validation-6214", "mrqa_squad-validation-6254", "mrqa_squad-validation-6268", "mrqa_squad-validation-6278", "mrqa_squad-validation-6279", "mrqa_squad-validation-6284", "mrqa_squad-validation-6349", "mrqa_squad-validation-6415", "mrqa_squad-validation-6447", "mrqa_squad-validation-6457", "mrqa_squad-validation-6474", "mrqa_squad-validation-6492", "mrqa_squad-validation-6517", "mrqa_squad-validation-6541", "mrqa_squad-validation-6622", "mrqa_squad-validation-6624", "mrqa_squad-validation-6766", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6807", "mrqa_squad-validation-6831", "mrqa_squad-validation-686", "mrqa_squad-validation-6879", "mrqa_squad-validation-6919", "mrqa_squad-validation-6933", "mrqa_squad-validation-696", "mrqa_squad-validation-7005", "mrqa_squad-validation-7026", "mrqa_squad-validation-7046", "mrqa_squad-validation-7051", "mrqa_squad-validation-7111", "mrqa_squad-validation-7135", "mrqa_squad-validation-7165", "mrqa_squad-validation-7170", "mrqa_squad-validation-7198", "mrqa_squad-validation-7203", "mrqa_squad-validation-7204", "mrqa_squad-validation-7208", "mrqa_squad-validation-7211", "mrqa_squad-validation-725", "mrqa_squad-validation-7260", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7292", "mrqa_squad-validation-7370", "mrqa_squad-validation-7387", "mrqa_squad-validation-7400", "mrqa_squad-validation-7417", "mrqa_squad-validation-7435", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-748", "mrqa_squad-validation-7500", "mrqa_squad-validation-7508", "mrqa_squad-validation-751", "mrqa_squad-validation-7637", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7758", "mrqa_squad-validation-7794", "mrqa_squad-validation-7851", "mrqa_squad-validation-7859", "mrqa_squad-validation-7885", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7949", "mrqa_squad-validation-7975", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8092", "mrqa_squad-validation-8123", "mrqa_squad-validation-8151", "mrqa_squad-validation-8171", "mrqa_squad-validation-8180", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8250", "mrqa_squad-validation-8301", "mrqa_squad-validation-8316", "mrqa_squad-validation-8338", "mrqa_squad-validation-8341", "mrqa_squad-validation-8349", "mrqa_squad-validation-835", "mrqa_squad-validation-8394", "mrqa_squad-validation-8401", "mrqa_squad-validation-8418", "mrqa_squad-validation-8428", "mrqa_squad-validation-8457", "mrqa_squad-validation-8478", "mrqa_squad-validation-8527", "mrqa_squad-validation-8599", "mrqa_squad-validation-862", "mrqa_squad-validation-8664", "mrqa_squad-validation-8677", "mrqa_squad-validation-8682", "mrqa_squad-validation-8690", "mrqa_squad-validation-8694", "mrqa_squad-validation-8700", "mrqa_squad-validation-8723", "mrqa_squad-validation-8724", "mrqa_squad-validation-8839", "mrqa_squad-validation-8840", "mrqa_squad-validation-8902", "mrqa_squad-validation-9036", "mrqa_squad-validation-9041", "mrqa_squad-validation-9047", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-929", "mrqa_squad-validation-9317", "mrqa_squad-validation-9456", "mrqa_squad-validation-9487", "mrqa_squad-validation-9492", "mrqa_squad-validation-9504", "mrqa_squad-validation-9524", "mrqa_squad-validation-9653", "mrqa_squad-validation-967", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9699", "mrqa_squad-validation-9761", "mrqa_squad-validation-9770", "mrqa_squad-validation-9787", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9971", "mrqa_squad-validation-9995", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1111", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1540", "mrqa_triviaqa-validation-1558", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1780", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-2137", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2560", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-292", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3748", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4393", "mrqa_triviaqa-validation-4590", "mrqa_triviaqa-validation-4655", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4892", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5094", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5181", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5660", "mrqa_triviaqa-validation-5780", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6026", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6129", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7450", "mrqa_triviaqa-validation-7612", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-799", "mrqa_triviaqa-validation-828"], "OKR": 0.8984375, "KG": 0.3953125, "before_eval_results": {"predictions": ["Cuba", "The General Conference", "Satya Nadella", "24\u201310", "achieving crime control via incapacitation and deterrence", "Cosgrove Hall", "eleven", "issues under their jurisdiction", "article 49", "Corliss", "Doctor of Pharmacy (Pharm. D.)", "a simple majority vote, usually through a \"written procedure\" of circulating the proposals and adopting if there are no objections", "father of the house when in his home", "Battle of Dalan Balzhut", "since the 1970s", "higher returns", "a US$10 a week raise over Tesla's US$18 per week salary", "1698", "Urgench", "clinical pharmacists", "exceeds any given number", "Enric Miralles", "178", "Franklin, Tennessee", "robert boyle", "two", "robert boyle", "Houston", "March 24", "CEO of an engineering and construction company", "Olivia Newton-John", "Fernando Gonzalez", "July 4", "Nazi Germany", "269,000", "banned substance cortisone", "ancient rituals", "collaborating with the Colombian government", "as many as 250,000", "Ricardo Valles de la Rosa", "Sporting Lisbon", "Tuesday", "leftist Workers' Party", "afmado", "Arthur E. Morgan III", "head for Italy", "behind the counter", "Sheikh Sharif Sheikh Ahmed", "Jason Voorhees", "Two United Arab Emirates based companies", "people who don't even know me", "Jacob", "strife in Somalia", "insurgency", "changed the world", "two years", "2026", "king Harold Godwinson", "fatty hump on their shoulders, drooping ears and a large dewlap", "australia", "every Rose Has its Thorn", "every aspect of public and private life", "Tokyo", "cVS"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6950284090909091}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.18181818181818182, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4298", "mrqa_squad-validation-2318", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-370", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-2620", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-10604", "mrqa_triviaqa-validation-2423", "mrqa_triviaqa-validation-3916"], "SR": 0.609375, "CSR": 0.6569940476190477, "EFR": 1.0, "Overall": 0.7463988095238095}, {"timecode": 21, "before_eval_results": {"predictions": ["half", "patient compliance", "Vince Lombardi", "CALIPSO", "1989", "an electrical generator", "festivals", "potential drug interactions, adverse drug reactions, and assess patient drug allergies", "international", "the spread of diseases from Europe,", "Atilla the Hun", "The Prospect Studios", "a school that has good, clear laws, fairly and democratically", "Warsaw Uprising Museum", "some extra costs", "Warsaw University of Technology", "Stanford University", "1978", "Citadel Media", "to abolish the state of Israel.", "in 2007", "continental European liberalism", "Good Neighbors", "an ornamental figure or illustration fronting the first page, or titlepage, of a book", "Paul Anka", "Wings", "gleddyf Rhydderch Hael", "Leonardo", "seven", "a Tin Star.", "1982", "54-Marquesses", "George Clooney", "Malvinas", "kami (paper)", "Wisconsin", "Norman Mailer", "NOW Magazine", "how to reply when someone thanks you.", "Amnesty International", "parsnip", "\"Archer\"", "Justin Bieber", "The Merry Wives of Windsor", "Lingerie Football League (LFL)", "26", "MexicanTrainFun", "Hadrian", "grapevines", "Jakarta", "white star and crescent", "The Seven Year Itch", "Anita Brookner", "state", "Frogmore Estate", "Kittim", "September 4, 2000 to February 25, 2003", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "\"The Double Life of V\u00e9ronique\"", "Thomas Jefferson", "\"Hillbilly Handfishin'\"", "22-10.", "arizonensis", "the ceiling"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6182539682539683}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, false, false, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.5, 0.16666666666666669, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6361", "mrqa_squad-validation-8021", "mrqa_squad-validation-6224", "mrqa_squad-validation-1831", "mrqa_squad-validation-9595", "mrqa_squad-validation-8598", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2069", "mrqa_triviaqa-validation-5289", "mrqa_triviaqa-validation-6743", "mrqa_triviaqa-validation-3521", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-1013", "mrqa_triviaqa-validation-987", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-2484", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-7375", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-3071", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-2808", "mrqa_triviaqa-validation-4815", "mrqa_triviaqa-validation-2725", "mrqa_naturalquestions-validation-5006", "mrqa_naturalquestions-validation-1722", "mrqa_hotpotqa-validation-2937", "mrqa_searchqa-validation-9630"], "SR": 0.515625, "CSR": 0.6505681818181819, "EFR": 1.0, "Overall": 0.7451136363636364}, {"timecode": 22, "before_eval_results": {"predictions": ["steam", "a freshwater lake", "a family member", "Rhin", "local producer prices", "Queen Bees", "coal", "LGBT", "When the Methodists in America were separated from the Church of England", "to denote unknown or unexplored territory", "CBS and NBC", "differences in value added by labor, capital and land", "Battle of Fort Bull", "Vince Lombardi", "Treaty provisions", "Albert Einstein", "X-ray imaging", "as soon as they enter into force", "the eighteenth century", "light energy", "one (or more)", "Bangladesh", "shark River Park", "1994", "adult reality show", "a face-to-face interview", "The Rosie Show", "Addis Ababa", "a remote part of northwestern Montana", "next year", "pirates", "giovanni bologna", "Michael Partain", "terrorize", "2,700-acre", "Copts", "reverse the Taliban's momentum and stabilize the country's government.", "Lindsey oil refinery", "Benazir Bhutto", "Red Cross, UNHCR and UNICEF", "Jacob", "Battlefield helicopter crews", "380,000", "Larry Ellison", "an Airbus A320-214", "$50", "a head injury", "Venus Williams", "1912", "prostate cancer", "Larry Zeiger", "pro-democracy activists", "pliers", "basic infrastructure", "Afghan homes and compounds", "Hot Wings", "transmitted these messages over military telephone or radio communications nets using formal or informally developed codes built upon their native languages", "siegfried", "Newton", "Tony Burke", "47,818", "argo", "Corinthian", "Queen Wilhelmina"], "metric_results": {"EM": 0.625, "QA-F1": 0.6970016891891893}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.1081081081081081, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-8325", "mrqa_squad-validation-5764", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-3412", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2801", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-1286", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-387", "mrqa_newsqa-validation-3001", "mrqa_naturalquestions-validation-4496", "mrqa_triviaqa-validation-5322", "mrqa_triviaqa-validation-5431", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-7611", "mrqa_searchqa-validation-4084"], "SR": 0.625, "CSR": 0.6494565217391304, "EFR": 1.0, "Overall": 0.7448913043478261}, {"timecode": 23, "before_eval_results": {"predictions": ["Brompton district of the Royal Borough of Kensington and Chelsea", "Golden Gate Bridge", "The Wars of Religion", "winter of 1973\u201374", "steam", "International Contemporary Music Festival Warsaw Autumn", "10 February 1763", "major cities", "his hotel room", "Oxygen therapy", "Tibetan Buddhism", "transubstantiation", "Swahili", "plantar fasciitis", "necessity", "Ancient Egypt", "The catechism", "Court of Justice of the European Union (CJEU)", "tenfold", "San Joaquin Light & Power Building", "Crips", "Overland Park, Kansas", "White Horse", "In the category of pornographic websites", "period dependent", "16 March 1987", "south", "twenty-three", "beer and soft drinks", "skiing and mountaineering", "East Lothian", "Tim Burton", "Robert Paul \"Robbie\" Gould III", "Fred Derry", "The The Onion", "Mary Bonauto, Susan Murray", "Malayalam", "Edith Cavell", "end of the 17th century", "Jenn Brown", "League of the Three Emperors", "Viacom Media Networks", "Oakland, California", "1993", "RKO", "Sully", "David Abelevich", "Bill Clinton", "Martin Scorsese", "Tryphosa Duncan", "Haitian Revolution", "Senator (1935\u201347)", "over 600", "Macau Peninsula, Macau", "Kalahari Desert", "$1.84 billion", "during the winter of the 2017", "The Peppercorn Pioneer", "Harold Wilson", "Ronald Cummings", "Jaipur", "Sioux City", "Luxor", "Arnold Schoenberg"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6647073412698413}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.22222222222222224, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.8, 0.4, 0.0, 0.5, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-680", "mrqa_squad-validation-10500", "mrqa_squad-validation-3053", "mrqa_squad-validation-629", "mrqa_hotpotqa-validation-2880", "mrqa_hotpotqa-validation-2755", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5030", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-583", "mrqa_hotpotqa-validation-3003", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-5564", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-4134", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-8696", "mrqa_triviaqa-validation-6557", "mrqa_newsqa-validation-1616"], "SR": 0.5625, "CSR": 0.6458333333333333, "EFR": 1.0, "Overall": 0.7441666666666666}, {"timecode": 24, "before_eval_results": {"predictions": ["suburban shopping", "Thermochemical techniques", "was lost in the 5th Avenue laboratory fire of March 1895", "Orthodox Christians", "July 23, 1963", "twelve", "gain support from China for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda", "lay servants", "Hitler's secret police demanded to know if they were hiding a Jew in their house.\"", "Levi's Stadium", "Wednesdays", "over 17.5 million", "104 \u00b0F (40 \u00b0C)", "blue-green algae", "Acute oxygen toxicity (causing seizures, its most feared effect for divers)", "New England Patriots", "chemical bonds", "Harrods", "extra-legal ownership", "HSH Nordbank Arena", "The two-hour finale.", "Tuesday afternoon.", "Sunday", "The new star is next to the iconic Hollywood headquarters of Capitol Records,", "intent of justice.", "to share personal information.", "David Beckham", "strict interpretation of the law", "\"She was focused so much on learning that she didn't notice,\"", "three out of four", "Barack Obama", "right-wing extremist groups.", "The contraband cell phones were found behind bars or in transit to Texas inmates in 2008.", "The aviation industry", "Venus Williams", "an Italian car parts manufacturing company.", "April 2010.", "The document mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "Iran's parliament speaker", "It came after a period of years in which a number of match-fixing allegations were made,", "Tigris and Euphrates", "saying Chaudhary's death was warning to management.", "the liberty to come and go with her face uncovered", "Bangladesh", "forgery and flying without a valid license", "plastic surgery research", "Four Americans", "\"Purvis continued to choke [the student] and told him, 'Don't you ever mess with my car again'\"", "\"Slumdog Millionaire\"", "34 civilians", "The embassy has confirmed that the injured are being moved by military transport to Cairo, and has mobilized staff to identify and to assist those Americans involved,\"", "100 million", "in a basement", "6-2 6-1", "former U.S. secretary of state.", "Hope Alice Williams", "The standing rib roast, also known as prime rib, is a cut of beef from the Primal rib,", "Leo", "constellation of Orion", "George A. Romero", "50 best cities to live in.\"", "Whitehorse", "The Viga", "Tomorrowland"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6280931919787345}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9714285714285714, 1.0, 0.9032258064516129, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.7058823529411764, 0.0, 1.0, 1.0, 0.29629629629629634, 1.0, 0.4444444444444445, 1.0, 0.0, 0.23529411764705882, 0.0, 1.0, 0.0, 1.0, 0.9473684210526315, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.8, 0.6666666666666666, 0.1111111111111111, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5010", "mrqa_squad-validation-8452", "mrqa_squad-validation-6998", "mrqa_squad-validation-170", "mrqa_squad-validation-7162", "mrqa_squad-validation-3642", "mrqa_squad-validation-3599", "mrqa_squad-validation-7510", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-57", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1680", "mrqa_newsqa-validation-937", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-119", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-2907", "mrqa_naturalquestions-validation-3206", "mrqa_naturalquestions-validation-6074", "mrqa_triviaqa-validation-7252", "mrqa_searchqa-validation-12546"], "SR": 0.453125, "CSR": 0.638125, "EFR": 1.0, "Overall": 0.7426250000000001}, {"timecode": 25, "before_eval_results": {"predictions": ["more equality in the income distribution", "Lunar Module Pilot", "Apollo 11", "few", "linebacker", "unicellular organisms", "high wages", "evenly round the body", "ABC", "the solution", "He was shown to be physically imposing, an equal in stature to the secular German princes with whom he would join forces to spread Lutheranism.", "Sweden", "macrophages and lymphocytes", "14,000", "Tony Hawk", "C. J. Anderson", "phosphate (PO3\u22124) groups in the biologically important energy-carrying molecules ATP and ADP, in the backbone and the purines", "average workers", "Major", "1909", "psilocybin", "Scotland", "Raimond Gaita", "Andrew Preston", "the Haitian Revolution", "saloon-keeper and Justice of the Peace", "July", "Wayman Lawrence Tisdale", "Bath, Maine", "26,000", "1848", "The Dressmaker", "American musician, singer, songwriter and actor", "Dark Heresy", "North Holland", "Elena Verdugo", "1874 until 1994", "1698", "UFC 50  UFC 50: The War of '04", "a few", "1987", "Mwabvi river", "John M. Dowd", "Amber Heard", "the 2011 Pulitzer Prize in General Nonfiction", "Reverend Lovejoy", "1888-1954", "Patterns of Sexual Behavior", "George Raft", "UFC Fight Pass", "Washington, D.C.", "Nicole Kidman, Meryl Streep and Julianne Moore", "\"Odorama\"", "February 22, 1968", "Bourbon County", "Long Island", "Jacques Cousteau", "Bodhidharma", "Joe Hart", "eight Indians whom the rebels accused of collaborating with the Colombian government,", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "fjord", "Afghanistan", "1879"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6110870726495727}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307691, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.15384615384615383, 0.3076923076923077, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6453", "mrqa_squad-validation-7791", "mrqa_squad-validation-2599", "mrqa_squad-validation-3624", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1048", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-4935", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1381", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-5715", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-2681", "mrqa_naturalquestions-validation-5143", "mrqa_triviaqa-validation-310", "mrqa_newsqa-validation-1042", "mrqa_newsqa-validation-1848", "mrqa_searchqa-validation-4623", "mrqa_naturalquestions-validation-7387"], "SR": 0.53125, "CSR": 0.6340144230769231, "EFR": 1.0, "Overall": 0.7418028846153847}, {"timecode": 26, "before_eval_results": {"predictions": ["gender roles and customs", "80,000", "complexity classes", "9 March 1508", "reduce consumer costs", "2020", "their greatest common divisor is one", "Sophocles", "Jacksonville's low latitude", "1726", "environmental factors", "backing for the uprising", "around 28,000", "The Reconstruction of Religious Thought in Islam", "July", "Harvey Martin", "water pump", "state senators", "Pakistan's High Commission in India", "it", "nearly all of Britain's troops in Iraq", "Sharon Tate", "almost 100 vessels", "40", "ClimateCare, one of Europe's most experienced providers of carbon offsets,", "The apartment building collapsed", "he was one of 10 gunmen who attacked several targets in Mumbai", "the United States", "policing the world and Africa in particular", "the area of the 11th century Preah Vihear temple", "Monday night", "the 45-year-old future president", "about 12 million", "some of our competitors", "Iran", "his grandfather", "Tsvangirai", "in 1995", "Turkey", "President Obama", "piers Morgan", "an Omani national", "in East Java", "protective shoes", "Thaksin Shinawatra", "gasoline", "diplomatic relations", "auxiliary lock", "prostate cancer", "The ACLU", "fight back against Israel in Gaza", "the soldiers", "off the coast of Dubai", "cancer", "five minutes before commandos descended from ropes that dangled from helicopters", "benzodiazepines", "retinal ganglion cell axons", "cotton", "Samson", "Jack Ridley", "bioelectromagnetics", "peter", "p.m.", "tornado"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6076780273621306}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08695652173913045, 0.0, 0.0, 0.0, 0.8, 0.2857142857142857, 0.18181818181818182, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.625, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7213", "mrqa_squad-validation-7041", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-3716", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-1133", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-3346", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2349", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-1846", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1292", "mrqa_naturalquestions-validation-3368", "mrqa_hotpotqa-validation-2944", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-8943", "mrqa_searchqa-validation-11260"], "SR": 0.484375, "CSR": 0.6284722222222222, "EFR": 1.0, "Overall": 0.7406944444444445}, {"timecode": 27, "before_eval_results": {"predictions": ["Milka, Angelina and Marica", "the Working Group chairs", "nine nations", "Stanley Steamer", "nerves", "rediscovery of \"Christ and His salvation\"", "a program of coordinated, evolving projects sponsored by the National Science Foundation", "nervous breakdown", "NewcastleGateshead", "cholecalciferol", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "the government and the National Assembly and the Senate", "vocational subjects", "four nations", "an eight-year term", "Calendar for Fixing the Seasons", "Land\u2019s End", "Good Morning to All", "The Simpsons", "Brazil", "salmon", "Annabel Croft", "D.\u00a0W. Griffith", "fruits", "James Bond", "Chester", "skunk", "John Wayne", "the University of Cambridge", "Argentina", "a pink feminine car", "Nicole Brown Simpson", "\"Oriver Twist\"", "cello", "Morten Skovsby", "the Nile", "Newcastle", "the meadows", "Charlie Brown", "Joanne Harris", "algae", "At Last the 1948 Show", "parsnip", "Port", "\"World\u2019s Greatest Athlete\u201d", "turbine", "waterfowl", "Walter Hagen", "Dirty Dancing", "long-term", "apples, blueberries, bananas and more.", "Bank of England", "Texas", "Cahaba", "4 September 1936", "35 to 40 hours per week", "neuro-orthopaedic", "an American painter and writer who wrote the autobiography \"The Bite in the Apple\" about her relationship with Apple co-founder Steve Jobs\"", "assassinated", "Fargo, North Dakota,", "diabetes", "a slogan", "Douglas MacArthur", "flavonoids"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5747491839477726}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4285714285714285, 1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4846", "mrqa_squad-validation-6426", "mrqa_squad-validation-4404", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-3021", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-4339", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-1541", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-237", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-7193", "mrqa_hotpotqa-validation-260", "mrqa_newsqa-validation-847", "mrqa_searchqa-validation-16770", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-7657"], "SR": 0.53125, "CSR": 0.625, "EFR": 0.9666666666666667, "Overall": 0.7333333333333334}, {"timecode": 28, "before_eval_results": {"predictions": ["quantum electrodynamics", "Genghis Khan", "westerns and detective series", "one astronaut on a limited Earth orbital mission,", "NBC Blue Network", "Friday", "CD40", "\"Big Five\"", "high pressure", "\"exterminate\" all non-Dalek beings", "City council", "a Bachelor of Education", "Miocene", "Rhin", "in the kingdom", "James Bond", "Madagascar", "shoulders", "Muriel Spark", "Japan", "james Savile", "argument form", "eryl Hobson", "\"Comedy Playhouse\u201d", "leopons", "frog", "Verruckt", "o-s-c-a-are", "weather", "stew", "harold", "leonais", "bridge", "New Jersey", "fluorine", "the Northern line between Tottenham Court Road and Warren Street stations", "harold wilson", "Manchester", "bbc", "\"black\"", "Venezuela", "Derona Water", "saxophone", "\"It is legal to buy cannabis seeds in the UK?,", "star", "November 5, 2013", "Bosnian", "whale", "eagle", "floor-length", "Blind Beggar pub", "Vatican Crypt", "tiger", "woodbeard", "Andrew Garfield", "Gil - galad", "Delilah Rene", "Tianhe Stadium", "Guinea, Myanmar, Sudan and Venezuela", "all day starting at 10 a.m.", "Rent", "james bennett", "Washington", "Mineola"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5562297077922078}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true], "QA-F1": [1.0, 0.5714285714285715, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6352", "mrqa_squad-validation-3791", "mrqa_squad-validation-5906", "mrqa_squad-validation-7778", "mrqa_squad-validation-1996", "mrqa_triviaqa-validation-1109", "mrqa_triviaqa-validation-5563", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-364", "mrqa_triviaqa-validation-7662", "mrqa_triviaqa-validation-515", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-2087", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-4045", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7280", "mrqa_triviaqa-validation-506", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-5656", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-6264", "mrqa_triviaqa-validation-1848", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-2491", "mrqa_searchqa-validation-7462"], "SR": 0.46875, "CSR": 0.6196120689655172, "EFR": 0.9705882352941176, "Overall": 0.733040060851927}, {"timecode": 29, "before_eval_results": {"predictions": ["ten times their own weight", "nine", "a year", "2003", "Africa", "Nederrijn", "a lack of understanding of the legal ramifications, or due to a fear of seeming rude", "Elie Metchnikoff", "a global to a domestic scale", "50%", "7\u20134\u20132\u20133 system", "Connectional Table", "2nd century BCE", "1060s", "George Washington Carver", "\"The Pelican Brief\"", "panther", "Vietnam", "oscar", "marinara", "pannier", "dollar", "225 H.O.", "oven", "diphthong", "a standing committee on Agriculture", "VISICANA", "Austria", "oscar", "Seventy-six trombones", "red", "australia", "ER", "crustacean", "chicken run", "Robert Bork", "Luzon", "vainberg", "The Big Sleep", "quid pro quo", "bismarck", "white Cliffs of Dover", "Prague", "notes", "vanya", "Harlem", "Yitzhak Rabin", "Quiz", "oscar", "otto", "bumblebee", "ermine", "Demi Moore", "oven", "central plains", "49", "a trillionth of a trillionths of a second in age", "higgins", "southern Jasper County", "Jenji Kohan", "public opinion", "a monthly allowance", "Via Vai", "Black Abbots"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6182291666666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7537", "mrqa_squad-validation-8490", "mrqa_searchqa-validation-13985", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-8718", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-2393", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-1650", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-14802", "mrqa_searchqa-validation-16521", "mrqa_searchqa-validation-893", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-14246", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-14606", "mrqa_searchqa-validation-1761", "mrqa_searchqa-validation-11380", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-6317", "mrqa_triviaqa-validation-834", "mrqa_triviaqa-validation-1871", "mrqa_hotpotqa-validation-1415", "mrqa_newsqa-validation-3833"], "SR": 0.5625, "CSR": 0.6177083333333333, "EFR": 1.0, "Overall": 0.7385416666666667}, {"timecode": 30, "UKR": 0.767578125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1087", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-119", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1440", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-2140", "mrqa_hotpotqa-validation-227", "mrqa_hotpotqa-validation-2293", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-243", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3502", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-3861", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3966", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4419", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-4699", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-4974", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5247", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5478", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5729", "mrqa_hotpotqa-validation-5745", "mrqa_hotpotqa-validation-5896", "mrqa_hotpotqa-validation-635", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-774", "mrqa_hotpotqa-validation-88", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-965", "mrqa_naturalquestions-validation-10162", "mrqa_naturalquestions-validation-10406", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1549", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-3390", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4250", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-8696", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9986", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1162", "mrqa_newsqa-validation-1214", "mrqa_newsqa-validation-1552", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-1584", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2462", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2545", "mrqa_newsqa-validation-2549", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2847", "mrqa_newsqa-validation-2912", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3001", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3027", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3183", "mrqa_newsqa-validation-3267", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3400", "mrqa_newsqa-validation-3412", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-370", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-400", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-498", "mrqa_newsqa-validation-542", "mrqa_newsqa-validation-696", "mrqa_newsqa-validation-804", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-868", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11799", "mrqa_searchqa-validation-1243", "mrqa_searchqa-validation-12558", "mrqa_searchqa-validation-12609", "mrqa_searchqa-validation-12771", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13966", "mrqa_searchqa-validation-14126", "mrqa_searchqa-validation-14373", "mrqa_searchqa-validation-14606", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-16371", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-320", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-3841", "mrqa_searchqa-validation-4175", "mrqa_searchqa-validation-4619", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5392", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-7385", "mrqa_searchqa-validation-7560", "mrqa_searchqa-validation-7611", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7920", "mrqa_searchqa-validation-801", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-917", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9604", "mrqa_squad-validation-10009", "mrqa_squad-validation-10018", "mrqa_squad-validation-10025", "mrqa_squad-validation-10027", "mrqa_squad-validation-10064", "mrqa_squad-validation-101", "mrqa_squad-validation-10264", "mrqa_squad-validation-10294", "mrqa_squad-validation-10370", "mrqa_squad-validation-10474", "mrqa_squad-validation-10491", "mrqa_squad-validation-10500", "mrqa_squad-validation-1068", "mrqa_squad-validation-1081", "mrqa_squad-validation-1148", "mrqa_squad-validation-1156", "mrqa_squad-validation-1272", "mrqa_squad-validation-1273", "mrqa_squad-validation-1355", "mrqa_squad-validation-1371", "mrqa_squad-validation-1550", "mrqa_squad-validation-159", "mrqa_squad-validation-167", "mrqa_squad-validation-1684", "mrqa_squad-validation-1758", "mrqa_squad-validation-1769", "mrqa_squad-validation-1889", "mrqa_squad-validation-195", "mrqa_squad-validation-1977", "mrqa_squad-validation-1980", "mrqa_squad-validation-1996", "mrqa_squad-validation-2140", "mrqa_squad-validation-2146", "mrqa_squad-validation-2169", "mrqa_squad-validation-2191", "mrqa_squad-validation-2329", "mrqa_squad-validation-237", "mrqa_squad-validation-2411", "mrqa_squad-validation-2423", "mrqa_squad-validation-2453", "mrqa_squad-validation-2461", "mrqa_squad-validation-2467", "mrqa_squad-validation-2468", "mrqa_squad-validation-2473", "mrqa_squad-validation-2523", "mrqa_squad-validation-2540", "mrqa_squad-validation-256", "mrqa_squad-validation-258", "mrqa_squad-validation-2616", "mrqa_squad-validation-2634", "mrqa_squad-validation-2709", "mrqa_squad-validation-2778", "mrqa_squad-validation-2834", "mrqa_squad-validation-2885", "mrqa_squad-validation-2900", "mrqa_squad-validation-2909", "mrqa_squad-validation-2994", "mrqa_squad-validation-3028", "mrqa_squad-validation-3048", "mrqa_squad-validation-3063", "mrqa_squad-validation-3081", "mrqa_squad-validation-3125", "mrqa_squad-validation-3165", "mrqa_squad-validation-3166", "mrqa_squad-validation-317", "mrqa_squad-validation-318", "mrqa_squad-validation-319", "mrqa_squad-validation-32", "mrqa_squad-validation-3207", "mrqa_squad-validation-3233", "mrqa_squad-validation-3264", "mrqa_squad-validation-3267", "mrqa_squad-validation-327", "mrqa_squad-validation-3403", "mrqa_squad-validation-343", "mrqa_squad-validation-3469", "mrqa_squad-validation-3522", "mrqa_squad-validation-3641", "mrqa_squad-validation-3740", "mrqa_squad-validation-3751", "mrqa_squad-validation-3791", "mrqa_squad-validation-3837", "mrqa_squad-validation-3860", "mrqa_squad-validation-3921", "mrqa_squad-validation-4080", "mrqa_squad-validation-4122", "mrqa_squad-validation-4144", "mrqa_squad-validation-415", "mrqa_squad-validation-4267", "mrqa_squad-validation-4303", "mrqa_squad-validation-4325", "mrqa_squad-validation-4366", "mrqa_squad-validation-4429", "mrqa_squad-validation-4437", "mrqa_squad-validation-4534", "mrqa_squad-validation-4557", "mrqa_squad-validation-4662", "mrqa_squad-validation-4670", "mrqa_squad-validation-472", "mrqa_squad-validation-4795", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-4958", "mrqa_squad-validation-5003", "mrqa_squad-validation-5010", "mrqa_squad-validation-5054", "mrqa_squad-validation-5077", "mrqa_squad-validation-5078", "mrqa_squad-validation-5134", "mrqa_squad-validation-5162", "mrqa_squad-validation-5179", "mrqa_squad-validation-5185", "mrqa_squad-validation-5205", "mrqa_squad-validation-5256", "mrqa_squad-validation-5319", "mrqa_squad-validation-5351", "mrqa_squad-validation-5382", "mrqa_squad-validation-5400", "mrqa_squad-validation-5457", "mrqa_squad-validation-5473", "mrqa_squad-validation-5474", "mrqa_squad-validation-5557", "mrqa_squad-validation-5600", "mrqa_squad-validation-5607", "mrqa_squad-validation-5609", "mrqa_squad-validation-5611", "mrqa_squad-validation-5672", "mrqa_squad-validation-5684", "mrqa_squad-validation-5754", "mrqa_squad-validation-5760", "mrqa_squad-validation-5840", "mrqa_squad-validation-5906", "mrqa_squad-validation-5939", "mrqa_squad-validation-5948", "mrqa_squad-validation-596", "mrqa_squad-validation-5966", "mrqa_squad-validation-6024", "mrqa_squad-validation-604", "mrqa_squad-validation-6086", "mrqa_squad-validation-6142", "mrqa_squad-validation-6161", "mrqa_squad-validation-6224", "mrqa_squad-validation-6227", "mrqa_squad-validation-6254", "mrqa_squad-validation-6268", "mrqa_squad-validation-6278", "mrqa_squad-validation-6284", "mrqa_squad-validation-6292", "mrqa_squad-validation-6349", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6380", "mrqa_squad-validation-6415", "mrqa_squad-validation-643", "mrqa_squad-validation-6434", "mrqa_squad-validation-6447", "mrqa_squad-validation-6457", "mrqa_squad-validation-6474", "mrqa_squad-validation-6492", "mrqa_squad-validation-654", "mrqa_squad-validation-6622", "mrqa_squad-validation-6624", "mrqa_squad-validation-6650", "mrqa_squad-validation-6744", "mrqa_squad-validation-6766", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6807", "mrqa_squad-validation-6812", "mrqa_squad-validation-6879", "mrqa_squad-validation-6941", "mrqa_squad-validation-7005", "mrqa_squad-validation-7026", "mrqa_squad-validation-7049", "mrqa_squad-validation-7051", "mrqa_squad-validation-708", "mrqa_squad-validation-7162", "mrqa_squad-validation-7170", "mrqa_squad-validation-7198", "mrqa_squad-validation-7208", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7260", "mrqa_squad-validation-7263", "mrqa_squad-validation-7284", "mrqa_squad-validation-7370", "mrqa_squad-validation-7387", "mrqa_squad-validation-7400", "mrqa_squad-validation-7448", "mrqa_squad-validation-7458", "mrqa_squad-validation-7492", "mrqa_squad-validation-7500", "mrqa_squad-validation-7508", "mrqa_squad-validation-7550", "mrqa_squad-validation-7562", "mrqa_squad-validation-7665", "mrqa_squad-validation-767", "mrqa_squad-validation-7794", "mrqa_squad-validation-7851", "mrqa_squad-validation-7859", "mrqa_squad-validation-7934", "mrqa_squad-validation-7936", "mrqa_squad-validation-7949", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8123", "mrqa_squad-validation-8171", "mrqa_squad-validation-8180", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8234", "mrqa_squad-validation-8289", "mrqa_squad-validation-83", "mrqa_squad-validation-8319", "mrqa_squad-validation-8325", "mrqa_squad-validation-8338", "mrqa_squad-validation-8394", "mrqa_squad-validation-8418", "mrqa_squad-validation-8428", "mrqa_squad-validation-8476", "mrqa_squad-validation-8478", "mrqa_squad-validation-8527", "mrqa_squad-validation-8599", "mrqa_squad-validation-862", "mrqa_squad-validation-8652", "mrqa_squad-validation-8664", "mrqa_squad-validation-8723", "mrqa_squad-validation-8724", "mrqa_squad-validation-8771", "mrqa_squad-validation-8839", "mrqa_squad-validation-8840", "mrqa_squad-validation-8902", "mrqa_squad-validation-9020", "mrqa_squad-validation-9036", "mrqa_squad-validation-9243", "mrqa_squad-validation-9247", "mrqa_squad-validation-9247", "mrqa_squad-validation-9486", "mrqa_squad-validation-9492", "mrqa_squad-validation-9504", "mrqa_squad-validation-9653", "mrqa_squad-validation-9678", "mrqa_squad-validation-9682", "mrqa_squad-validation-9761", "mrqa_squad-validation-9762", "mrqa_squad-validation-9770", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-999", "mrqa_squad-validation-9995", "mrqa_triviaqa-validation-1111", "mrqa_triviaqa-validation-1159", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-122", "mrqa_triviaqa-validation-1363", "mrqa_triviaqa-validation-1413", "mrqa_triviaqa-validation-1494", "mrqa_triviaqa-validation-1538", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1741", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1977", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2376", "mrqa_triviaqa-validation-2618", "mrqa_triviaqa-validation-2667", "mrqa_triviaqa-validation-2693", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-310", "mrqa_triviaqa-validation-3140", "mrqa_triviaqa-validation-3267", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3471", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3501", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3620", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3799", "mrqa_triviaqa-validation-3881", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-399", "mrqa_triviaqa-validation-4045", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4590", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5318", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5571", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5915", "mrqa_triviaqa-validation-5990", "mrqa_triviaqa-validation-6006", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6110", "mrqa_triviaqa-validation-6244", "mrqa_triviaqa-validation-6312", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6666", "mrqa_triviaqa-validation-6675", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6850", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-727", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-7375", "mrqa_triviaqa-validation-7675", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-791", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-987"], "OKR": 0.8671875, "KG": 0.47734375, "before_eval_results": {"predictions": ["mid-Eocene", "2004", "the algorithm deciding this problem returns the answer yes", "their disciples", "data sampling is biased away from the center of the Amazon basin", "downward", "Westinghouse Electric", "captured Fort Beaus\u00e9jour in June 1755", "Saxon chancellery", "Advanced Steam movement", "July 23, 1963", "Stanford University", "Florida", "Spanish power in North America meant the disappearance of a strong ally and counterweight to British expansion,", "a dot", "Elijah Muhammad", "bamboo", "Benjamin Franklin", "Guatemala", "Moscow", "Good Times", "Mount Hood", "The Backstreet Boys", "the ball's progress", "Red", "a magnetic compass", "the Black Maria", "Walt Kelly", "the evaporator", "the QWERTY keyboard", "kozo", "The Trial", "Raytheon", "red", "a burn", "Thomas Hardy", "Signs", "(Revere)", "water", "cream", "adultery", "Sunday", "Syracuse", "(Mary) Biddle", "a commissioned officer", "the breath", "Toulouse", "dualism", "(Mary) Griffith", "French Guiana", "a Dutchboy", "a fruitcake", "viola", "beautiful black and white", "Bill Irwin", "George Harrison", "perfume", "every year", "1999 Odisha cyclone", "15,000 people", "two remaining crew members", "Cameroon", "any records showing that Barlow and the girl were married and any evidence of them having a child.", "the Indian Ocean"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5527173913043478}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.17391304347826086, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1655", "mrqa_squad-validation-4390", "mrqa_squad-validation-10273", "mrqa_squad-validation-10300", "mrqa_squad-validation-10304", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-14508", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-4646", "mrqa_searchqa-validation-11972", "mrqa_searchqa-validation-7106", "mrqa_searchqa-validation-12239", "mrqa_searchqa-validation-1561", "mrqa_searchqa-validation-10370", "mrqa_searchqa-validation-5735", "mrqa_searchqa-validation-13224", "mrqa_searchqa-validation-11367", "mrqa_searchqa-validation-14007", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-3933", "mrqa_searchqa-validation-5043", "mrqa_searchqa-validation-9552", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-4605", "mrqa_searchqa-validation-3782", "mrqa_searchqa-validation-2901", "mrqa_searchqa-validation-8256", "mrqa_searchqa-validation-6586", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-929", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-2076"], "SR": 0.484375, "CSR": 0.6134072580645161, "EFR": 1.0, "Overall": 0.7451033266129032}, {"timecode": 31, "before_eval_results": {"predictions": ["Maria Fold and Thrust Belt", "DVB-S2 standard", "E.I. du Pont", "a shortage of male teachers.", "water", "John D. Rockefeller", "waste", "his own men", "Gaelic", "New Orleans", "six", "half as much", "John Pell, Lord of Pelham Manor", "Madonna", "a doses", "Treasure Island", "San Antonio", "Chicago", "Hindu funeral", "Chicago", "Close Encounters of the Third Kind", "malaria", "Chicago", "Louis XV", "Shakira", "Yokohama", "Chicago", "Samuel Morse", "the highest mountain in Venezuela", "Pete Rose", "Hannibal", "Mercury", "Naples", "Iran", "Oahu", "Chicago", "Lincoln", "Chicago", "Blue Nile", "insulin", "Newt Gingrich", "Alexander Hamilton", "Colossus of Rhodes", "Quisp Cereal", "Nepal", "Ernest Hemingway", "Stephen Hawking", "center", "gurus", "a Lamb Souvlaki", "Los Angeles", "the bumblebee", "FDR", "Faubourg Saint- Honoror", "season two", "spectroscopic notation", "Switzerland", "Keeping theBallRolling", "Giuseppe Fortunino Francesco Verdi", "South Africa", "military treatment of al-Qahtani,\"", "\"brain hacking\"", "the heads of federal executive departments who form the Cabinet of the United States", "it can refer to either peace between two entities ( especially between man and God or between two countries )"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5985503740970073}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.4, 1.0, 0.23529411764705882, 1.0, 0.0, 0.10526315789473684]}}, "before_error_ids": ["mrqa_squad-validation-2668", "mrqa_squad-validation-4528", "mrqa_squad-validation-6128", "mrqa_squad-validation-341", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-460", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-10333", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-12500", "mrqa_searchqa-validation-3737", "mrqa_searchqa-validation-6966", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-4343", "mrqa_searchqa-validation-15563", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-16250", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-11452", "mrqa_naturalquestions-validation-585", "mrqa_triviaqa-validation-902", "mrqa_hotpotqa-validation-3953", "mrqa_newsqa-validation-3818", "mrqa_naturalquestions-validation-8982", "mrqa_naturalquestions-validation-645"], "SR": 0.515625, "CSR": 0.6103515625, "EFR": 1.0, "Overall": 0.7444921875}, {"timecode": 32, "before_eval_results": {"predictions": ["O2", "one of the first peer-to-peer network architectures", "the Kenyan Coast", "SyFy", "1530", "the temperance movement", "1963", "18", "British troops", "Harvard Yard", "all war", "Mongol peace", "the pancreas", "red", "rani", "endive", "the 18th century", "Flamenco", "Nero", "the Old Manse", "Kenny", "Rio de Janeiro", "the earth", "Tonto", "Helen Hayes", "a Bible", "Harald", "Ma Barker", "the St. Valentine's Day Massacre", "Ratatouille", "Spain", "Louis C. Tiffany", "the pig", "Jericho", "John Cabot", "George Orwell", "Friedrich Nietzsche", "eggs", "A Christmas Story", "Raymond", "Hawaii", "rhythmic", "a falcon", "a prayer", "the Hospitallers", "Aragorn", "(Mary) O'Keeffe", "Athens", "ROXANNE", "the Jefferson family", "(Oxmedo)", "trans fat", "Caspar Weinberger", "the Ninja Turtles", "Amerigo Vespucci", "1979", "an eclipse", "Kept thinkin I could never live without you by my side", "Girls' Generation", "Guangzhou, China", "Oxygen", "\"The oceans are kind of the last frontier for use and development,\"", "$627", "blew up an ice jam"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6858825051759835}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 0.08695652173913045, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3457", "mrqa_squad-validation-4676", "mrqa_searchqa-validation-13674", "mrqa_searchqa-validation-7410", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-4138", "mrqa_searchqa-validation-11603", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-1105", "mrqa_searchqa-validation-11804", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-10006", "mrqa_searchqa-validation-2889", "mrqa_searchqa-validation-12476", "mrqa_searchqa-validation-16241", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-13887", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-7715", "mrqa_searchqa-validation-16405", "mrqa_searchqa-validation-6217", "mrqa_triviaqa-validation-6912", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-3453"], "SR": 0.59375, "CSR": 0.6098484848484849, "EFR": 1.0, "Overall": 0.744391571969697}, {"timecode": 33, "before_eval_results": {"predictions": ["Colony of Victoria Act 1855", "853,382", "Cestum veneris", "the traditional Mongolian aristocracy", "nonfunctional pseudogenes", "Northern Europe and the Mid-Atlantic temperate climate", "to coordinate the response to the embargo", "Pliocene", "Apollo Program Director", "yellow fever outbreaks", "thylakoid network", "the phytoplankton (planktonic plants)", "Mars", "Lake Nyasaland", "regal", "Michael Holding", "Manfred von Richthofen", "( Frankie) Laine", "Backyard Kerplunk Game (The goal is to get out the sticks without any balls falling out)", "the Archive of American Folk Song,", "90", "Marsupials", "Florence", "Rodgers and Hammerstein", "Washington", "plants that will do well with the sun, soil, and water", "South africa", "British Airways", "Hippety Hopper", "relationship with each other,", "three", "Peter Paul Rubens", "Goran Ivanisevic", "potatoes", "June 14th", "Judy Garland", "Mel Brooks", "Han Solo", "Tina Turner", "Angevin", "Hydrogen", "Tesla", "magnesium, magnesium, iron, zinc, molybdenum, and selenium", "David Frost", "Charlie Chaplin", "Edward VIII", "cardamom", "The Siberian tiger", "Sousa", "New Zealand", "the giraffe", "the Crusades", "(Sue)", "big house", "Kevin Garnett", "1980", "Led Zeppelin", "Marco Hietala", "Zed", "$60 billion", "Best Children's Hospitals", "the 1930s", "Portugal", "5 liters"], "metric_results": {"EM": 0.578125, "QA-F1": 0.656547619047619}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.2857142857142857]}}, "before_error_ids": ["mrqa_squad-validation-7151", "mrqa_squad-validation-9869", "mrqa_squad-validation-4319", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5689", "mrqa_triviaqa-validation-1261", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-2686", "mrqa_triviaqa-validation-4608", "mrqa_triviaqa-validation-3543", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3831", "mrqa_triviaqa-validation-1265", "mrqa_triviaqa-validation-6336", "mrqa_triviaqa-validation-5914", "mrqa_triviaqa-validation-2241", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-3527", "mrqa_naturalquestions-validation-2008", "mrqa_naturalquestions-validation-2732", "mrqa_hotpotqa-validation-2711", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-15800", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-4054"], "SR": 0.578125, "CSR": 0.6089154411764706, "EFR": 0.9629629629629629, "Overall": 0.7367975558278868}, {"timecode": 34, "before_eval_results": {"predictions": ["powerful", "typhoon", "municipal building inspector", "Battle of Hastings", "gold", "Victoria", "western European", "Muslim and Chinese siege engines", "four", "the town council", "tea or porridge", "tell them the truth about why you broke up!", "January 24, 2006", "William Shakespeare", "Steve Jobs", "The Da Vinci Code", "the Russian air force,", "10 below", "18th", "byproducts emitted during the process of burning and melting raw materials.", "the Screening Room", "help women \" learn how to dance and feel sexy,\"", "misdemeanor assault", "wings", "Sub-Saharan Africa", "in a tenement in the Mumbai suburb of Chembur,", "john williams", "New Haven firefighter", "left Brooklyn, New York, for Miami Beach, Florida,", "he spent the first night in his car.\"", "More than 100 soldiers", "mpire of the Sun", "genocide", "Susan Atkins", "she felt good for me to talk about her,\"", "\"The argument got heated, and Murray demanded he leave,\"", "Alaska or Hawaii", "South Africa", "burned over 65 percent of his body after being set on fire,", "attempted murder", "Intensifying violence, food shortages and widespread drought", "Kerstin Fritzl", "India", "Hakeemullah Mehsud", "LEDs", "the complexity of the human body, its anatomy, and the importance of leading a healthy way of life.\"", "Cambodian territory", "saturn", "red", "more than 2.5 million copies,", "support for same-sex civil unions,", "Hezbollah", "venus will release the final results of its 30-day test drilling operation in the South Atlantic archipelago", "his brother, who died in action in the United States Army", "Kristine Smirnoff", "lyonesse", "football", "Clovis I", "Willis (Sears) Tower", "First Amendment", "concierge", "nixon", "nikola", "june"], "metric_results": {"EM": 0.359375, "QA-F1": 0.494628081622325}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.8750000000000001, 0.8, 1.0, 0.0, 0.923076923076923, 0.0, 0.8571428571428571, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.16, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 0.10526315789473682, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6968", "mrqa_squad-validation-8219", "mrqa_squad-validation-6253", "mrqa_squad-validation-8517", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1413", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-1336", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-3276", "mrqa_newsqa-validation-3660", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-3889", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-1783", "mrqa_triviaqa-validation-2930", "mrqa_hotpotqa-validation-721", "mrqa_hotpotqa-validation-1674", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-5716", "mrqa_triviaqa-validation-7531"], "SR": 0.359375, "CSR": 0.6017857142857144, "EFR": 1.0, "Overall": 0.742779017857143}, {"timecode": 35, "before_eval_results": {"predictions": ["4,222,000", "1271", "17", "Falls", "phlogiston", "ENR", "single-tape Turing machines", "Pitt", "fixed annual carriage fees of \u00a330m for the channels with both channel suppliers able to secure additional capped payments if their channels meet certain performance-related targets", "On the Councils and the Church", "Honorary freemen", "Congress", "he didn't know if Woods' wife, Elin Nordegren, would appear with her husband.", "Sri Lanka's Tamil rebels", "daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.\"", "passengers on the Miva Marmara", "37th", "Liverpool Street Station heading for Aldgate East.", "black and Hispanic students", "soldiers have driven the terrorists and extremists from many strongholds in Iraq.", "one Iraqi soldier, a U.S. military spokesman said.", "Department of Homeland Security Secretary Janet Napolitano", "glamour and hedonism", "This will be the second time since the 1990s that the army has been sent in to combat Mafia crime in southern Italy,", "Math teacher Mawise Gumba", "Former U.S. soldier Steven Green", "78,000 parents of children ages 3 to 17.", "March 22,", "Afghanistan and India", "iPhone 4S", "we seek a new way forward, based on mutual interest and mutual respect.", "two", "Joe Jackson's request \"will be considered as are all requests for money from Michael's estate,\"", "environmental", "Little Rock Nine", "NATO's International Security Assistance Force", "U.S. senators who couldn't resist taking the vehicles for a spin.\"", "summer", "Bob Johnson", "Kim Il Sung", "137", "$250,000", "Communist Party of Nepal (Unified Marxist-Leninist)", "Diego Maradona", "April 22.", "She said Cain suggested meeting over dinner, then tried to reach up her skirt after the meal -- and when she protested, he told her, \"You want a job, right?\"", "Unseeded Frenchwoman", "MS Columbus", "California-based Current TV", "Elena Kagan", "Rambosk", "15-month investigation, at least a part of which was conducted undercover.\"", "a president who understands the world today, the future we seek and the change we", "carrying an amino acid to the protein synthetic machinery of a cell ( ribosome ) as directed by a three - nucleotide sequence ( codon ) in a messenger RNA ( mRNA )", "1800", "New Zealand artist Nicholas Garland", "james chadwick", "Patrick Dempsey", "Kristy Lee Cook", "Clint Eastwood", "Italy", "an all-female a cappella singing group", "General Motors", "Doctor of Philosophy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6407029112192155}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.782608695652174, 0.0, 0.0, 0.6, 0.28571428571428575, 0.0, 0.25, 0.4444444444444445, 0.375, 0.09523809523809523, 0.6666666666666666, 0.5714285714285715, 0.875, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-2837", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1788", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-2642", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-2929", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-3871", "mrqa_newsqa-validation-234", "mrqa_triviaqa-validation-4532", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-5297"], "SR": 0.53125, "CSR": 0.5998263888888888, "EFR": 1.0, "Overall": 0.7423871527777778}, {"timecode": 36, "before_eval_results": {"predictions": ["new form", "Greg Brady", "the mid-sixties", "less than a year", "the time complexity", "the 7th century,", "William Tyndale", "Henry Cole", "released Islamists from prison and welcomed home exiles in tacit exchange for political support in his struggle against leftists.", "military action,", "mixed martial arts", "Rick and Morty", "Ringo Starr", "Lord Byron", "Resorts World Genting", "World of Wonder", "Vernier, Switzerland", "the Ruul", "Nebula Award, the Philip K. Dick Award, and the Hugo Award", "United States and Canada", "musician", "Paris Motor Show", "Jon Hamm", "Abdul Razzak Yaqoob", "Vilnius", "Mr. Tumnus", "elderships", "Gerard Marenghi", "Bolton, England", "Conservative", "1966", "Amundsen Sea.", "Liverpool and England international player", "romantic comedy", "Mani", "Tunisian", "people working in film and the performing arts,", "Christian Duguay", "antigua & Barbuda, Argentina, South Africa,", "Boston, Providence, Hartford, New York City, Philadelphia, Wilmington, Baltimore, and Washington, D.C.", "Bob Mould", "the German nuclear weapon project", "electric currents and magnetic fields", "Oklahoma Sooners", "books, films and other", "Galway", "Razor Ramon", "illnesses", "Russia", "southern (Dolomitic) Alps", "Bisexuality", "400 MW", "Germany", "allows the fuel pressure to be controlled via pulse - width modulation of the pump voltage", "International Baccalaureate", "a pepper", "Eva Per\u00f3n", "15", "fill more than 1 million", "the Rio Grande", "jai-alai", "Michael Jackson and Lionel Richie", "George Halas", "Florida"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6379381613756614}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.888888888888889, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7000000000000001, 1.0, 0.6666666666666666, 0.0, 0.5, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1701", "mrqa_squad-validation-9567", "mrqa_hotpotqa-validation-5254", "mrqa_hotpotqa-validation-4963", "mrqa_hotpotqa-validation-1059", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-3409", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-820", "mrqa_hotpotqa-validation-4473", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1283", "mrqa_naturalquestions-validation-6148", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-6179", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3460", "mrqa_searchqa-validation-6003", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-1223", "mrqa_naturalquestions-validation-5468"], "SR": 0.515625, "CSR": 0.5975506756756757, "EFR": 0.967741935483871, "Overall": 0.7354803972319093}, {"timecode": 37, "before_eval_results": {"predictions": ["not having a residence permit", "Torchwood (2006\u20132011)", "p-adic norm", "sedimentary", "Doritos", "30", "the dinophyte nucleus", "six", "Blum complexity axioms", "electromagnetic force", "15,000 people", "Boston, Massachusetts", "Mudvayne", "\"Aloha \u02bbOe\"", "Jeffrey Adam \"Duff\" Goldman (born December 17, 1974)", "69.7 million litres", "Sir Charles Benedict Ainslie, CBE (born 5 February 1977)", "an English Grand Prix motorcycle road racer and Formula One driver", "Kait Parker", "Revolution Studios", "Lord's Resistance Movement", "June 26, 1970", "Barbara Bush (n\u00e9e Pierce)", "2008", "Cookstown", "1978", "Colonel", "Giuseppe Verdi", "madkel S. Eriksen, Tor Erik Hermansen, Saul Milton, Will Kennard, James Fauntleroy II, Takura Tendayi, and Rihanna herself.", "North Carolina", "Love", "Arthur Freed", "the Cuban Revolution", "the lead roles of Timmy Sanders and Jack in the series \"Granite Flats\"", "Maine", "Oklahoma Sooners", "the University College of North Staffordshire", "more than 230", "Northern Irish", "since 1736", "Joachim Trier", "Barbara Lee Alexander", "Washington, D.C.", "Andrzej Go\u0142ota", "Patrick Dempsey", "Flaw", "former president of Guggenheim Partners", "Derry City F.C.", "My Gorgeous Life", "67,038", "\"Nana\" Patekar", "KBS2", "2005", "Jack Gleeson ( born 20 May 1992 )", "the 18th century", "the Thomas Cup", "Caernarfon", "Barack Obama", "Ben Freeth", "San Antonio", "the opera", "Robert Mugabe", "More than 150,000", "around 3.5 percent of global greenhouse emissions."], "metric_results": {"EM": 0.59375, "QA-F1": 0.6888512529137529}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.1818181818181818, 0.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.5, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7626", "mrqa_squad-validation-8958", "mrqa_squad-validation-10483", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2185", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2855", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4196", "mrqa_hotpotqa-validation-87", "mrqa_hotpotqa-validation-923", "mrqa_hotpotqa-validation-2698", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-5052", "mrqa_naturalquestions-validation-6359", "mrqa_triviaqa-validation-5021", "mrqa_newsqa-validation-3529", "mrqa_searchqa-validation-11958", "mrqa_newsqa-validation-3393"], "SR": 0.59375, "CSR": 0.5974506578947368, "EFR": 1.0, "Overall": 0.7419120065789474}, {"timecode": 38, "before_eval_results": {"predictions": ["deforestation", "21 to 11", "a commune (gmina)", "lower lake", "the Yuan dynasty", "a quantity surveyor", "Squillace", "chloroplasts", "the wedding banquet", "thermal shielding material", "David Yates", "an American chef, author and television personality", "an Roman Empire", "265 million", "the County of York", "Lev Ivanovich Yashin", "Germanicus", "Johnny Galecki", "Edmonton, Alberta", "Brooklyn", "Symphony No. 7", "a working-class young man who spends his weekends dancing and drinking at a local Brooklyn discoth\u00e8que", "Indianapolis Motor Speedway", "Central University of India, Aligarh Muslim University", "Boxing Day, 2004", "an additional conservation law for total energy, of which kinetic energy of motion is one element", "\"Barney Miller\"", "Daniel Andre Sturridge", "horror", "Philippe of Belgium", "Oded Fehr", "Hans Rosenfeldt", "January 15, 1975", "\"I Am Furious (Yellow)\"", "Nikolai Trubetzkoy", "books, films and other media", "1770", "John Meston", "Tottenham", "Operation Neptune", "\"The Future\"", "Ready Player One", "Cecily Legler Strong", "\"My Own Worst Enemy\"", "Gillian Leigh Anderson", "SKUM", "Tom Ewell", "the National Mall in Washington, D.C., across from the Washington Monument", "1976", "James Knox Polk", "the Sun", "1987", "Thorgan Ganael Francis Hazard", "quarterback", "hairpin turn", "flat oven-baked Italian bread product", "ABBA", "Fareed Zakaria", "use of torture and indefinite detention", "Indiana", "Nixon's", "Vice President", "May 30, 2017", "Mahatma Gandhi"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7167106331168831}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7272727272727273, 0.28571428571428575, 0.0, 0.6666666666666666, 0.8, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-973", "mrqa_squad-validation-1028", "mrqa_squad-validation-8792", "mrqa_hotpotqa-validation-188", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-4024", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3973", "mrqa_hotpotqa-validation-4563", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-304", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-1848", "mrqa_hotpotqa-validation-3603", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4616", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-2914", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-9979", "mrqa_triviaqa-validation-3529", "mrqa_searchqa-validation-3074", "mrqa_naturalquestions-validation-5537"], "SR": 0.578125, "CSR": 0.5969551282051282, "EFR": 1.0, "Overall": 0.7418129006410257}, {"timecode": 39, "before_eval_results": {"predictions": ["11\u201313th century AD", "75%", "the defects justifying rebellion must be much more serious than those justifying disobedience", "Sonderungsverbot", "1543", "Edward the Confessor", "July 2014", "Warszawa", "polytechnics became new universities", "the plague", "a Zen monastery", "George Jones", "an arch", "Life of Pi", "the Circus", "Nixon", "the endangered pacarana", "Lorna Luft", "Jeop Study Set XI Flashcards", "the Chicago Cubs", "the Peashooter", "the Tame", "August Wilson", "accordion", "the Volkswagen Passat", "Mad About You", "a promissory note", "James Patterson", "Great Britain", "Luciano Pavarotti", "Maria Sharapova", "Daniel Defoe", "\"The Secrets of a Fire King\"", "floats", "the Industrial Canal", "Dragnet", "the Labyrinth of Crete", "the neurological exammuscle", "a whitener", "the Beatles", "sculptor Leonard Craske", "the Bordeaux region of France", "the United States dollar", "a waistcoat", "a worthless thing or endeavor", "Tila Tequila", "Ashton Kutcher", "Emancipation Proclamation", "the pottery industry", "Billy Joel", "the Lintons", "the Caribbean Sea", "the Edict of Nantes", "1995", "Superintendent Dave Kelly", "Leeds", "Angostura bitters", "pop music and popular culture", "4,530", "100", "in a hotel near Fort Bragg", "Obeth Kandjoze", "New England ( Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, and Vermont )", "dorsally on the forearm"], "metric_results": {"EM": 0.375, "QA-F1": 0.45884415064102557}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.125, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6, 0.0, 0.3076923076923077, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6794", "mrqa_squad-validation-7136", "mrqa_squad-validation-2684", "mrqa_searchqa-validation-4961", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-4136", "mrqa_searchqa-validation-4117", "mrqa_searchqa-validation-14709", "mrqa_searchqa-validation-13399", "mrqa_searchqa-validation-7504", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-116", "mrqa_searchqa-validation-13589", "mrqa_searchqa-validation-12341", "mrqa_searchqa-validation-3436", "mrqa_searchqa-validation-16699", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-16623", "mrqa_searchqa-validation-7297", "mrqa_searchqa-validation-2218", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-14815", "mrqa_searchqa-validation-7777", "mrqa_searchqa-validation-4513", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-11152", "mrqa_searchqa-validation-2794", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-4986", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14037", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-8851", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-1391", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-4351"], "SR": 0.375, "CSR": 0.59140625, "EFR": 1.0, "Overall": 0.740703125}, {"timecode": 40, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1063", "mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1509", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2298", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3163", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3630", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4024", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4987", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5037", "mrqa_hotpotqa-validation-5052", "mrqa_hotpotqa-validation-5168", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-649", "mrqa_hotpotqa-validation-686", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-88", "mrqa_hotpotqa-validation-881", "mrqa_naturalquestions-validation-1783", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-4532", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6359", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7554", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9778", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1685", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2084", "mrqa_newsqa-validation-209", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3017", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3393", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4099", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-806", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-860", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-937", "mrqa_searchqa-validation-10053", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1184", "mrqa_searchqa-validation-11854", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15483", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16699", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2218", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2794", "mrqa_searchqa-validation-2795", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3074", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4117", "mrqa_searchqa-validation-4228", "mrqa_searchqa-validation-4467", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4723", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5043", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-6365", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-6672", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7514", "mrqa_searchqa-validation-7575", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7690", "mrqa_searchqa-validation-7777", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9784", "mrqa_searchqa-validation-9894", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10203", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-103", "mrqa_squad-validation-10316", "mrqa_squad-validation-10489", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1311", "mrqa_squad-validation-1343", "mrqa_squad-validation-1379", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1481", "mrqa_squad-validation-1506", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-157", "mrqa_squad-validation-1640", "mrqa_squad-validation-167", "mrqa_squad-validation-1670", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-1977", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2208", "mrqa_squad-validation-225", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2354", "mrqa_squad-validation-2375", "mrqa_squad-validation-2628", "mrqa_squad-validation-2648", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-2739", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3097", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-3146", "mrqa_squad-validation-3193", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3420", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-3653", "mrqa_squad-validation-3661", "mrqa_squad-validation-3664", "mrqa_squad-validation-3713", "mrqa_squad-validation-3725", "mrqa_squad-validation-3740", "mrqa_squad-validation-3759", "mrqa_squad-validation-3791", "mrqa_squad-validation-3853", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3941", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4023", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4267", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4415", "mrqa_squad-validation-4429", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4446", "mrqa_squad-validation-4478", "mrqa_squad-validation-4490", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-472", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4846", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5012", "mrqa_squad-validation-5077", "mrqa_squad-validation-5185", "mrqa_squad-validation-5256", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5373", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5846", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6128", "mrqa_squad-validation-6224", "mrqa_squad-validation-6242", "mrqa_squad-validation-6279", "mrqa_squad-validation-6292", "mrqa_squad-validation-630", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6520", "mrqa_squad-validation-6541", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6871", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-696", "mrqa_squad-validation-7006", "mrqa_squad-validation-7136", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7663", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7871", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8065", "mrqa_squad-validation-8103", "mrqa_squad-validation-820", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-8505", "mrqa_squad-validation-8554", "mrqa_squad-validation-8561", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8969", "mrqa_squad-validation-8977", "mrqa_squad-validation-9020", "mrqa_squad-validation-9068", "mrqa_squad-validation-9102", "mrqa_squad-validation-9145", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9368", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-9744", "mrqa_squad-validation-9787", "mrqa_squad-validation-980", "mrqa_squad-validation-9800", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9944", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1189", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1792", "mrqa_triviaqa-validation-1884", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3675", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5040", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-6033", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6912", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-91"], "OKR": 0.841796875, "KG": 0.4828125, "before_eval_results": {"predictions": ["England", "Jimmy Kimmel", "an invasion of Western Europe", "\"The Day of the Doctor\"", "mannerist architecture", "3", "the same message routing methodology as developed by Baran", "Jonathan Stewart", "two", "Balvenie Castle", "Vernier, Switzerland", "1966", "Selden", "Henry Kaiser", "841", "a creek", "20 March to 1 May 2003", "Newfoundland and Labrador", "9 November 1967", "Westchester County", "French Canadians", "Towards the Sun", "Secret Intelligence Service", "Portal A Interactive", "Windigo", "aging issues", "1949", "Churros", "Tom Kartsotis", "Terrence Jones", "Joseph Cotten", "Thomas Allen", "Eisstadion Davos", "Tampa Bay Lightning", "Vishal Bhardwaj", "15", "University of Vienna", "four", "KlingStubbins", "the seventh season", "pornographicstar", "torpedoes", "1951", "Saint Motel", "Allan McNish", "1995", "1689", "David Villa", "Champion Jockey", "Office", "Russell Humphreys", "He served as director of the Saint Petersburg Conservatory", "Mathew Sacks", "Sons of Anarchy Motorcycle Club", "Mel Tillis", "Baseball", "sand", "a pregnancy", "A New York appeals court", "3800", "a mirror", "sewer", "Fred Astaire", "Douglas MacArthur"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7057291666666666}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9362", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-5848", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-553", "mrqa_hotpotqa-validation-79", "mrqa_hotpotqa-validation-5460", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-5514", "mrqa_hotpotqa-validation-4007", "mrqa_naturalquestions-validation-7845", "mrqa_naturalquestions-validation-4288", "mrqa_newsqa-validation-1387", "mrqa_newsqa-validation-815", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-10901"], "SR": 0.59375, "CSR": 0.5914634146341464, "EFR": 1.0, "Overall": 0.7355583079268293}, {"timecode": 41, "before_eval_results": {"predictions": ["c1600", "absolute value", "20", "James Hutton", "force-free magnetic fields", "six", "Graduates from the polytechnics and colleges", "BBC Wales", "guitar", "books, films and other media", "Sindbad", "Ben Ainslie", "Pac-12 Conference", "Rochdale", "Jay Park", "Sergeant Purley Stebbins", "11 November 1918", "Fort Bragg, North Carolina", "Drudge Report", "Homebrewing", "an anvil", "UVM Agriculture Department and the Agricultural Experiment Station", "Andy Garc\u00eda", "Eielson Air Force Base", "1 December 1948", "drummer Seb Rochford", "David Dunn", "\"Charmed\"", "White Horse", "postal delivery", "Europop", "private Ivy League research university", "Leofric", "Visigoths", "Prince Sung-won", "providing the voices of Leonardo and Rocksteady in the original \" Teenage Mutant Ninja Turtles\" animated series and Shotaro Kaneda in the 1989 original English dub of \"Akira\"", "Schaffer", "Cold Spring", "Richardson, Texas, United States", "Wal-Mart Canada Corp.", "southwestern", "October 30, 1964", "Richard Arthur", "Presbyterian Church", "CD Castell\u00f3n", "left-hand or right-hand batsman", "the German Empire", "three", "River Welland", "Ben Stokes", "the site of Canada's first train robbery", "Don Johnson", "\"Cs\u00e1sz\u00e1ri \u00e9s Kir\u00e1lyi Hadsereg\"", "13 February", "slide clamps or a cross-table", "Fred", "Mildred Pierce", "February 2008", "Daniel Radcliffe", "Nostradamus", "a tortoise", "Leo", "Pablo Picasso", "Kosovo"], "metric_results": {"EM": 0.53125, "QA-F1": 0.64140625}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12500000000000003, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 0.4, 0.4, 0.5, 0.0, 1.0, 0.8, 0.4, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8472", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2991", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-2069", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-1201", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-4363", "mrqa_hotpotqa-validation-1365", "mrqa_naturalquestions-validation-182", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-1845", "mrqa_newsqa-validation-1275", "mrqa_searchqa-validation-4054"], "SR": 0.53125, "CSR": 0.5900297619047619, "EFR": 0.9666666666666667, "Overall": 0.7286049107142858}, {"timecode": 42, "before_eval_results": {"predictions": ["Oxygen therapy", "a series of New York hotels", "its energy content", "Baiju", "July 24", "S-IVB-200", "over half", "New Orleans, Louisiana", "German", "GE Appliances", "Taipei City", "John Joseph Travolta", "The authorship of Titus Andronicus", "National Football Conference", "wich children's novelist", "25 August 1949", "over 20 million records worldwide", "The Captain Matchbox Whoopee Band", "Sippin' on Some Syrup", "40 million albums", "22,500 acres", "Citizens for a Sound Economy", "Woodsy owl", "Hellenism", "Lufthansa Heist", "Mauritian", "The LA Galaxy", "Rowan Atkinson", "first baseman and third baseman", "Rymill Park", "Alonso L\u00f3pez", "Tampa Bay Storm quarterback", "in the series \"Runaways\"", "Las Vegas", "House of Habsburg-Lorraine", "Tian Tan Buddha", "Hopi", "a British astronomer and composer of German and Czech-Jewish origin", "Martin Truex Jr.", "Secretary of Defense", "twice", "Puli Alam", "1998", "Lionel Hollins", "Pamelyn Wanda Ferdin", "Brown Mountain Overlook", "bioelectromagnetics", "VH1", "Kentucky", "Dra\u017een Petrovi\u0107", "SAS Technical Services", "Hawaii", "Scott Eastwood", "Lenny Jacobson", "July 1, 1923", "radionuclides", "recommendations for Reducing False Positives Related to Use of Behavioral Manifestations", "Marine units", "a racially-tinged remark made by his former caddy", "a tornado", "in the \"baggage train\" of Wellington", "60 Minutes", "Nixon", "a ballad"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5943185286935286}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.0, 0.7499999999999999, 0.0, 0.5, 0.28571428571428575, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.2, 0.28571428571428575, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1247", "mrqa_squad-validation-3891", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-3886", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-2373", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-3300", "mrqa_hotpotqa-validation-5125", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-2646", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1446", "mrqa_triviaqa-validation-2361", "mrqa_triviaqa-validation-7704", "mrqa_newsqa-validation-1859", "mrqa_newsqa-validation-2811", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-14021", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-7720"], "SR": 0.46875, "CSR": 0.5872093023255813, "EFR": 1.0, "Overall": 0.7347074854651162}, {"timecode": 43, "before_eval_results": {"predictions": ["19", "Creon", "unbalanced torque", "Oxygen storage methods", "April 1959", "the water level", "the Daleks, the Cybermen, and the Master", "95", "Lana Del Rey", "Beldam / Other Mother", "Mathew Horne", "Brazil and Paraguay", "Adam", "Christopher Kennedy Masterson", "1987", "Kanawha River", "September 9, 2012", "The seafloor must have originated on the Earth's great fiery welts, like the Mid-Atlantic Ridge and the East Pacific Rise", "Bart Cummings", "Hermann Ebbinghaus", "optic chiasma", "New Mexico", "The centuries - old Jedi Grand Master of an unknown species", "Atticus Finch", "The decision effectively overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "Jos Plateau", "Butch or Killer", "1975", "Eddie Van Halen", "Parker's pregnancy", "Aibak, first ruler of the Delhi Sultanate", "1951", "season five", "North Atlantic Drift", "Abid Ali Neemuchwala", "the Thane of Lochaber", "1998", "early 20th century", "March 31, 2017", "The sales area is primarily concentrated in the Southern United States, and has been sold as far west as Las Vegas, as far north as Indianapolis and Denver, and as far east as Richmond, Virginia", "Ernest Hemingway", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "Sara Gilbert", "Jesse Frederick James Conaway", "alpaca fiber and mohair from Angora goats", "the Hongwu Emperor of the Ming Dynasty", "The 133rd overall episode overall,", "Elena Anaya", "Gary Grimes as Hermie, Jerry Houser as his best friend Oscy, Oliver Conant as their nerdy young friend Benjie", "The tuatara", "a chimera ( a mixture of several animals ), who would probably be classified as a carnivore overall", "gastrocnemius muscle", "late to mid-2000s", "Wilkie Collins", "aircraft carrier", "Vyto Ruginis", "Archie Andrews", "five", "two tickets to Italy", "(John) Shaft", "The Laughing Cavalier", "Saturday Night Live", "Bono", "The Royal Ballet"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6090356691919192}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.2, 0.5, 0.3333333333333333, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.35, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.22222222222222224, 1.0, 0.1904761904761905, 0.0, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3688", "mrqa_squad-validation-7615", "mrqa_naturalquestions-validation-10073", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-9756", "mrqa_naturalquestions-validation-2966", "mrqa_naturalquestions-validation-8205", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-7239", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-554", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-1882", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-8908"], "SR": 0.546875, "CSR": 0.5862926136363636, "EFR": 0.9310344827586207, "Overall": 0.7207310442789969}, {"timecode": 44, "before_eval_results": {"predictions": ["a green algal derived chloroplast", "bilaterians", "Ward", "The Swiss cities", "the goals he receives from his superior", "7 January 1900", "education", "in the $24,000-30,000 price range", "10 years", "U.S.-backed Iraqi forces and gunmen.", "millions of Americans", "arson", "Sunday", "Citizens", "a music video", "at least nine", "NATO to do more to stop the Afghan opium trade", "forgery and flying without a valid license", "The 19-year-old woman", "amazed at their band's amazing impact", "in a canyon in the path of the blaze", "recanted her claims that she was lured to a dorm and assaulted in a bathroom stall", "Ronaldinho", "shock, quickly followed by speculation about what was going to happen next,\"", "Illness", "Utah Valley Regional Medical Center", "a number of calls", "Frank Ricci", "200", "Manchester United", "African National Congress", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "the charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "137", "New Year's Day", "South Africa", "Unseeded Frenchwoman Aravane Rezai", "in a muddy barley field", "after nine years.", "Tutsi ethnic minority and the Hutu majority", "the island's dining scene", "sovereignty over them.", "Kgalema Motlanthe", "1918-1919", "Newcastle", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Germany's \"Holocaust shame,\"", "The EU naval force", "the Genocide Prevention Task Force", "President Obama", "U.S. military bases in the Pacific Ocean territory of Guam", "Hong Kong and Shenzhen, a city in mainland China", "201-262-2800", "neuropsychology", "Rumplestiltskin", "Saturn", "Hyundai", "South America", "WB Television Network", "George P. Shultz", "Parody", "cartoonist, author, art critic and stage designer", "decorate", "Jack Johnson"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5841897685647686}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.45454545454545453, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.3636363636363636, 0.0, 0.923076923076923, 0.923076923076923, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4, 0.0, 0.0, 0.2222222222222222, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-844", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-1398", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-3808", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3817", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-1704", "mrqa_newsqa-validation-1428", "mrqa_newsqa-validation-263", "mrqa_naturalquestions-validation-2839", "mrqa_searchqa-validation-5272", "mrqa_triviaqa-validation-25", "mrqa_triviaqa-validation-4612"], "SR": 0.484375, "CSR": 0.5840277777777778, "EFR": 0.9696969696969697, "Overall": 0.7280105744949495}, {"timecode": 45, "before_eval_results": {"predictions": ["Jane Kim", "BBC National Orchestra of Wales", "to the southeast part of Khwarzemia to form, with the first division, a pincer attack on Samarkand", "Algeria", "Qara Khitai", "worst-case time complexity", "Robert Goulet", "piano", "Ramen", "Beethoven", "corn", "Edmund Burke", "a song", "John Schaar", "the White House lawn", "Se sitcom", "Medieval Times Dinner & Tournament", "\"Marcus Welby, M.D.\"", "corn dog", "Marlon Brando", "TV Series Finale", "zoology", "cheese", "The Book of Judges", "a ground level effort", "Howard Hughes", "Lake Michigan", "religious leaders", "Hermann Rorschach", "Amritsar", "Teddy Roosevelt", "Southeast Asia", "the Otis family", "Lance Armstrong", "It's a Wonderful Life", "\"The Passing of Arthur\"", "Cewek123.com", "The Winds of War", "the Black Rooster", "the Hundred Years' War", "Wellington", "Bangkok", "Castle Rock", "carbon fiber", "Women in", "Oklahoma", "Serer", "Italian media magnate", "a helicopter", "aces", "\"The Raven\"", "azu", "Marshall Sahlins", "more than 600 restaurants in 42 states", "Leo Arnaud", "The Squeee", "Debbie Rowe", "\"GEORGES Bizet\u2019s Carmen", "extended play", "sulfur mustard", "Lieutenant Colonel Iceal Hambleton", "\"Top Gun.\"", "fake his own death", "\"We have run the economy into the ground, taken a great and open and proud nation and turned it into a kind of paranoid and closed one"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5048611111111111}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.5, 0.5, 1.0, 0.5, 0.8, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6125", "mrqa_squad-validation-1710", "mrqa_searchqa-validation-7347", "mrqa_searchqa-validation-10923", "mrqa_searchqa-validation-5572", "mrqa_searchqa-validation-11405", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-16776", "mrqa_searchqa-validation-9674", "mrqa_searchqa-validation-15634", "mrqa_searchqa-validation-15808", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-16463", "mrqa_searchqa-validation-3040", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-4957", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-5644", "mrqa_searchqa-validation-5759", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-338", "mrqa_searchqa-validation-1341", "mrqa_searchqa-validation-6209", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-5441", "mrqa_searchqa-validation-14904", "mrqa_searchqa-validation-4954", "mrqa_searchqa-validation-11073", "mrqa_searchqa-validation-13554", "mrqa_searchqa-validation-2849", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-6692", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-7659", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-3528", "mrqa_newsqa-validation-2689", "mrqa_newsqa-validation-729"], "SR": 0.359375, "CSR": 0.5791440217391304, "EFR": 0.9512195121951219, "Overall": 0.7233383317868505}, {"timecode": 46, "before_eval_results": {"predictions": ["Ed McCaffrey", "oxygen-16", "December 1517", "a blue British police box", "Georgia", "Four thousand", "Mirzapur, Uttar Pradesh", "JPS", "an investor couple in Austin, Texas", "Bonnie Lipton", "Sara Gilbert", "1981 -- 86", "the Christian biblical canon", "1971", "Butter Island off North Haven, Maine in the Penobscot Bay", "15 December 2017", "the part of the gastrointestinal tract between the stomach and the large intestine", "Todd Griffin", "such famous figures as Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "2017 / 18 Divisional Round", "Dunedin, Port Chalmers and on the Otago Peninsula, Saint Bathans in Central Otago and at the Cape Campbell Lighthouse in Marlborough", "2018", "Cyndi Grecco", "Eliana", "Christopher Columbus", "starch", "General Inquisition", "Electoral College", "Captain Jones", "Lynne", "June 8, 2009", "fertilization", "The management team", "The Enchantress", "Americans", "Herod", "Gibraltar", "13 February", "Boston Celtics", "E Elaine Davidson", "off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "an explosion", "the uppermost layer of the dermis", "Total Drama Action", "18", "food and clothing", "Empiricism", "senators", "the blood, brain, and other tissues", "March 29, 2018", "Acid rain", "UN General Assembly", "South Pacific", "Mt Kenya", "vanilla", "1995 to 1996", "John Snow", "March 13, 2013", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "his company Polo", "immigration detainees", "lily Allen", "a Purple Heart", "Caltech"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6431919642857142}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714285, 1.0, 0.2666666666666667, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5714285714285715, 0.4, 0.7428571428571429, 0.6666666666666666, 1.0, 0.5714285714285715, 0.2857142857142857, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-3341", "mrqa_naturalquestions-validation-2429", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1304", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-9985", "mrqa_naturalquestions-validation-9726", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-9582", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-7264", "mrqa_naturalquestions-validation-5008", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-692", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-4472", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-135"], "SR": 0.515625, "CSR": 0.5777925531914894, "EFR": 0.967741935483871, "Overall": 0.7263725227350721}, {"timecode": 47, "before_eval_results": {"predictions": ["Pittsburgh Steelers", "Sainte Foy in Quebec", "Arley D. Cathey", "1930", "current Doctor Who", "productivity", "nine hours from Coordinated Universal Time ( UTC \u2212 09 : 00 )", "three", "New England Patriots", "24 hours later", "Brooklyn Heights", "hydrogen", "URL of a web page above the page in an address bar", "Massachusetts", "the left of the dinner plate", "Daniel A. Dailey", "the leaves of the plant species Stevia rebaudiana", "Captain Jones", "Freddie Highmore", "Squamish, British Columbia, Canada", "Dan Enright", "Jason Marsden", "Tandi, in Lahaul", "1912", "Etienne de Mestre", "Paris", "Lauren Tom", "two - third of the total members present and voted in favour", "Miami Heat", "The Intolerable Acts", "accommodationism", "pilgrimages to Jerusalem", "Shirley Mae Jones", "Haytham Kenway", "741 weeks from 1973 to 1988", "the head of the Imperial Family and the traditional head of state of Japan", "frontal lobe", "McKim Marriott", "Lizzy Greene", "C peptide - A chain bond", "Walter Brennan", "Tabaqui", "Keeley Clare Julia Hawes", "following graduation with a Bachelor of Medicine, Bachelor of surgery degree and start the UK Foundation Programme", "Georgia Groome", "1773", "Dr. Rajendra Prasad", "Emma Swan ( Jennifer Morrison )", "Mahatma Gandhi", "B - Rabbit", "Jonathan Goldstein", "Malvolio", "formic acid", "transuranic elements", "western Caribbean Sea", "The final of 2011 AFC Asian Cup", "Stephen Crawford Young", "Great Northern Railway", "The Stooges comedic farce entitled \"Three Little Beers,\"", "in a volatile zone along the equator between South America and Africa.", "The People's Republic of Arizona is in the market for nuclear warheads", "Brazil", "rodeo", "William Safire"], "metric_results": {"EM": 0.578125, "QA-F1": 0.659546921838911}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.8, 0.33333333333333337, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3529411764705882, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.5263157894736842, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7971", "mrqa_squad-validation-7875", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-215", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8341", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-8986", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-3922", "mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-2889", "mrqa_naturalquestions-validation-2605", "mrqa_triviaqa-validation-1912", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-2784", "mrqa_newsqa-validation-1444"], "SR": 0.578125, "CSR": 0.5777994791666667, "EFR": 1.0, "Overall": 0.7328255208333334}, {"timecode": 48, "before_eval_results": {"predictions": ["6,000 square kilometres (2,300 sq mi)", "about 10,000", "1998", "materials melted near an impact crater", "the Rhine Gorge", "New Brunswick", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "3D modeling", "Kevin McKidd", "Abanindranath Tagore CIE", "Donny Osmond", "arm", "2017", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "a pop ballad", "to petition for a governmental redress of grievances", "those at the bottom of the economic government whom the state ( in Roosevelt's view and in the general social humanitarian approach ) needed to help", "China", "to be unfair", "art of the book and architecture", "Buddhism", "a greeting", "Siddharth Arora / Vibhav Roy as Ishaan Anirudh Sinha", "bicameral Congress", "1984", "Geoffrey Zakarian", "the fourth century", "2006", "eleven", "new wave rock band The Fixx", "ancient Rome", "Buffalo Lookout", "Charles Carson", "July 2012", "Spanish / Basque", "an idiom for the most direct path between two points", "March 15, 1945", "September 1972", "air moisture", "the naos", "SURFACE AREA OF ROOTS", "`` drift '' across the ocean bed", "a competitor or team in a sport or other tournament who is given a preliminary ranking for the purposes of the draw", "2.5", "Terry O'Neill", "to obtain a U.S. passport", "endocrine", "the Royal Air Force ( RAF )", "It plays a key role in chain elongation", "Sharyans Resources", "Old Trafford", "Icarus", "kermadec islands", "Gloucestershire", "Cambridge", "Chad", "Johnnie Ray", "William Corcoran Eustis", "\"it is impossible to turn back the tide of globalization.\"", "U.S. and NATO forces.", "Columbia, Missouri.", "a thunderbolt", "a twirler", "man"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5439981610024713}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.07407407407407408, 0.0, 0.25, 0.33333333333333337, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.16, 0.0, 0.0, 0.0, 0.0689655172413793, 0.0, 0.4444444444444445, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.07142857142857142, 0.25, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3026", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-9837", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-9079", "mrqa_naturalquestions-validation-949", "mrqa_naturalquestions-validation-9361", "mrqa_naturalquestions-validation-4868", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-8648", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-7948", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-1314", "mrqa_hotpotqa-validation-4240", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-3301", "mrqa_searchqa-validation-9451"], "SR": 0.453125, "CSR": 0.5752551020408163, "EFR": 0.9428571428571428, "Overall": 0.7208880739795919}, {"timecode": 49, "before_eval_results": {"predictions": ["Leonard Bernstein", "Boolean circuits", "Yale University", "Wednesdays", "with the help of the military", "the cavities and surfaces of blood vessels and organs throughout the body", "alveolar bone", "the court from its members for a three - year term", "the object is placed further away from the mirror / lens than the focal point and this real image is inverted", "China", "420 mg", "W. Edwards Deming", "Fa Ze Banks", "in the phospholipids", "cartilage", "`` Nearer, My God, to Thee ''", "freedmen joined the ranks of farmers and the urban working class", "Eurasian Plate", "Ukraine", "diffuse interstellar medium ( ISM ) of gas and dust", "October 28, 2007", "2018", "Barbara Eve Harris", "Walmart", "Celtic", "Ra\u00fal Eduardo Esparza", "late as the 1890s", "C\u03bc and C\u03b4", "medical care provided on an outpatient basis, including diagnosis, observation, consultation, treatment, intervention, and rehabilitation services", "Justice Harlan", "2017", "52 days", "Travis Tritt and Marty Stuart", "Missi Hale", "1939", "the base of the right ventricle", "Acts passed by the Congress of the United States and its predecessor, the Continental Congress", "in New Mexico", "Bart Howard", "Jason Flemyng", "Brad Dourif", "Steve Mazzaro & Missi Hale", "a single, implicitly structured data item in a table", "Kyla Pratt", "Ariana Clarice Richards", "glucose", "pathology", "Lionel Hardcastle", "Debbie Gibson", "on the bank's own funds and signed by a cashier", "2017", "Tavares", "Dealings with the Firm of Dombey and Son: Wholesale, Retail and for Exportation", "bison", "The Duchess", "\"Back to the Future\"", "Adam Levine", "Westchester", "\"it should stay that way.\"", "more than 200.", "a weapon", "Dobermann", "Kirsten Gillibrand", "Elie Wiesel"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5748081140350876}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.2222222222222222, 0.1111111111111111, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4210526315789474, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5113", "mrqa_naturalquestions-validation-8217", "mrqa_naturalquestions-validation-2402", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-1330", "mrqa_naturalquestions-validation-5812", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-6052", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-2988", "mrqa_hotpotqa-validation-745", "mrqa_newsqa-validation-2655", "mrqa_newsqa-validation-1176", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-5647"], "SR": 0.515625, "CSR": 0.5740624999999999, "EFR": 1.0, "Overall": 0.732078125}, {"timecode": 50, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1245", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2368", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2596", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-3953", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5882", "mrqa_hotpotqa-validation-604", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-634", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-88", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10382", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-10639", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2280", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-3074", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-4026", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5170", "mrqa_naturalquestions-validation-5486", "mrqa_naturalquestions-validation-5696", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-6399", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-662", "mrqa_naturalquestions-validation-6987", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7942", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-9004", "mrqa_naturalquestions-validation-9029", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9684", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-1911", "mrqa_newsqa-validation-2006", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2076", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2593", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3472", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4142", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_newsqa-validation-937", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1184", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-13419", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14635", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4723", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-578", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7297", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9442", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9820", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10041", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10160", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-103", "mrqa_squad-validation-10316", "mrqa_squad-validation-10416", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1247", "mrqa_squad-validation-1311", "mrqa_squad-validation-1379", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1481", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-157", "mrqa_squad-validation-1640", "mrqa_squad-validation-167", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2648", "mrqa_squad-validation-267", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3112", "mrqa_squad-validation-3125", "mrqa_squad-validation-3146", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-366", "mrqa_squad-validation-3661", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3941", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4478", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5077", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5337", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5846", "mrqa_squad-validation-5857", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6125", "mrqa_squad-validation-6128", "mrqa_squad-validation-6224", "mrqa_squad-validation-6279", "mrqa_squad-validation-6292", "mrqa_squad-validation-630", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6520", "mrqa_squad-validation-6541", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-7006", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7615", "mrqa_squad-validation-7627", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7864", "mrqa_squad-validation-7871", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8031", "mrqa_squad-validation-8065", "mrqa_squad-validation-8103", "mrqa_squad-validation-8143", "mrqa_squad-validation-820", "mrqa_squad-validation-821", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-844", "mrqa_squad-validation-8472", "mrqa_squad-validation-8554", "mrqa_squad-validation-8561", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8792", "mrqa_squad-validation-8958", "mrqa_squad-validation-8977", "mrqa_squad-validation-9068", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9623", "mrqa_squad-validation-9643", "mrqa_squad-validation-9680", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-973", "mrqa_squad-validation-9787", "mrqa_squad-validation-9837", "mrqa_squad-validation-9900", "mrqa_squad-validation-9944", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3805", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5655", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6044", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6557", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867"], "OKR": 0.79296875, "KG": 0.4328125, "before_eval_results": {"predictions": ["Rankine cycle", "1849", "Bureau of Buddhist and Tibetan Affairs (Xuanzheng Yuan)", "More than 1 million", "October 16, 2012", "Liverpool Bay", "\"Little Dixie\"", "Sadar Bazaar", "Burning Man", "The Wachowskis", "Revengers Tragedy", "first and only U.S. born world grand prix champion", "Jessica Lange", "Boeing EA-18G Growler", "Starlite", "Edward P. \"Ed\" Mangano", "Urijah Faber", "1980", "Ben R. Guttery", "Wiz Khalifa", "Matthew Ryan Kemp", "January 15, 1975", "the Mediterranean", "Valeri Bure", "German Shepherd", "March 19, 2017", "Albert", "the Millennium Olympic Games/Games of the New Millennium", "the Mikoyan design bureau", "Vyd\u016bnas", "Greg Hertz", "seven", "1999", "a few", "Venus", "the 924", "\"The Process\"", "1973", "House of Fraser", "Duval County, Florida", "William Allen White", "Tel Aviv University", "Stephen Crawford Young", "through YouTube.", "94", "Rabat", "Brenton Thwaites", "Centers for Medicare & Medicaid Services (HCFA)", "2014", "Romeo Montague", "Mark Neveldine and Brian Taylor", "Lonestar", "in Middlesex County", "2018", "bachata", "Treaty of Amiens", "EMI", "Mariette", "in a Nazi concentration camp,", "Paul Ryan", "calls his family's survival \"a miracle,\"", "Enigma", "Chicago", "Guadalajara"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6659615124458875}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.8333333333333334, 0.0, 0.0, 1.0, 1.0, 0.2727272727272727, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.12500000000000003, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8351", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-527", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-4862", "mrqa_hotpotqa-validation-4520", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-5296", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-453", "mrqa_hotpotqa-validation-212", "mrqa_naturalquestions-validation-1431", "mrqa_naturalquestions-validation-2743", "mrqa_triviaqa-validation-6409", "mrqa_triviaqa-validation-6055", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2853", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-13144"], "SR": 0.546875, "CSR": 0.5735294117647058, "EFR": 1.0, "Overall": 0.696190257352941}, {"timecode": 51, "before_eval_results": {"predictions": ["356 \u00b1 47 tonnes per hectare", "acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs", "more than half", "5K", "all-Gemini veteran crew", "the United States", "the strawberry Family", "Hu Jintao", "$3 billion, with further foreign direct investment exceeding $40 billion during the operations phase.", "Basel", "9-1", "Afghanistan's Helmand province,", "she was lifted from the lifeboat onto Carpathia, the rescue ship, in a mail sack,\"", "Steven Green", "Roma", "public opinion in Turkey.\"", "Costa Rica.", "\"learn how to dance and feel sexy,\"", "a few months and lose weight and then go back to exactly how you were living before and expect everything to be the same,\"", "six", "\" Days of our Lives,\"", "\"Quiet Nights,\"", "Michael Partain", "Frank Ricci", "Stoke City.", "Australian officials", "Kim Il Sung", "Indonesia", "Karen Floyd", "a \"happy ending\" to the case.", "Brett Cummins", "workers have pulled a body from underneath the rubble of a collapsed apartment building", "Keating Holland", "Anil Kapoor", "MS Columbus", "late Thursday to form a government of national reconciliation.", "The Wall Street Journal Europe", "inability to \"turn it off\"", "Thursday", "Nineteen", "off east  Africa", "fake his own death by crashing his private plane into a Florida swamp.", "sailing", "Mugabe's opponents", "$14.1 million", "Kaka", "south-central Washington,", "543", "Jesus Christ", "maintain an \"aesthetic environment\" and ensure public safety,", "the FBI.", "New York Post's Page 6 gossip column.", "Branford College", "St. John's, Newfoundland and Labrador", "September 24, 2012", "Erinyes", "the liver", "The Cross Foxes Inn", "21 kilometres south-east of Adelaide, in the Adelaide Hills", "St James's Palace", "Salisbury", "The Saiga Antelope", "bopple nut", "Livia"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6217053345959596}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.375, 1.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.8333333333333333, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7272727272727272, 0.0, 0.0, 1.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2474", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2991", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-3547", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-844", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1523", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-2388", "mrqa_naturalquestions-validation-3788", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-5096", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-2306", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-191", "mrqa_hotpotqa-validation-3324", "mrqa_searchqa-validation-2005", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-15795"], "SR": 0.515625, "CSR": 0.5724158653846154, "EFR": 0.9354838709677419, "Overall": 0.6830643222704714}, {"timecode": 52, "before_eval_results": {"predictions": ["education, sanitation, and traffic control within the city limits", "\u00a342,090", "Genghis Khan", "Emergency Highway Energy Conservation Act", "\"training Day\"", "\"The Brothers\"", "the full 24 hours, given fair weather", "25 October 1921", "George Washington Bridge", "\"Red Rock West\"", "Larry Eustachy", "shortstop", "Stephen King", "Sports Illustrated", "the United Football League", "41st", "\"The School Boys\"", "In Pursuit", "British", "Memphis, Tennessee", "Fort Snelling", "Martin Lee Truex Jr.", "\"The Onion\"", "\"Shoot Straight from Your Heart.\"", "1963", "Saint Louis County,", "Republican", "Anno 2053", "the Wabanaki Confederacy", "Martin Ingerman", "New York Islanders", "The Division of Cook", "Bigfoot", "\"The King of Chutzpah\"", "$10\u201320 million", "Julianne Moore", "beer", "Francis the Talking Mule", "Mexico", "model", "16,575", "\"To Save a Life\"", "superhero roles as the Marvel Comics", "October 13, 1980", "HBO World Championship Boxing", "\" College Football Scoreboard\"", "1930", "Buffalo", "Monash", "Godiva", "science fiction", "1978", "December 9, 2017", "the Deathly Hallows", "Tom Robinson", "Muhammad Ali", "Elvis Presley", "Sri Lanka", "April 6, 1994", "from Texas and Oklahoma to points east, with 8 to 10 inches of snow possible in some locales,", "Brown-Waite", "Lagos", "Cuba", "aikido"], "metric_results": {"EM": 0.578125, "QA-F1": 0.700606684981685}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.07692307692307691, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7246", "mrqa_squad-validation-6150", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-2791", "mrqa_hotpotqa-validation-4568", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-797", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-3112", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-5155", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2802", "mrqa_naturalquestions-validation-10416", "mrqa_triviaqa-validation-5506", "mrqa_newsqa-validation-409", "mrqa_newsqa-validation-1016", "mrqa_searchqa-validation-5286", "mrqa_searchqa-validation-9382"], "SR": 0.578125, "CSR": 0.5725235849056604, "EFR": 1.0, "Overall": 0.695989091981132}, {"timecode": 53, "before_eval_results": {"predictions": ["his friends Johannes Bugenhagen and Philipp Melanchthon", "Renaissance", "Warszawa", "Gods of Egypt", "Levon Helm", "three", "Zack Snyder", "the Qin dynasty", "University of Vienna", "the University of Texas at Austin", "the Virginia Destroyers of the United Football League", "1970s and 1980s", "1983", "Headless Body", "aeronautical engineer", "Gerard Marenghi", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "UDC", "the University of Southern California", "a record 20", "invoicing the employees' work based on an hourly rate, measuring the work effectiveness and project management", "October 13, 1980", "Russell T Davies", "nearly 8 km", "Tony Aloupis", "London", "Antonio Salieri", "the USS Essex", "Lonestar", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Merck & Co.", "Louth", "A123 Systems, LLC", "Northern Ireland", "shock cavalry", "Robert L. Stone", "American", "Flushed Away", "Unibet Premier League Darts", "four", "Division of Cook", "Macau, Macau", "New York Shakespeare Festival", "Princess Aisha bint Hussein", "October 25, 1881", "John II Casimir Vasa", "Nina Stibbe", "2001", "1994", "Art of Dying", "the Hawaii House of Representatives", "RAF Mount Pleasant", "Akshay Kumar", "Dr. Rajendra Prasad", "a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company, founded in 2012 in Lebanon with the collaboration of Lebanese, French and Italian engineers", "Flemish", "Jennifer Ellison", "the Mongols", "to do jobs that Arizonans wouldn't do.", "Robert Barnett", "more than a million residents who have been displaced by fighting", "Mark Twain", "peameal", "Covington"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6721072330447331}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.8333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.5, 0.45454545454545453, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.888888888888889, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2547", "mrqa_squad-validation-608", "mrqa_hotpotqa-validation-4578", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-3872", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-3654", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2891", "mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-5179", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-118", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-4900", "mrqa_naturalquestions-validation-392", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-1999", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-3178", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-9008"], "SR": 0.53125, "CSR": 0.5717592592592593, "EFR": 1.0, "Overall": 0.6958362268518519}, {"timecode": 54, "before_eval_results": {"predictions": ["\u00a341,004", "environmental degradation", "1876", "a Standard Model", "The United States of America", "50 JJB Sports Fitness Clubs", "The Cherokee Nation", "Gatwick Airport", "Berea College", "Daniel Espinosa", "Bill Boyd", "John Monash", "1860", "25 November 2015", "Trilochanapala", "the Jacobite leader \"Bonnie Prince\" Charles Edward Stuart", "Martin Truex Jr.", "Objectivism", "Ronald Ryan", "Hennepin County", "Samuel Beckett", "Barbara Bush", "Carl David Tolm\u00e9 Runge", "James Weldon Johnson", "Tom Jones", "sarod", "Bank of China Tower", "two", "Enemy", "40,400", "more than 100 countries", "for crafting and voting on legislation", "Flaw", "237 square miles", "Matthieu Vaxivi\u00e8re", "handheld", "Jennifer Taylor", "Sam", "Nye County", "Prospero", "Orlando Predators", "Charlie Wilson", "\"The King of Chutzpah\"", "Afghanistan", "Girls' Generation", "Division I", "2269", "Vernier, Switzerland", "Park Hyung-Sik", "Britain", "Newell Highway", "11 September 1996", "104 colonists and Discovery", "My Summer Story", "Samantha Jo `` Mandy '' Moore", "6", "jerry", "Ben Watson", "Three aid workers", "Mexico", "the United States if provoked.", "Stones from the River", "contempt", "potato"], "metric_results": {"EM": 0.546875, "QA-F1": 0.628125}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-2072", "mrqa_squad-validation-7629", "mrqa_squad-validation-5540", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-413", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-1965", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-5373", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-4584", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2531", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-4696", "mrqa_hotpotqa-validation-3076", "mrqa_hotpotqa-validation-2005", "mrqa_hotpotqa-validation-3208", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-2168", "mrqa_hotpotqa-validation-1892", "mrqa_naturalquestions-validation-3962", "mrqa_triviaqa-validation-4877", "mrqa_newsqa-validation-213", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-14967", "mrqa_searchqa-validation-3400"], "SR": 0.546875, "CSR": 0.5713068181818182, "EFR": 1.0, "Overall": 0.6957457386363636}, {"timecode": 55, "before_eval_results": {"predictions": ["The Earth's mantle", "deep spiritual despair", "Three", "the printing press", "closed on 366 for eight wickets", "Pixar's", "Auckland", "Brazil", "9 percent", "killed a limo driver", "higher insurance rates", "Osama bin Laden's sons", "\"release\" civilians,", "1940's Japan.", "The oceans", "228", "23 years.", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "Sharon Bialek", "15,000", "cell phones", "The sole survivor of the crash that killed Princess Diana", "a \"prostitute\" and threatening to oust another from his country.", "declared the charity of kidnapping the children and concealing their identities.", "North Korea intends to launch a long-range missile in the near future,", "21", "(Friday the 13th)\"", "to bring closure to the families of three missing military men,", "four people believed to be illegal immigrants", "150", "1994", "Al-Shabaab, the radical Islamist militia that controls the city", "The situation has been exacerbated by drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "U.S. Secretary of State Hillary Clinton", "those traveling near the Somali coast", "Basilan", "a skilled hacker", "on supporting full marriage equality,\"", "the college campus.", "President Sheikh Sharif Sheikh Ahmed", "20,000", "Department of Homeland Security Secretary Janet Napolitano", "fighting charges of Nazi war crimes for well over two decades.", "12-1", "second", "criminals who had fired on an army patrol shot and killed the students, the Interior Ministry said.", "Mashhad", "543", "Fullerton, California,", "(Buying a Prius) shows the world that you love the environment and hate using fuel,\"", "the Russian air force", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun,\"", "Kimberlin Brown", "Philippe Petit", "the year 2026", "Hugh Hefner", "Switzerland", "the Seventies", "1997", "bronze", "Mary Astor", "Tuesday", "wildebeest", "Parris Island"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6384133643376706}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3870967741935484, 1.0, 0.5, 1.0, 0.0, 0.26666666666666666, 0.125, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.25, 0.0, 0.2857142857142857, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.7058823529411764, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-1225", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-653", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-4143", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-1234", "mrqa_newsqa-validation-3203", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1432", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-19", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-1190", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-2398", "mrqa_newsqa-validation-2850", "mrqa_triviaqa-validation-6942"], "SR": 0.546875, "CSR": 0.5708705357142857, "EFR": 0.9655172413793104, "Overall": 0.6887619304187191}, {"timecode": 56, "before_eval_results": {"predictions": ["hormones", "microscopic analysis of oriented thin sections of geologic samples", "nearly $41 trillion", "in Eisleben, Saxony,", "The EU naval force", "Gyanendra", "a head injury.", "Haitians", "by the time the Presidents Day holiday weekend", "Harrison Ford", "Al-Shabaab,", "Don Draper", "\"It's really nice to have people who eat anything really appreciate the vegan treats,\"", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "President Sheikh Sharif Sheikh Ahmed", "two satellites", "Sgt. Jason Bendett", "150", "the hiring of hundreds of foreign workers for a construction project", "CNN/Opinion Research Corporation", "Amitabh Bachchan", "\"Most of my friends have put in at least a couple hours,\"", "the Sri Lankan cricket team", "\"Nothing But Love\"", "(Saturn owners)", "The European Union", "Five of us for the United States and two against us because they were stranded in Japan\"", "the job bill's controversial millionaire's surtax,", "Los Angeles County Fire Department", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "one count of attempted murder in the second degree", "ordered the immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guant Bay, Cuba.", "to \"move down\" from the new-car market because a new model is simply out of their reach.", "\"Drug trafficking is a transnational threat, and therefore national initiatives have their limitations,\"", "Wigan Athletic", "Stephen Vance", "Adam Lambert", "Chester Stiles, 38,", "President Bush", "1,073 immigration detainees", "James Newell Osterberg", "\"A Lion Among Men,\"", "Monday and Tuesday", "New York-based Human Rights Watch", "Larry Ellison,", "84-year-old", "Leo Frank", "\"This is not something that anybody can reasonably anticipate,\"", "It wasn't appreciated how much of an impact it can have on a patient's quality of life,\"", "North Korea", "Jaime Andrade", "\"The Cycle of Life,\"", "certain actions taken by employers or unions that violate the National Labor Relations Act", "14 December 1972 UTC", "24", "triathlon", "Thomas Chippendale", "Andy Murray", "Martin \"Marty\" McCann", "Captain B.J. Hunnicutt", "Jackson Storm", "arsenic", "the 'X'", "Australian universities"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5948476529166691}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 0.9473684210526316, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.17391304347826086, 0.4615384615384615, 0.33333333333333337, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.6666666666666666, 0.0, 0.11764705882352941, 0.5714285714285715, 0.0, 0.0, 0.4, 0.048780487804878044, 0.6956521739130436, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5038", "mrqa_squad-validation-2212", "mrqa_newsqa-validation-1574", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-3121", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3181", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-2936", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-827", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-94", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2013", "mrqa_naturalquestions-validation-290", "mrqa_hotpotqa-validation-2702", "mrqa_searchqa-validation-10114", "mrqa_searchqa-validation-6692"], "SR": 0.46875, "CSR": 0.569078947368421, "EFR": 0.9705882352941176, "Overall": 0.6894178115325077}, {"timecode": 57, "before_eval_results": {"predictions": ["Sunday Times University of the Year", "probabilistic", "d'Hondt", "Gene Barry", "Muhammad", "Djokovic", "peptide bonds", "Around 1200", "Atlanta, Georgia", "Jessica Simpson", "Kevin Spacey", "Bart Cummings", "Jodie Foster", "Tlaxcalan warriors led by Hern\u00e1n Cort\u00e9s and Xicotencatl the Younger", "Andrew Michael Harrison", "virtual reality simulator", "Edward Seton", "Lake Powell", "September of that year", "two", "Lori Rom", "`` speed limit '' omitted and an additional panel stating the type of hazard ahead", "a stray wandering the streets of Moscow", "Daniel A. Dailey", "Antigonon leptopus", "James Chadwick", "Krypton", "United Nations", "Mason Alan Dinehart", "skeletal muscle and the brain", "These films are being tracked down by SS agents like Blake for dispatch to Hitler for an as - yet - unknown purpose", "Lou Rawls", "in 2017,", "New York City", "Pat McCormick", "`` Nelson's Sparrow ''", "American swimmer Michael Phelps", "Seattle, Washington", "five", "the recommended. HTML", "Steve Russell", "D.J.", "Philippe Petit", "United States", "October 28, 2007", "the top 50 accounts", "post translational modification", "Hon July Moyo", "`` Skid Row ''", "Mankombu Sambasivan Swaminathan", "1994", "Chris Rea", "Homo Floresiensis", "Tim Peake", "the knife", "Sir Seretse Goitsebeng Maphiri Khama", "Essex", "Kentucky River", "about 50", "Israel", "five female pastors", "host", "Narcissus", "a person"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6545255908307379}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 1.0, 1.0, 0.09090909090909091, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-5925", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-2733", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-5048", "mrqa_naturalquestions-validation-3614", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-4177", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5211", "mrqa_triviaqa-validation-5598", "mrqa_hotpotqa-validation-1218", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-2275", "mrqa_searchqa-validation-1213", "mrqa_searchqa-validation-3021"], "SR": 0.578125, "CSR": 0.5692349137931034, "EFR": 0.8888888888888888, "Overall": 0.6731091355363985}, {"timecode": 58, "before_eval_results": {"predictions": ["Tomingaj, near Gra\u010dac", "Class I MHC molecules", "2011", "as of October 1, 2015", "Frankie Muniz", "De Wayne Warren", "The Continental Congress", "Tom Brady", "1997", "2016", "plant food", "RAM", "Andaman and Nicobar Islands", "W. Edwards Deming", "LED illuminated display", "1804", "1078", "Billie Jean King", "`` Great G minor symphony ''", "a major fall in stock prices", "1940", "Ben Findon, Mike Myers and Bob Puzey", "Allison Janney", "$19.8 trillion", "Jonathan Goldstein", "six degrees", "frontal lobe", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "MGM Resorts International", "Davos", "asphyxia", "4.5 pounds or 2.04 kg", "Ron Clements and John Musker", "January 1, 1976", "Bill Pullman", "July 4", "the variety of headdresses worn by Muslim women", "a theory ( see Stryker and Vryan, 2003, for a clear distinction between the two as it pertains to interactionist - inspired conceptualizations ) can be assessed on the basis of effective conceptualizations", "eight", "production -- possibility frontier ( PPF ) or production possibility curve ( PPC )", "Athens", "New England", "the customer's account", "Germany", "a mark that reminds of the Omnipotent Lord", "two installments", "loosely on Eminem's actual upbringing, and follows white rapper B - Rabbit ( Eminem ) and his attempt to launch a career in a genre dominated by African - Americans", "Chicago metropolitan area", "Jonny Buckland", "Professor Eobard Thawne", "Natya Shastra", "The actors are married in real life and also still recur in their roles that made them famous", "Triumph", "railway", "fibrous", "1964", "Epic Records", "mathematician and physicist", "Elisabeth Fritzl,", "gasoline", "\"The U.S. subcontracted out an assassination program against al Qaeda... in early 2006.\"", "ticks", "Joe Biden", "Blitzkrieg"], "metric_results": {"EM": 0.328125, "QA-F1": 0.47200520833333326}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false], "QA-F1": [0.5, 0.75, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.1904761904761905, 0.4, 0.8, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.6666666666666666, 0.12500000000000003, 1.0, 0.4444444444444445, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.13333333333333336, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1227", "mrqa_squad-validation-6618", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-9523", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-3093", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-8005", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-1799", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-7906", "mrqa_naturalquestions-validation-6564", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-7509", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-1507", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-10135", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-5185", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-3246", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-7751", "mrqa_hotpotqa-validation-4280", "mrqa_hotpotqa-validation-1822", "mrqa_hotpotqa-validation-4856", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-509", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-7402"], "SR": 0.328125, "CSR": 0.5651483050847458, "EFR": 0.9767441860465116, "Overall": 0.6898628732262514}, {"timecode": 59, "before_eval_results": {"predictions": ["\u2212F", "100", "productivity gap", "Toby Keith", "John Adams", "Abbot Suger", "senators", "1966", "Justice Lawrence John Wargrave", "Isaiah Amir Mustafa", "Effy", "MGM Resorts International", "Joudeh Al - Goudia family ( who were added to the original arrangement in the time of Saladin, the Muslim conqueror who seized the holy city from the Crusaders in 1187 )", "Hulne Priory in Northumberland", "`` Lady Arbuthnot's Chamber ''", "538", "a 1993 American comedy - drama film directed by Fred Schepisi, adapted from the Pulitzer Prize - nominated John Guare play of the same name", "Turner Layton", "`` 1 lakh of people ''", "Richard Stallman", "Ferm\u00edn Francisco de Lasu\u00e9n", "to manage the characteristics of the beer's head", "Sean O' Neal", "Brazil", "950 pesos ( approximately $ 18 )", "Best Art Direction, Best Makeup, and Best Visual Effects", "the ACU", "16.5 quadrillion BTUs", "an Easter egg", "`` Gaddocker's Diet Beer,", "the Speaker of the House of Representatives", "cell - mediated, cytotoxic innate immunity", "Miami Heat", "Terry Kath", "since 3, 1, and 4", "pathology", "reared", "Sarah Silverman", "the Coriolis force", "Number 4, Privet Drive, Little Whinging in Surrey, England", "Bon Jovi", "produced with constant technology and resources per unit of time", "amino acids glycine and arginine", "April 6, 1917", "regulatory site", "`` Suicide & Obsession ''", "October 27, 2017", "All three", "1912", "Zilphia Horton", "Bryan Cranston", "July 2, 1776", "Sachin Tendulkar", "Christian Dior", "\"party\"", "John Vereker", "841", "VH1", "September 21.", "Sen. Barack Obama", "Robert", "Wendell, North Carolina", "cinnamon", "The Tiger lancifolium"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6239694622507123}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.14814814814814814, 0.0, 0.0, 1.0, 0.08333333333333334, 1.0, 0.4, 1.0, 1.0, 0.7692307692307692, 0.4, 1.0, 0.8, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-10333", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-6207", "mrqa_naturalquestions-validation-6999", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-8676", "mrqa_naturalquestions-validation-5454", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-686", "mrqa_naturalquestions-validation-2092", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-6091", "mrqa_triviaqa-validation-4198", "mrqa_hotpotqa-validation-992", "mrqa_newsqa-validation-1976", "mrqa_searchqa-validation-1784", "mrqa_searchqa-validation-10417"], "SR": 0.53125, "CSR": 0.5645833333333333, "EFR": 0.9666666666666667, "Overall": 0.687734375}, {"timecode": 60, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1097", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-1231", "mrqa_hotpotqa-validation-1245", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-1618", "mrqa_hotpotqa-validation-1625", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-1686", "mrqa_hotpotqa-validation-1743", "mrqa_hotpotqa-validation-1756", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1806", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2031", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2164", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2203", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2368", "mrqa_hotpotqa-validation-2389", "mrqa_hotpotqa-validation-2392", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2511", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3181", "mrqa_hotpotqa-validation-3203", "mrqa_hotpotqa-validation-3228", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3317", "mrqa_hotpotqa-validation-3345", "mrqa_hotpotqa-validation-336", "mrqa_hotpotqa-validation-3392", "mrqa_hotpotqa-validation-3437", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-3613", "mrqa_hotpotqa-validation-3649", "mrqa_hotpotqa-validation-3825", "mrqa_hotpotqa-validation-3832", "mrqa_hotpotqa-validation-3865", "mrqa_hotpotqa-validation-391", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4186", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4327", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4519", "mrqa_hotpotqa-validation-4527", "mrqa_hotpotqa-validation-467", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4966", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5292", "mrqa_hotpotqa-validation-5302", "mrqa_hotpotqa-validation-5333", "mrqa_hotpotqa-validation-5375", "mrqa_hotpotqa-validation-5389", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-5541", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5616", "mrqa_hotpotqa-validation-5737", "mrqa_hotpotqa-validation-5818", "mrqa_hotpotqa-validation-632", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-810", "mrqa_hotpotqa-validation-88", "mrqa_naturalquestions-validation-10049", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10439", "mrqa_naturalquestions-validation-10509", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-2010", "mrqa_naturalquestions-validation-2016", "mrqa_naturalquestions-validation-2110", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2605", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3522", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-412", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-4354", "mrqa_naturalquestions-validation-4531", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-525", "mrqa_naturalquestions-validation-592", "mrqa_naturalquestions-validation-6009", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-662", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-7062", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-7480", "mrqa_naturalquestions-validation-7502", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-7517", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-7942", "mrqa_naturalquestions-validation-8103", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-8154", "mrqa_naturalquestions-validation-8175", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-8326", "mrqa_naturalquestions-validation-8423", "mrqa_naturalquestions-validation-853", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8870", "mrqa_naturalquestions-validation-9029", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-9276", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-950", "mrqa_naturalquestions-validation-9684", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-9931", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-1296", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-1592", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2039", "mrqa_newsqa-validation-2072", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2283", "mrqa_newsqa-validation-2317", "mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-2474", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2574", "mrqa_newsqa-validation-2583", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-2594", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-2892", "mrqa_newsqa-validation-296", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3257", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-354", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3642", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3675", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3758", "mrqa_newsqa-validation-3883", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-492", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-644", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-841", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-862", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-927", "mrqa_searchqa-validation-10521", "mrqa_searchqa-validation-11260", "mrqa_searchqa-validation-11438", "mrqa_searchqa-validation-11442", "mrqa_searchqa-validation-1254", "mrqa_searchqa-validation-12987", "mrqa_searchqa-validation-13081", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13246", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-13611", "mrqa_searchqa-validation-13634", "mrqa_searchqa-validation-13735", "mrqa_searchqa-validation-14037", "mrqa_searchqa-validation-146", "mrqa_searchqa-validation-14635", "mrqa_searchqa-validation-14744", "mrqa_searchqa-validation-14988", "mrqa_searchqa-validation-15114", "mrqa_searchqa-validation-15366", "mrqa_searchqa-validation-15685", "mrqa_searchqa-validation-16240", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2276", "mrqa_searchqa-validation-2598", "mrqa_searchqa-validation-2631", "mrqa_searchqa-validation-2917", "mrqa_searchqa-validation-2977", "mrqa_searchqa-validation-3086", "mrqa_searchqa-validation-3569", "mrqa_searchqa-validation-3739", "mrqa_searchqa-validation-4098", "mrqa_searchqa-validation-4675", "mrqa_searchqa-validation-4741", "mrqa_searchqa-validation-5205", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5647", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-578", "mrqa_searchqa-validation-5795", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-5910", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7551", "mrqa_searchqa-validation-785", "mrqa_searchqa-validation-7873", "mrqa_searchqa-validation-8393", "mrqa_searchqa-validation-8705", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-9274", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9442", "mrqa_searchqa-validation-9508", "mrqa_searchqa-validation-9760", "mrqa_searchqa-validation-9820", "mrqa_squad-validation-1", "mrqa_squad-validation-10020", "mrqa_squad-validation-10027", "mrqa_squad-validation-10054", "mrqa_squad-validation-10137", "mrqa_squad-validation-10160", "mrqa_squad-validation-10206", "mrqa_squad-validation-1028", "mrqa_squad-validation-10309", "mrqa_squad-validation-10316", "mrqa_squad-validation-10416", "mrqa_squad-validation-10500", "mrqa_squad-validation-1051", "mrqa_squad-validation-1098", "mrqa_squad-validation-1148", "mrqa_squad-validation-1311", "mrqa_squad-validation-1394", "mrqa_squad-validation-1424", "mrqa_squad-validation-1467", "mrqa_squad-validation-1474", "mrqa_squad-validation-1516", "mrqa_squad-validation-1544", "mrqa_squad-validation-167", "mrqa_squad-validation-1695", "mrqa_squad-validation-1736", "mrqa_squad-validation-1758", "mrqa_squad-validation-1771", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2122", "mrqa_squad-validation-218", "mrqa_squad-validation-2191", "mrqa_squad-validation-2292", "mrqa_squad-validation-2292", "mrqa_squad-validation-2315", "mrqa_squad-validation-2648", "mrqa_squad-validation-2719", "mrqa_squad-validation-2736", "mrqa_squad-validation-3048", "mrqa_squad-validation-3069", "mrqa_squad-validation-3112", "mrqa_squad-validation-3146", "mrqa_squad-validation-327", "mrqa_squad-validation-328", "mrqa_squad-validation-338", "mrqa_squad-validation-3469", "mrqa_squad-validation-3599", "mrqa_squad-validation-3642", "mrqa_squad-validation-3661", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3907", "mrqa_squad-validation-3921", "mrqa_squad-validation-3942", "mrqa_squad-validation-402", "mrqa_squad-validation-4168", "mrqa_squad-validation-4173", "mrqa_squad-validation-4304", "mrqa_squad-validation-4404", "mrqa_squad-validation-4429", "mrqa_squad-validation-4442", "mrqa_squad-validation-4646", "mrqa_squad-validation-4655", "mrqa_squad-validation-466", "mrqa_squad-validation-4662", "mrqa_squad-validation-478", "mrqa_squad-validation-4802", "mrqa_squad-validation-4875", "mrqa_squad-validation-4896", "mrqa_squad-validation-4953", "mrqa_squad-validation-4958", "mrqa_squad-validation-5077", "mrqa_squad-validation-5311", "mrqa_squad-validation-5322", "mrqa_squad-validation-5337", "mrqa_squad-validation-5396", "mrqa_squad-validation-5428", "mrqa_squad-validation-5457", "mrqa_squad-validation-547", "mrqa_squad-validation-5503", "mrqa_squad-validation-5537", "mrqa_squad-validation-5611", "mrqa_squad-validation-5635", "mrqa_squad-validation-5754", "mrqa_squad-validation-5857", "mrqa_squad-validation-5877", "mrqa_squad-validation-5927", "mrqa_squad-validation-5967", "mrqa_squad-validation-6086", "mrqa_squad-validation-61", "mrqa_squad-validation-6115", "mrqa_squad-validation-6122", "mrqa_squad-validation-6125", "mrqa_squad-validation-6128", "mrqa_squad-validation-6150", "mrqa_squad-validation-6224", "mrqa_squad-validation-6279", "mrqa_squad-validation-6312", "mrqa_squad-validation-6361", "mrqa_squad-validation-6380", "mrqa_squad-validation-6434", "mrqa_squad-validation-6453", "mrqa_squad-validation-6474", "mrqa_squad-validation-6726", "mrqa_squad-validation-6777", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-6879", "mrqa_squad-validation-6887", "mrqa_squad-validation-6919", "mrqa_squad-validation-7006", "mrqa_squad-validation-7165", "mrqa_squad-validation-7211", "mrqa_squad-validation-7217", "mrqa_squad-validation-725", "mrqa_squad-validation-7284", "mrqa_squad-validation-729", "mrqa_squad-validation-7537", "mrqa_squad-validation-7615", "mrqa_squad-validation-7627", "mrqa_squad-validation-7665", "mrqa_squad-validation-7683", "mrqa_squad-validation-7816", "mrqa_squad-validation-7835", "mrqa_squad-validation-7877", "mrqa_squad-validation-7991", "mrqa_squad-validation-802", "mrqa_squad-validation-8103", "mrqa_squad-validation-8143", "mrqa_squad-validation-820", "mrqa_squad-validation-8210", "mrqa_squad-validation-8244", "mrqa_squad-validation-8245", "mrqa_squad-validation-8250", "mrqa_squad-validation-8338", "mrqa_squad-validation-8401", "mrqa_squad-validation-8566", "mrqa_squad-validation-8599", "mrqa_squad-validation-8792", "mrqa_squad-validation-8958", "mrqa_squad-validation-9068", "mrqa_squad-validation-9151", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9346", "mrqa_squad-validation-9541", "mrqa_squad-validation-9595", "mrqa_squad-validation-9643", "mrqa_squad-validation-9701", "mrqa_squad-validation-9709", "mrqa_squad-validation-973", "mrqa_squad-validation-9787", "mrqa_squad-validation-9796", "mrqa_squad-validation-9837", "mrqa_squad-validation-999", "mrqa_triviaqa-validation-101", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1200", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1956", "mrqa_triviaqa-validation-2026", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2229", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2500", "mrqa_triviaqa-validation-2697", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2745", "mrqa_triviaqa-validation-3534", "mrqa_triviaqa-validation-3547", "mrqa_triviaqa-validation-3594", "mrqa_triviaqa-validation-3774", "mrqa_triviaqa-validation-3794", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-3977", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4183", "mrqa_triviaqa-validation-4198", "mrqa_triviaqa-validation-4404", "mrqa_triviaqa-validation-4532", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4975", "mrqa_triviaqa-validation-5102", "mrqa_triviaqa-validation-5199", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-5506", "mrqa_triviaqa-validation-561", "mrqa_triviaqa-validation-5903", "mrqa_triviaqa-validation-6141", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6610", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-7018", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7351", "mrqa_triviaqa-validation-7531", "mrqa_triviaqa-validation-867"], "OKR": 0.80078125, "KG": 0.4265625, "before_eval_results": {"predictions": ["1957", "The Judicial Council", "Sports Night", "April 1917", "Kim Basinger", "Ki Toy Johnson", "24", "Shenzi", "the 1840s", "September 1980", "the Oxford English Dictionary", "Edward Kenway ( Matt Ryan )", "2018", "cell surface receptor complex known as Interleukin - 10", "the Red Sea", "the spinal cord", "1999", "the United Kingdom", "Montreal Canadiens ( 24 )", "March 9, 2018", "Malina Weissman", "49", "2009", "Alan Eustace", "2019", "Chelsea", "Ra\u00fal Eduardo Esparza", "Shawn Wayans", "1807", "eleven", "Doc '' Brown", "Lady Gaga", "Mitch Murray", "May 2017", "during the day", "the Jos Plateau", "Barbara Windsor", "scrolls", "Charles Cavaroc v", "hydrogen", "Madison, Wisconsin, United States", "Starscream", "abolished most civil liberties and transferred state powers to the Reich government", "Macon Blair", "8 December 1985", "a hostname ( www.example.com )", "1998", "Alex Ryan", "Eukarya", "the development of electronic computers in the 1950s", "exceeds the normal resting rate", "a liquid crystal on silicon ( LCoS ) ( based on an LCo S ) (based on anLCoS chip from Himax ), field - sequential color system, LED illuminated display", "\u201cDance Stance\u201d", "the manly virtues", "1500m", "1991", "Urijah Christopher Faber", "1993 to 1996", "Spanish Davis Cup hero Fernando Verdasco", "Teen Patti", "called Chu \"uniquely suited to be our next secretary of energy\"", "The Rio Grande", "the kidneys", "the Cobb salad"], "metric_results": {"EM": 0.5, "QA-F1": 0.5917497144059645}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.0, 0.07407407407407407, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 0.060606060606060615, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8368", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-5997", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-578", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-5738", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-359", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4505", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-3347", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-9811", "mrqa_naturalquestions-validation-8229", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-754", "mrqa_triviaqa-validation-5198", "mrqa_triviaqa-validation-24", "mrqa_triviaqa-validation-2359", "mrqa_hotpotqa-validation-2625", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-674"], "SR": 0.5, "CSR": 0.5635245901639344, "EFR": 0.875, "Overall": 0.6804392930327869}, {"timecode": 61, "before_eval_results": {"predictions": ["Charles Darwin's", "to manage the pharmacy department and specialised areas in pharmacy practice allowing pharmacists the time to specialise in their expert field as medication consultants spending more time working with patients and in research", "interacting and working directly with students", "Burma", "peace", "Red Bank", "La Toya Jackson", "an overcast day outside you should pay attention to the kind of clouds in the sky.", "Edward of Bordeaux", "Lost in America", "Louis XVIII", "a tomato", "Nunavut, Canada", "a monkey", "the sea", "a turquoise", "the moses", "carousel", "a mug", "a jackson's sucessor from NY< portrayed as an aristocrat by whigs,", "a barrel", "lilah", "a chakra (Sanskrit cakra, \"wheel\")", "the plague", "Wall Street", "Tiger Woods", "a New York taxi", "Colorado", "Broadway (Manhattan)", "Eastman Kodak Company", "Juicy Couture", "a skirting board", "You Are What You Are", "Istanbul", "New York (Port Washington, L.I.)", "the opium trade", "Brad Paisley", "a peter", "Maharishi Mahesh Yogi", "New York, Vermont", "Theodore Roosevelt", "a violin", "The Lamb", "Austin Butts", "Elvis Presley", "The Washington Post", "Man Ray", "a bass (crumhorn and rauschpfiefe)", "Oedipus", "the Bay of Biscay", "The Mystery of Edwin Drood", "Harry Goodman", "Gorakhpur Junction", "about 1500 BC", "Reveille", "Spain", "peter pypelinckx", "Katy", "Tool", "The entity", "Mark Helfrich", "Steven Green", "Saturday's Hungarian Grand Prix.", "One group -- People Against Switching Sides (PASS) -- unsuccessfully challenged the constitutionality of the change in the country's Supreme Court."], "metric_results": {"EM": 0.359375, "QA-F1": 0.46625744047619044}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, false, false], "QA-F1": [0.5, 0.45000000000000007, 1.0, 0.0, 0.0, 1.0, 0.0, 0.14285714285714288, 0.5, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.47619047619047616]}}, "before_error_ids": ["mrqa_squad-validation-5179", "mrqa_squad-validation-6324", "mrqa_searchqa-validation-9701", "mrqa_searchqa-validation-9357", "mrqa_searchqa-validation-10839", "mrqa_searchqa-validation-9796", "mrqa_searchqa-validation-16918", "mrqa_searchqa-validation-3756", "mrqa_searchqa-validation-7409", "mrqa_searchqa-validation-16254", "mrqa_searchqa-validation-6457", "mrqa_searchqa-validation-14750", "mrqa_searchqa-validation-12163", "mrqa_searchqa-validation-2710", "mrqa_searchqa-validation-7733", "mrqa_searchqa-validation-2006", "mrqa_searchqa-validation-1041", "mrqa_searchqa-validation-2346", "mrqa_searchqa-validation-8006", "mrqa_searchqa-validation-3540", "mrqa_searchqa-validation-2713", "mrqa_searchqa-validation-764", "mrqa_searchqa-validation-14742", "mrqa_searchqa-validation-2011", "mrqa_searchqa-validation-7030", "mrqa_searchqa-validation-7772", "mrqa_searchqa-validation-4774", "mrqa_searchqa-validation-8059", "mrqa_searchqa-validation-174", "mrqa_searchqa-validation-5057", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-5924", "mrqa_searchqa-validation-8268", "mrqa_searchqa-validation-1369", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-4354", "mrqa_searchqa-validation-3030", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-6155", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-1319"], "SR": 0.359375, "CSR": 0.5602318548387097, "EFR": 1.0, "Overall": 0.704780745967742}, {"timecode": 62, "before_eval_results": {"predictions": ["Saul Alinsky", "in the spring of 1329", "to protect their tribal lands from commercial interests", "Legally Blonde", "Equus", "Shia LaBeouf", "the Teamsters Union", "New Zealand", "a microwave oven", "Roosevelt", "Budapest", "feminism", "John Hersey", "tofu", "the roof", "Queen Victoria", "Judge Advocate General's Corps", "Destiny's Child", "the Ottoman Empire", "Narnia", "Vermont", "a rolling stone", "ex-wife", "Austria", "Dante", "an alto flute", "Kosher Wines", "The Hague", "(Mary Virginia) Martin", "\"All About Eve\"", "U2", "Laos", "Saint Andrew", "\"Walden\"", "\"The Color Purple\"", "Anthony Fokker", "an artist, dancer and choreographer who popularized Indian dance through his effective use of western theatrical techniques in combination", "Ypres", "Vladimir Lenin", "Bots", "a carpet", "lampoon", "stars", "a centaur", "India", "Treasury", "nekropolis", "Israel", "\"Little Women\"", "the ceiling", "Belgium", "the Vietnam War", "15,000 BC", "Jewish audiences", "October 6, 2017", "Japan", "Fred Gwynne", "Vancouver", "TD Garden", "Mitsubishi", "zoonotic", "seeking help", "Zimbabwe President", "a fight outside of an Atlanta strip club"], "metric_results": {"EM": 0.5, "QA-F1": 0.60445916473877}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false], "QA-F1": [1.0, 0.8571428571428571, 0.3157894736842105, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.30769230769230765]}}, "before_error_ids": ["mrqa_squad-validation-8158", "mrqa_squad-validation-4314", "mrqa_searchqa-validation-14081", "mrqa_searchqa-validation-7735", "mrqa_searchqa-validation-2017", "mrqa_searchqa-validation-13130", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-13817", "mrqa_searchqa-validation-1212", "mrqa_searchqa-validation-13068", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-12070", "mrqa_searchqa-validation-6169", "mrqa_searchqa-validation-4548", "mrqa_searchqa-validation-15346", "mrqa_searchqa-validation-9729", "mrqa_searchqa-validation-8773", "mrqa_searchqa-validation-1728", "mrqa_searchqa-validation-13426", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-12122", "mrqa_searchqa-validation-12255", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-11763", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-7004", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-3027", "mrqa_triviaqa-validation-4487", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-85"], "SR": 0.5, "CSR": 0.5592757936507937, "EFR": 0.96875, "Overall": 0.6983395337301588}, {"timecode": 63, "before_eval_results": {"predictions": ["Montpellier", "39", "easier credit to the lower and middle income earners", "Argentine striker Pedro Calomino", "May 5, 2015", "Julia Verdin", "Dennis Potter", "1909 Cuban-American Major League Clubs Series", "Martin Ingerman", "\"The Godfather Part II\"", "to kill on a hunt", "October 21, 2016", "Memphis", "William Corcoran Eustis", "queen consort of Hanover", "the Washington Huskies", "The club", "December 24, 1973", "Christianity", "Polihale State Park", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "its riverside location", "322,520", "\"North Greenwich Arena", "Ringo Starr", "KlingStubbins", "FC Bayern Munich", "four", "Elsie Audrey Mossom", "Rice University", "Albert Bridge", "Count Schlieffen", "the Mexican War on Drugs", "Erreway", "The Pale", "Oracle", "Katherine Harris", "Hong Kong", "Ice Princess", "Hellenism", "the North Atlantic Ocean", "A. E. Housman", "Cyclic Defrost", "\"Losing My Religion\"", "the Fundamentalist Church of Jesus Christ of Latter-Day Saints", "1998", "Boston University", "dachshund", "Frank Sinatra", "Woking, England", "Army & Navy Stores", "The 2016\u201317 Dallas Stars season", "Kate Walsh", "November 3, 2007", "a constitutional right", "Cybill Shepherd", "Pamplona", "the narwhal", "$40 billion during the operations phase.", "a new GI Bill that expands education benefits for veterans who have served since the 9/11 attacks, provides a 13-week extension of unemployment benefits and more than $2 billion in disaster assistance", "the Democratic VP candidate", "Haile Selassie", "cherry", "Gettysburg"], "metric_results": {"EM": 0.625, "QA-F1": 0.7106646825396825}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.4444444444444445, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.1142857142857143, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3034", "mrqa_squad-validation-639", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-151", "mrqa_hotpotqa-validation-5855", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-3897", "mrqa_hotpotqa-validation-1319", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3544", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-4160", "mrqa_hotpotqa-validation-5723", "mrqa_triviaqa-validation-51", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-163", "mrqa_searchqa-validation-14467"], "SR": 0.625, "CSR": 0.560302734375, "EFR": 1.0, "Overall": 0.704794921875}, {"timecode": 64, "before_eval_results": {"predictions": ["six", "Six", "rear - view mirror ( or rearview mirror )", "Scorpions", "Fighter Command", "to fit in", "Kyrie Irving, winner of the 2014 All - Star Game MVP,", "over 300,000", "Canada", "Jesse Frederick James Conaway", "B.R. Ambedkar", "early 1988", "Malibu, California beach intercut with scenes of them driving an orange campervan", "the American Civil War", "moist temperate climates, with much of the world's production occurring near the 48th parallel north", "a simple majority", "American singer and songwriter Lana Del Rey", "Nicole Gale Anderson", "TLC - All That", "Charles Perrault", "Djokovic", "punk rock", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "declared neutrality and worked to broker a peace", "1986", "between 1765 and 1783", "annuity", "Anthony Caruso as Johnny Rivers", "Michael Crawford", "Inequality of opportunity was higher in the transition economies of Central and Eastern Europe and Central Asia than in some other developed economies in Western Europe ( except France,", "Koine Greek : apokalypsis, meaning `` unveiling '' or `` revelation '' ( before title pages and titles, books were commonly known by their first words", "4 September 1936", "the United States by American Broadcasting Company ( ABC ), produced by Michael De Luca and Jennifer Todd and directed by Glenn Weiss", "sometime between 124 and 800 CE, with some theories dating the earliest Polynesian settlements to the 10th or even 13th century", "biscuit", "Greek mythology, first mentioned in Homer's Odyssey as the home of the Phaeacians and the last destination of Odysseus in his 10 - year journey before returning home to Ithaca", "Audrey II", "Master Christopher Jones", "RMS Titanic", "the right side of the heart to the lungs", "Pittsburgh in 2008", "the sea witch character who appears in the fairy tale `` The Little Mermaid '' by Hans Christian Andersen", "15 February 1998", "Thomas Jefferson's", "Arthur Chung", "advisory speed signs are classified as warning signs, not regulatory signs", "March 31, 2013", "semi-automatic, but not fully automatic, firearms by Swiss citizens and foreigners with permanent residence", "1987", "Rajendra Prasad", "Donna", "until December 1800", "\"golden wedding anniversary\"", "Western Samoa", "the Canadian Inuit Dog", "a micronutrient-rich diet", "M\u00e6rsk Mc- Kinney M\u00f8ller", "December 13, 1920", "raping and killing a 14-year-old Iraqi girl.", "verify the authenticity of the voice on the tape.", "$627,", "A Portrait of the Artist", "Crete", "a brownie"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6164272359584859}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.6, 1.0, 0.5, 1.0, 1.0, 0.5, 0.22222222222222218, 0.25, 1.0, 0.4, 0.16666666666666669, 0.5714285714285715, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.2, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8591", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-462", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-7067", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2989", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-3342", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3515", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-4414", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-171", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-3505", "mrqa_hotpotqa-validation-5873", "mrqa_hotpotqa-validation-5688", "mrqa_searchqa-validation-1924"], "SR": 0.46875, "CSR": 0.5588942307692308, "EFR": 0.9411764705882353, "Overall": 0.6927485152714933}, {"timecode": 65, "before_eval_results": {"predictions": ["1856", "NASA", "Matt Flinders", "2026", "in the pouring rain", "in a compact layout to combine keys which are usually kept separate", "Ra\u00fal Eduardo Esparza", "1963", "April 1979", "1792", "Charles Carson", "November 5, 2017", "not being pushed around by big labels, managers, and agents and being told what to do", "In 1974", "6,259", "blood plasma and lymph in the `` intravascular compartment '' ( inside the blood vessels and lymphatic vessels )", "O'Meara", "classical architecture", "September 19, 2017", "almost all officeholders", "Rashida Jones", "Nodar Kumaritashvili", "Olympia", "2000", "J.P. Zenger High", "Georges Auguste Escoffier", "After Shawn's kidnapping", "1904", "The FCI accepted the long - haired type in 2010, listing it as the variety b --", "Liam Cunningham", "before the first year begins", "John Pyper - Ferguson", "Woody Paige", "Albert Einstein", "the brain", "the North Cascades range of, Washington", "David Ben - Gurion", "T.J. Miller", "1995", "toys or doorbell installations", "Dirk Benedict", "New Croton Reservoir in Westchester and Putnam counties", "the source of the donor organ", "to prevent further offense by convincing the offender that their conduct was wrong", "Bill Henderson", "thunderstorms", "Federer", "1981", "1878", "St. John's, Newfoundland and Labrador", "Elvis Presley", "southern Anatolia", "Tom Shales", "All Things Must Pass", "Toronto", "Salvatore \"Salvie\" Testa", "Newark, New Jersey at the Prudential Center", "National Mall in Washington, D.C.", "75.", "Grand Ronde, Oregon.", "1994", "Buddha", "Oslo", "Coretta Scott King"], "metric_results": {"EM": 0.5, "QA-F1": 0.6355853544030788}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.1142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8205128205128205, 0.0, 0.0, 0.7000000000000001, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5263157894736842, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-2710", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-4520", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-5093", "mrqa_naturalquestions-validation-10583", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-2778", "mrqa_naturalquestions-validation-2781", "mrqa_naturalquestions-validation-2648", "mrqa_naturalquestions-validation-410", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-5259", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-7387", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-8095", "mrqa_naturalquestions-validation-9383", "mrqa_triviaqa-validation-2642", "mrqa_hotpotqa-validation-242", "mrqa_hotpotqa-validation-4896", "mrqa_hotpotqa-validation-5725", "mrqa_searchqa-validation-3274"], "SR": 0.5, "CSR": 0.5580018939393939, "EFR": 0.9375, "Overall": 0.6918347537878788}, {"timecode": 66, "before_eval_results": {"predictions": ["24 March 1879", "shopping destinations", "saliva", "Robert Louis Stevenson", "South African", "Ben Whishaw", "Neighbours", "laurie bacall", "Wawrinka", "Vienna", "Terrell", "National Space Centre", "lyon", "\u201cThe Nutcracker\u201d", "climate", "Take That", "predominantly Muslim part of the world", "USS Cole", "car", "in the British charts", "Denmark", "Conduction", "Willy Russell", "beets", "Celsius", "Henkel", "linda abner", "Alex Murphy", "aromatherapy", "Lieutenant-General", "barbeor", "Fenn Street School", "Fran\u00e7ois Hollande", "Johannesburg", "The Great Leap", "laurie bacall", "Madrid", "The Vikings in England", "Statue of Liberty", "wine director/Sommelier", "pasta", "blue", "hugh laurie", "Ned Sherrin", "The Pillow Book", "Sally Ride", "archery", "wool", "1992", "Billy Fury", "California", "Anton Chekhov", "1898", "The Third Five - year Plan", "`` Killer Within ''", "Shepardson Microsystems", "Nan Britton", "Venice", "Haleigh Cummings", "The oceans are growing crowded, and governments are increasingly trying to plan their use.", "Capitol Hill,", "the bark", "the tonka bean", "the Fertile Crescent"], "metric_results": {"EM": 0.53125, "QA-F1": 0.547048611111111}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-355", "mrqa_triviaqa-validation-1997", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-4013", "mrqa_triviaqa-validation-951", "mrqa_triviaqa-validation-2430", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-6648", "mrqa_triviaqa-validation-1094", "mrqa_triviaqa-validation-3407", "mrqa_triviaqa-validation-5318", "mrqa_triviaqa-validation-304", "mrqa_triviaqa-validation-4790", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-1688", "mrqa_triviaqa-validation-598", "mrqa_triviaqa-validation-4421", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-3673", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-4915", "mrqa_triviaqa-validation-4946", "mrqa_hotpotqa-validation-376", "mrqa_newsqa-validation-4165", "mrqa_searchqa-validation-11467"], "SR": 0.53125, "CSR": 0.5576026119402986, "EFR": 1.0, "Overall": 0.7042548973880598}, {"timecode": 67, "before_eval_results": {"predictions": ["1850", "Ed McCaffrey", "Dutch", "integrated circuit", "Abu Dhabi", "Spain, Mexico and France", "Humberside Airport", "Jeff Meldrum", "Central Avenue", "2013", "Martin Truex Jr.", "compact car", "tyachty", "gaius Julius Caesar Augustus Germanicus", "Detroit, Michigan", "924", "Birmingham, Alabama", "Sean", "Brian Friel", "Paul W. S. Anderson", "heavy metal", "Nebraska Cornhuskers women's basketball", "Frank Fertitta", "beer", "Alain Robbe-Grillet", "Awake", "Helena Sternlicht", "British", "Nickelodeon", "2,615", "McG", "\"Invader (Invasor)\"", "john Bowen", "eurydice", "Sean Yseult", "Kansas City Crime Family", "34", "1926 Paris during the period of the Lost Generation", "8 August 1907", "Jenji Kohan", "Sir William Collins", "Beatrice Tinsley", "Prada", "\"Crossed: Psychopath\"", "singer, songwriter, actress, and radio and television presenter", "1903", "Warsaw", "\"Slaughterhouse-Five\"", "2016", "Spiderwick Chronicles", "the Mediterranean", "1959", "Tom Brady", "on the left hand ring finger", "plant food, mainly grass and sedges, which were supplemented with herbaceous plants, flowering plants, shrubs, mosses, and tree matter", "egypt", "Waterloo", "ballet", "rocket", "animation", "oregon", "horror", "plumbeus", "China"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6724826388888889}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.8, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9852", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-529", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3031", "mrqa_hotpotqa-validation-3752", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-3997", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-558", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-4195", "mrqa_triviaqa-validation-1630", "mrqa_newsqa-validation-1662", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3020", "mrqa_searchqa-validation-7217"], "SR": 0.5625, "CSR": 0.5576746323529411, "EFR": 1.0, "Overall": 0.7042693014705883}, {"timecode": 68, "before_eval_results": {"predictions": ["the time of the Miocene", "observer status", "Moton Field, the Tuskegee Army Air Field,", "O'Meara", "digitization of social systems", "vehicles inspired by theJeep that are suitable for use on rough terrain", "Poems : Series 1", "South Dakota", "Ali Daei", "Hungary", "Herbert Hoover", "a judicial officer, of a lower or puisne court, elected or appointed by means of a commission ( letters patent ) to keep the peace", "The Bellamy Brothers", "the 1980s", "Hellenismos", "Thomas and Martha Wayne", "Wakanda and the Savage Land", "August Darnell", "Lori Rom", "Nodar Kumaritashvili", "T - Bone Walker", "New Mexico", "Kingsholm Stadium", "FIGG Bridge Engineers", "scrolls dating back to the 12th and 13th centuries", "architecture", "Emmett Lathrop `` Doc '' Brown", "c. 1000 AD", "Baker, California, USA", "majority of members present at that time", "the passing of the year", "Christopher Allen Lloyd", "The Actor's Home", "Moscazzano", "the Lower Mainland in Vancouver", "St Pancras International", "The Comma Johanneum", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Wednesday, September 21, 2016", "a turlough", "Mark Williams", "a substance that fully activates the receptor that it binds to ) while under other conditions, behaves as an antagonist", "Tommy Shaw", "All Hallows'Day", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "fresh water", "March 2018", "American singer Elvis Presley", "pulmonary circulation", "a Raja Dhilu", "1881", "1997", "Moon River", "the Prussian Third Army", "Nessie", "Boyd Gaming", "\"The Legend of Tarzan\"", "Love Actually", "five", "Washington", "managing his time.", "Robin Hood", "Lafayette", "Virgin Atlantic"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6783556547619047}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.24000000000000002, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.72, 1.0, 0.28571428571428575, 0.09523809523809525, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2272", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-5511", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-4870", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-9602", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-9749", "mrqa_naturalquestions-validation-8545", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-42", "mrqa_naturalquestions-validation-6843", "mrqa_triviaqa-validation-6858", "mrqa_hotpotqa-validation-4345", "mrqa_newsqa-validation-4073", "mrqa_searchqa-validation-7633", "mrqa_searchqa-validation-11403"], "SR": 0.5625, "CSR": 0.5577445652173914, "EFR": 0.9642857142857143, "Overall": 0.6971404309006212}, {"timecode": 69, "before_eval_results": {"predictions": ["Labor", "the hosts have the responsibility to ensure orderly delivery of packets", "the western part of the Republic of Botswana in southern Africa", "a minor basilica", "\"The Independent\"", "Royal College of Music", "Sullivan University", "Archbishop of Canterbury", "Canada's first train robbery", "The club will participate in the Premier League", "1941", "Tennessee", "Hard Workin' Man", "Kennedy Road", "August 1973", "Hawaii County, Hawaii", "Summer Olympic Games", "the Minnesota Timberwolves of the National Basketball Association (NBA)", "David Kossoff", "a priest", "England", "coaxial", "\"Sal\" Avellino", "racehorse breeder", "Colin Vaines", "Indraneil S Penguinpta", "win world titles in four weight classes", "Dark Heresy", "Texas Tech University", "American actor", "Scott Mosier", "Paradise, Nevada", "Adelaide", "Erinsborough", "interstate commerce", "George Balanchine", "\"Northern Lights\"", "Southern Rock Allstars", "\"Alceste\"", "Prada", "the islands concerned are sometimes referred to as the Kingdom of Mann and the Isles", "Mary Bonauto, Susan Murray", "David Wells", "Future", "Macomb County", "wine and cellar door region", "Boston, Massachusetts", "Juventus of Italy", "23 June 1912", "Matthew Abraham \"Matt\" Groening ( ; born February 15, 1954) is an American cartoonist, writer, producer, animator, and voice actor", "Dan Crow", "statesman", "1923", "Harrison Ford", "Bill Pullman", "setts", "Kwame Nkrumah", "a docked yacht", "draquila -- Italy Trembles.", "positive signal", "ConAgra Foods plant", "the Bears", "pikes", "James Watt"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6321585213032581}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.21052631578947367, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4777", "mrqa_hotpotqa-validation-4134", "mrqa_hotpotqa-validation-5440", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-114", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-1431", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5679", "mrqa_hotpotqa-validation-4712", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-1008", "mrqa_hotpotqa-validation-5472", "mrqa_hotpotqa-validation-5041", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-2407", "mrqa_naturalquestions-validation-622", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-6018", "mrqa_newsqa-validation-627", "mrqa_searchqa-validation-9505", "mrqa_searchqa-validation-1977"], "SR": 0.53125, "CSR": 0.5573660714285714, "EFR": 1.0, "Overall": 0.7042075892857144}, {"timecode": 70, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1522", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2175", "mrqa_hotpotqa-validation-2306", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4478", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4579", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4721", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1263", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1840", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-325", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4177", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-4903", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6087", "mrqa_naturalquestions-validation-6300", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-644", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8072", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-1915", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-200", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3351", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-94", "mrqa_searchqa-validation-10269", "mrqa_searchqa-validation-10839", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-10998", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12507", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15590", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-16971", "mrqa_searchqa-validation-1761", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-3828", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-4966", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-893", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9626", "mrqa_searchqa-validation-9729", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10338", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1048", "mrqa_squad-validation-10491", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1655", "mrqa_squad-validation-1769", "mrqa_squad-validation-1862", "mrqa_squad-validation-1956", "mrqa_squad-validation-1964", "mrqa_squad-validation-1977", "mrqa_squad-validation-2112", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-237", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3554", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-3965", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4286", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4615", "mrqa_squad-validation-4755", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4807", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5256", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6266", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-664", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-6924", "mrqa_squad-validation-6959", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7203", "mrqa_squad-validation-7211", "mrqa_squad-validation-7357", "mrqa_squad-validation-7407", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7793", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-7979", "mrqa_squad-validation-823", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-8566", "mrqa_squad-validation-8664", "mrqa_squad-validation-867", "mrqa_squad-validation-8837", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9567", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-9944", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1625", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2412", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4615", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4826", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6371", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-679", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7302", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.826171875, "KG": 0.47734375, "before_eval_results": {"predictions": ["the Song dynasty", "manage the pharmacy department and specialised areas", "$106.5 million", "a president who understands the world today, the future we seek and the change we", "about the police tried to cover it up,\"", "45 minutes,", "Stephen Worgu", "his health and about a comeback.", "a full garden and pool, a tennis court, or several heli-pads", "\"utterly baseless,\"", "a bag", "the procedures", "laundry service.", "The son of Gabon's former president", "Kingdom City", "killing of a 15-year-old boy", "Arlington National Cemetery", "spending billions to revitalize the nation's economy,", "the challenges a pregnancy", "The EU naval force", "Bob Bogle", "composer of \"Phantom of the Opera\" and \"Cats\"", "19-12", "$273 million", "20,000-capacity O2 Arena.", "immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "calls the Internet \"one of the most magnificent expressions of freedom and free enterprise in history\"", "Lucky Dube", "Unseeded", "\"Republicans and Democrats in Congress agreed to provide these vital funds without tying the hands of our commanders and without an artificial timetable of withdrawal from Iraq.\"", "82", "150", "burns", "celebrities with hectic lifestyles and little time to sleep,", "Ennis", "Jacob", "duct tape", "flooding and debris", "5,600", "the fact that the teens were charged as adults.", "one", "al-Maliki", "Mugabe's opponents", "Kenyan and Somali", "snowmen", "88-year-old", "Kenner, Louisiana", "suicide bombing", "\"Watchmen\"", "the U.S. Agency for International Development,", "a building in the Kenyan capital of Nairobi", "1998.", "MacFarlane", "Indirect rule", "Kryptonite", "the Cherokee Indians", "jumbly", "James Stewart", "Robert Matthew Hurley (born June 28, 1971) is an American basketball coach", "Sulla", "Gondorian soldier", "Howard Hughes", "Harry Truman", "The Curse of the Black Pearl"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5899001949917898}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5714285714285714, 0.5714285714285715, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 0.9655172413793104, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-900", "mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3927", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-1970", "mrqa_newsqa-validation-2094", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-2655", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-3437", "mrqa_newsqa-validation-3330", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-3671", "mrqa_newsqa-validation-1481", "mrqa_newsqa-validation-2945", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-2297", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-3839", "mrqa_hotpotqa-validation-4760", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-10796"], "SR": 0.484375, "CSR": 0.556338028169014, "EFR": 1.0, "Overall": 0.7188457306338029}, {"timecode": 71, "before_eval_results": {"predictions": ["playing cards", "the divinity of Jesus", "\"Regular Folks\" Ordinary People 1932: \"Magnificent Inn\" Grand Hotel", "St. Mark", "Romeo & Juliet", "Alicia Keys", "debts", "Bear Island", "Google Groups (usenet groups)", "yodeling", "The Criminal Justice Information Services Division,", "Spartacus", "the Isle of Man,", "sphinx", "The Rolloff", "Bob Hope", "the United States Senate", "Texas A&M University", "Bell Telephone Hour", "the Python", "The Nations of the Bible", "Stanford", "The Globe Sessions", "1927", "Transportation", "Mexico City", "Atolls", "a cat", "Bosnia and Herzegovina", "Christopher Columbus", "freight ton", "a tribunal", "\"pan rabbit,\"", "the hyoid horns", "Joan of Arc", "Maternity", "Mulan", "Namibia", "Abraham Lincoln", "Alaska", "President Bush", "Jim Thorpe", "a wombat", "the phi phenomenon", "Pushkin", "dessert sized portions", "the 1066 Battle of Hastings", "An Officer and a Gentleman", "Roger Bannister", "Summer Was employed as a life guard", "Monopoly", "Volkswagen", "Alice Cooper", "Miami Heat of the National Basketball Association ( NBA )", "Jurchen Aisin Gioro clan", "a black Ferrari", "the Netherlands", "Harvard", "1978", "1866", "276,170", "New Haven, Connecticut, firefighter", "stand down.", "Wednesday."], "metric_results": {"EM": 0.515625, "QA-F1": 0.5777777777777777}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1151", "mrqa_searchqa-validation-0", "mrqa_searchqa-validation-16512", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-5826", "mrqa_searchqa-validation-16238", "mrqa_searchqa-validation-13867", "mrqa_searchqa-validation-7661", "mrqa_searchqa-validation-5494", "mrqa_searchqa-validation-16635", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-14499", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-14020", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-1918", "mrqa_searchqa-validation-402", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-3840", "mrqa_searchqa-validation-10902", "mrqa_searchqa-validation-10456", "mrqa_searchqa-validation-14301", "mrqa_searchqa-validation-1396", "mrqa_searchqa-validation-10857", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-4454", "mrqa_naturalquestions-validation-9639", "mrqa_triviaqa-validation-7650", "mrqa_hotpotqa-validation-55"], "SR": 0.515625, "CSR": 0.5557725694444444, "EFR": 1.0, "Overall": 0.7187326388888889}, {"timecode": 72, "before_eval_results": {"predictions": ["baptism in the Small Catechism", "three", "Montpelier", "City of Hope", "New Orleans", "a router", "dollar", "Herbert Hoover", "crawfish", "the Doric Order", "Emil Von Behring", "Senator", "Yeast", "Lewis and Clark", "Mount Everest", "saxophones", "Etch A Sketch", "Peter Paul Rubens", "grapefruit", "Fermi", "larvae", "Jesus Christ", "The Juilliard School", "Ralph Lauren", "Mary Poppins", "air", "Kansas City", "Abe Vigoda", "a steak", "Franklin Pierce", "a calculating machine", "magnesium", "four", "occipital", "Mao Zedong", "Coretta Scott", "the uterus", "the Northland College Lumberjacks", "a magnums", "clouds", "the hog", "the Spanish American War", "George Rogers Clark", "Jack Palance", "a dose", "Columbus Day", "Southern Africa", "the Democratic Republic of Congo", "a cucumber", "a calves", "thesaurus", "Prince William and Kate Middleton", "Pac - 12 Conference Champions Stanford Cardinal", "Todd Bridges", "August 18, 1998", "feet", "the Frog", "Berlin", "Tybalt", "evangelical Christian", "Windermere", "6-2", "Kaka", "last year's"], "metric_results": {"EM": 0.4375, "QA-F1": 0.56875}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2448", "mrqa_searchqa-validation-16859", "mrqa_searchqa-validation-6176", "mrqa_searchqa-validation-1119", "mrqa_searchqa-validation-3445", "mrqa_searchqa-validation-4928", "mrqa_searchqa-validation-16619", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-12215", "mrqa_searchqa-validation-15701", "mrqa_searchqa-validation-14222", "mrqa_searchqa-validation-12825", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-11217", "mrqa_searchqa-validation-11175", "mrqa_searchqa-validation-15456", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-12319", "mrqa_searchqa-validation-4301", "mrqa_searchqa-validation-2531", "mrqa_searchqa-validation-14033", "mrqa_searchqa-validation-3608", "mrqa_searchqa-validation-13169", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-15708", "mrqa_naturalquestions-validation-3926", "mrqa_triviaqa-validation-4521", "mrqa_triviaqa-validation-449", "mrqa_hotpotqa-validation-1680", "mrqa_newsqa-validation-802", "mrqa_newsqa-validation-1750"], "SR": 0.4375, "CSR": 0.554152397260274, "EFR": 1.0, "Overall": 0.7184086044520548}, {"timecode": 73, "before_eval_results": {"predictions": ["IgG", "the green chloroplast lineage", "Ted Shackleford", "Belle", "Discworld", "prometheus", "Hulk Hogan", "the Spartans", "Rodgers & Hammerstein", "Jamaica", "Wawrinka", "Marillion", "Verdi", "Roy Keane", "a second-year", "Sarah Vaughan", "sheep", "Sven Goran Eriksson", "Monaco", "Chaplin", "Cary Grant", "Mozart", "Mary Quant", "Tina Turner", "Culloden", "Abraham", "the Swordfish", "Chile", "the Porteous Riots", "Philip II of Spain", "Julian Fellowes", "the cornea", "motocross", "Russell Crowe", "The Penguin", "a basket", "Marni Nixon", "Thom Yorke", "Hokkaby", "a sister and brother", "milk", "Some Like It Hot", "The Life and Opinions of Tristram Shandy", "Mahatma Gandhi", "charlind Leigh", "Erik Thorvaldson", "Churchill Downs", "Honda", "a dungeon", "the narwhal", "Joseph Smith", "Adidas", "Benzodiazepines", "in southern Anatolia", "Babur", "Andes", "Kirk Kerkorian", "Fountains of Wayne", "debate preparation.", "four decades", "snow, sleet, freezing drizzle", "Coldgreen", "Helena", "a Dachshund"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6107142857142858}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3651", "mrqa_triviaqa-validation-6558", "mrqa_triviaqa-validation-4021", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-4491", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-5147", "mrqa_triviaqa-validation-4323", "mrqa_triviaqa-validation-4342", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3518", "mrqa_triviaqa-validation-5890", "mrqa_triviaqa-validation-1384", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-4252", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-3242", "mrqa_hotpotqa-validation-2521", "mrqa_newsqa-validation-1019", "mrqa_searchqa-validation-6441", "mrqa_searchqa-validation-9755", "mrqa_searchqa-validation-5816"], "SR": 0.578125, "CSR": 0.5544763513513513, "EFR": 0.8888888888888888, "Overall": 0.6962511730480481}, {"timecode": 74, "before_eval_results": {"predictions": ["ships", "$10\u201320 million", "London", "1944", "Alfonso Cuar\u00f3n", "FC Bayern Munich", "DeskMate", "J. K. Rowling", "1967", "Anandji Virji Shah", "Jena Malone", "\"Le Divorce\"", "Conservatorio Verdi", "The Bye Bye Man", "Esteban Ocon", "pinball machine", "Eric Morecambe", "400 MW", "\"personal earnings\" (such as salary and wages)", "1991", "Patricia Arquette", "Brig Gen Augustine Warner Robins", "1993", "deborah debGeneres", "Newcastle upon Tyne, England", "the Recording Industry Association of America", "Zero Mostel", "Donald Duck", "2 March 1972", "NXT Tag Team Championship", "john rich", "Matt Groening", "Jean Acker", "1938", "Norse mythology", "National Hockey League (NHL)", "Vincent Landay", "political activist", "Syracuse", "dachshund", "bullfighting and cockfighting", "Venus", "Ford Field in Detroit, Michigan", "largest country", "\"We'll Burn That Bridge\"", "Summerlin, Nevada", "battlefield of Europe", "1887", "\"The Ones Who Walk Away from Omelas\"", "Philip K. Dick", "Union Pacific Railroad", "Have I Told You Lately", "159 counties", "an undisclosed location", "Via Appia", "Port Moresby", "Bruce Alexander", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "boyhood experience in a World War II internment camp", "Lillo Brancato Jr.", "eyes", "ben", "penny Lane", "two"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7357786493477283}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2666666666666667, 1.0, 0.7368421052631579, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-517", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-3652", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-5209", "mrqa_hotpotqa-validation-625", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-4640", "mrqa_hotpotqa-validation-1213", "mrqa_hotpotqa-validation-3192", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-142", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-4349", "mrqa_naturalquestions-validation-8464", "mrqa_newsqa-validation-1759", "mrqa_searchqa-validation-9976", "mrqa_searchqa-validation-3831", "mrqa_triviaqa-validation-1193"], "SR": 0.640625, "CSR": 0.555625, "EFR": 1.0, "Overall": 0.718703125}, {"timecode": 75, "before_eval_results": {"predictions": ["counterflow", "John Brown", "Donald", "for the red - bed country of its watershed", "a best - of - seven playoff", "Seven", "the 4th century", "I Don't Feel at Home in This World Anymore", "Carolyn Sue Jones", "President pro tempore", "Geoffrey Dyson Palmer", "John Smith", "July 2010", "a major worldwide economic downturn", "requiring all non-U.S. ('foreign') financial institutions ( TFIs ) to search their records for customers", "the breast or lower chest of beef or veal", "Edward V", "Presley Smith", "flour and water", "the Gupta Empire", "the dimension sign", "Dr. Rajendra Prasad", "September 1995", "1946", "Tandi", "France", "Richard Parker", "Ryan Pinkston", "Fulton, Arkansas", "differential erosion", "Bumpy Robinson", "2018", "lithium", "the arms of the king of Ireland", "Katharine Hepburn -- Ethel Thayer", "his cousin D\u00e1in", "An elevator with a counterbalance approximates an ideal Atwood machine and thereby relieves the driving motor from the load of holding the elevator cab", "the Speaker", "in the 1970s", "the `` 0 '' trunk code", "Siddharth Arora / Vibhav Roy", "the epidermis", "in the books of Exodus and Deuteronomy", "Archduke Franz Ferdinand of Austria", "a blighted ovum or anembryonic gestation", "The Mandate of Heaven", "85 %", "Tony Orlando and Dawn", "R&B singer Lou Rawls", "Norman", "a crust of potato", "the Forth Bridge", "Wooden Heart (Muss I Denn)", "Billy Liar", "Golden Globe Award", "Equus (play)", "port of Baltimore west to Sandy Hook", "bribing other wrestlers to lose bouts,", "President Mohamed Anwar al-Sadat", "Another high tide", "Labor Day", "David Beckham", "Ear", "Sarajevo"], "metric_results": {"EM": 0.4375, "QA-F1": 0.6133957365841074}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.4210526315789473, 0.6666666666666666, 0.3333333333333333, 1.0, 0.35294117647058826, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 0.2857142857142857, 0.0, 0.3333333333333333, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 0.8333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-10260", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-9322", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-2768", "mrqa_naturalquestions-validation-9421", "mrqa_naturalquestions-validation-7059", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-8064", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-9644", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-9214", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10616", "mrqa_triviaqa-validation-5282", "mrqa_triviaqa-validation-3197", "mrqa_hotpotqa-validation-1727", "mrqa_newsqa-validation-1123", "mrqa_searchqa-validation-177"], "SR": 0.4375, "CSR": 0.5540707236842105, "EFR": 0.9444444444444444, "Overall": 0.707281158625731}, {"timecode": 76, "before_eval_results": {"predictions": ["New Orleans", "2010", "1901", "1992", "the Election Commission of India", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "a convergent plate boundary", "Debbie Gibson", "Polly Walker", "the President", "employment in which a person works a minimum number of hours defined as such by his / her employer", "Castleford", "James Chadwick", "Davos, a mountain resort in Graub\u00fcnden, in the eastern Alps region of Switzerland", "Mark Lowry", "Portuguese and Spanish - French", "Andrew Lloyd Webber", "in Whistler, British Columbia", "To capitalize on her publicity", "In December 1971", "Holden Nowell", "grades 1 ( threshold 85 %, a distinction ), 2 ( 70 -- 84 % ), 3 ( 55 -- 69 % ) & 4 ( 40 -- 54 % )", "The courts", "actions taken by employers or unions that violate the National Labor Relations Act of 1935 ( 49 Stat. 449 ) 29 U.S.C. \u00a7 151 -- 169 ( also known as the NLRA and the Wagner Act", "October 27, 1904", "The Parlement de Bretagne ( Administrative and judicial centre of Brittany, Breton : Breujo\u00f9 Breizh )", "Vijaya Mulay", "Gracyn Shinyei as Emily Montgomery", "Thomas Jefferson", "Ren\u00e9 Verdon", "Francis Ford Coppola", "whether they wish to collect a jackpot prize in cash or annuity", "a young girl", "July 21, 1861", "Lead and lead dioxide", "Notts County ( 1894 ) ; Tottenham Hotspur ( 1901 ) ; Barnsley ( 1912 ) ; West Bromwich Albion ( 1931 ) ; Sunderland ( 1973 ), Southampton ( 1976 ) and West Ham United ( 1980 )", "Mary Chapin Carpenter", "November 2016", "Hellenism", "ended Russia's participation in World War I", "January 2002", "parashah ( or parshah / p\u0251\u02d0r\u0283\u0259 / or parsha )", "Divyanka Tripathi", "save, rescue, savior", "1906", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "modestly and cover their breasts and genitals", "the government - owned Panama Canal Authority", "divergent tectonic plate boundary", "Middle Eastern alchemy", "Friedman Billings Ramsey", "the Hooded Claw", "Culloden", "The Comitium", "McLaren-Honda", "Alexander Gorsky", "Lev Yashin", "August 4, 2000", "Turkey,", "the Southeast,", "biological", "King of Spain", "high jump", "Jane Addams"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6767250385394041}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.09090909090909091, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.25, 0.22222222222222224, 0.0, 1.0, 0.25, 1.0, 0.8656716417910448, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.32, 1.0, 0.0, 1.0, 0.7999999999999999, 0.6666666666666666, 0.0, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 0.3846153846153846, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-8673", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-1000", "mrqa_naturalquestions-validation-1039", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-1656", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-5951", "mrqa_naturalquestions-validation-7089", "mrqa_naturalquestions-validation-868", "mrqa_naturalquestions-validation-9753", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-3865", "mrqa_searchqa-validation-1499"], "SR": 0.578125, "CSR": 0.5543831168831168, "EFR": 0.9259259259259259, "Overall": 0.7036399335618085}, {"timecode": 77, "before_eval_results": {"predictions": ["lay servants", "Randy Newman", "Daman and Diu -- Daman", "Macon Blair", "Ann Gillespie", "A turlough, or turlach", "Katrina Kaif", "the 1830s", "helps digestion by breaking the bonds linking amino acids, a process known as proteolysis", "the 6th century AD", "John Young", "Wilt Chamberlain and LeBron James", "2005", "Sir Rowland Hill", "Menorca", "one", "the Isthmus of Corinth", "Chris Rea", "Jodie Foster", "honey bees", "Cress", "Renhe Sports Management Ltd", "his brother", "A 30 - something man ( XXXX ), is a London underworld criminal who has established himself as one of the biggest cocaine suppliers in the city", "Christina Giles", "scrolls dating back to the 12th century", "in 1546", "Emily Perkins", "Atlantic Ocean", "Joseph M. Scriven", "Nick Hancock", "c. 1000 AD", "a statistical advantage for the casino that is built into the game", "in the original Star Wars film in 1977", "the modern state system", "MacFarlane", "Marie Van Brittan Brown", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "Richie Cunningham", "16,801", "Kepner", "The Beatles", "the final episode of the series", "the summer of 1979", "10 May 1940", "December 14, 2017", "2018 and 2019", "May 2010", "the pyloric valve", "Rome, Italy", "Aegisthus", "the portrait of the monarch", "Peter Ackroyd", "seal", "political correctness", "professional wrestler, actor, and hip hop musician", "1944", "the City of Los Angeles", "on a Taurus XL rocket from Vandenberg Air Force Base in California at 1:55 a.m. PT", "The opposition group, also known as the \"red shirts,\" is demanding that the prime minister dissolve the parliament within 15 days.", "the northwest coast of the world", "ulna", "Sahara", "Carisa Cunningham,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7003036194316437}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.7741935483870968, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.09523809523809523, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5833333333333334, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-2942", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-6671", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-9781", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-8491", "mrqa_triviaqa-validation-4659", "mrqa_newsqa-validation-958", "mrqa_newsqa-validation-4196", "mrqa_newsqa-validation-2674", "mrqa_searchqa-validation-14207", "mrqa_searchqa-validation-7968", "mrqa_searchqa-validation-9180"], "SR": 0.5625, "CSR": 0.5544871794871795, "EFR": 0.9285714285714286, "Overall": 0.7041898466117217}, {"timecode": 78, "before_eval_results": {"predictions": ["Ernest Gimson", "Emily Perkins", "the TV studio in the Hollywood Masonic Temple ( now known as the El Capitan Entertainment Centre )", "Frank Oz", "Mickey Rourke", "those colonists of the Thirteen Colonies who rebelled against British control", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "Krypton", "foster global monetary cooperation, secure financial stability, facilitate international trade, promote high employment and sustainable economic growth, and reduce poverty around the world", "Hundreds or even thousands", "Iowa", "the Ramones", "helped gather enough support for the Constitution to ensure its ratification and lead to the adoption of the first ten amendments, the Bill of Rights", "Gabrielle - Suzanne Barbot de Villeneuve", "Johannes Gutenberg", "Cress", "rocks and minerals", "Devastator", "Lord's, on 15 July 2004 between Middlesex and Surrey", "April 1979", "Buddhist missionaries", "2014 Winter Olympics in Sochi, Russia", "1974", "gravitation", "Supplemental oxygen", "Nancy Jean Cartwright", "the heart", "the coffee shop Monk's", "increased productivity, trade, and secular economic trends", "1986", "2018", "1960", "Garbi\u00f1e Muguruza", "Jurriaen Aernoutsz", "the Sunni Muslim family", "3", "Ali Daei", "the intersection of Del Monte Blvd and Esplanade Street", "the primal rib", "Abigail Hawk", "Bed and breakfast", "J.P. Zenger High", "September 19, 2017", "Super Bowl LII", "Nickelback", "a donor molecule", "Massachusetts", "the Behavioral Analysis Unit", "October 29 - 30, 2012", "in 886 AD", "British citizens", "Carpathia", "Scharnhorst", "Tasmania", "Prussia", "Gregg Harper", "Stravinsky's \"The Rite of Spring\"", "Mogadishu", "Linda Hogan,", "Hakeemullah Mehsud", "Joplin", "Time Use Survey", "Lord of the Rings", "hate crime"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6449672528931613}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false], "QA-F1": [0.0, 1.0, 0.8695652173913044, 1.0, 1.0, 0.5882352941176471, 1.0, 0.0, 0.4827586206896552, 0.0, 1.0, 1.0, 0.5517241379310345, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-5599", "mrqa_naturalquestions-validation-1315", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-4021", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-6523", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-504", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-4079", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-3881", "mrqa_hotpotqa-validation-5298", "mrqa_newsqa-validation-2391", "mrqa_searchqa-validation-12140", "mrqa_searchqa-validation-3740", "mrqa_searchqa-validation-6610"], "SR": 0.53125, "CSR": 0.5541930379746836, "EFR": 0.9333333333333333, "Overall": 0.7050833992616035}, {"timecode": 79, "before_eval_results": {"predictions": ["in collenchyma tissue", "Dollywood", "Chicken Run", "an Fender Stratocaster", "travel", "Roger Williams", "Amherst", "Annika Sorenstam", "breakfast", "Christmas Around the", "John Keats", "Ford Fairlane", "Lindsay Davenport", "Vanessa Hudgens", "50 million cells per litre (quart)", "Starsky and Hutch", "John Locke", "Canterbury", "the Ottoman Empire", "Phil of the Future", "21", "Nacho Libre", "India", "Mork & Mindy", "Tommy Tutone", "Twin-lens reflex camera", "a backfire", "the Rhine & the Main", "Mentor", "Virgin", "Angel Gabriel", "Indiana", "Danny Elfman", "a triangle", "the Horn of Africa", "Captain", "driving Miss Daisy", "hippopotamus", "Oktoberfest", "Pope John Paul II", "Viggo Mortensen", "Fugu", "Edgar Allan Poe", "chervil", "Aston Martin", "the House of Commons", "Edward R. Murrow", "Gandhi", "Houston Rockets", "fief", "Xerox", "The Order of the Phoenix", "American country music artist Toby Keith", "advisory speed signs are classified as warning signs, not regulatory signs", "Thailand", "Walt Whitman", "Otto von Bismarck", "Dire Straits", "Hannaford Brothers Company", "motor ships", "Iran's parliament speaker", "Cannes Film Festival,", "Tuesday.", "Help!"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7212053571428572}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8288", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-2001", "mrqa_searchqa-validation-5773", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-15298", "mrqa_searchqa-validation-14485", "mrqa_searchqa-validation-10982", "mrqa_searchqa-validation-5412", "mrqa_searchqa-validation-766", "mrqa_searchqa-validation-2126", "mrqa_searchqa-validation-16351", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9466", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-4209", "mrqa_searchqa-validation-15443", "mrqa_naturalquestions-validation-922", "mrqa_hotpotqa-validation-5724", "mrqa_newsqa-validation-44"], "SR": 0.65625, "CSR": 0.55546875, "EFR": 1.0, "Overall": 0.7186718750000001}, {"timecode": 80, "UKR": 0.666015625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1290", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1399", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-289", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3582", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4150", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4349", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4537", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2202", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2349", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2355", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3368", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6634", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7461", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7564", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9283", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-10377", "mrqa_searchqa-validation-10784", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-10900", "mrqa_searchqa-validation-11241", "mrqa_searchqa-validation-11361", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-14131", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2273", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3324", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-364", "mrqa_searchqa-validation-3695", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5673", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7165", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9626", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1769", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-237", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3554", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4286", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4755", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6266", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7793", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-855", "mrqa_squad-validation-8664", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9247", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_squad-validation-9944", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-1321", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1625", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1681", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1871", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-2785", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3697", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3917", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4432", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5431", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6947", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.787109375, "KG": 0.45390625, "before_eval_results": {"predictions": ["the wedding banquet", "David Copperfield", "Stalin", "Indiana Jones", "the Demon Barber of Fleet Street", "the Sex Pistols", "Cosmopolitan", "changelings", "right", "the California power company accused of polluting a city's", "(Sarah) Abbott", "Biggie Smalls", "a rope", "Sherlock Holmes", "Ring", "green", "the Vietnam", "Star Trek", "the cliffs", "giant", "Bangkok", "the Sahara", "Puget Sound", "The Golden Age of Murder", "the Erie Canal", "(1942)", "China", "(J. H. Ingraham)", "one", "the Vicious Circle", "Passover", "(Winnie)Pooh", "Austin Powers", "offensive", "the manatee", "(Mohammed)", "the XFiles", "the Bachelor", "West Virginia", "La Salle", "the JFK assassination", "John", "China", "a pigeon", "Jyacheslav Molotov", "(Paul) McCartney", "(3)", "(Jack) Robinson", "to hurt Tammy Wynette as a person", "electrons", "Zbigniew Brzezinski", "June 27, 2008", "Nick Curran", "2018", "fur", "D", "mongoose", "the United States Food and Drug Administration (FDA)", "John Rockwell", "\"Cisleithanian\" half of Austria-Hungary (1867\u20131918)", "Mexico", "one of everything,", "a dozen", "a cloak"], "metric_results": {"EM": 0.453125, "QA-F1": 0.503125}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.20000000000000004, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-2824", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-8047", "mrqa_searchqa-validation-16560", "mrqa_searchqa-validation-15870", "mrqa_searchqa-validation-16217", "mrqa_searchqa-validation-3404", "mrqa_searchqa-validation-5808", "mrqa_searchqa-validation-519", "mrqa_searchqa-validation-15471", "mrqa_searchqa-validation-6747", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-2389", "mrqa_searchqa-validation-4805", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8613", "mrqa_searchqa-validation-2689", "mrqa_searchqa-validation-5237", "mrqa_searchqa-validation-14009", "mrqa_searchqa-validation-5858", "mrqa_searchqa-validation-10196", "mrqa_searchqa-validation-3499", "mrqa_searchqa-validation-2502", "mrqa_searchqa-validation-12464", "mrqa_searchqa-validation-6188", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-5440", "mrqa_triviaqa-validation-1379", "mrqa_triviaqa-validation-6859", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-5324", "mrqa_hotpotqa-validation-4669", "mrqa_newsqa-validation-3124", "mrqa_triviaqa-validation-2123"], "SR": 0.453125, "CSR": 0.5542052469135803, "EFR": 0.9714285714285714, "Overall": 0.6865330136684304}, {"timecode": 81, "before_eval_results": {"predictions": ["Antonio Verrio", "South Africa", "Germany", "the Red Sea", "a ping pong ball", "Superhighways", "a howitzer", "the Teenitans", "Snowball", "New Zealand", "the Blue Meanies", "drop", "E", "the ring", "Joan Bayley", "Coleridge", "a tabby", "St. Francis", "antonyms", "the Raggedy Ann", "a pilsner malt", "Joe Louis", "The Flying Dutchman", "Kentucky", "Iraq", "Province of Newfoundland", "a tail", "Honolulu", "E", "novelist", "Dolphins", "King Solomon", "A House Divided", "a pyramid", "Virginia Woolf", "high jump", "a title", "Poseidon", "Cesare Borgia", "Urdu", "lamb", "man", "Sulfur", "Advil", "Fred (Frederick) Zeller", "Arnold", "a pint", "Joel Osteen", "Ambrose Bierce", "Paris", "Azores", "Susan Seaforth", "1936", "Upon closure at birth", "St. Paul", "Sedbergh", "Renzo Piano", "Sydney", "34 days", "Si Da Ming Bu", "250,000", "Philip Markoff", "Kitty Kelley", "Hungary"], "metric_results": {"EM": 0.5, "QA-F1": 0.5756944444444444}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5520", "mrqa_searchqa-validation-671", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-6004", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-8458", "mrqa_searchqa-validation-4865", "mrqa_searchqa-validation-10497", "mrqa_searchqa-validation-1169", "mrqa_searchqa-validation-9487", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-7447", "mrqa_searchqa-validation-7243", "mrqa_searchqa-validation-3172", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-11906", "mrqa_searchqa-validation-12352", "mrqa_searchqa-validation-4659", "mrqa_searchqa-validation-531", "mrqa_searchqa-validation-16622", "mrqa_searchqa-validation-11688", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-9799", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-5511", "mrqa_hotpotqa-validation-5146", "mrqa_newsqa-validation-373", "mrqa_newsqa-validation-834", "mrqa_newsqa-validation-4004"], "SR": 0.5, "CSR": 0.5535442073170731, "EFR": 1.0, "Overall": 0.6921150914634147}, {"timecode": 82, "before_eval_results": {"predictions": ["between June and September", "six", "September 29, 1910", "Ligurian", "Spanish", "over 100 million", "Walcha", "Eleanor of Aquitaine", "Philip Glass", "122,067", "1971", "Ian Rush", "May 4, 2004", "Jan Kazimierz", "1844", "Theodore Robert Bundy", "held the American record for the most time in space (381.6 days)", "Buffalo", "26 August 2013", "as", "128 pages", "2", "Bruce Springsteen Band", "Hindi", "an organ", "the Innviertel region of western Upper Austria", "Rogue One", "god", "Trilochanpala", "political geographer", "\"Funeral\"", "\"Tainy Sledstviya\"", "constant support from propaganda campaigns", "Westchester County", "\"The Worm\"", "New York", "Northampton", "Suspiria", "Russian film", "Laurel, Mississippi", "India Today", "VIMN Russia", "McLaren-Honda", "January 28, 2016", "Debbie Reynolds", "steamy pictorials of celebrities", "Stage Stores, Inc.", "two", "the 8th and 16th centuries", "Jerry Michael Glanville (born October 14, 1941)", "Hindi", "beneath the liver", "Golden Gate Bridge", "The Intolerable Acts", "Dik Browne", "tobacco", "Transformers: Age of Extinction", "NATO", "Libreville, Gabon.", "planning processes are urgently needed", "profundo", "a reel", "Kentucky", "December 1, 2009"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6577876984126985}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.8, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.2857142857142857, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1766", "mrqa_hotpotqa-validation-4216", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-3819", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5288", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-573", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-1013", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4531", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-141", "mrqa_hotpotqa-validation-43", "mrqa_triviaqa-validation-6333", "mrqa_searchqa-validation-14857", "mrqa_searchqa-validation-4061", "mrqa_naturalquestions-validation-2169"], "SR": 0.53125, "CSR": 0.5532756024096386, "EFR": 1.0, "Overall": 0.6920613704819278}, {"timecode": 83, "before_eval_results": {"predictions": ["Imperial Secretariat", "rural", "1985", "1851", "novelty songs", "Albert", "Norman", "1930 American Pre-Code musical film directed by John Francis Dillon", "Captain", "\"Mrs. Eastwood & Company\"", "November 23, 2011", "CBS", "The R-8 Human Rhythm Composer", "Joulupukki", "C. S. Lewis", "United Holy Salvation Army", "\"Nebo Zovyot\"", "FEMSA", "Football Club Barcelona", "100 countries", "Nigel Patrick", "Nokia Sugar Bowl", "Jonathan Daniel Hamm", "Gian Carlo Menotti", "1883", "Dougray Scott", "January 15, 1975", "Tim Whelan", "Montreal", "Long Island", "Kentucky Music Hall of Fame", "The Oklahoma Sooners", "trans-Pacific flight", "The United States House of Representatives", "Anthony Lynn", "Alfred Graf von Schlieffen", "graffiti artists", "Justin Adler", "the Empire of Japan", "Lieutenant Colonel Iceal E. \"Gene\" Hambleton", "VfL Wolfsburg", "27 November 1956", "2015", "ESPN", "actor", "May", "Hillary Rodham Clinton", "1981", "Suffolk County", "Juergen M. Geissinger", "Paul Hindemith", "Inequality of opportunity", "If These Dolls Could Talk", "to bring", "John Steinbeck", "in god we trust", "Special Forces Command", "\"A Whiter Shade of Pale\"", "Matthew Fisher", "$250,000", "Olivetti S.p.A.", "Frisbee", "a typewriter", "May 29, 2018"], "metric_results": {"EM": 0.625, "QA-F1": 0.7136532738095238}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8075", "mrqa_hotpotqa-validation-2800", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-3880", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-572", "mrqa_hotpotqa-validation-1944", "mrqa_hotpotqa-validation-1094", "mrqa_hotpotqa-validation-4931", "mrqa_hotpotqa-validation-1228", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-4771", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-2004", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-4225", "mrqa_naturalquestions-validation-10512", "mrqa_triviaqa-validation-3097", "mrqa_searchqa-validation-11756"], "SR": 0.625, "CSR": 0.5541294642857143, "EFR": 1.0, "Overall": 0.692232142857143}, {"timecode": 84, "before_eval_results": {"predictions": ["Apollo 17", "Dane William De Haan", "Julian Dana William McMahon", "Ericsson (\"Telefonaktiebolaget L. M. Ericsson\")", "Casey Bond", "S6 Edge+", "John Ford", "Hallett Cove", "June", "Boston Celtics", "Hong Kong", "Captain Beefheart & His Magic Band", "Frank Lowy", "Chuck Yeager", "Southbank", "De La Soul", "Guangzhou, China", "J.R. R. Tolkien", "World War II", "31 October 1783", "Japanese", "largest Mission Revival Style building in the United States", "Skipton Castle", "1965", "The Clash of Triton", "Amon Leopold G\u00f6th", "Aqua", "Colonial colleges", "Tottenham Hotspur", "Linda Ronstadt", "Pim Fortuyn", "Steven Selling", "Spurs", "Vilyam \"Willie\" Genrikhovich Fisher", "Juan Francisco Antonio Hilari\u00f3n Zea D\u00edaz", "France", "Anne, Princess Royal", "E Street Band", "King Duncan", "Bardot", "February 11, 1863", "1985", "40 million", "On March 27, 1977, two Boeing 747 passenger jets, KLM Flight 4805 and Pan Am Flight 1736, collided on the runway at Los Rodeos Airport (now Tenerife North Airport)", "General Motors", "singer", "Miami Gardens", "Marvel Comics", "the bounty hunter Jango Fett", "The series", "Austral L\u00edneas A\u00e9reas", "November 27, 2017", "The epidermis", "May 18, 2010", "The Passenger Pigeon", "parlophone", "horse", "The Casalesi Camorra clan", "Jason Chaffetz", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "Anne Rice", "improvisation", "Neptune", "538"], "metric_results": {"EM": 0.578125, "QA-F1": 0.704933608058608}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.4, 1.0, 1.0, 0.0, 0.8, 0.15384615384615385, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-4991", "mrqa_hotpotqa-validation-3620", "mrqa_hotpotqa-validation-1555", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3265", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-4866", "mrqa_hotpotqa-validation-2174", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-2329", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-89", "mrqa_hotpotqa-validation-5501", "mrqa_hotpotqa-validation-5579", "mrqa_naturalquestions-validation-7513", "mrqa_triviaqa-validation-7398", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-2098"], "SR": 0.578125, "CSR": 0.5544117647058824, "EFR": 0.9259259259259259, "Overall": 0.6774737881263617}, {"timecode": 85, "before_eval_results": {"predictions": ["Baillie-PSW", "liberalisation", "Steve", "Roger Chaffee", "Wilt Chamberlain", "The Man", "meathead", "a swami", "the Snowy Mountains", "Spin", "La Gioconda", "Grand Central Liquor", "Es Selamu", "The Devil Wears Prada", "heresy", "Cleveland", "Chile", "Iridill", "black pudding", "Newton", "Union Square", "reflection", "The Sleeping Beauty", "California", "Rachel Carson", "1066", "a stick", "WD-40", "Bonobos", "the sound barrier", "Hillary Clinton", "Adam", "a metronome", "the pupil", "The Firebird", "Larry Fortensky", "the gang of child criminals,", "3", "Guantnamo Bay Naval Station", "an American politician who is the 44th and current President of the United States.", "Yond Cassius", "Turkish", "the family plays", "The liver", "Picabo Street", "John Paul Jones", "the sound of fingernails", "a poncho", "\"Pig Crap\"", "\"God bless Captain Vere!\"", "Utah", "Richie Cunningham", "April 1917", "the fourth Anglo - Mysore war during which Tipu Sultan was killed.", "weasel", "a person whose occupation is mainly to cut, dress, groom, style and shave", "fractal geometry", "Aamir Khan", "five times", "The Grandmaster", "The National Infrastructure Program, as he called it,", "that he knew the owner of the home, a Vietnam veteran who had given him permission to enter the house and take painkillers or other pills whenever he wanted.", "the Employee Free Choice act in Lafayette Square in Washington on Monday.", "1960"], "metric_results": {"EM": 0.453125, "QA-F1": 0.4911458333333334}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.5333333333333333, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-3796", "mrqa_searchqa-validation-5299", "mrqa_searchqa-validation-11783", "mrqa_searchqa-validation-5370", "mrqa_searchqa-validation-8407", "mrqa_searchqa-validation-9850", "mrqa_searchqa-validation-7784", "mrqa_searchqa-validation-13659", "mrqa_searchqa-validation-1215", "mrqa_searchqa-validation-9247", "mrqa_searchqa-validation-8909", "mrqa_searchqa-validation-589", "mrqa_searchqa-validation-3351", "mrqa_searchqa-validation-854", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-10218", "mrqa_searchqa-validation-16625", "mrqa_searchqa-validation-11956", "mrqa_searchqa-validation-3371", "mrqa_searchqa-validation-12376", "mrqa_searchqa-validation-2715", "mrqa_searchqa-validation-6776", "mrqa_searchqa-validation-14066", "mrqa_searchqa-validation-15775", "mrqa_searchqa-validation-5830", "mrqa_searchqa-validation-15030", "mrqa_naturalquestions-validation-6749", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-2405", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-5675", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2344"], "SR": 0.453125, "CSR": 0.553234011627907, "EFR": 1.0, "Overall": 0.6920530523255815}, {"timecode": 86, "before_eval_results": {"predictions": ["over 200 awards", "the beetles", "the Cherokee Nation", "Danny Elfman", "The blank space", "Vladivostok", "Brian Cowen", "Illinois", "Linus", "Seurat", "Pompeii", "achaia", "The tuba", "a megaton", "Michigan", "webbing", "September", "Madrid", "Roger Williams", "Washington", "the retina", "statistic", "dance", "the life and Times of John Barrymore", "Moldova", "Andrew Johnson", "a novel", "spaghettification", "Rosalind", "gluttony", "a catalog", "Marx", "German", "words", "Baghdad", "Merlin", "Thomas Stearns Eliot", "Oomf", "the Bells", "George Washington Carver", "Tommy Franks", "Nyx", "Tigger", "Songs of Innocence", "Sarah", "Ashley Judd", "Jean", "Saudi Arabia", "Dustbin lids", "Kufic", "focal length", "Organisms in the domains of Archaea and Bacteria", "Lady Gaga", "West Quoddy Head in Lubec, Maine", "Portugal", "Puccini", "Rainbow", "Harry Booth", "Vilnius Old Town", "35,402", "three", "Zimbabwe", "OneLegacy, an organ procurement agency in Southern California,", "Orson Welles"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6479166666666666}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true], "QA-F1": [0.8, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8000", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-838", "mrqa_searchqa-validation-5818", "mrqa_searchqa-validation-1912", "mrqa_searchqa-validation-16603", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-16202", "mrqa_searchqa-validation-3026", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-1985", "mrqa_searchqa-validation-15649", "mrqa_searchqa-validation-9217", "mrqa_searchqa-validation-11033", "mrqa_searchqa-validation-8437", "mrqa_searchqa-validation-10315", "mrqa_searchqa-validation-8327", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-14277", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-765", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1276"], "SR": 0.578125, "CSR": 0.5535201149425287, "EFR": 1.0, "Overall": 0.6921102729885058}, {"timecode": 87, "before_eval_results": {"predictions": ["the fear of transferring power to the ethnic Chinese under their rule", "peat moss", "linguistic analysis", "Oliver Parker", "Walt Disney and Ub Iwerks", "French mathematician and physicist", "India", "novelty songs", "Barbados", "The Captain Matchbox Whoopee Band", "Guillermo del Toro", "The Rookie", "antisemitic", "Commissioner", "\"Kitty Hawk\"", "Scandinavian design", "Linda Maria Ronstadt", "\"Naked Killer\"", "the corner of North Avenue at Techwood Drive on the campus of the Georgia Institute of Technology in Atlanta", "Imperial War Museums (IWM)", "\"Trzy kolory\"", "100 and 200 meters dash", "Scottish national team", "William Bradford", "200,167", "Boston, Providence, Hartford", "Rose Theatre", "Columbine", "\"Pastime Paradise\"", "Sir William Collins", "G\u00e9rard Depardieu", "Lionsgate", "the 13th century", "he abdicated", "Zara Kate Bate", "Louisiana Tech University", "Creech Air Force Base, Nevada", "around 8000 BC", "Rochdale", "Andrew Lloyd Webber", "12", "1955", "number 8", "Henry Lau", "Terry the Tomboy", "\"Teenage Dream\"", "2015", "Federal Bureau of Prisons", "Mick Jackson", "Mayor Ed Lee", "two", "Matt Monro", "on the lateral side", "Jason Flemyng", "Elizabeth I", "Ghostbusters II", "(Stephen) O'CONglas", "homicide.", "its intention to set up headquarters in Dublin.", "the release of the four men", "Bolivia", "Art Brut", "Spain", "vomiting"], "metric_results": {"EM": 0.53125, "QA-F1": 0.646422847985348}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.47619047619047616, 0.0, 0.0, 0.20000000000000004, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.15384615384615385, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8413", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-4359", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-4226", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-1157", "mrqa_hotpotqa-validation-4727", "mrqa_hotpotqa-validation-648", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-4821", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2644", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4736", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-906", "mrqa_hotpotqa-validation-5438", "mrqa_naturalquestions-validation-9218", "mrqa_triviaqa-validation-459", "mrqa_triviaqa-validation-4077", "mrqa_newsqa-validation-3726", "mrqa_newsqa-validation-3806", "mrqa_searchqa-validation-2770"], "SR": 0.53125, "CSR": 0.5532670454545454, "EFR": 1.0, "Overall": 0.6920596590909092}, {"timecode": 88, "before_eval_results": {"predictions": ["Scottish rivers", "Heracles", "New Kids on the Block", "Van Allen", "Camilla Parker Bowles", "(Stephen) Schreiber", "a scorpion", "a one year period", "souvlaki", "Charles Dana Gibson", "Philip Seymour Hoffman", "ACTIVE", "chicken pox", "Japan", "chickens", "St. Patrick", "Lebanon", "George Sand", "Over the hifls", "Wellness Center hours", "the Holy Grail", "shalom", "the Cumberland Gap", "Belgium", "Hollaback Girl", "Michigan Wolverines", "Poor Richard's Almanack", "red", "Al Jolson", "a violin", "Hestia", "Transformers", "glory", "Rand McNally & Company", "Scrabble", "Martin Luther King III", "Duct tape", "Henry Cavendish", "confer", "a gap", "Bill & George Clinton", "Condoleezza Rice", "John Glenn", "the bluest eye", "Spinal Tap", "Janet Reno", "(Casey) Stengel", "Monticello", "Eric Clapton", "saguaro", "Louisiana", "Beijing", "Charlotte Hornets", "An appellate court", "Sweden", "Lucas McCain", "Auckland", "hunt", "1501", "35", "billboards with an image of the burning World Trade Center", "At least 14 bodies", "Fullerton, California", "1936"], "metric_results": {"EM": 0.71875, "QA-F1": 0.8002961601307189}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.9411764705882353, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-862", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-1908", "mrqa_searchqa-validation-5433", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-11022", "mrqa_searchqa-validation-13166", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-3306", "mrqa_searchqa-validation-7825", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-10409", "mrqa_naturalquestions-validation-4653", "mrqa_triviaqa-validation-6874", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-405", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-798"], "SR": 0.71875, "CSR": 0.555126404494382, "EFR": 1.0, "Overall": 0.6924315308988764}, {"timecode": 89, "before_eval_results": {"predictions": ["white light", "Fame", "Bresaola", "Hermione Gingold", "Yellowfin", "John", "bamboo", "Stratosphere Las Vegas", "Wizardry", "The Constant Gardener", "the Beatles", "Mercury and Venus", "Hallmark Cards", "the sick", "Peter Pan", "magnesium", "the Space Shuttle Columbia", "chicken", "the Romanov dynasty", "Jeannette Rankin", "(Stephen) Escoffier", "(James) Hemingway", "Billie", "her sister", "George Harrison", "the heel", "Superbad", "Emily Post", "a drum", "Morris West", "New Zealand", "Europe and Asia", "Sacco and Vanzetti", "the angel of the LORD", "Copenhagen", "Tennessee", "Mozart", "a spare tire", "(Stephen) Ross Perot", "(Henry) Ford", "Salmon", "a system of abstract codes", "U.S. presidents", "Necessity", "a car", "the Oakland Raiders", "an owl", "(Julius) Caesar", "cipher", "Elvis Presley", "an American sitcom and a spin-off of Sanford and Son that aired on NBC from December 4, 1975 to March 11, 1976", "giant molecular clouds ( GMC )", "Thomas Edison", "Terrell Suggs", "Benjamin Franklin", "(James) Bolam", "Fluorine", "He received an excellent education in both Arabic and Greek, having studied Arabic grammar with the famous poet Abul \u02bfAla Al-Ma\u02bfarri", "University of California", "11 November 1918", "Donald Duck", "Dr. Maria Siemionow, the head of plastic surgery research at the Cleveland, Ohio, hospital,", "in a remote part of northwestern Montana", "Father of Liberalism"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5983784859464207}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, false, true], "QA-F1": [0.5, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2608695652173913, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.4, 0.9090909090909091, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8865", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-11947", "mrqa_searchqa-validation-10914", "mrqa_searchqa-validation-13380", "mrqa_searchqa-validation-5198", "mrqa_searchqa-validation-1068", "mrqa_searchqa-validation-8059", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-12675", "mrqa_searchqa-validation-1036", "mrqa_searchqa-validation-15046", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-13001", "mrqa_searchqa-validation-10002", "mrqa_searchqa-validation-2698", "mrqa_searchqa-validation-3173", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-8542", "mrqa_searchqa-validation-16904", "mrqa_searchqa-validation-12099", "mrqa_searchqa-validation-13523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-8714", "mrqa_searchqa-validation-8388", "mrqa_searchqa-validation-16902", "mrqa_searchqa-validation-12427", "mrqa_searchqa-validation-9117", "mrqa_searchqa-validation-14302", "mrqa_searchqa-validation-16659", "mrqa_naturalquestions-validation-9272", "mrqa_naturalquestions-validation-3474", "mrqa_hotpotqa-validation-5637", "mrqa_newsqa-validation-110", "mrqa_newsqa-validation-1679", "mrqa_newsqa-validation-3677"], "SR": 0.421875, "CSR": 0.5536458333333334, "EFR": 1.0, "Overall": 0.6921354166666667}, {"timecode": 90, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1012", "mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1768", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1892", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2703", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-3632", "mrqa_hotpotqa-validation-3660", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4128", "mrqa_hotpotqa-validation-4197", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4410", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-514", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5391", "mrqa_hotpotqa-validation-5407", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1763", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-3951", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-6887", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-7858", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1088", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-1961", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2863", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3014", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-807", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-10784", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12479", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13183", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-15583", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15806", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-15958", "mrqa_searchqa-validation-162", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-3695", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6591", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8830", "mrqa_searchqa-validation-8896", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9562", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9599", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-10416", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1577", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-2390", "mrqa_squad-validation-2468", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3898", "mrqa_squad-validation-3925", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4795", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-519", "mrqa_squad-validation-5373", "mrqa_squad-validation-5472", "mrqa_squad-validation-5520", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-855", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9569", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_triviaqa-validation-1016", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4718", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-575", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.8359375, "KG": 0.50078125, "before_eval_results": {"predictions": ["nonviolent", "anchors", "Joseph", "Sideways", "Clothes", "Thomas Wolfe", "Porsche", "Snow White", "Missouri", "a pyramid", "Subway", "Tila Tequila", "proton and an electron", "cells", "Bloomingdale\\'s", "Richard Nixon", "Marie Osmond", "bumblebees", "loyal", "French", "the transition movement", "Barney Miller", "the Constitution", "the bullseye", "Thomas Wolfe", "Lynette \"Squeaky\" Fromme", "Thursday", "robe", "Matthew", "Fenway Park", "Pennsylvania", "the Liberty Bell", "Cardinal Richelieu", "Guinevere", "James Jeffords", "the Wachowski brothers", "a clef", "Brazil", "Montgomery", "Sindbad", "dicttus", "John Brown", "a piranha", "26.2 miles", "the pupil", "Amish TV", "Anthony Michael Hall", "Lord Baden-Powell", "judo", "Don Knotts", "messenger", "Spencer Treat Clark", "beloved", "northwest Washington", "Charlie Harper", "\"No 'Stairway to Heaven\"", "Denmark", "WB Television Network", "Senior Service", "Eric Edward Whitacre", "martial arts,", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use as he arrived in Africa for his first visit to the continent as pope.", "Eleven", "Jackson Storm"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7286458333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8002", "mrqa_searchqa-validation-4101", "mrqa_searchqa-validation-8352", "mrqa_searchqa-validation-10414", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-13607", "mrqa_searchqa-validation-8459", "mrqa_searchqa-validation-6649", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-15019", "mrqa_searchqa-validation-13537", "mrqa_searchqa-validation-13842", "mrqa_searchqa-validation-5754", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-5235", "mrqa_naturalquestions-validation-10610", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-3947", "mrqa_triviaqa-validation-5922", "mrqa_hotpotqa-validation-565", "mrqa_hotpotqa-validation-5559", "mrqa_newsqa-validation-1663"], "SR": 0.65625, "CSR": 0.5547733516483517, "EFR": 1.0, "Overall": 0.7173609203296702}, {"timecode": 91, "before_eval_results": {"predictions": ["1330 Avenue of the Americas", "25", "gossip Girl", "a pool of blood beneath his head.", "war funding", "elections", "military trials", "Missouri", "A Colorado prosecutor", "hostile war zones,", "Authorities in Fayetteville, North Carolina,", "Tsvangirai", "Isabella, Emma, Olivia, Sophia, Ava, Emily, Madison, Abigail, Chloe and Mia.", "Illlinois.", "Blacks and Hispanics", "Mohamed Alanssi,", "Wednesday night", "33-year-old", "3", "re-negotiating a follow-up to the Strategic Arms Reduction Treaty and nonproliferation.", "Australian Environment Minister Peter Garrett", "two", "smile softly at the paralyzed bodies they hold.", "equator,", "police", "2004.", "Inter Milan", "the Airbus A330-200", "deaths linked to the practices,", "many riders say it's worth the cost to avoid the traffic hassles of the oft-congested I-70.", "The Washington Post", "suicide bombing", "that a U.S. helicopter crashed in northeastern Baghdad as a result of clashes between U,S.-backed Iraqi forces and gunmen.", "her boyfriend,", "Argentine", "poems", "alcohol poisoning,", "Harvard symbologist Robert Langdon", "guzzlers", "intricate Flemish tapestries", "Taliban militants", "additional information", "The federal officers' bodies", "sixth world title", "violent government crackdown seeped out.", "The Tinkler", "a new model is simply out of their reach.", "costs $50 less, while the Nook has twice the storage space and a longer advertised battery life", "part", "dental work done, including removal of his diamond-studded braces.", "The wings, included in the sale,", "introverted feeling ( Fi ) and Extroverted Intuition ( Ne )", "one season", "Lady Gaga", "Quran", "Enid", "sweden", "Club Deportivo Castell\u00f3n", "Bishop's Stortford", "2000", "New York City", "the final scene", "Lady Sings the Blues", "sweden"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5486880970336853}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, true, true, true, false, false, true, false], "QA-F1": [0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 0.058823529411764705, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.35294117647058826, 0.5, 0.46153846153846156, 0.4, 0.6, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5972", "mrqa_newsqa-validation-3098", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-4203", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-1855", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3193", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-3855", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3506", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-2084", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-1532", "mrqa_triviaqa-validation-6046", "mrqa_searchqa-validation-12610", "mrqa_searchqa-validation-10962", "mrqa_triviaqa-validation-1028"], "SR": 0.4375, "CSR": 0.5534986413043479, "EFR": 1.0, "Overall": 0.7171059782608695}, {"timecode": 92, "before_eval_results": {"predictions": ["The teacher also needs to be enthusiastic about the subject matter they are teaching", "Saturn", "a nuclear weapon", "150", "\"We are a nation of Christians and Muslims, Jews and Hindus -- and nonbelievers.\"", "Tim Clark, Matt Kuchar and Bubba Watson", "\"She was focused so much on learning that she didn't notice,\"", "\"utterly baseless,\"", "(Opry) Mills,", "Somalia's piracy problem was fueled by environmental and political events.", "safety issues in the company's cars", "innovative, exciting skyscrapers", "Kurdish militant group", "Two pages -- usually high school juniors who serve Congress as messengers", "\"The only path to reach long-lasting security and peace is through the rule of law and strengthening of our institutions,\"", "Sunday", "New smuggling routes across the Red Sea", "put a lid on the marking of Ashura", "the Bronx.", "Congress", "sculptures", "eight", "kerstin and two of her brothers,", "Zimbabwe", "\"release\" civilians,", "Stratfor", "potential revenues from oil and gas", "in order to tell her the devastating news.", "In the future, TSA will inform passengers that they have the option to resolve the alarm through a visual inspection of the article in lieu of removing the item in question.", "\"The people kill him with the blocks,", "mother", "in his 60s,", "to clean up Washington State's decommissioned Hanford nuclear site,", "Haiti", "four months ago,", "(CNN) tells stories of different women coping with breast cancer in five vignettes.", "it has not intercepted any", "an acid attack by a spurned suitor.", "Provo, Utah", "\"We look forward to the day soon when our president stands with that pro-equality majority,\"", "five victims by helicopter, one who died, two in critical condition and two in serious condition.", "22", "38", "India", "iCloud service", "Bob Bogle,", "Jason Chaffetz", "eight", "CNN's \"Good Morning America\" and \"The View.\"", "Sunday.", "101", "charbagh", "boxing", "southeastern United States", "penicillin, Television,", "the Last Post", "Columbus", "Indian", "Science professor and writer", "Operation Julin", "Sorbo", "Weeds", "neurotransmitters", "largest wars that had ever taken place"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5775616897399986}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, true, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 1.0, 1.0, 0.0, 0.9411764705882353, 1.0, 0.0, 0.0, 0.33333333333333337, 0.5714285714285715, 0.28571428571428575, 0.2, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.06666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2180", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-2793", "mrqa_newsqa-validation-3928", "mrqa_newsqa-validation-4146", "mrqa_newsqa-validation-2902", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-389", "mrqa_newsqa-validation-810", "mrqa_newsqa-validation-2447", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-435", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-1831", "mrqa_newsqa-validation-258", "mrqa_newsqa-validation-3034", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-561", "mrqa_newsqa-validation-1274", "mrqa_naturalquestions-validation-9459", "mrqa_triviaqa-validation-2429", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-5647", "mrqa_searchqa-validation-7630", "mrqa_naturalquestions-validation-2042"], "SR": 0.484375, "CSR": 0.552755376344086, "EFR": 0.9696969696969697, "Overall": 0.7108967192082112}, {"timecode": 93, "before_eval_results": {"predictions": ["the Kingdom of Qocho", "World Trade Center Transportation Hub", "Olivia Olson", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ),", "England", "Michael Schumacher", "Sajjad Delafrooz", "one season", "Phillipa Soo as Natasha, Lucas Steele as Anatole, Amber Gray as H\u00e9l\u00e8ne, Brittain Ashford as Sonya, Nick Choksi as Dolokhov, Shaina Taub as Mary", "Thespis", "eurozone", "Keith Thibodeaux", "2014", "Arunachal Pradesh", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "the inner core", "Tatsumi", "Gaget, Gauthier & Co.", "13", "during World War II", "Cherbourg in France", "Sheev Palpatine", "1999", "73", "the ulnar nerve", "the medulla oblongata", "Judy Garland, Carole Landis, Dean Martin, and Ethel Merman", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Hellenic polytheist", "three", "Spanish explorers", "statutory law", "Menorca", "20 - year period", "Tim McGraw", "infection, irritation, or allergies", "between the Eastern Ghats and the Bay of Bengal", "Mary Elizabeth ( Margaret Hoard )", "Nueva Extremadura", "International Orange", "in the duodenum", "October 6, 2017", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "China ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "singer - songwriter Pebe Sebert", "Arthur Chung", "semilunar pulmonary valve", "Lana Del Rey", "Sergeant - Major James Hewson", "October 3, 2013", "J.H. Ingraham", "faggot", "Parchman Farm", "SpongeBob", "1994", "848", "2004", "racially-tinged remark", "poor families", "positive signal", "lizard", "Gone With the Wind", "Charles Chaplin", "the Papua New Guinea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6182311178404929}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.7499999999999999, 0.37499999999999994, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9142857142857143, 0.8, 0.4444444444444445, 1.0, 0.18181818181818182, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8244", "mrqa_naturalquestions-validation-10088", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-1162", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-3391", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-5986", "mrqa_naturalquestions-validation-10469", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-6968", "mrqa_naturalquestions-validation-3737", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-2666", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-7233", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-3175", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-7457", "mrqa_hotpotqa-validation-455", "mrqa_newsqa-validation-2812", "mrqa_newsqa-validation-3446", "mrqa_searchqa-validation-8023", "mrqa_searchqa-validation-16519"], "SR": 0.515625, "CSR": 0.5523603723404256, "EFR": 0.9032258064516129, "Overall": 0.6975234857584077}, {"timecode": 94, "before_eval_results": {"predictions": ["8", "Mickey Gilley", "June 25, 1952", "Long Island", "The Keeping Hours", "John \"John\" Alexander Florence", "Queensland", "1995 to 2012", "Los Angeles", "One of the AL's eight charter franchises", "1.23 million", "The interview", "Pan Am Railways, Inc. (PAR), formerly known as Guilford Rail System (GRS) before March 2006", "102,984", "England", "321,520", "Mary Harron", "Newcastle upon Tyne, England", "Fiapre", "a card (or cards) during a card game", "University of Vienna", "1998", "capitol building", "Ericsson", "first settlement of German immigrants in Australia and was named after the village of Klemzig, near Z\u00fcllichau in southeastern Brandenburg in the German state of Prussia", "William Bradford", "Free and Sovereign State of Tamaulipas", "highland regions of Scotland", "2015", "Richard Arthur, former owner of the land on which it was built,", "Elizabeth River", "\"Sheen Michaels Entertainment\"", "Nikita Khrushchev", "Charlyn Marie \" Chan\" Marshall", "Europe", "Pound Puppies and the Legend of Big Paw", "November 8, 2016", "War Is the Answer", "drawings", "Philip K. Dick", "Northern Ireland", "the junction with Interstate 95", "Cambridge University", "Mineola", "Ministry of European Integration (Albania)", "\"Fatman\" McCabe", "The R-8 Human Rhythm Composer", "London", "North Carolina", "Steven John Carell", "The entity", "Have I Told You Lately", "when an individual noticing that the person in the photograph is attractive, well groomed, and properly attired, assumes, using a mental heuristic", "1,350", "armada", "The Chatham House Rule", "cartoons", "Guam", "The Devil Went Down to Georgia.", "a share in the royalties", "a macadamia", "Travertine", "Final Cut Pro", "Anne Eliot"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6741939484126984}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 1.0, 0.2857142857142857, 0.4, 0.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.888888888888889, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6923076923076924, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4274", "mrqa_hotpotqa-validation-63", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-3718", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-2336", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-4743", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-4402", "mrqa_hotpotqa-validation-1996", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-5760", "mrqa_hotpotqa-validation-5177", "mrqa_naturalquestions-validation-4740", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-871", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3993", "mrqa_searchqa-validation-15161", "mrqa_searchqa-validation-5324", "mrqa_triviaqa-validation-6256"], "SR": 0.5625, "CSR": 0.5524671052631579, "EFR": 0.9285714285714286, "Overall": 0.7026139567669173}, {"timecode": 95, "before_eval_results": {"predictions": ["YouTube", "soccer", "Rockbridge County", "Rawhide", "John Sullivan", "John Monash", "Ice Princess", "Antonio Vivaldi", "Terry Malloy", "Alfred Preis", "professional wrestler, actor, and hip hop musician", "2009", "Twitch Interactive, a subsidiary of Amazon.com", "(500) Days of Summer", "Accokeek, Maryland", "self-immolation of a 19-year-old student named Romas Kalanta", "Cartoon Network Studios", "Love", "Argand lamp", "Krypto Report", "Keelung", "Adam Levine", "Pennacook", "Michael Crawford", "The Five", "The Lykan Hypersport", "Bundesliga", "133d Air Refueling Squadron", "Honolulu", "from 1993 to 1996", "Hannaford", "John Lennon", "The BFG", "Sophie Winkleman", "In 2017, Pachulia won his first NBA Championship", "1954", "Tie Domi", "Harold Lipshitz, March 20, 1931", "American serial killer couple", "Target Corporation", "George A. Romero", "Nassau County Executive", "number five", "British racing driver", "26 September 1961", "Stapleton Cotton", "Arctic tundra soil near Ny-\u00c5lesund in Norway", "1983", "Linux Format", "French Canadians", "Kew", "twice", "Meghalaya", "December 15, 2017", "le Fosche Notturne Spoglie", "Love Is All Around", "pamphlets, posters, ballads", "12-1", "to make life a little easier for these families by organizing the distribution of wheelchair, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "if a security officer were to pull a gun on an armed individual in a mall, it could result in \"the gunfight at the 'OK corral,'", "a connector", "apples", "the late Jurassic period", "Greenland"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7035962301587302}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 0.33333333333333337, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 0.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2740", "mrqa_hotpotqa-validation-4791", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-5506", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-511", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-5546", "mrqa_hotpotqa-validation-1808", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-111", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-1886", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-3039", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-982", "mrqa_searchqa-validation-4407", "mrqa_searchqa-validation-8536", "mrqa_searchqa-validation-11462"], "SR": 0.609375, "CSR": 0.5530598958333333, "EFR": 1.0, "Overall": 0.7170182291666667}, {"timecode": 96, "before_eval_results": {"predictions": ["in the biosphere", "won", "Thomas Jefferson", "Adam", "Jesus'birth", "Paul Rudd", "John F. Kennedy", "the end", "S", "a piece of foam insulation", "Mount Baker - Snoqualmie National Forest and Nooksack Falls in the North Cascades range of, Washington", "Garfield Sobers", "The stability, security, and predictability of British law and government", "Hal Derwin", "Madison, Wisconsin, United States", "Gaelic \u00d3 Brad\u00e1in'descendant of Brad\u00e1n ', a personal name meaning'salmon '", "the hour of death", "rum", "13 February", "comprehend and formulate language", "John DiMaggio", "Charles Crozat Converse", "Kiss", "China ( formerly the Soviet Union ), France, the United Kingdom, and the United States", "at", "Pakistan", "Danny Veltri", "Australian Peter Siddle", "Ole Einar Bj\u00f8rndalen", "Marie Fredriksson", "Amitabh Bachchan", "Twin Pines / Lone Pine Mall", "Del and Rodney", "Speaker of the House of Representatives", "538", "a man with a face described as looking like the devil - two protrusions emanating from his forehead ( like horns ), eyes burning like'fire in a cave '", "March 2016", "1924", "Hank Williams", "Siddharth Arora / Vibhav Roy", "Vinati Amar Pethawala", "4 January 2011", "Johnny Cash", "Cool Hand Luke", "an optional message body", "an optional message body", "Kida", "2005", "remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "1961", "The person who has existence in two parallel worlds", "\u201cMy Favorite Martian,\u201d", "Perseus", "piscina", "Justin Adler", "Melbourne", "G\u00f6tene in Sweden", "Ronald Cummings,", "in Egypt.", "Stuart Gaffney, media director", "a hundredth", "brandy", "(Arthur) Miller", "Renzo Piano"], "metric_results": {"EM": 0.546875, "QA-F1": 0.642745335701485}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.38095238095238093, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08333333333333333, 0.5, 0.25, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.12500000000000003, 0.3333333333333333, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 1.0, 1.0, 0.7741935483870968, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7853", "mrqa_naturalquestions-validation-3714", "mrqa_naturalquestions-validation-6121", "mrqa_naturalquestions-validation-7224", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-1990", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-6698", "mrqa_naturalquestions-validation-2512", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-1144", "mrqa_naturalquestions-validation-7152", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-3558", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-5293", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-3184", "mrqa_triviaqa-validation-6227", "mrqa_hotpotqa-validation-2687", "mrqa_newsqa-validation-339", "mrqa_newsqa-validation-260", "mrqa_searchqa-validation-5290"], "SR": 0.546875, "CSR": 0.5529961340206185, "EFR": 1.0, "Overall": 0.7170054768041236}, {"timecode": 97, "before_eval_results": {"predictions": ["23 November 1946", "Afghanistan", "C. J. Cherryh", "October 17, 1938", "German", "1978", "Trey Parker and Matt Stone", "Robert \"Bobby\" Bunda", "the late eighteenth century", "the \"Father of Liberalism\"", "Grave Digger", "1854", "Thriller", "various names, including, of course, the Supersuckers", "March 23, 2017", "Vladimir Valentinovich Menshov", "Shery", "American", "The Colorado Rockies", "at least 96", "the Bears", "EN World web site that has hosted the awards since their inception in 2001", "John Andr\u00e9", "Brian Bosworth", "Lynwood", "Hawaii County, Hawaii", "Mel Blanc", "January 2016", "Philadelphia", "nine", "Finding Nemo", "1957", "House of Commons", "Terry the Tomboy", "1995 teen drama \"Kids\"", "the Switzerland national team", "Polk County, Georgia", "Charles L. Clifford", "Taoiseach", "2018\u201319 UEFA Europa League group stage", "island of Spitsbergen", "Ronald Ryan", "2017", "10 October 2010", "Che Guevara", "Marigold Newey", "three", "Warsaw, Poland", "horror film", "Virginia", "the capital of French Indochina", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "Strabo", "1922", "Bono", "a mosaic", "Indonesia", "a \"procedure on her heart,\"", "British", "Briton Carl Froch", "Kosovo", "Oh Henry", "James Earl Ray", "john le Carr\u00e9"], "metric_results": {"EM": 0.671875, "QA-F1": 0.8081713935574231}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 0.47058823529411764, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.8, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.9142857142857143, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-1858", "mrqa_hotpotqa-validation-2657", "mrqa_hotpotqa-validation-4354", "mrqa_hotpotqa-validation-4154", "mrqa_hotpotqa-validation-874", "mrqa_hotpotqa-validation-2442", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-3418", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-840", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-1911", "mrqa_newsqa-validation-2546", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-302"], "SR": 0.671875, "CSR": 0.5542091836734694, "EFR": 0.9047619047619048, "Overall": 0.6982004676870748}, {"timecode": 98, "before_eval_results": {"predictions": ["the plain of Marathon", "a clove hitch", "birds", "John Knox", "Sheryl Crow", "Rich Girl", "circus wagons", "endive", "midwife", "Patricia Arquette", "One billion", "tuberculosis", "Judges", "Milan", "Abu Musab al-Zarqawi", "Sanford and Son", "repent", "New Mexico", "Henri Matisse", "forty-niners", "Australia", "Monarch", "Pluto", "Quisp Cereal", "1803", "The Prose Works of John Milton", "bloom", "a catalog", "Hydra", "The petroleum sector", "Tin", "Irish Coffee", "Rome", "The activities begin when a child turns three and can start soccer,", "Jack Dempsey", "Vladivostok", "an earthquake", "Bizarro", "month", "sancire", "A 4.0 GPA", "New Kids on the Block", "Louis the German", "Cutpurse", "a rectangle", "Princeton", "7th Heaven", "Plato The lunar", "Mali", "a kite", "Minnesota, North Star State, Gopher State", "Amartya Sen ( 1998, Economics )", "its genome", "1883", "drake", "George Edalji (pronounced \"Aydlji\"", "Newfoundland and Labrador", "Kentucky Wildcats", "MGM Resorts International", "North West England", "Sporting Lisbon", "Vicente Carrillo Leyva,", "bodyguard Trevor Rees and the back of Princess Diana's head", "Allison Janney"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6176609848484849}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, false, true], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14941", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-11657", "mrqa_searchqa-validation-12806", "mrqa_searchqa-validation-7151", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-2401", "mrqa_searchqa-validation-4343", "mrqa_searchqa-validation-14595", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-6005", "mrqa_searchqa-validation-1262", "mrqa_searchqa-validation-1069", "mrqa_searchqa-validation-10077", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-12932", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-4038", "mrqa_searchqa-validation-9456", "mrqa_searchqa-validation-4943", "mrqa_searchqa-validation-5833", "mrqa_searchqa-validation-2890", "mrqa_naturalquestions-validation-5726", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-3985", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-730", "mrqa_hotpotqa-validation-5799", "mrqa_newsqa-validation-2960"], "SR": 0.546875, "CSR": 0.554135101010101, "EFR": 0.9655172413793104, "Overall": 0.7103367184778822}, {"timecode": 99, "UKR": 0.6875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1022", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-1131", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1137", "mrqa_hotpotqa-validation-1209", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1266", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-1536", "mrqa_hotpotqa-validation-1693", "mrqa_hotpotqa-validation-1734", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1748", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-1768", "mrqa_hotpotqa-validation-1790", "mrqa_hotpotqa-validation-1794", "mrqa_hotpotqa-validation-183", "mrqa_hotpotqa-validation-184", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-196", "mrqa_hotpotqa-validation-1961", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2138", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-2254", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2343", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2567", "mrqa_hotpotqa-validation-2580", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-2657", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-2895", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2929", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3073", "mrqa_hotpotqa-validation-3189", "mrqa_hotpotqa-validation-3298", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3396", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3535", "mrqa_hotpotqa-validation-3660", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3723", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4022", "mrqa_hotpotqa-validation-4028", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-4041", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-4274", "mrqa_hotpotqa-validation-4276", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4311", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4383", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-4486", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-4654", "mrqa_hotpotqa-validation-4704", "mrqa_hotpotqa-validation-4719", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4775", "mrqa_hotpotqa-validation-4849", "mrqa_hotpotqa-validation-489", "mrqa_hotpotqa-validation-4942", "mrqa_hotpotqa-validation-4946", "mrqa_hotpotqa-validation-499", "mrqa_hotpotqa-validation-5275", "mrqa_hotpotqa-validation-5325", "mrqa_hotpotqa-validation-5339", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-556", "mrqa_hotpotqa-validation-5576", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5588", "mrqa_hotpotqa-validation-5657", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5792", "mrqa_hotpotqa-validation-5820", "mrqa_hotpotqa-validation-5836", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-652", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-675", "mrqa_hotpotqa-validation-846", "mrqa_hotpotqa-validation-904", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-10380", "mrqa_naturalquestions-validation-10620", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1154", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-133", "mrqa_naturalquestions-validation-1443", "mrqa_naturalquestions-validation-1491", "mrqa_naturalquestions-validation-1504", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1613", "mrqa_naturalquestions-validation-1629", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-2011", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-2395", "mrqa_naturalquestions-validation-2552", "mrqa_naturalquestions-validation-2558", "mrqa_naturalquestions-validation-2722", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-3404", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-344", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3840", "mrqa_naturalquestions-validation-3898", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-4113", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-4597", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-4816", "mrqa_naturalquestions-validation-5069", "mrqa_naturalquestions-validation-5200", "mrqa_naturalquestions-validation-5230", "mrqa_naturalquestions-validation-5292", "mrqa_naturalquestions-validation-5446", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-5662", "mrqa_naturalquestions-validation-5835", "mrqa_naturalquestions-validation-5942", "mrqa_naturalquestions-validation-601", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-6020", "mrqa_naturalquestions-validation-6951", "mrqa_naturalquestions-validation-7158", "mrqa_naturalquestions-validation-7402", "mrqa_naturalquestions-validation-7490", "mrqa_naturalquestions-validation-7741", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8163", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-8446", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8584", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8928", "mrqa_naturalquestions-validation-9075", "mrqa_naturalquestions-validation-9410", "mrqa_naturalquestions-validation-9874", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-9931", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-1074", "mrqa_newsqa-validation-1077", "mrqa_newsqa-validation-1228", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-1621", "mrqa_newsqa-validation-1723", "mrqa_newsqa-validation-1774", "mrqa_newsqa-validation-1790", "mrqa_newsqa-validation-1862", "mrqa_newsqa-validation-1949", "mrqa_newsqa-validation-2035", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2159", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-2333", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-2388", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2475", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2926", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3218", "mrqa_newsqa-validation-3316", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-337", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-3560", "mrqa_newsqa-validation-3570", "mrqa_newsqa-validation-381", "mrqa_newsqa-validation-3841", "mrqa_newsqa-validation-3847", "mrqa_newsqa-validation-3920", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-3954", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-416", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-44", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-585", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-799", "mrqa_newsqa-validation-892", "mrqa_searchqa-validation-1069", "mrqa_searchqa-validation-10772", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-10845", "mrqa_searchqa-validation-11403", "mrqa_searchqa-validation-1154", "mrqa_searchqa-validation-11562", "mrqa_searchqa-validation-11600", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-12321", "mrqa_searchqa-validation-12446", "mrqa_searchqa-validation-12515", "mrqa_searchqa-validation-12615", "mrqa_searchqa-validation-12757", "mrqa_searchqa-validation-12806", "mrqa_searchqa-validation-131", "mrqa_searchqa-validation-13396", "mrqa_searchqa-validation-1363", "mrqa_searchqa-validation-13903", "mrqa_searchqa-validation-14082", "mrqa_searchqa-validation-14332", "mrqa_searchqa-validation-14354", "mrqa_searchqa-validation-14657", "mrqa_searchqa-validation-14669", "mrqa_searchqa-validation-14679", "mrqa_searchqa-validation-14773", "mrqa_searchqa-validation-14925", "mrqa_searchqa-validation-15078", "mrqa_searchqa-validation-15188", "mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-15583", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-15800", "mrqa_searchqa-validation-15832", "mrqa_searchqa-validation-162", "mrqa_searchqa-validation-16546", "mrqa_searchqa-validation-16659", "mrqa_searchqa-validation-1686", "mrqa_searchqa-validation-16912", "mrqa_searchqa-validation-1809", "mrqa_searchqa-validation-190", "mrqa_searchqa-validation-1977", "mrqa_searchqa-validation-2063", "mrqa_searchqa-validation-2073", "mrqa_searchqa-validation-2307", "mrqa_searchqa-validation-2372", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2507", "mrqa_searchqa-validation-2513", "mrqa_searchqa-validation-2560", "mrqa_searchqa-validation-2578", "mrqa_searchqa-validation-2671", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2891", "mrqa_searchqa-validation-3375", "mrqa_searchqa-validation-3400", "mrqa_searchqa-validation-371", "mrqa_searchqa-validation-3727", "mrqa_searchqa-validation-4115", "mrqa_searchqa-validation-4228", "mrqa_searchqa-validation-4307", "mrqa_searchqa-validation-4409", "mrqa_searchqa-validation-4550", "mrqa_searchqa-validation-4912", "mrqa_searchqa-validation-5108", "mrqa_searchqa-validation-5496", "mrqa_searchqa-validation-5602", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6632", "mrqa_searchqa-validation-6784", "mrqa_searchqa-validation-6908", "mrqa_searchqa-validation-7046", "mrqa_searchqa-validation-7168", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-7362", "mrqa_searchqa-validation-7402", "mrqa_searchqa-validation-7554", "mrqa_searchqa-validation-7706", "mrqa_searchqa-validation-8431", "mrqa_searchqa-validation-8518", "mrqa_searchqa-validation-8755", "mrqa_searchqa-validation-8830", "mrqa_searchqa-validation-8932", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-9422", "mrqa_searchqa-validation-9562", "mrqa_searchqa-validation-959", "mrqa_searchqa-validation-9599", "mrqa_searchqa-validation-9894", "mrqa_searchqa-validation-9916", "mrqa_searchqa-validation-9984", "mrqa_squad-validation-10064", "mrqa_squad-validation-10168", "mrqa_squad-validation-10203", "mrqa_squad-validation-10309", "mrqa_squad-validation-10333", "mrqa_squad-validation-10370", "mrqa_squad-validation-1042", "mrqa_squad-validation-1182", "mrqa_squad-validation-1247", "mrqa_squad-validation-1349", "mrqa_squad-validation-1404", "mrqa_squad-validation-157", "mrqa_squad-validation-1956", "mrqa_squad-validation-1977", "mrqa_squad-validation-2118", "mrqa_squad-validation-2140", "mrqa_squad-validation-225", "mrqa_squad-validation-2390", "mrqa_squad-validation-2521", "mrqa_squad-validation-256", "mrqa_squad-validation-2573", "mrqa_squad-validation-258", "mrqa_squad-validation-2599", "mrqa_squad-validation-2628", "mrqa_squad-validation-2668", "mrqa_squad-validation-2684", "mrqa_squad-validation-2719", "mrqa_squad-validation-2778", "mrqa_squad-validation-3063", "mrqa_squad-validation-3138", "mrqa_squad-validation-3141", "mrqa_squad-validation-3165", "mrqa_squad-validation-3204", "mrqa_squad-validation-3460", "mrqa_squad-validation-3522", "mrqa_squad-validation-3599", "mrqa_squad-validation-3713", "mrqa_squad-validation-3782", "mrqa_squad-validation-3898", "mrqa_squad-validation-3958", "mrqa_squad-validation-4072", "mrqa_squad-validation-4122", "mrqa_squad-validation-4157", "mrqa_squad-validation-4162", "mrqa_squad-validation-4164", "mrqa_squad-validation-4304", "mrqa_squad-validation-4311", "mrqa_squad-validation-4421", "mrqa_squad-validation-4429", "mrqa_squad-validation-4490", "mrqa_squad-validation-4579", "mrqa_squad-validation-4803", "mrqa_squad-validation-4806", "mrqa_squad-validation-4875", "mrqa_squad-validation-5010", "mrqa_squad-validation-5081", "mrqa_squad-validation-5156", "mrqa_squad-validation-5373", "mrqa_squad-validation-5520", "mrqa_squad-validation-5555", "mrqa_squad-validation-5754", "mrqa_squad-validation-5839", "mrqa_squad-validation-5891", "mrqa_squad-validation-6019", "mrqa_squad-validation-6166", "mrqa_squad-validation-6214", "mrqa_squad-validation-6227", "mrqa_squad-validation-6524", "mrqa_squad-validation-6611", "mrqa_squad-validation-6613", "mrqa_squad-validation-6854", "mrqa_squad-validation-692", "mrqa_squad-validation-708", "mrqa_squad-validation-7111", "mrqa_squad-validation-7162", "mrqa_squad-validation-7211", "mrqa_squad-validation-751", "mrqa_squad-validation-7559", "mrqa_squad-validation-7588", "mrqa_squad-validation-7639", "mrqa_squad-validation-7683", "mrqa_squad-validation-7752", "mrqa_squad-validation-7831", "mrqa_squad-validation-7864", "mrqa_squad-validation-7902", "mrqa_squad-validation-7934", "mrqa_squad-validation-8245", "mrqa_squad-validation-8476", "mrqa_squad-validation-8527", "mrqa_squad-validation-867", "mrqa_squad-validation-9082", "mrqa_squad-validation-9145", "mrqa_squad-validation-9317", "mrqa_squad-validation-9362", "mrqa_squad-validation-9587", "mrqa_squad-validation-9643", "mrqa_squad-validation-9653", "mrqa_triviaqa-validation-1051", "mrqa_triviaqa-validation-148", "mrqa_triviaqa-validation-1532", "mrqa_triviaqa-validation-1630", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-2029", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-2136", "mrqa_triviaqa-validation-2142", "mrqa_triviaqa-validation-2314", "mrqa_triviaqa-validation-2537", "mrqa_triviaqa-validation-2550", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2715", "mrqa_triviaqa-validation-3039", "mrqa_triviaqa-validation-3057", "mrqa_triviaqa-validation-3081", "mrqa_triviaqa-validation-3292", "mrqa_triviaqa-validation-3405", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-3947", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-424", "mrqa_triviaqa-validation-4321", "mrqa_triviaqa-validation-4343", "mrqa_triviaqa-validation-4408", "mrqa_triviaqa-validation-4420", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-4869", "mrqa_triviaqa-validation-4888", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5309", "mrqa_triviaqa-validation-5885", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-621", "mrqa_triviaqa-validation-6360", "mrqa_triviaqa-validation-6516", "mrqa_triviaqa-validation-6571", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-7239", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-7362", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-977"], "OKR": 0.796875, "KG": 0.47265625, "before_eval_results": {"predictions": ["arthur square", "St Keegan Dan", "worse", "Montreal", "Goat Island", "Sean \"Puff Daddy\" Combs", "Moses", "Richard Walter Jenkins", "manganese", "Leander Club", "Tripoli", "a window", "Sir Winston Churchill", "Tom Hanks", "Jamaica", "a Compact Pussycat", "Strangeways", "Paul Gauguin", "rod", "Tim Brooke- Taylor", "secret state police", "Shania Twain", "manganese", "Azovs'ke", "J. M. W. Turner", "an ear canal", "Morten Skovsby", "Taco Bell", "Malaysia", "Ford Motor Company", "man", "paper sales", "Dilbert", "a horizon", "Erik Aunapuu", "David Bowie", "24", "Ibrox Stadium", "mark", "The Cheshire Cat", "Kent", "East of Eden", "manganese", "agincourt", "Greece", "\u00ef\u00bf\u00bdhad been waiting for reinforcements that never came.", "an architect who, in the opinion of select Pritzker Prize jury, has made profound achievements in the world of architecture.", "Richard Rodgers and Oscar Hammerstein", "Dr John Sentamu", "a dice game", "green", "Jenny Slate", "an unknown recipient", "Donna Mills", "February 9, 1994", "Hexachrome", "13 October 1958", "Revolutionary Armed Forces of Colombia,", "Sheikh Sharif Sheikh Ahmed", "allegations that a dorm parent mistreated students at the school.", "Thurgood Marshall", "Inigo Montoya", "The King of the Hill", "prevent any contaminants in the sink from flowing into the potable water system by siphonage"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6059027777777777}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false], "QA-F1": [0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5247", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-1447", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-3105", "mrqa_triviaqa-validation-1876", "mrqa_triviaqa-validation-3077", "mrqa_triviaqa-validation-6410", "mrqa_triviaqa-validation-2114", "mrqa_triviaqa-validation-3946", "mrqa_triviaqa-validation-5485", "mrqa_triviaqa-validation-7633", "mrqa_triviaqa-validation-7287", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-1930", "mrqa_triviaqa-validation-3808", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-6274", "mrqa_triviaqa-validation-4339", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-2308", "mrqa_hotpotqa-validation-4558", "mrqa_searchqa-validation-11101", "mrqa_naturalquestions-validation-5297"], "SR": 0.546875, "CSR": 0.5540625, "EFR": 1.0, "Overall": 0.70221875}]}