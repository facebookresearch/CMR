{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5390, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["1978", "587,000 square kilometres", "itinerant farmers", "Gamal Abdul Nasser", "39", "Philo of Byzantium", "cnidarians", "the dukes", "Christopher Eccleston", "Ed Lee", "Keck and Mithouard", "Word and Image Department", "Water (H2O) and carbon dioxide (CO2)", "BBC Radio 5 Live", "Baptism", "achievement-oriented motivations (\"pull\")", "2 million", "inferior", "until 1796", "dummy upper stages filled with water", "Variable lymphocyte receptors", "progressive folk-rock", "$32 billion", "Derek Wolfe", "Basel", "1937", "tourism", "white", "Midsummer\u2019s Night", "installed electrical arc light based illumination systems", "2016", "all large cases of the problem are hard", "photooxidative damage", "five", "iteratively", "the Sun", "Climate fluctuations during the last 34 million years", "Bible translation", "Hayri Abaza", "a better understanding of the Mau Mau command structure", "Turnagain Lane", "monophyletic", "adaptive immune system", "The Dornbirner Ach", "water flow through the body cavity", "CBSE", "a cubic interpolation formula", "Writers Guild of America", "education", "five", "14th to the 19th century", "extended structure", "the Lisbon Treaty", "the Romantic Rhine", "2012", "No Child Left Behind", "The Deadly Assassin and Mawdryn undead", "higher than normal O2 exposure for a fee", "three to five", "TFEU article 294", "The Northern Chinese were ranked higher", "Tracy Wolfson", "local building authority regulations and codes of practice", "The WB"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8807426948051948}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4257", "mrqa_squad-validation-486", "mrqa_squad-validation-5362", "mrqa_squad-validation-291", "mrqa_squad-validation-1862", "mrqa_squad-validation-9520", "mrqa_squad-validation-3104", "mrqa_squad-validation-3609", "mrqa_squad-validation-8256"], "SR": 0.859375, "CSR": 0.859375, "EFR": 0.8888888888888888, "Overall": 0.8741319444444444}, {"timecode": 1, "before_eval_results": {"predictions": ["euphoric", "Khorasan", "higher", "bigamy", "pathogens", "the geographical area it covers", "ships", "12 January", "7.5%", "Ren\u00e9 Lalique", "The Hoppings", "the Scots", "double or triple", "wage or salary", "Each packet is labeled with a destination address, source address, and port numbers", "Maria Fold and thrust Belt", "Yinchuan", "between AD 0\u20131250", "Chivas", "26", "Amazonia: Man and Culture in a Counterfeit Paradise", "1550", "two", "1850", "Greg Brady", "colonialism", "22 October 2006", "10,000", "Foreign Protestants Naturalization Act", "art posters", "Americans", "its unpaired electrons", "Los Angeles", "over 100,000", "wealth", "chromoplasts", "1,548", "The upper Rhine and upper Danube are easily crossed", "Peter Pratt and Geoffrey Beevers", "2010", "Germany and Austria", "Immunoproteomics", "solid economic growth", "ditch digger", "1754\u20131763", "Danny Trevathan", "Jochi", "1999", "rubisco", "August 1914", "public", "affordable housing", "seven", "John Mearsheimer and Robert Pape", "Business Connect", "1550 to 1900", "rules that conflict with morality", "Bauhaus", "2015", "French", "EBSCO", "The official vegetable of Washington State is a sweet onion", "I've been exiled to Siberia!", "The definition of a cloister is a secluded monastery or any... a place of religious seclusion"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8233840811965812}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_squad-validation-3190", "mrqa_squad-validation-4677", "mrqa_squad-validation-360", "mrqa_squad-validation-9802", "mrqa_squad-validation-9372", "mrqa_squad-validation-7701", "mrqa_squad-validation-6891", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5763", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-206"], "SR": 0.796875, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 2, "before_eval_results": {"predictions": ["2016", "expansion", "Shirley and Johnson", "Miller", "1892 to 1894", "26", "99.4", "St Thomas Becket", "they are homebound", "two", "Moscone Center", "Germany", "January", "about thirty", "10", "Greg Olsen", "Prince of P\u0142ock", "electron microscopy", "computability theory", "1275", "Fred Silverman", "liquid nitrogen", "Quasiturbine", "UK", "Daniel 8:9\u201312, 23\u201325", "Ex post facto laws", "cameras", "time and space hierarchy theorems", "$105 billion", "six", "55 mph", "The Bachelor", "child-killers", "Warfare and the long occupation", "Cretaceous\u2013Paleogene extinction", "almost a month", "George B. Storer", "CD40", "94", "their dispersed population and distance from the Scottish Parliament in Edinburgh", "difference in potential energy", "George Westinghouse", "Geneva", "Court of Justice", "Lake \u00dcberlingen", "Budapest", "Genoese traders", "the \"simple people\"", "Stanford University", "formal language", "systematic economic inequalities", "stealing", "lunar new year", "Steve McQueen", "alcohol", "Sedgefield", "Sirhan Sirhan", "a wooden comb", "Asia", "People!  and The Carnabeats", "a Yemeni cleric and his personal assistant", "the 3rd Sun", "Xherdan Shaqiri", "Potomac River"], "metric_results": {"EM": 0.734375, "QA-F1": 0.765625}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6337", "mrqa_squad-validation-7165", "mrqa_squad-validation-1703", "mrqa_squad-validation-5921", "mrqa_squad-validation-3540", "mrqa_squad-validation-4096", "mrqa_squad-validation-8534", "mrqa_squad-validation-4206", "mrqa_squad-validation-3945", "mrqa_squad-validation-4771", "mrqa_squad-validation-1509", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4028", "mrqa_newsqa-validation-817", "mrqa_searchqa-validation-13857", "mrqa_hotpotqa-validation-1902"], "SR": 0.734375, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 3, "before_eval_results": {"predictions": ["Religious Coalition for Reproductive Choice", "Waal", "malaria parasite", "three", "over $40 million", "SyFy", "Ollie Treiz", "five", "younger", "nearly three hundred years", "1 July 1851", "the world's economy", "nine", "the property owner", "1916", "twice", "its unpaired electrons", "Golovin", "how or whether this connection is relevant on microscales", "Matt Smith", "Baltimore", "noble", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "CBS and NBC", "mediaeval music", "southern California", "26", "Masovian Primeval Forest", "Apollo 1 backup crew", "2008", "a hemicycle", "the Connectional Table", "waldzither", "a bishop", "performance", "destruction of the forest", "10.0%", "Outlaws", "Africa", "an occupancy permit", "one", "Dignity Health", "Battle of Jumonville Glen", "Kony Ealy", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "Napoleon", "2016", "kennedy", "kennedy", "Bones", "The History Book Club", "kennedy", "California's 33rd congressional district", "yerevan", "The Vampire Armand", "The Solar System is located within the disk", "Hugh S. Johnson", "The Five Stages of Sleep", "a person who smuggles what across the U.S. border", "various", "1896", "Carrousel du Louvre", "last year's Gaza campaign", "Odense"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7113935291858678}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19047619047619044, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6008", "mrqa_squad-validation-10427", "mrqa_squad-validation-438", "mrqa_squad-validation-6118", "mrqa_squad-validation-2612", "mrqa_squad-validation-10068", "mrqa_squad-validation-4360", "mrqa_squad-validation-6426", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-6097", "mrqa_searchqa-validation-4289", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-7616", "mrqa_hotpotqa-validation-3780", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-1749", "mrqa_hotpotqa-validation-5584"], "SR": 0.65625, "CSR": 0.76171875, "retrieved_ids": ["mrqa_squad-train-23502", "mrqa_squad-train-33057", "mrqa_squad-train-16061", "mrqa_squad-train-21445", "mrqa_squad-train-5590", "mrqa_squad-train-25753", "mrqa_squad-train-41202", "mrqa_squad-train-48023", "mrqa_squad-train-11519", "mrqa_squad-train-43122", "mrqa_squad-train-39326", "mrqa_squad-train-39862", "mrqa_squad-train-63705", "mrqa_squad-train-63682", "mrqa_squad-train-12280", "mrqa_squad-train-4362", "mrqa_squad-validation-3609", "mrqa_searchqa-validation-206", "mrqa_squad-validation-5921", "mrqa_searchqa-validation-3272", "mrqa_squad-validation-7701", "mrqa_triviaqa-validation-3751", "mrqa_squad-validation-5362", "mrqa_squad-validation-4677", "mrqa_triviaqa-validation-4028", "mrqa_squad-validation-4206", "mrqa_hotpotqa-validation-5763", "mrqa_squad-validation-486", "mrqa_squad-validation-1703", "mrqa_newsqa-validation-817", "mrqa_hotpotqa-validation-1902", "mrqa_squad-validation-8256"], "EFR": 1.0, "Overall": 0.880859375}, {"timecode": 4, "before_eval_results": {"predictions": ["13", "he miscalculated the political implications", "178", "1st century BC", "The Five Doctors", "Combined Statistical Area", "Bryan Davies", "1060s", "Fresno", "the British were able to prevent the arrival of French relief ships in the naval Battle of the Restigouche", "Utopia", "John Debney", "BBC Dead Ringers", "July 11, 1962", "Light", "49\u201315", "pancake-shaped circular disks", "The energy crisis", "nine", "very low tuition fees", "increase its bulk and decrease its density", "paying his rent at the Hotel New Yorker", "it would do more harm than good", "John Debney", "symbiotic", "Larry Roberts", "CBS", "leftist/communist/nationalist insurgents/opposition", "RSA", "procurement of Armoured Personnel Carriers", "France's claim to the region was superior to that of the British", "male sex hormones such as testosterone seem to be immunosuppressive", "very weak", "private citizen", "Florida", "August 1992", "71%", "Samuel Phillips", "1962", "Newcastle Diamonds", "five", "demographics and economic ties", "UNICEF", "Morgan Tsvangirai", "civilians", "maintain an \"aesthetic environment\" and ensure public safety", "Pakistani territory", "four months ago", "right-wing extremist groups", "remains committed to British sovereignty", "led authorities to a $13 million global crime ring", "Don Draper", "had his Irish heritage uncovered by genealogists in 2002", "the body", "1979", "a large primate species", "John", "1641", "House of Commons", "\"You're out\"", "The ballot", "plants", "white", "Montezuma"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6862245334161452}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.16, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.21052631578947364, 0.0, 1.0, 0.0, 0.125, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2591", "mrqa_squad-validation-7792", "mrqa_squad-validation-2717", "mrqa_squad-validation-10269", "mrqa_squad-validation-7715", "mrqa_squad-validation-236", "mrqa_squad-validation-8811", "mrqa_squad-validation-6874", "mrqa_squad-validation-7713", "mrqa_squad-validation-8374", "mrqa_squad-validation-6595", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-7901", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-430", "mrqa_hotpotqa-validation-2418", "mrqa_searchqa-validation-8027", "mrqa_triviaqa-validation-1088"], "SR": 0.640625, "CSR": 0.7375, "EFR": 1.0, "Overall": 0.86875}, {"timecode": 5, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "missile projects", "The Entertainment Channel", "from home viewers who made tape recordings of the show", "cattle and citrus", "quantum electrodynamics", "through confirmation and sometimes the profession of faith", "75th birthday", "a computational problem where a single output (of a total function) is expected for every input", "Plasmodium falciparum", "civil disobedience", "Masovian gothic style", "more wealth and income", "polynomial-time reduction", "2012", "European Parliament and the Council of the European Union", "antithetical", "polynomial time algorithm", "Fred Singer", "William of Volpiano and John of Ravenna", "water in equilibrium with air", "Arizona Cardinals", "91%", "co-chair of TAR WGI", "July 24", "nearly three hundred years", "Encoded Archival description (EAD)", "the Privy Council", "a citizen's relation to the state and its laws", "Golden Gate Bridge", "2011", "respiration", "created", "Fred Pierce", "2016", "390", "More than 1 million", "in the chloroplasts of C4 plants", "When the reaction occurs in a liquid solution, the solid formed is called the'precipitate '", "Jim Capaldi, Paul Carrack, and Peter Vale,", "Hanna's best friend, Mona Vanderwaal, informs the girls that she has also received texts from A", "in order to halt it following brake failure", "in the 1960s, but lost each series to the Boston Celtics,", "boys", "1998", "Geophysicists", "between Glen Miller Road in Trenton and the Don Valley Parkway / Highway 402 Junction in Toronto", "April 25 -- 30 in Park Avenue, just outside the Waldorf - Astoria Hotel", "Tsetse can be distinguished from other large flies by two easily observed features", "December 1, 2009", "in sequence with each heartbeat", "World War II", "Richard Seddon", "Switzerland", "the \"Black Abbots\"", "1919", "Subha", "Roger Federer", "Gov. Bobby Jindal", "July 4", "real traditionalists", "men", "a variety of this 3-letter defensive secretion produced by cephalopods", "Australia"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6220472503905409}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.4, 1.0, 1.0, 0.9411764705882353, 0.5, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.7058823529411764, 0.0, 0.0, 0.0, 0.0, 0.09523809523809523, 0.0, 0.0, 0.9333333333333333, 0.0, 0.0625, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-7647", "mrqa_squad-validation-2705", "mrqa_squad-validation-1600", "mrqa_squad-validation-6670", "mrqa_squad-validation-988", "mrqa_squad-validation-2160", "mrqa_squad-validation-3556", "mrqa_squad-validation-8189", "mrqa_squad-validation-8671", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2555", "mrqa_triviaqa-validation-7172", "mrqa_hotpotqa-validation-3223", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-2328", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-10685"], "SR": 0.53125, "CSR": 0.703125, "EFR": 0.9666666666666667, "Overall": 0.8348958333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["2007", "1973", "Siegfried", "June", "27 September 2001", "teachers who are friendly and supportive", "500", "northern China", "18 February 1546", "tree growth", "Karl von Miltitz", "a supervisory church body", "via the ballast tanks of ships", "1279", "plant health status from a satellite platform", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "opposite end from the mouth", "models", "river Deabolis", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "October", "three", "chameleon circuit", "center of the Earth", "there are more poor people in the United States and Western Europe than in China (due to a greater tendency to take on debts).[unreliable source?] Anthony Shorrocks", "geographic scholars under colonizing empires", "Lippe", "colonyism", "10,000", "layered basaltic lava flows", "political parties", "$5,000,000", "a second Gleichschaltung or similar event in the future", "transient", "11 February 2012", "L lubricates the heart", "25 years after the release of their first record", "Dominicanos ( Dominicans ), which is the adjective form of `` Domingo '', and the revolutionaries named their newly independent country La Rep\u00fablica Dominicana", "two", "The permanent members of the United Nations Security Council ( also known as the Permanent Five, Big Five, or P5 ) are the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council", "first Sunday after Easter", "When all the numbers required to win a prize have been marked off", "9.0 -- 9.1 ( M ) undersea megathrust earthquake", "The Jungle Book ( 1894 ) is a collection of stories by English author Rudyard Kipling", "American Horror Story", "Reuben Kincaid, opposite Shirley Jones's character", "Redskins", "Symphony No. 40 in G minor, KV. 550", "the Garden of Gethsemane, where Jesus went to pray the night before His crucifixion", "Parliament Square, London", "Armagh", "St Paul's Cathedral", "Sydney", "Australian", "Battle of Britain and the Battle of Malta", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help", "Casalesi Camorra clan", "PC processing unit (CPU)", "colonel", "Queen Wilhelmina", "The Mole", "Santa Maria a Poppiena", "Crete", "-ie"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5623793514758189}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.06666666666666667, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.8, 0.09523809523809525, 0.0, 0.0, 0.13043478260869568, 0.0, 0.0, 0.0, 0.5, 0.8571428571428571, 0.0, 0.0, 0.4, 0.07692307692307691, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2095", "mrqa_squad-validation-4506", "mrqa_squad-validation-2468", "mrqa_squad-validation-3620", "mrqa_squad-validation-9408", "mrqa_squad-validation-10412", "mrqa_squad-validation-7554", "mrqa_squad-validation-9865", "mrqa_squad-validation-9798", "mrqa_squad-validation-6962", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1058", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-4084", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-1430"], "SR": 0.453125, "CSR": 0.6674107142857143, "retrieved_ids": ["mrqa_squad-train-73030", "mrqa_squad-train-7742", "mrqa_squad-train-80331", "mrqa_squad-train-29730", "mrqa_squad-train-52885", "mrqa_squad-train-29157", "mrqa_squad-train-65369", "mrqa_squad-train-19612", "mrqa_squad-train-29200", "mrqa_squad-train-4943", "mrqa_squad-train-43963", "mrqa_squad-train-73075", "mrqa_squad-train-65255", "mrqa_squad-train-23486", "mrqa_squad-train-57502", "mrqa_squad-train-3838", "mrqa_triviaqa-validation-7616", "mrqa_searchqa-validation-8027", "mrqa_newsqa-validation-2955", "mrqa_squad-validation-7715", "mrqa_triviaqa-validation-2050", "mrqa_squad-validation-6118", "mrqa_squad-validation-486", "mrqa_naturalquestions-validation-6857", "mrqa_squad-validation-4771", "mrqa_newsqa-validation-3394", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-4028", "mrqa_squad-validation-1600", "mrqa_squad-validation-8189", "mrqa_naturalquestions-validation-2588", "mrqa_hotpotqa-validation-3223"], "EFR": 1.0, "Overall": 0.8337053571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["games and activities that will highlight the Bay Area's technology, culinary creations, and cultural diversity", "three days", "second and third run movies, along with classic films", "radiography", "the government and the National Assembly and the Senate", "lipophilic alkaloid toxins", "the Compromise of 1850", "a presidential representative democratic republic", "phagosomal", "1969", "preparation and approval process", "Community law", "elementary particles", "Downtown San Diego", "1954", "Bermuda 419 turf", "The Judiciary", "they are judged \" wrong\" by an individual conscience", "The Space Museum", "only a minority of the genetic material is kept in circular chromosomes while the rest is in branched, linear, or other complex structures", "The Hoppings funfair", "Decision problems", "seven", "Pickawillany", "denying having committed the crime", "56.2%", "Financial crisis of 2007\u201308", "Peter Davison", "Percy Shelley", "swimming-plates", "American Broadcasting-Paramount Theatres, Inc.", "Ray Henderson", "rises 735 feet ( 224 m )", "B.R. Ambedkar", "Kim Basinger", "John F. Kelly, to the post of White House Chief of Staff by President Donald Trump", "2001", "Western world", "two", "1973", "Bachendri Pal", "blood vessels and organs throughout the body", "gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Plank", "Ron Harper", "Dmitri Mendeleev", "The sacroiliac joint", "The Potteries", "South African", "John Chilcot", "the Apollo 11 lunar module landed in the moon\u2019s Sea of Tranquility", "Kind Hearts and Coronets", "Khilona", "The O2 Arena", "2004", "London and Buenos Aires", "Evan Bayh", "after Wood went missing off Catalina Island, near the California coast", "Juarez drug cartel", "volcanic eruptions", "ummi", "1876", "Luck of the", "between the three towns of Doncaster, Scunthorpe and Gainsborough"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6429762453199953}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.7567567567567568, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.18181818181818182]}}, "before_error_ids": ["mrqa_squad-validation-519", "mrqa_squad-validation-8496", "mrqa_squad-validation-4589", "mrqa_squad-validation-436", "mrqa_squad-validation-8732", "mrqa_squad-validation-6776", "mrqa_squad-validation-4730", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-7464", "mrqa_triviaqa-validation-3602", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-861", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-493", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-6517", "mrqa_hotpotqa-validation-1533"], "SR": 0.5625, "CSR": 0.654296875, "EFR": 0.9642857142857143, "Overall": 0.8092912946428572}, {"timecode": 8, "before_eval_results": {"predictions": ["Episcopal Areas", "\"sharia rather than the building of Islamic institutions,\" and rejection of Shia Islam.", "scoil phr\u00edobh\u00e1ideach", "1993", "Chinatown", "state, relative cost of living, and grade taught", "it stimulated his brain cells.", "home viewers who made tape recordings of the show", "the chosen machine model", "captive import policy", "the 1855 colonial constitution", "Rhine Gorge", "savanna or desert", "1967", "Vince Lombardi Trophy", "2003", "Kurt Vonnegut", "10", "to become more integral within the health care system", "ten-horsepower", "12", "1995\u201396 season", "San Diego International Airport", "hydrogen and helium", "two", "Golden Gate Bridge", "Elders", "1331", "The donor cells also become coated with IgG and are subsequently removed by macrophages in the reticuloendothelial system ( RES )", "Billy Beane", "Spain had more than compensated by recovering Menorca and by reducing the British threat to its colonies in and around the Caribbean", "Eurasian Plate", "1947", "John Dalton", "12", "it was on this day in 1930 when Declaration of Indian Independence ( Purna Swaraj ) was proclaimed by the Indian National Congress", "The Battle of Salamis ( / \u02c8s\u00e6l\u0259m\u026as / ; Ancient Greek : \u039d\u03b1\u03c5\u03bc\u03b1\u03c7\u03af\u03b1", "Road / Track ( no `` and '' )", "Pradyumna", "Phillipa Soo", "May 18, 2010", "St. John's, Newfoundland and Labrador", "Mangal Pandey", "Asuka", "one more proton and is less metallic than its predecessor", "The enthalpy of fusion of a substance", "an alternate meaning", "a true wireworm", "a self-governing colony", "50", "an Anglo-Saxon saint", "Jena Malone", "American", "1993", "civilians", "If huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica", "immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally", "Piedad Cordoba", "the Seven Years' War", "an extra $5,000/year", "Robert", "two numbers", "The Dark Tower", "Ministry of European Integration (Albania)"], "metric_results": {"EM": 0.5, "QA-F1": 0.6249074748319758}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 0.09523809523809523, 0.0, 0.0, 1.0, 0.0, 0.8163265306122449, 0.125, 0.6666666666666666, 1.0, 1.0, 1.0, 0.7272727272727272, 0.4, 1.0, 0.0, 0.5882352941176471, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5405405405405405, 1.0, 0.28571428571428575, 0.0, 0.6666666666666666, 0.0, 1.0, 0.888888888888889]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-2236", "mrqa_squad-validation-1445", "mrqa_squad-validation-7643", "mrqa_squad-validation-8984", "mrqa_squad-validation-6404", "mrqa_squad-validation-3667", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-1119", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-672", "mrqa_hotpotqa-validation-1086", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3581", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-1996", "mrqa_hotpotqa-validation-5760"], "SR": 0.5, "CSR": 0.6371527777777778, "EFR": 0.90625, "Overall": 0.7717013888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["Sports Night", "c1750", "Newton", "Ford", "Islamism", "51.6%", "public policy goals", "the superior and the norm", "1985", "standard model", "Theory of the Earth", "New Jersey, Rhode Island and Delaware", "1,967", "the AS-205 mission was canceled", "Earth", "prime elements and prime ideals", "T. J. Ward", "5-cylinder engine (no compound) with superheated steam", "often a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "primes", "1995", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "applied mathematics to the construction of calendars", "spy network", "Elizabeth", "1861\u20131865", "1968", "Daimler-Benz", "25", "Baldwin", "June", "John Schlesinger", "hamburgers", "Ellie Kemper", "Ezeiza International Airport", "leg injury", "insurance agent", "bioelectromagnetics", "Tennessee", "Innsbruck", "Andes", "Vishal Bhardwaj", "Lincoln Memorial University", "five aerial victories", "actress", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "Justin Bieber", "Nitty Gritty Dirt Band", "April", "Lucky the Leprechaun", "( zimtrekturkey.com", "1", "zimbabwe", "Dean Martin, Katharine Hepburn and Spencer Tracy", "between South America and Africa", "\"Quiet Nights\"", "Crandon, Wisconsin", "the Lone Ranger", "(highest point)", "(born November 8, 1965)", "(Cheryl of the Clue Crews)", "1922", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "John F. Kennedy"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5734844659017453}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 0.25, 0.0, 1.0, 1.0, 0.38095238095238093, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.15384615384615383, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.47058823529411764, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-361", "mrqa_squad-validation-7017", "mrqa_squad-validation-10506", "mrqa_squad-validation-276", "mrqa_squad-validation-3954", "mrqa_squad-validation-9087", "mrqa_squad-validation-3347", "mrqa_squad-validation-2315", "mrqa_squad-validation-6806", "mrqa_squad-validation-6154", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-4617", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-3750", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-1567", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-4050", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4272", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2324", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-2892", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5876"], "SR": 0.484375, "CSR": 0.621875, "retrieved_ids": ["mrqa_squad-train-54948", "mrqa_squad-train-24807", "mrqa_squad-train-19872", "mrqa_squad-train-15536", "mrqa_squad-train-77568", "mrqa_squad-train-29619", "mrqa_squad-train-22338", "mrqa_squad-train-14591", "mrqa_squad-train-39531", "mrqa_squad-train-27842", "mrqa_squad-train-74185", "mrqa_squad-train-48503", "mrqa_squad-train-8130", "mrqa_squad-train-12508", "mrqa_squad-train-13650", "mrqa_squad-train-63700", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-10615", "mrqa_hotpotqa-validation-1086", "mrqa_triviaqa-validation-6976", "mrqa_squad-validation-8189", "mrqa_squad-validation-8732", "mrqa_squad-validation-6874", "mrqa_squad-validation-2468", "mrqa_triviaqa-validation-4458", "mrqa_squad-validation-9372", "mrqa_squad-validation-7701", "mrqa_squad-validation-9865", "mrqa_hotpotqa-validation-5760", "mrqa_naturalquestions-validation-710", "mrqa_searchqa-validation-4084", "mrqa_squad-validation-10068"], "EFR": 1.0, "Overall": 0.8109375}, {"timecode": 10, "UKR": 0.794921875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1966", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4617", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-861", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5687", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9963", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-787", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-206", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8027", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10004", "mrqa_squad-validation-10010", "mrqa_squad-validation-10024", "mrqa_squad-validation-10038", "mrqa_squad-validation-10059", "mrqa_squad-validation-10068", "mrqa_squad-validation-10072", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10115", "mrqa_squad-validation-10124", "mrqa_squad-validation-10140", "mrqa_squad-validation-10232", "mrqa_squad-validation-10340", "mrqa_squad-validation-10340", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1172", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1311", "mrqa_squad-validation-1409", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1541", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1634", "mrqa_squad-validation-1637", "mrqa_squad-validation-1651", "mrqa_squad-validation-1703", "mrqa_squad-validation-1762", "mrqa_squad-validation-1817", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2095", "mrqa_squad-validation-2108", "mrqa_squad-validation-2160", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2315", "mrqa_squad-validation-2325", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2403", "mrqa_squad-validation-2461", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2612", "mrqa_squad-validation-2678", "mrqa_squad-validation-2711", "mrqa_squad-validation-2717", "mrqa_squad-validation-2752", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2861", "mrqa_squad-validation-2869", "mrqa_squad-validation-2902", "mrqa_squad-validation-291", "mrqa_squad-validation-2916", "mrqa_squad-validation-2934", "mrqa_squad-validation-2952", "mrqa_squad-validation-2985", "mrqa_squad-validation-3049", "mrqa_squad-validation-3104", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-322", "mrqa_squad-validation-3222", "mrqa_squad-validation-3223", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-334", "mrqa_squad-validation-3347", "mrqa_squad-validation-3416", "mrqa_squad-validation-343", "mrqa_squad-validation-3440", "mrqa_squad-validation-3524", "mrqa_squad-validation-3540", "mrqa_squad-validation-3556", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3609", "mrqa_squad-validation-361", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-3620", "mrqa_squad-validation-3660", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3713", "mrqa_squad-validation-3745", "mrqa_squad-validation-3751", "mrqa_squad-validation-3752", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3866", "mrqa_squad-validation-3871", "mrqa_squad-validation-3873", "mrqa_squad-validation-3954", "mrqa_squad-validation-3957", "mrqa_squad-validation-3962", "mrqa_squad-validation-3986", "mrqa_squad-validation-4026", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-418", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4206", "mrqa_squad-validation-4242", "mrqa_squad-validation-4246", "mrqa_squad-validation-4257", "mrqa_squad-validation-4260", "mrqa_squad-validation-4305", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4447", "mrqa_squad-validation-4451", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-45", "mrqa_squad-validation-453", "mrqa_squad-validation-4533", "mrqa_squad-validation-4547", "mrqa_squad-validation-4575", "mrqa_squad-validation-4589", "mrqa_squad-validation-4630", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-47", "mrqa_squad-validation-4707", "mrqa_squad-validation-4730", "mrqa_squad-validation-4771", "mrqa_squad-validation-4775", "mrqa_squad-validation-4832", "mrqa_squad-validation-486", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-5052", "mrqa_squad-validation-5099", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-519", "mrqa_squad-validation-5230", "mrqa_squad-validation-524", "mrqa_squad-validation-5250", "mrqa_squad-validation-5329", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5362", "mrqa_squad-validation-5364", "mrqa_squad-validation-539", "mrqa_squad-validation-5434", "mrqa_squad-validation-5440", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5558", "mrqa_squad-validation-5562", "mrqa_squad-validation-5597", "mrqa_squad-validation-5650", "mrqa_squad-validation-5671", "mrqa_squad-validation-5693", "mrqa_squad-validation-57", "mrqa_squad-validation-5753", "mrqa_squad-validation-5772", "mrqa_squad-validation-5783", "mrqa_squad-validation-5791", "mrqa_squad-validation-5881", "mrqa_squad-validation-5921", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-5999", "mrqa_squad-validation-6013", "mrqa_squad-validation-6042", "mrqa_squad-validation-6118", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6288", "mrqa_squad-validation-6291", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6491", "mrqa_squad-validation-6552", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6776", "mrqa_squad-validation-6787", "mrqa_squad-validation-6801", "mrqa_squad-validation-6805", "mrqa_squad-validation-6806", "mrqa_squad-validation-6852", "mrqa_squad-validation-6861", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6948", "mrqa_squad-validation-6958", "mrqa_squad-validation-6962", "mrqa_squad-validation-6996", "mrqa_squad-validation-7017", "mrqa_squad-validation-7026", "mrqa_squad-validation-7030", "mrqa_squad-validation-7035", "mrqa_squad-validation-71", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7173", "mrqa_squad-validation-7328", "mrqa_squad-validation-7331", "mrqa_squad-validation-734", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7380", "mrqa_squad-validation-7384", "mrqa_squad-validation-7395", "mrqa_squad-validation-742", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-758", "mrqa_squad-validation-7628", "mrqa_squad-validation-7629", "mrqa_squad-validation-764", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7713", "mrqa_squad-validation-7715", "mrqa_squad-validation-7723", "mrqa_squad-validation-7747", "mrqa_squad-validation-7774", "mrqa_squad-validation-7792", "mrqa_squad-validation-7793", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-7976", "mrqa_squad-validation-7993", "mrqa_squad-validation-8002", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-82", "mrqa_squad-validation-8232", "mrqa_squad-validation-8256", "mrqa_squad-validation-828", "mrqa_squad-validation-8319", "mrqa_squad-validation-8320", "mrqa_squad-validation-8338", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-847", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8671", "mrqa_squad-validation-8679", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-8728", "mrqa_squad-validation-8732", "mrqa_squad-validation-8796", "mrqa_squad-validation-8811", "mrqa_squad-validation-8839", "mrqa_squad-validation-8862", "mrqa_squad-validation-8872", "mrqa_squad-validation-8920", "mrqa_squad-validation-893", "mrqa_squad-validation-8930", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-916", "mrqa_squad-validation-9178", "mrqa_squad-validation-9240", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9331", "mrqa_squad-validation-9351", "mrqa_squad-validation-9408", "mrqa_squad-validation-9413", "mrqa_squad-validation-9470", "mrqa_squad-validation-9520", "mrqa_squad-validation-9532", "mrqa_squad-validation-959", "mrqa_squad-validation-96", "mrqa_squad-validation-9608", "mrqa_squad-validation-9647", "mrqa_squad-validation-9777", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-9865", "mrqa_squad-validation-988", "mrqa_squad-validation-9984", "mrqa_squad-validation-999", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1140", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3602", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-5871", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-6513", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6903", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7616"], "OKR": 0.9375, "KG": 0.43046875, "before_eval_results": {"predictions": ["November 1979", "Pittsburgh", "William Rainey Harper", "the classical element fire", "the political weakness of the Mughal state", "photosynthesis", "10th and 11th centuries", "53,000", "animosity toward each other", "giving her brother Polynices a proper burial", "lion, leopard, buffalo, rhinoceros, and elephant", "1998 NFL draft", "Widener Library", "a fee per unit of connection time", "Nobel Prize", "Bart Starr", "UK", "Australia", "7.8%", "Mars", "2014", "27", "Louis King", "Boston", "Venancio Flores", "Coronation Street", "approximately $700 million", "Art Deco", "Lindsey Islands", "Cher", "Brig Gen Augustine Warner Robins", "the flags of dependent territories", "Russian film", "Thriller", "Restoration Hardware", "a, b, and Q/Q0 values that are used in these \"dating\" equations are either missing, poorly defined, improperly measured or inaccurate", "Iron Man 3", "Tomasz Adamek", "Waylon Smithers", "Ordos City Science Flying Universe Science and Technology Co.", "The Vanishing", "Wendell Berry", "Che Guevara", "Indians", "gastrocnemius muscle", "1,228 km / h ( 763 mph )", "cells", "a compiler can derive machine code", "typhoid fever", "William Boyd", "Octopussy", "a Mercedes-Benz", "Labor Day", "actress", "The supplemental spending bill", "genocide", "off the coast of Dubai", "murder", "Stalin", "The Industrial Workers of the World", "tin", "a handful of fishermen", "cereal-&-milk", "cuthbert Calculus"], "metric_results": {"EM": 0.6875, "QA-F1": 0.745116341991342}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9929", "mrqa_squad-validation-366", "mrqa_squad-validation-4750", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-201", "mrqa_naturalquestions-validation-9885", "mrqa_triviaqa-validation-4960", "mrqa_newsqa-validation-163", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-14014", "mrqa_triviaqa-validation-5948"], "SR": 0.6875, "CSR": 0.6278409090909092, "EFR": 1.0, "Overall": 0.7581463068181818}, {"timecode": 11, "before_eval_results": {"predictions": ["the difference in potential energy", "\"Canny\" (a versatile word meaning \"good\", \"nice\" or \"very\"", "accessory pigments that override the chlorophylls' green colors", "62", "$20,000", "the Autons with the Nestene Consciousness and Daleks", "1080i HD", "the demand for higher quality housing increased", "104 \u00b0F (40 \u00b0C)", "conspiracy against Islam by the Western governments", "road engines", "religious texts", "Larry Ellison", "consultant", "Derek Wolfe and Malik Jackson", "luxurious parks and royal gardens", "the Establishment Clause of the First Amendment", "Centrum", "20%", "win an acquittal and avoid imprisonment or a fine", "commemorating fealty and filial piety", "Melissa Disney", "Theodore Roosevelt", "Andy Cole", "13 February", "200 to 500 mg up to 7 mg", "November 17, 1800", "right name tape", "Monk's Caf\u00e9", "Director of National Intelligence", "Francisco Pizarro", "Czech word, robota", "Kyla Pratt", "1945", "Michael Crawford", "a young husband and wife", "2017 / 18 Divisional Round", "New Croton Reservoir", "550 quadrillion Imperial gallons", "Kepner", "to increase the quality of a herd, or to introduce an outcross of bloodlines", "Annette Strean", "Anwar Sadat", "tin tin", "tin tin", "Richard Wagner", "tin tin tin", "Islamic philosophy", "Minnesota", "The group consists of thirteen members who are separated into three sub-units, each with different areas of specialization", "Scottish", "stoneware", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "nearly $162 billion in war funding", "The most important thing is that our family is still together, everybody is safe, and eventually we will rebuild again", "Juri Kibuishi", "Old Trafford", "The Truman Show", "Jack McCall", "diamond", "ice caps", "the tin-Iranian speaking population", "\"including taking any and all appropriate personnel actions including termination, discipline and referral of any wrongdoing for criminal prosecution.\"", "a paragraph about the king and crown prince"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5790008602508603}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.5, 0.4, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.3076923076923077, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.06666666666666667, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5193", "mrqa_squad-validation-7729", "mrqa_squad-validation-7162", "mrqa_squad-validation-9632", "mrqa_squad-validation-1850", "mrqa_squad-validation-7087", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-100", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-5538", "mrqa_triviaqa-validation-5275", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-4167", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-1265", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-2716", "mrqa_newsqa-validation-2668"], "SR": 0.484375, "CSR": 0.6158854166666667, "EFR": 0.9696969696969697, "Overall": 0.7496946022727273}, {"timecode": 12, "before_eval_results": {"predictions": ["San Joaquin Light & Power Building", "2003", "British", "Doritos", "live", "23.9%", "Daewoo", "1777", "34\u201319", "John D. Rockefeller", "Pole Mokotowskie", "Newcastle College", "63%", "Boulton", "can produce both eggs and sperm at the same time", "the laws of physics", "Air", "international drug suppliers, rather than consumers", "Oregon", "tuberculosis", "The Young Men's Christian Association", "the centre of Bleak House", "Charles Springall", "The Iron Duke", "Neil Armstrong", "28", "The Golden Child", "Scarborough", "Il Trovatore", "tunisia, and the morose dentist Walter \"Painless\" Waldowski, the contemplative Chaplain Father Mulcahy.", "The Kentucky Derby", "Calvin Coolidge", "Hindi", "The Magnificent Ambersons", "Nowhere Boy", "The Stereophonics", "Lancashire", "the recorder", "Michael J. Fox", "Japanese", "Poland", "October 2, 2017", "By 1912", "Ravi Shastri", "Bohrium", "Anthony Hopkins", "Ella Jane Fitzgerald", "Bill Cosby", "Anna Clyne", "aluminum foil", "Brazil", "in Hong Kong's Victoria Harbor", "school", "Herman Thomas", "Sunday", "anesthetic and sedative", "conifers", "the funerals", "South Africa", "the dachshund", "the Lincoln one-cent coin", "The Letter", "the Lichen Copper- Silver- Gold", "the pig"], "metric_results": {"EM": 0.625, "QA-F1": 0.7012400793650794}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.8, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4645", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-1765", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-4591", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-444", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4757", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-16664", "mrqa_searchqa-validation-13542"], "SR": 0.625, "CSR": 0.6165865384615384, "retrieved_ids": ["mrqa_squad-train-31017", "mrqa_squad-train-13722", "mrqa_squad-train-18051", "mrqa_squad-train-5162", "mrqa_squad-train-19213", "mrqa_squad-train-73474", "mrqa_squad-train-6405", "mrqa_squad-train-13678", "mrqa_squad-train-4188", "mrqa_squad-train-5998", "mrqa_squad-train-37914", "mrqa_squad-train-81668", "mrqa_squad-train-20600", "mrqa_squad-train-48256", "mrqa_squad-train-29134", "mrqa_squad-train-72130", "mrqa_searchqa-validation-2892", "mrqa_squad-validation-2612", "mrqa_squad-validation-4206", "mrqa_searchqa-validation-9423", "mrqa_squad-validation-7647", "mrqa_naturalquestions-validation-3442", "mrqa_searchqa-validation-9419", "mrqa_squad-validation-3620", "mrqa_triviaqa-validation-5948", "mrqa_squad-validation-366", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-4039", "mrqa_squad-validation-4506", "mrqa_squad-validation-7087", "mrqa_squad-validation-6776", "mrqa_naturalquestions-validation-7464"], "EFR": 0.9583333333333334, "Overall": 0.7475620993589744}, {"timecode": 13, "before_eval_results": {"predictions": ["Elders", "WatchESPN", "permanent pulmonary fibrosis", "third and fourth", "February 1, 2016", "Antigone", "1972", "over fifty", "Oursel", "\u00a341,004", "1806-07", "Executive Vice President of Football Operations and General Manager", "British", "1920s", "poet", "the number of social services that people can access wherever they move", "john Dryden", "21", "Hester and William Fields", "cyanoguttatus", "Ecuador", "Brussels", "Hitler", "Egypt", "Barrow, Carlisle, Whitehaven and Workington", "photographer", "Paul Rudd", "Uganda", "dogs", "coffee, espresso drinks, blended coffees, teas, pastries, gelato", "Leicestershire", "Bill herschels", "andrew Mitchell", "gold", "Istanbul", "wale", "diana ejaz Khan", "Los Angeles", "L. Pasteur", "sally", "White Christmas", "the nucleus", "12 November 2010", "November 1975", "Afonso IV", "southern Turkey", "\"Barney Miller\"", "Yellow fever", "Nye County", "\"Traumnovelle\" (\"Dream Story\")", "the \"Pour le M\u00e9rite\"", "San Francisco, California", "rebecca Barnett", "1-0", "Terra Firma", "June 6, 1944", "19", "backbreaking labor", "Curly Lambeau", "a New English Dictionary on Historical", "Jimmy Carter", "Jimmy Lee", "john dillard", "cheese"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6208019666199813}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.25, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.5, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7762", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-5352", "mrqa_triviaqa-validation-5849", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-1966", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-1349", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-804", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-9672", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-2852", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-4067", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-3413"], "SR": 0.53125, "CSR": 0.6104910714285714, "EFR": 1.0, "Overall": 0.7546763392857143}, {"timecode": 14, "before_eval_results": {"predictions": ["Xbox One", "tyrosinase", "low skilled jobs becoming more tradeable", "2005", "Art Deco style in painting and art", "The Prospect Studios", "Advanced Steam movement", "three", "the Moscone Center in San Francisco", "since 2001", "identified change orders or project changes that increased costs", "comedies and family-oriented series", "case law by the Court of Justice, international law and general principles of European Union law", "oxide compounds such as silicon dioxide", "278", "bacterial fusion", "comets", "Melvil Dewey", "saxophone", "alamy", "7", "China", "the foot", "classic building toy", "March 19", "order Diptera", "krakatoa", "kelp", "fruit", "a multiple telegraph", "Eirik albertsson", "tungsten", "lebdingnagian", "Black September", "the Dartford Warblers", "jodie Foster", "geodetics", "comets", "sea salamander", "Niveditha, Diwakar, Shruti", "Tagalog or English", "2013", "1861", "President James Madison", "Lou Rawls", "Southern Illinois University Carbondale", "Marco Hietala", "New Orleans, Louisiana", "Worcester", "North Carolina", "1944", "March 22", "August 19, 2007", "comets", "Ike", "Alice Horton", "J. Crew", "john Harvard", "Montserrat", "lymphoma and blood disease", "comets", "five thieves enter the Riviera Casino in Las Vegas with guitar cases full of guns, and stage a daring robbery.", "comets", "Valeri Vladimirovich \"Val\" Bure"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5355654761904762}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 0.28571428571428575, 1.0, 0.5, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7534", "mrqa_squad-validation-512", "mrqa_squad-validation-3670", "mrqa_squad-validation-1546", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-992", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-6032", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-2189", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1418", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-4545", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-3783", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-12049"], "SR": 0.4375, "CSR": 0.5989583333333333, "EFR": 0.9722222222222222, "Overall": 0.7468142361111111}, {"timecode": 15, "before_eval_results": {"predictions": ["the Treaties establishing the European Union", "Isaac Newton", "draftsman", "mesoglea", "Associate Membership", "his own men", "Broncos", "through sponsors", "61.1%", "Baden-W\u00fcrttemberg", "Most Western countries, and some others,", "lion, leopard, buffalo, rhinoceros, and elephant", "25-minute", "Shakespeare Quotes", "Terrence Malick", "Ceefax", "Ireland", "William Wakefield", "konya", "Homo sapiens", "prophet Samuel", "monastic", "otters", "Joseph w.", "mountain", "roger jays", "three reservoirs", "London euston", "the Spice Girls", "record of the debits and credits relating to the person, business, etc.", "feet", "South Dakota", "17", "Moldova", "AFC Wimbledon", "Old Sparky", "Northern Ireland", "roger roger", "Rocketship X-M", "the country", "Thomas Jefferson", "David to avenge his daughter", "Ra\u00fal Eduardo Esparza", "The Lykan Hypersport", "cellular activities", "commercial", "44", "model, actress and television host", "Fred Willard", "Jeff Meldrum", "Canadian", "her landlord", "Afghanistan", "the Kurdish Workers' Party", "Saturday", "Jaime Andrade", "Bill Haas", "tritonic", "the infield", "the Jerry Lewis MDA Labor Day Telethon", "the Maryland area", "Pennsylvania", "fuel-producing", "Anil Kapoor."], "metric_results": {"EM": 0.453125, "QA-F1": 0.5221354166666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6128", "mrqa_squad-validation-2086", "mrqa_squad-validation-8278", "mrqa_triviaqa-validation-3742", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-239", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2434", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-533", "mrqa_triviaqa-validation-3956", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-487", "mrqa_triviaqa-validation-1998", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-1116", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1507", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-1446", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-3979"], "SR": 0.453125, "CSR": 0.58984375, "retrieved_ids": ["mrqa_squad-train-64892", "mrqa_squad-train-85470", "mrqa_squad-train-63176", "mrqa_squad-train-798", "mrqa_squad-train-32687", "mrqa_squad-train-36127", "mrqa_squad-train-52814", "mrqa_squad-train-67974", "mrqa_squad-train-42270", "mrqa_squad-train-27657", "mrqa_squad-train-76625", "mrqa_squad-train-10365", "mrqa_squad-train-47425", "mrqa_squad-train-21243", "mrqa_squad-train-1734", "mrqa_squad-train-20246", "mrqa_squad-validation-8256", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-5801", "mrqa_hotpotqa-validation-562", "mrqa_newsqa-validation-3889", "mrqa_squad-validation-438", "mrqa_triviaqa-validation-4028", "mrqa_squad-validation-486", "mrqa_naturalquestions-validation-3", "mrqa_searchqa-validation-5440", "mrqa_squad-validation-9865", "mrqa_hotpotqa-validation-4757", "mrqa_triviaqa-validation-5849", "mrqa_triviaqa-validation-7683", "mrqa_naturalquestions-validation-7754", "mrqa_squad-validation-9520"], "EFR": 0.9714285714285714, "Overall": 0.7448325892857144}, {"timecode": 16, "before_eval_results": {"predictions": ["12 December 1964", "seawater", "a protest that blocked heavy traffic passing over the A13, Brenner Autobahn, en route to Italy", "5 million", "Van de Graaff generator", "bounding", "between 1835 and 1842", "Vivienne Westwood", "absolution", "France", "megaprojects", "income inequality", "1960s to the mid-1970s", "W. Edwards Deming", "the Maryland Senate", "1947, 1956, 1975, 2015 and 2017", "Paul Lynde", "Kevin Kline", "a computer maintenance utility", "Saint Alphonsa", "the President of the United States", "Ed Roland", "Kristy Swanson", "a compact layout to combine keys which are usually kept separate", "Missouri River", "in desperation, with only a small chance of success and time running out on the clock", "Lex Luger and Rick Rude", "Blue laws", "Earth", "1923", "the medieval walls and towers of the Cit\u00e9 de Carcassonne in the town of CarcASSonne in Aude, France", "The U.S. state of Georgia is known as the `` Peach State ''", "Anna Faris", "Norris and Ross McWhirter", "23 September 1889", "October 22, 2017", "the modern Olympics began in 1896", "1997", "two All-Beef Patties", "Missouri", "Australia and Ireland", "Kaiser", "Llandudno", "albinism", "round five of the 2017 season", "John Schlesinger", "Clitheroe Football Club", "1998", "Neighbours", "$7.3 billion", "France's famous Louvre museum", "Kenneth Cole", "many people sought out Tutsis and moderate Hutus", "10", "Citizens", "her decades-long portrayal of Alice Horton", "khamsin", "saprophytes", "The Treasure of the Sierra Madre", "the Arabian Peninsula", "barry manilow", "copper", "The Ansonia Hotel", "Chiltern Hills"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5818789199062032}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.058823529411764705, 1.0, 0.060606060606060615, 0.0, 1.0, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8369", "mrqa_squad-validation-4332", "mrqa_squad-validation-2297", "mrqa_squad-validation-7577", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-6912", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-3943", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2757", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-15859", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-14243"], "SR": 0.53125, "CSR": 0.5863970588235294, "EFR": 0.9333333333333333, "Overall": 0.7365242034313726}, {"timecode": 17, "before_eval_results": {"predictions": ["Henry Plitt", "charging their students tuition", "lysozyme and phospholipase A2", "tensions over slavery and the power of bishops in the denomination", "The Jetsons", "Thomas Edison and Nikola Tesla", "the established Church", "any terrestrial distance", "greenhouse gas", "\"scariest TV show of all time\"", "The Scottish Parliament", "beer", "Arabic", "lunar module", "lucky", "hitler", "the Eiffel Tower", "hitler", "rf", "cosmetics", "yellow fever", "geometry", "Alfie", "a second", "Mexico", "the Lewis and Clark Bicentennial", "Maria Full of Grace", "the Pro-Jig Clamp Set", "Arby's", "hitler", "the pupil", "albright", "Union Pacific", "the Lord of the Rings", "tarantulas", "Marvell", "Mars", "16", "Sean O' Neal", "Muhammad", "Baker, California", "April 1979", "the International Border", "a Horse With No Name", "Missouri", "avocado", "British European Airways", "Tahrir Square", "Hercules", "14 December 1990", "Gianna", "first freshman to finish as the runner-up", "a series of bilateral treaties", "gemeinn\u00fctzige", "1868", "The Da Vinci Code", "Paul McCartney", "buckling under pressure from the ruling party.", "Chesley \"Sully\" Sullenberger", "\"Abbey Road.\"", "tennis", "In Time", "Milira", "Pangaea"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6219494047619047}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-15676", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-7518", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-2953", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-9764", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-3951", "mrqa_searchqa-validation-9998", "mrqa_searchqa-validation-16905", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-4902", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-5401", "mrqa_newsqa-validation-2129"], "SR": 0.578125, "CSR": 0.5859375, "EFR": 1.0, "Overall": 0.749765625}, {"timecode": 18, "before_eval_results": {"predictions": ["1960", "Saudi", "Smiljan", "Muqali", "2016", "by technique", "2008", "41", "Middleton Railway", "increasing access to education", "Penance", "Intermezzo", "Political battleground states", "Bangladesh", "Warren Gamaliel Harding", "Joy Division", "a Lesson from Aloes", "a percussion instrument", "the Constitution", "a Tibetan antelope", "glaciers", "Wells Fargo", "canticle", "sanguine", "Maine", "a Fokker", "The Pilgrim's Progress", "a Holy Grail", "Macbeth", "Chocolate Factory", "Mountain Dew", "Engelbert Humperdinck", "Mike McCready", "a paddle sport", "Smokey Robinson", "Lhasa", "a bat ray", "statute or the Constitution itself", "known as Tadheus", "2028", "Patrick Walshe", "Massachusetts", "Wisconsin", "Marc Brunel", "Czech Republic", "Donegal", "Sir Edwin Landseer", "Albert Einstein", "hot springs", "Academy Award for Best Animated Feature", "Aircraft", "Minnesota", "Lisburn Distillery Football Club", "(Wilton) Mall", "2002\u201303", "9 a.m.", "a civil disturbance call,", "2008", "known for his role as Ralph Cifaretto on the HBO series \"The Sopranos,\"", "Lance Cpl. Maria Lauterbach and her fetus", "near Warsaw, Kentucky", "Percy Sledge", "Dublin", "Herald of Free Enterprise"], "metric_results": {"EM": 0.625, "QA-F1": 0.6838541666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.4, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.16666666666666669, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1227", "mrqa_searchqa-validation-15758", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-7430", "mrqa_searchqa-validation-1831", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9637", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-3955", "mrqa_hotpotqa-validation-218", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-541", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2573"], "SR": 0.625, "CSR": 0.5879934210526316, "retrieved_ids": ["mrqa_squad-train-49082", "mrqa_squad-train-50761", "mrqa_squad-train-83465", "mrqa_squad-train-21377", "mrqa_squad-train-64947", "mrqa_squad-train-18087", "mrqa_squad-train-21564", "mrqa_squad-train-29868", "mrqa_squad-train-36229", "mrqa_squad-train-13033", "mrqa_squad-train-86040", "mrqa_squad-train-25569", "mrqa_squad-train-68564", "mrqa_squad-train-84354", "mrqa_squad-train-54288", "mrqa_squad-train-31006", "mrqa_naturalquestions-validation-100", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-2179", "mrqa_hotpotqa-validation-4757", "mrqa_triviaqa-validation-3742", "mrqa_hotpotqa-validation-5760", "mrqa_naturalquestions-validation-5876", "mrqa_triviaqa-validation-2050", "mrqa_searchqa-validation-5440", "mrqa_squad-validation-276", "mrqa_squad-validation-2236", "mrqa_hotpotqa-validation-2418", "mrqa_triviaqa-validation-1924", "mrqa_hotpotqa-validation-204", "mrqa_naturalquestions-validation-4785", "mrqa_squad-validation-9798"], "EFR": 1.0, "Overall": 0.7501768092105264}, {"timecode": 19, "before_eval_results": {"predictions": ["Ice Ages", "Oligocene", "Gosforth Park", "Fridays", "refuse to sign bail until certain demands are met, such as favorable bail conditions, or the release of all the activists", "encourage investment", "*R\u012bnaz", "Coldplay", "\"zip\" the mouth shut when the animal is not feeding, by forming intercellular connections with the opposite adhesive strip", "J\u00f3zsef Pulitzer", "\"Spitting Image\"", "left fielder", "Ellesmere Port, United Kingdom", "Punjabi/Pashtun", "KXII", "Who's That Girl", "Manor of More", "Europe", "Adelaide", "Woodsy owl", "\"The Snowman\"", "1994\u201395", "Satchmo, Satch or Pops", "Philip Livingston", "hamburgers", "C. J. Cherryh", "Holston River", "1939", "Tabasco", "8/7c", "Iceal \"Gene\" Hambleton", "Cleveland Cleveland", "\"The Deer Hunter\"", "striker", "a vegetarian dish called Buddha's delight", "The Washington Post", "\"Secrets and Lies\"", "Tom Brady", "the endocrine ( hormonal ) systems", "one - point perspective, and their vanishing point corresponds to the oculus, or `` eye point '', from which the image should be viewed for correct perspective geometry", "Dorothy Gale", "master carpenter Anthony Mayfield on behalf of an investor couple in Austin, Texas", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "Cape Town", "the French Revolution", "the Arabic word \"al-jebr\"", "Rajasthan", "a little bitter", "Diogenes", "Disney", "Dr. Jennifer Arnold and husband Bill Klein", "American", "Gary Player", "the Defense of Marriage Act", "closed on 366 for eight wickets on the opening day", "the United States", "Ovid", "Martin Van Buren", "(Tyrtaeus)", "The Pentagon", "a Beanie Baby", "Wigan Athletic", "Iowa", "social networking sites"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5875088839762823}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.6206896551724138, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.19354838709677416, 0.16, 0.0, 0.5882352941176471, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5456", "mrqa_squad-validation-6931", "mrqa_squad-validation-4497", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-5075", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2898", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-15311", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1182"], "SR": 0.46875, "CSR": 0.58203125, "EFR": 1.0, "Overall": 0.748984375}, {"timecode": 20, "UKR": 0.783203125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2555", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3302", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9175", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-560", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-13604", "mrqa_searchqa-validation-13784", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-15676", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-16905", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2639", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3131", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-3894", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-9998", "mrqa_squad-validation-10004", "mrqa_squad-validation-10013", "mrqa_squad-validation-10024", "mrqa_squad-validation-1006", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10199", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1462", "mrqa_squad-validation-1496", "mrqa_squad-validation-1512", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1546", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1637", "mrqa_squad-validation-1684", "mrqa_squad-validation-1762", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-199", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2705", "mrqa_squad-validation-2723", "mrqa_squad-validation-276", "mrqa_squad-validation-2834", "mrqa_squad-validation-2869", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-302", "mrqa_squad-validation-3049", "mrqa_squad-validation-3063", "mrqa_squad-validation-3092", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3398", "mrqa_squad-validation-3416", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3540", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3610", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-3871", "mrqa_squad-validation-3925", "mrqa_squad-validation-3950", "mrqa_squad-validation-3986", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4127", "mrqa_squad-validation-4179", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-4246", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4497", "mrqa_squad-validation-4506", "mrqa_squad-validation-4533", "mrqa_squad-validation-4649", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-4707", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5193", "mrqa_squad-validation-5230", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5562", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5866", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-641", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6891", "mrqa_squad-validation-6942", "mrqa_squad-validation-6996", "mrqa_squad-validation-7096", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-715", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7380", "mrqa_squad-validation-7534", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7701", "mrqa_squad-validation-7708", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7792", "mrqa_squad-validation-7850", "mrqa_squad-validation-7956", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-8231", "mrqa_squad-validation-8287", "mrqa_squad-validation-8362", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8732", "mrqa_squad-validation-878", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9331", "mrqa_squad-validation-9578", "mrqa_squad-validation-96", "mrqa_squad-validation-9606", "mrqa_squad-validation-9608", "mrqa_squad-validation-9632", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-980", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-988", "mrqa_squad-validation-9966", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1201", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1719", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2377", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2731", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4469", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4586", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-4801", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5525", "mrqa_triviaqa-validation-5538", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6661", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7132", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-804"], "OKR": 0.90625, "KG": 0.4515625, "before_eval_results": {"predictions": ["America's Funniest Home Videos", "the Romantic Rhine", "philanthropy", "1986", "Republic of Kenya", "PNU and ODM camps", "pseudorandom number generators", "\u20ac5,000", "a baffle plate", "force", "Spain", "Havana", "Harriet Beecher Stowe", "The MIT Blackjack Team", "The Silence of the Lambs", "tragedy of norman", "The Bladder", "Pulsed Laser", "Admiral Richard E. Byrd", "Resident Evil", "Lake Mead", "James Earl Ray", "President Lyndon B. Johnson", "The Complete Poems", "The bass trumpet", "The Boer War", "Anne Frances Reagan", "Dame Agatha Christie", "Cleveland", "Oscar Wilde", "Word of the Day", "calamitat", "Lake Alakol", "Today's Flashback", "kerosene", "a squash blossom necklace", "The World War I `` Unknown '' is a recipient of the Medal of Honor, the Victoria Cross, and several other foreign nations'highest service awards", "Carol Ann Susi", "Peru", "Santa Fe", "Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "Dirk Benedict", "Akshay Kumar", "algal-growth processes", "Japan", "James", "Nigeria", "Francis Scott Key", "The Truman Show", "Sherlock Holmes", "Leslie Knope", "1978", "University of Missouri", "Nicolas Winding Refn", "Osaka International Airport", "$26 billion", "2013\u201314 Premier League", "Pixar's", "Inter Milan", "the fact that the teens were charged as adults.", "If  your ex's loved ones ask why you broke up", "Samir Kuntar", "two years ago", "April 21, 1947"], "metric_results": {"EM": 0.5, "QA-F1": 0.5981026785714285}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4, 0.8, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.06666666666666667, 1.0, 0.0, 0.2857142857142857, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.7499999999999999, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9161", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-10745", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-4655", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-1958", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-4475", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-11905", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5780", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-7625", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-6060", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1034"], "SR": 0.5, "CSR": 0.578125, "EFR": 1.0, "Overall": 0.7438281250000001}, {"timecode": 21, "before_eval_results": {"predictions": ["punts", "arrest", "Confucian propriety and ancestor veneration", "5,000 years", "Barnett Center", "immunomodulators", "the RSA algorithm", "the Black Ahab", "Francis Scott Key", "the 1906 San Francisco earthquake", "piccolo", "Alice's", "Sri Lanka", "a Rolling Stone", "Maria Schneider", "Fred Foy", "tears", "the Old Curiosity Shop", "pearl", "Australia", "slide slide", "Europa", "Pope John XXIII", "the sun", "the Mercury program", "M&M Mars", "slide", "John Edwards", "Ismail Haniyeh", "soldiers", "Congo", "Van Reek & Spassky", "Bombay", "Rome", "lymphatic", "Bed and breakfast", "1038", "19 June 2018", "Mahatma Gandhi", "Virginia Dare", "Lionel Hardcastle", "iOS, watchOS, and tvOS", "The Shard", "The Blue Boy", "Allende", "Sue Ryder", "Adrian Edmondson", "sense of taste", "Tony Curtis", "the lead roles of Timmy Sanders and Jack", "Centre of Excellence", "The Prodigy", "Melesha O'Garro", "New Journalism", "Boyd Gaming", "Douglas Jackson", "Sunday.", "Six", "April 13", "Juan Martin Del Potro", "Opryland", "Anil Kapoor", "soldiers have been shot over the last day in Somalia,", "1960"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6500248015873016}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, false, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-5408", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-14219", "mrqa_searchqa-validation-12504", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-14559", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-14337", "mrqa_searchqa-validation-14613", "mrqa_searchqa-validation-743", "mrqa_searchqa-validation-16149", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-16301", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7058", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-639", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-3501"], "SR": 0.5625, "CSR": 0.5774147727272727, "retrieved_ids": ["mrqa_squad-train-61498", "mrqa_squad-train-70488", "mrqa_squad-train-57199", "mrqa_squad-train-84293", "mrqa_squad-train-46774", "mrqa_squad-train-23991", "mrqa_squad-train-72192", "mrqa_squad-train-42495", "mrqa_squad-train-29384", "mrqa_squad-train-4152", "mrqa_squad-train-77349", "mrqa_squad-train-32149", "mrqa_squad-train-84456", "mrqa_squad-train-28295", "mrqa_squad-train-32808", "mrqa_squad-train-41163", "mrqa_triviaqa-validation-92", "mrqa_hotpotqa-validation-1617", "mrqa_searchqa-validation-2716", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-3344", "mrqa_squad-validation-276", "mrqa_triviaqa-validation-6848", "mrqa_squad-validation-2297", "mrqa_naturalquestions-validation-10653", "mrqa_searchqa-validation-10963", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-3197", "mrqa_triviaqa-validation-2452", "mrqa_hotpotqa-validation-2418", "mrqa_squad-validation-10068", "mrqa_squad-validation-9372"], "EFR": 1.0, "Overall": 0.7436860795454545}, {"timecode": 22, "before_eval_results": {"predictions": ["Mike Carey", "Rankine cycle", "2014", "iger team", "11:28", "1321 to 1323", "AFC", "Tony Award for Best Lighting Design for \"Evita\", \"Cats\", and \"Les Mis\u00e9rables\",", "Rafael Palmeiro", "10", "Central Park", "Enkare Nairobi", "YouTube celebrity PewDie Pie", "Thrushcross Grange", "\"Linda McCartney's Life in Photography\", \"Some Like It Hot\", \"Kubrick's Napoleon: The Greatest Movie Never Made\", \"Saturday Night Live: The Book\",", "10 June 1921", "A Bug's Life", "Michelle Anne Sinclair", "Angel Parrish", "\"OSRIC\"", "Bundesliga", "\"Apprendi v. New Jersey\"", "cleaning services, support services, property services, catering services, security services and facility management services", "4,972", "Captain", "Mount Everest", "119", "British", "Red Dead Redemption", "Los Angeles Xtreme", "neo-v\u00f6lkisch movement", "Sugar Ray Robinson", "the rough patches of skin on its head which appear white due to parasitism by whale lice", "Jeffrey Adam \"Duff\" Goldman", "A Rush of Blood to the Head", "a political ideology", "Mendel", "John Adams", "Toto", "Pandit Jawaharlal Nehru", "2009", "James Arthur", "14", "Vanity Fair", "Route sixty-six", "The Welcome Stranger", "Pour Moi", "aardvark", "Jeremy Bates", "the officers,", "Sixteen", "Mexico", "Dr. Cade", "keeping our children from just this type of public exposure.\"", "pelvis and sacrum -- the triangular bone within the pelvis", "Brown and her family", "Rocco Ritchie", "the assassination of Martin Luther", "Armistice Day", "double-headed eagle", "Roger Federer", "Hannibal Lecter", "Venezuela", "Marie Fredriksson"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6701546717171717}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.26666666666666666, 0.8, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 0.4166666666666667, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3926", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-1386", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-1332", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-7692", "mrqa_naturalquestions-validation-4033", "mrqa_triviaqa-validation-3951", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-9888", "mrqa_searchqa-validation-7840", "mrqa_searchqa-validation-6337", "mrqa_naturalquestions-validation-9195"], "SR": 0.5625, "CSR": 0.5767663043478262, "EFR": 1.0, "Overall": 0.7435563858695653}, {"timecode": 23, "before_eval_results": {"predictions": ["the trans-Atlantic wireless telecommunications facility known as Wardenclyffe", "\"a day of rote learning and often wearying spiritual exercises.\"", "Pitt", "the meeting of the Church's General Assembly", "vocational subjects", "ESPN Deportes", "the Greek Goddess of Revenge", "smell", "Honshu", "\u201cbitter almond\u201d", "the Sator Square", "John Ritchie", "an English children's writer whose books have been among the world's best-sellers since the 1930s", "a barred, spiral galaxy", "the javelin throw", "milk", "Derbyshire, England", "James Cameron", "Emma Chambers", "eenezer Scrooge", "Spain", "writing", "Sherlock Holmes: The Man Who Never Lived And Will Never Die", "North by Northwest", "oxygen", "Alberto Juantorena", "William Randolph Hearst", "Henry III", "The Yardbirds", "Canada", "The Live Read of Space Jam", "Gerry Adams", "ambergris", "The Time Machine", "The Lion King", "5 September 1666", "pneumonoultramicroscopicsilicovolcanoconiosis", "William Strauss and Neil Howe", "florida", "Thorleif Haug", "Vermont boarding school Welton Academy", "1978", "July", "Ghana Technology University College", "Charles Quinton Murphy", "12", "Pakistan", "Washington, D.C.", "Michael Jordan", "Arnoldo Rueda Medina", "\" waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.\"", "the story of the Cowardly Lion", "a bank", "Eintracht Frankfurt", "al Gamaa al-Islamiyya,", "\"pattern matching.\"", "\"Bix\" Beiderbecke", "a black hole", "Canada", "Olivia Newton-John", "a wish", "Ren Lacoste", "a grill", "Alien Intelligences"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5339425511031114}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, true, false], "QA-F1": [0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.0, 0.7692307692307693, 0.6666666666666666, 0.0, 1.0, 0.4, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.06896551724137931, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1384", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-7071", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3759", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-2509", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10442", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-914", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-303", "mrqa_hotpotqa-validation-1516"], "SR": 0.4375, "CSR": 0.5709635416666667, "EFR": 0.9444444444444444, "Overall": 0.7312847222222223}, {"timecode": 24, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "40,000", "Geordie", "left foot", "Tesla items in unfriendly hands", "virgil", "Queen Victoria", "Tokyo", "February", "Loki", "Samuel Johnson", "6 Litres", "West Point", "33", "Caernarfon", "curling", "Avro Lancaster", "bare-knuckle prizefights", "1987", "Joe Willie Kirk", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands,", "\"Ain't No Mountain High Enough\"", "ambulance driver", "a pair of lookalikes", "New Hampshire", "New Zealand", "U2", "right", "Christian Wulff", "soup made by The Soup Dragon and blue string pudding", "a reddish-purple berry", "The Beatles\u2019 Liverpool", "Jesse Garon Presley", "(Dan) Bedingfield", "epernay", "senators, each of whom represents a single state in its entirety, with each state being equally represented by two senators, regardless of its population, serving staggered terms of six years", "Buddhism appealed to Chinese intellectuals and elites and the development of gentry Buddhism was sought as an alternative to Confucianism and Daoism", "the Mongol Yuan Dynasty", "65,507 bytes", "A turlough, or turlach", "Health is usually measured in hit points or health points, shortened to HP", "George Warren Barnes", "The Grandmaster", "Nanna Popham Britton", "\"The Nick Cannon Show\"", "Marvel Comics", "Taylor Swift", "Sierra Leone", "September 8, 2017", "CNN's Max Foster", "seven", "the Russian air force", "38 people", "3,000 kilometers (1,900 miles)", "Paul McCartney and Ringo Starr", "lightning strikes", "Clifford Odets", "degaussing", "Fargo", "it wasn't meaty enough", "tried & treed", "Washington Irving", "a grizzly bear", "Carrie Underwood"], "metric_results": {"EM": 0.5, "QA-F1": 0.5948683261183261}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 0.5, 0.0, 0.5, 0.0, 0.0, 0.05714285714285715, 0.09523809523809523, 0.0, 0.5, 0.5, 0.5555555555555556, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1598", "mrqa_triviaqa-validation-5317", "mrqa_triviaqa-validation-5958", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-183", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-2205", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-4092", "mrqa_newsqa-validation-2763", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-9185"], "SR": 0.5, "CSR": 0.568125, "retrieved_ids": ["mrqa_squad-train-56512", "mrqa_squad-train-9698", "mrqa_squad-train-24102", "mrqa_squad-train-81097", "mrqa_squad-train-31958", "mrqa_squad-train-68119", "mrqa_squad-train-36499", "mrqa_squad-train-70858", "mrqa_squad-train-56308", "mrqa_squad-train-49577", "mrqa_squad-train-37182", "mrqa_squad-train-71578", "mrqa_squad-train-4902", "mrqa_squad-train-56303", "mrqa_squad-train-63896", "mrqa_squad-train-77618", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-590", "mrqa_squad-validation-7643", "mrqa_searchqa-validation-10875", "mrqa_squad-validation-1850", "mrqa_searchqa-validation-2716", "mrqa_searchqa-validation-2892", "mrqa_naturalquestions-validation-34", "mrqa_searchqa-validation-12504", "mrqa_naturalquestions-validation-2837", "mrqa_triviaqa-validation-5075", "mrqa_naturalquestions-validation-9672", "mrqa_newsqa-validation-940", "mrqa_naturalquestions-validation-5483", "mrqa_triviaqa-validation-7625"], "EFR": 1.0, "Overall": 0.7418281250000001}, {"timecode": 25, "before_eval_results": {"predictions": ["Gaelic", "destroy the antichrist", "The Mask of Anarchy", "Japanese", "it is neither zero nor a unit", "carmen Miranda", "a 7", "Italy", "rounders", "Albert Finney", "The Daily Mirror", "Fort Sumter", "aeschylus", "Giuseppe Verdi", "Jordan", "the Sutherland Brothers", "Downton Abbey", "Groucho Marx", "raclette", "a lie detector", "daedalus", "Una Stubbs", "muscle", "a sea otter", "Fringillidae", "paul b\u00e4umer", "May", "al bundy", "paul c\u00e9zanne", "puffer", "A-ha", "a head-on collision with a 1950 Ford Tutor", "alsatian", "All Star", "jesse", "Noel Kahn", "Mitch Murray", "Rajendra Prasad", "an upright triangle", "Alamodome and city of San Antonio", "Chris Rea", "retinal ganglion cell axons and glial cells", "Restoration Hardware", "the final of 2011 AFC Asian Cup", "James Harrison", "Canada's first train robbery", "the northwest tip of Canisteo Peninsula in Amundsen Sea", "Bardot", "E22", "Booches Billiard Hall", "1998", "AbdulMutallab", "Lebanese", "the two remaining crew members from the helicopter", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "Mashhad, Iran.", "China", "Wallachia", "Flamin' Hot", "vodka", "Tim Duncan", "a hot dish", "Ukraine", "angels"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5803323412698412}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2564", "mrqa_squad-validation-6864", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-4411", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-3316", "mrqa_hotpotqa-validation-3008", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-1771", "mrqa_searchqa-validation-16661", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-4640", "mrqa_searchqa-validation-9293"], "SR": 0.546875, "CSR": 0.5673076923076923, "EFR": 1.0, "Overall": 0.7416646634615385}, {"timecode": 26, "before_eval_results": {"predictions": ["defensins", "that the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks at the bottom of the sea", "5 to 15 years", "the pre-game and halftime coverage", "sternum", "Finland", "carbon", "richmond", "haockney", "jet streams", "Rajasthan", "richmond", "Exile", "Argentina", "yachts", "kia", "power station", "the troposphere", "the Battle of Goose Green", "Ellen DeGeneres", "charlie drake", "Spain", "richmond marriot", "richmond", "tintoretto", "Venus", "120 beats per minute", "Hans Lippershey", "Gryffindor", "richmond", "Wilkie Collins", "yunte Huang", "Canada", "Baton Rouge", "Alison", "Helena", "The Inn at Newport Ranch", "Herbert Hoover", "neha dangal", "Rumplestiltskin", "Max Martin", "Sean Yseult", "Mount Everest", "18.7 miles", "actor and filmmaker", "audio", "Tony Stewart", "Stage Stores", "at least $20 million to $30 million", "carbon emissions", "40 militants and six Pakistan soldiers", "Gen. Stanley McChrystal", "Natalie Cole", "$627", "WFTV", "a mot", "The Man Without A Country", "Deimos", "Benazir Bhutto", "Dracula", "Ipanema", "a sitar", "the band's logo in gold lettering over black sleeve", "John von Neumann"], "metric_results": {"EM": 0.53125, "QA-F1": 0.612545803205003}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 0.4651162790697675, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.38095238095238093, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.22222222222222224, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.823529411764706, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-3443", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-3022", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7148", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-5619", "mrqa_triviaqa-validation-2360", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-6043", "mrqa_triviaqa-validation-4663", "mrqa_naturalquestions-validation-6936", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-2839", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5513", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-2806", "mrqa_searchqa-validation-2896", "mrqa_searchqa-validation-3888", "mrqa_naturalquestions-validation-3556"], "SR": 0.53125, "CSR": 0.5659722222222222, "EFR": 0.9666666666666667, "Overall": 0.7347309027777779}, {"timecode": 27, "before_eval_results": {"predictions": ["three hours", "Cosgrove Hall", "American Civil Rights Movement", "inquiries and scrutinise legislation", "Armin Meiwes", "Katherine Harris", "\"Vera Cruz\"", "NPR", "Nikhil Banerjee", "Croatian", "the Food and Agriculture Organization", "Kashmiri,", "2010 to 2012", "Dorothy", "the lack of any perceptible change in an adult female (for instance, a change in appearance or scent) when she is \"in heat\" and near ovulation", "the United States Navy's only boot camp, located near North Chicago, in Lake County, Illinois", "Preston, Lancashire, UK", "Mark Dayton", "Black Panther Party", "Chiba, Japan.", "S\u00f8nderjyskE", "The Sun", "the University of Missouri-Kansas City in Kansas City, Missouri", "October 3, 2017", "Walt Disney and Ub Iwerks", "River Clyde", "Chicago", "Moonstruck", "the 1933 Revolt of the Sergeants that overthrew the provisional government of Carlos Manuel de C\u00e9spedes y Quesada", "Seventeen", "Kim Sung-su", "ten years of probation and subsequently ordered him to therapy at a long-term in-patient facility,", "Outside", "Paul Teutul Jr.", "bypasses, to cross major bridges, and to provide direct intercity connections", "Governor Al Smith", "Charles Carson", "Chris Sarandon", "Beijing", "the fourth season", "Schadenfreude", "Poland", "John Mortimer", "norland", "Japanese silvergrass", "Daily Express", "usa", "red", "the privileged ethnicity,", "Saturday", "\"The Book\"", "Iowa's critical presidential caucuses on January 3.", "partying ways and the infamous bong photo.", "12.3 million", "the United States government can develop and deploy more effective strategies", "Doom", "Benjamin Franklin", "Theodore Levitt,", "Forrest", "edmond ii", "WD-40", "an apple", "falconry", "lemon"], "metric_results": {"EM": 0.4375, "QA-F1": 0.517640142580158}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.08, 0.35294117647058826, 0.5, 1.0, 0.4, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7272727272727273, 0.2857142857142857, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5285", "mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-1214", "mrqa_hotpotqa-validation-700", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-5227", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-8404", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-225", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-4019", "mrqa_newsqa-validation-1891", "mrqa_newsqa-validation-2723", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6504", "mrqa_searchqa-validation-13505"], "SR": 0.4375, "CSR": 0.5613839285714286, "retrieved_ids": ["mrqa_squad-train-67507", "mrqa_squad-train-43941", "mrqa_squad-train-28343", "mrqa_squad-train-61885", "mrqa_squad-train-69375", "mrqa_squad-train-12828", "mrqa_squad-train-51596", "mrqa_squad-train-3881", "mrqa_squad-train-26653", "mrqa_squad-train-20476", "mrqa_squad-train-31850", "mrqa_squad-train-4286", "mrqa_squad-train-70472", "mrqa_squad-train-42987", "mrqa_squad-train-18216", "mrqa_squad-train-78276", "mrqa_searchqa-validation-6517", "mrqa_naturalquestions-validation-5510", "mrqa_triviaqa-validation-239", "mrqa_searchqa-validation-13367", "mrqa_naturalquestions-validation-1169", "mrqa_squad-validation-9865", "mrqa_triviaqa-validation-273", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-4289", "mrqa_searchqa-validation-16661", "mrqa_squad-validation-486", "mrqa_hotpotqa-validation-3964", "mrqa_naturalquestions-validation-9953", "mrqa_triviaqa-validation-948", "mrqa_squad-validation-3556", "mrqa_squad-validation-8811"], "EFR": 0.9722222222222222, "Overall": 0.7349243551587301}, {"timecode": 28, "before_eval_results": {"predictions": ["Lucas Horenbout", "Barnett Center", "UNESCO's World Heritage list", "punk rock", "Texas", "Argentine cuisine", "Valley Falls", "1942", "Estelle Sylvia Pankhurst", "Rhode Island School of Design", "Battleship", "Arthur Miller", "1964", "Abigail", "unidentified flying objects", "Dame Eleen June Atkins", "Toshi Ichiyanagi", "Africa", "a jersey or uniform that a sports team wear in games instead of its home outfit or its away outfit", "Catwoman", "Nassau County", "Duncan Kenworthy", "Mr. Church", "Edward Robert Martin Jr.", "23", "western Upper Austria", "World War II", "Minette Walters", "\"Le Divorce\"", "Princeton University", "Three's Company", "Rungrado 1st", "Linux Format", "1881", "August 30", "Guy Carawan", "presidential representative democratic republic", "Sir Henry Cole", "the Corinthian and Saronic Gulfs", "Mel Gibson", "Obama", "Switzerland", "edward llius", "iron", "9", "the solar system", "The Word", "the \"cliff effect.\"", "Felipe Calderon", "Cpl. Richard Findley", "Manmohan Singh", "The patient, who prefers to be anonymous,", "Fernando Caceres", "YouTube", "Gettysburg", "Vermont", "sheep", "Botanya", "Abbey Theatre", "Nile", "out-of-body", "the United States, NATO member states, Russia and India", "Vertikal-T", "U.S. District Judge Ricardo Urbina"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6619419642857143}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.75, 1.0, 1.0, 0.09999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.8571428571428571, 0.4, 0.28571428571428575]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4562", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4642", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-35", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-1092", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-6054", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-1563"], "SR": 0.546875, "CSR": 0.5608836206896552, "EFR": 1.0, "Overall": 0.7403798491379311}, {"timecode": 29, "before_eval_results": {"predictions": ["over the winter of 1973\u201374", "smartphones", "three hundred sixty", "Francois Truffaut", "Final Cut Pro", "alcohol", "the chiaroscuro", "an American Tragedy", "a hen", "Dairy Queen", "Roger Bacon", "Gene Autry", "voice pitch", "toasted harvesters", "Sydney", "offbeat", "Alexander Graham Bell", "the Gulf War", "the Colorado River", "(Ossining, New York)", "the South Beach diet", "hair", "the Intihuatana pyramid", "Phnom Penh", "Fairbanks", "50 Cent", "Star Trek", "Ramses the Great", "Kevin Costner", "Rhode Island", "a mysterious old mansion and its equally fascinating master", "The Andy Griffith Show", "The World's Largest Hotel Chain", "Steve Russell", "the retina", "a normally inaccessible mini-game in the 2004 video game Grand Theft Auto : San Andreas, developed by Rockstar North", "Massachusetts", "flawed democracy", "currently 24 judges, against a maximum possible strength of 31", "The Lightning Thieves", "mathematics", "The 2014 Apple iPad Air commercial", "Diana Ross and Lionel Richie", "9", "Taiwan", "Sinclair Lewis", "elton john", "Fort Bragg", "September 23, 1935", "$10.5 million (USD 8 million) payment and an apology by the federal government", "\"Dinotopia\"", "Colonel Patrick John Mercer", "Alien Resurrection", "the arrival of the first Spanish conquistadors in the region of North America now known as Texas in 1519", "The new Touch, now the most popular iPod, will be available in both black and white and get a $30 price cut, to $299 for 32 gigs and $399 for 64.", "543 elected members", "Deutschneudorf", "ammonia leaks and a fire that was not extinguished", "seven", "Jeffrey Jamaleldine", "30,000", "four", "a solitary figure who is not understood by others, but is actually wise", "November 1961"], "metric_results": {"EM": 0.484375, "QA-F1": 0.572441620879121}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.19999999999999998, 0.5, 1.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 0.2, 0.13333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-526", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-1424", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-4657", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-9950", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-11919", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-11578", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-4104", "mrqa_triviaqa-validation-1650", "mrqa_triviaqa-validation-7237", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-7195", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-4754", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-2766", "mrqa_naturalquestions-validation-8728"], "SR": 0.484375, "CSR": 0.5583333333333333, "EFR": 1.0, "Overall": 0.7398697916666668}, {"timecode": 30, "UKR": 0.765625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2691", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-281", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-443", "mrqa_hotpotqa-validation-4465", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-468", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4874", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5760", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9997", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2093", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2536", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-926", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10275", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-11919", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12504", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-1584", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16301", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-1958", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3376", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-472", "mrqa_searchqa-validation-4755", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6489", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10395", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1496", "mrqa_squad-validation-1541", "mrqa_squad-validation-1598", "mrqa_squad-validation-1637", "mrqa_squad-validation-1833", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2108", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-2376", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3398", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-360", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3800", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-387", "mrqa_squad-validation-3871", "mrqa_squad-validation-3926", "mrqa_squad-validation-3957", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4186", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-424", "mrqa_squad-validation-4332", "mrqa_squad-validation-4360", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4506", "mrqa_squad-validation-4547", "mrqa_squad-validation-4649", "mrqa_squad-validation-4677", "mrqa_squad-validation-4775", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-5054", "mrqa_squad-validation-510", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5334", "mrqa_squad-validation-5348", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6118", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-629", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-6638", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6996", "mrqa_squad-validation-703", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7850", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-824", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8534", "mrqa_squad-validation-8687", "mrqa_squad-validation-8732", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-90", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9413", "mrqa_squad-validation-9451", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-9802", "mrqa_squad-validation-9849", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-2387", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-3001", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-362", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4192", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4910", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5017", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-636", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-6688", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7087", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-746", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-804", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-922"], "OKR": 0.875, "KG": 0.49296875, "before_eval_results": {"predictions": ["increasing access to education", "Arab", "Chester", "The 725-mile Veracruz", "safety issues in the company's cars", "Brian Thomas,", "Roland S. Martin", "The three men entered the E.G. Buehrle Collection -- among the finest collections of Impressionist and post-Impressionist art in the world", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry", "The U.S.-Mexico border", "a music video on his land.", "Monterrey,", "head", "file papers shortly with an appeals court seeking an emergency stay", "Long troop deployments", "$89", "the estate with its 18th-century sights, sounds, and scents.", "27-year-old", "Amanda Knox's", "\"falling space debris,\"", "Chinese tourists", "Cameron-Ritchie", "The Transportation Security Administration", "the Gulf", "Columbia, Missouri.", "The apartment building", "skull.", "At least 40", "the Democratic VP candidate", "1831", "Daniel Radcliffe", "Arkansas", "Nick Adenhart", "a ranking used in combat sports", "the nucleus", "the date on which the Constitution of India came into effect on 26 January 1950", "the cavities and surfaces of blood vessels and organs", "sea water", "each state's DMV", "currently starring as Phyllis Summers on The Young and the Restless", "Dilbert", "dpurves Member", "Dim sum", "King George III", "sons of chicAGO", "algebra", "Rosslyn Chapel", "Acid house", "Oahu", "Edgar Linton", "Rigoletto", "\"Saw II\"", "The Hertz Corporation", "Elliot Fletcher", "The Lottery by Shirley Jackson", "Berkeley", "Robert Devereux", "the Wessex", "Wyoming", "Mall of America", "Mayo", "Joseph Cotten,", "Bangkok", "Jimmy Ellis"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5512715215911727}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.10526315789473684, 0.9393939393939393, 0.0, 0.5714285714285715, 0.25, 0.0, 0.0, 0.7499999999999999, 1.0, 0.2222222222222222, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3846153846153846, 1.0, 0.8695652173913044, 0.888888888888889, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2271", "mrqa_newsqa-validation-4033", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-1440", "mrqa_newsqa-validation-3096", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-3301", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-268", "mrqa_triviaqa-validation-285", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-4972", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-9692", "mrqa_hotpotqa-validation-5460"], "SR": 0.421875, "CSR": 0.5539314516129032, "retrieved_ids": ["mrqa_squad-train-72666", "mrqa_squad-train-37038", "mrqa_squad-train-71110", "mrqa_squad-train-5065", "mrqa_squad-train-50548", "mrqa_squad-train-62399", "mrqa_squad-train-19487", "mrqa_squad-train-65421", "mrqa_squad-train-28552", "mrqa_squad-train-49530", "mrqa_squad-train-42226", "mrqa_squad-train-4660", "mrqa_squad-train-74429", "mrqa_squad-train-65675", "mrqa_squad-train-54187", "mrqa_squad-train-16681", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-9877", "mrqa_squad-validation-8984", "mrqa_hotpotqa-validation-2313", "mrqa_newsqa-validation-1468", "mrqa_triviaqa-validation-7058", "mrqa_searchqa-validation-9998", "mrqa_hotpotqa-validation-1019", "mrqa_triviaqa-validation-3272", "mrqa_searchqa-validation-11905", "mrqa_triviaqa-validation-3506", "mrqa_naturalquestions-validation-8433", "mrqa_newsqa-validation-4067", "mrqa_hotpotqa-validation-5760", "mrqa_squad-validation-5362", "mrqa_searchqa-validation-2783"], "EFR": 0.972972972972973, "Overall": 0.7320996349171753}, {"timecode": 31, "before_eval_results": {"predictions": ["1969", "Highly combustible materials", "the monsoons", "2002", "Joanne Wheatley", "Clarence Williams", "invertebrates", "off the southernmost tip of the South American mainland, across the Strait of Magellan", "Detective Superintendent Dave Kelly", "warplanes", "The Drew Las Vegas", "1957", "Virginia", "bilaterally symmetrical", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "the breast or lower chest of beef or veal", "1933", "john Ratzenberger", "southern Anatolia", "halogenated paraffin hydrocarbons", "Professor Eobard Thawne", "Dr. Lexie Grey", "Number 4, Privet Drive, Little Whinging in Surrey, England", "1939", "Alex Skuby", "the Church of England", "AD 1786", "Bacon", "October 3, 2013", "In the television series's fourth season", "one of the most common words in scripture", "Action Jackson", "the giraffe", "1883", "Dyfed-Powys Police,", "Elizabeth I", "Indonesia", "the crow", "Itzhak Stern", "2013 Cannes Film Festival", "Conservative Party", "1967", "Hjernevask", "corn", "4,000", "1966", "Knox's aunt Janet Huff", "\"I am sick of life -- what can I say to you?\"", "possible victims of physical and sexual abuse.", "bipartisan", "early detection and helping other women cope with the disease.\"", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds", "16", "Hector Berlioz", "Omaha", "Communist", "ice hockey", "Manchin", "Cock Robin", "Halloween", "the first trans-Pacific flight", "2 April 1977", "Phil Collins"], "metric_results": {"EM": 0.40625, "QA-F1": 0.553997693088063}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.7058823529411764, 0.0, 0.07999999999999999, 1.0, 1.0, 1.0, 0.0, 0.9, 0.4, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.4, 1.0, 0.3333333333333333, 0.7272727272727272, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.3076923076923077, 1.0, 1.0, 1.0, 0.19354838709677416, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3491", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-6374", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5473", "mrqa_hotpotqa-validation-5", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-491", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-6195", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-6554", "mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-3464"], "SR": 0.40625, "CSR": 0.54931640625, "EFR": 0.9210526315789473, "Overall": 0.7207925575657895}, {"timecode": 32, "before_eval_results": {"predictions": ["worst and average case complexity", "pores in the epidermis", "Charles Lebrun", "Ajay Tyagi", "the Earth", "28 %", "West Norse sailors", "a single epididymal tubule ( luminal diameter. 15 -. 25 mm ) to the lumen of the vas deferens", "plant matter that contained spores of dung fungus", "Andrew Lloyd Webber", "New England Patriots", "John J. Flanagan", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Ra\u00fal Eduardo Esparza", "1916", "Milira", "1989", "Ledger", "New England Patriots XX, XXXI, XXXVI, XXXVIII, XXXIX, XLII", "USS Chesapeake", "Christopher Lloyd", "Pashto and Persian as \u0647\u0646\u062f\u0648\u06a9\u0634 \u202c", "Pakistan", "a combination of genetics and the male hormone dihydrotestosterone", "Nazi Germany and Fascist Italy", "saliva and plaque fluid", "Rocinante", "on the heads of Christians on Ash Wednesday", "Alastair Cook", "Brad Dourif", "Robert Irsay", "the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "Disney", "a tittle", "(George) West", "high cooking", "Otto von Bismarck", "a moustache", "(Vince) Lombardi", "Sid Vicious", "\"Kids\"", "Toby Kennish", "Free Range Films", "Gambaga", "a pioneer in watch design, manufacturing and distribution", "in their home country", "Draco Youlanda Ruby Clinton-Baddeley", "can I", "his comments", "can lead to blocked airways, cardiovascular collapse, and even death.", "on-loan David Beckham claimed his first goal in Italian football.", "bartering -- trading goods and services without exchanging money", "the area was sealed off, so they did not know casualty figures.", "Bob Bogle", "(Count von) Zeppelin", "(Henry) Tudor", "Tsingtao Lager", "incognito", "a bats", "Galileo", "Life Among the Lowly", "Charlotte Gainsbourg and Willem Dafoe", "2,000 euros ($2,963)", "Seasons of My Heart"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5221229274538097}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true], "QA-F1": [0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.14814814814814814, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1700", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-3593", "mrqa_triviaqa-validation-2415", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-599", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-461", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-4471", "mrqa_searchqa-validation-9195", "mrqa_searchqa-validation-16579"], "SR": 0.46875, "CSR": 0.546875, "EFR": 0.9705882352941176, "Overall": 0.7302113970588235}, {"timecode": 33, "before_eval_results": {"predictions": ["D\u00fcrer", "ten", "Charles Perrault", "2", "relaxation if one becomes too anxious through methods such as progressive muscle relaxation, breathing exercises, and meditation, or the use of energizing techniques", "seven", "Bachendri Pal", "7 July", "usually in May", "Arthur Chung", "The Outback", "Andy Serkis", "France", "Sean O'Neal", "Himadri Station", "257,083", "ensure party discipline in a legislature", "Empiricism", "beneath the liver", "George II", "1920s", "Under normal conditions", "Divyanka Tripathi", "May 26, 2017", "in formal education during the Roman Empire", "Mason Alan Dinehart", "a federal law intended to check the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "Buffalo Lookout", "The Lykan Hypersport", "Geoffrey Dyson Palmer", "Jupiter", "March 26, 1973", "1991", "Pete Seeger", "Sweden", "1915", "bankside power station", "onion", "trombone", "24", "Theodore Roosevelt Mason", "Route 37 East", "Lucy Muringo Gichuhi", "Do Kyung-soo", "Teotihuacan", "Travis County", "1936", "no other designer", "Arsene Wenger", "Derek Mears", "Bob Bogle", "Miguel Cotto", "recite her poetry", "Pakistani officials,", "the Nile", "Pushing Daisies", "Uncle 1 hit", "shiner", "(George) Newton", "Nefertiti", "milk", "three years", "This is not a project for commercial gain.", "Zhanar Tokhtabayeba"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6167410714285715}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.5, 1.0, 0.7999999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5520", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-7403", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-2217", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-3294", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-1042", "mrqa_hotpotqa-validation-5451", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3076", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-15769", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-3415"], "SR": 0.5625, "CSR": 0.5473345588235294, "retrieved_ids": ["mrqa_squad-train-18749", "mrqa_squad-train-21272", "mrqa_squad-train-9080", "mrqa_squad-train-60718", "mrqa_squad-train-54311", "mrqa_squad-train-38155", "mrqa_squad-train-58406", "mrqa_squad-train-66104", "mrqa_squad-train-80947", "mrqa_squad-train-51521", "mrqa_squad-train-3967", "mrqa_squad-train-25962", "mrqa_squad-train-74428", "mrqa_squad-train-50789", "mrqa_squad-train-26204", "mrqa_squad-train-14244", "mrqa_triviaqa-validation-331", "mrqa_hotpotqa-validation-5273", "mrqa_searchqa-validation-16905", "mrqa_newsqa-validation-1561", "mrqa_naturalquestions-validation-3189", "mrqa_newsqa-validation-2324", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-2033", "mrqa_naturalquestions-validation-2250", "mrqa_hotpotqa-validation-2328", "mrqa_newsqa-validation-1440", "mrqa_hotpotqa-validation-5265", "mrqa_squad-validation-6864", "mrqa_newsqa-validation-2787", "mrqa_naturalquestions-validation-9056", "mrqa_searchqa-validation-16664"], "EFR": 0.9642857142857143, "Overall": 0.7290428046218488}, {"timecode": 34, "before_eval_results": {"predictions": ["Warfare and the long occupation left the city disrupted after the war.", "high", "September 1995", "Prince William County, Virginia", "Walter Mondale", "Vienna", "hot summers and mild winters", "A footling breech", "Ireland", "John McConnell", "the times sign", "skeletal muscle, while the remainder is distributed in the blood, brain, and other tissues", "At the end of the episode, she is seen smacking a fly on her mirror and removes its corpse", "13 June 1990", "on the lateral side", "the university", "Bob Dylan", "Malware", "Konakuppakatil Gopinathan Balakrishnan", "May 19, 2008", "IB Diploma Program and the IB Career - related Program", "Jack Barry", "Catherine Tramell", "2026", "more than a million", "April 13, 2018", "The border between the Cocos Plate and North American Plate, along the Pacific Coast of Mexico", "Jacques Cousteau", "1963", "Fix You", "minced meat", "change their attitudes and personal behavior so they would learn to be nonviolent in any relationship", "Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Imola", "Wigan", "astronaut", "Dante Alighieri", "david hockney", "\"Li'l Abner,\"", "Tokyo", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "Irish Chekhov", "Cody Miller", "Selina D'Arcy", "ten", "Tallahassee City Commission", "Roosevelt Island", "air support.", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Sabina Guzzanti", "two Israeli soldiers,", "north Georgia mountains,", "The American Civil Liberties Union", "Zimbabwe", "the boy", "baldness", "parliaments", "Archer Daniels Midland", "the potato", "Part I", "The Leyden jar", "21", "Aniston, Demi Moore and Alicia Keys", "grocery store"], "metric_results": {"EM": 0.5, "QA-F1": 0.5754195661364779}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [0.6153846153846153, 1.0, 0.0, 0.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5454545454545454, 0.3333333333333333, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7096", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-692", "mrqa_naturalquestions-validation-1049", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9387", "mrqa_triviaqa-validation-2357", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-3633", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1018", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-9057", "mrqa_searchqa-validation-14489", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-4177"], "SR": 0.5, "CSR": 0.5459821428571429, "EFR": 0.96875, "Overall": 0.7296651785714287}, {"timecode": 35, "before_eval_results": {"predictions": ["algorithms", "1964", "A severe famine", "the Internet", "participate in Iraq's government.", "more than 4,000", "Nasser Medical Institute", "an annual road trip,", "the one described by former CIA officer John Kiriakou.", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "Wednesday night", "\"Empire of the Sun,\"", "apps and allow content to be stored on remote servers instead of the users' iPod,", "gun charges,", "UNICEF", "dismissed all charges", "shooting himself three times in the head", "Kris Allen,", "Janet and La Toya,", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "a secret U.S. program to assassinate terrorists in Iraq.", "They're big, strong, and fierce -- and they wear little blue booties.", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "one", "NATO fighters", "750", "one", "black, red or white,", "Phay Siphan, secretary of the Cambodian Council of Ministers.", "last week", "more than 9,500 energy-efficient light-emitting diodes", "two and a half hours.", "on Anjuna beach in Goa", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "personal and artistic freedom from the Beatles", "Tokyo", "Madison's proposed Virginia Plan", "Judith Cynthia Aline Keppel", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "Convention's first act, on 10 August 1792, was to establish the French First Republic and officially strip the king of all political powers", "Fu\u00dfball", "manager of Elvis Presley", "a collapsible support assembly", "Indy", "Nadia Comaneci", "Florida", "an older man about 50 and a young fellow about 24", "1862", "World War I", "Meghan Markle", "the acid house and later rave scenes", "Pease Air National Guard Base", "South African", "1754", "chiffon", "Artemis", "rodeo", "a psalm", "The Sun Also Rises", "overzealous", "Tequila Sunrise", "Hawley Harvey", "Vladivostok", "cussler"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5346024269955192}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.07142857142857144, 0.5263157894736842, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.8, 0.33333333333333337, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.5, 0.22222222222222224, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1859", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-1791", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-2980", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-10311", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-4688", "mrqa_triviaqa-validation-68", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-7594", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3028", "mrqa_hotpotqa-validation-4603", "mrqa_hotpotqa-validation-1034", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-9366", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-4322"], "SR": 0.40625, "CSR": 0.5421006944444444, "EFR": 0.9210526315789473, "Overall": 0.7193494152046783}, {"timecode": 36, "before_eval_results": {"predictions": ["harassment and, at least to the bystander, somewhat inane", "2014", "the 1989 album Sleeping with the Past", "Philadelphia ( Revelation 3 : 7 - 13 )", "John Barry", "9 May 2017", "2020 National Football League ( NFL ) season", "a Norwegian town circa 1879", "the 18th century", "before the first year begins", "Vicente Fox", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "President", "Supplemental oxygen", "1961", "45 %", "his cousin D\u00e1in", "1933", "Earle Hyman", "federal nervous system, project their axons to skeletal muscles ( such as the muscles of the limbs, abdominal, and intercostal muscles ), which are involved in locomotion", "the rate at which soil is able to absorb rainfall or irrigation", "chain elongation", "1", "referee", "Thomas Edison", "In the television series's fourth season", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "President pro tempore", "September 4, 2000", "somatic cell nuclear transfer", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Eddie Murphy", "Payson, Lauren, and Kaylie", "Bonita Melody Lysette", "Johnnie L. Cochran Jr.", "a pieman", "Singapore", "California", "clive", "kalium", "Rob Reiner", "1994", "Valtellina", "Yasiin Bey", "FIFA Women's World Cup", "University of Texas Longhorns", "Carol Ann Duffy", "May 2000,", "people around the world commented, pondered, and paid tribute", "putting a personal and human face on the issue... there's nothing more crucial,\"", "the family's blog", "his fleet of trucks used to pick up cargo.", "more than 200.", "1959.", "Luxor Las Vegas", "Tennessee", "Vatican City", "Thomas Alva Edison", "the Gulf of Tonkin", "When the moon hits your eye", "Saint Telemachus", "Fareed Zakaria", "three", "federal security, prosperity and freedom."], "metric_results": {"EM": 0.40625, "QA-F1": 0.5195339948397013}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 0.7499999999999999, 0.33333333333333337, 0.0, 0.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.7272727272727272, 0.33333333333333337, 0.0, 1.0, 1.0, 0.8405797101449275, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7014", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-9371", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3187", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-7394", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-257", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-2855", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15986", "mrqa_searchqa-validation-1056", "mrqa_searchqa-validation-9778", "mrqa_searchqa-validation-10986", "mrqa_newsqa-validation-4175"], "SR": 0.40625, "CSR": 0.5384290540540541, "retrieved_ids": ["mrqa_squad-train-69912", "mrqa_squad-train-21747", "mrqa_squad-train-17676", "mrqa_squad-train-5808", "mrqa_squad-train-39253", "mrqa_squad-train-5884", "mrqa_squad-train-14837", "mrqa_squad-train-55853", "mrqa_squad-train-60496", "mrqa_squad-train-27767", "mrqa_squad-train-47510", "mrqa_squad-train-41376", "mrqa_squad-train-19087", "mrqa_squad-train-40902", "mrqa_squad-train-81884", "mrqa_squad-train-12680", "mrqa_triviaqa-validation-5619", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-5713", "mrqa_squad-validation-7729", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-1888", "mrqa_naturalquestions-validation-7217", "mrqa_squad-validation-8374", "mrqa_newsqa-validation-205", "mrqa_triviaqa-validation-1058", "mrqa_newsqa-validation-856", "mrqa_searchqa-validation-7050", "mrqa_newsqa-validation-1182", "mrqa_squad-validation-2564", "mrqa_hotpotqa-validation-1617"], "EFR": 0.8947368421052632, "Overall": 0.7133519292318635}, {"timecode": 37, "before_eval_results": {"predictions": ["warmer", "Brooke Wexler", "Chris Martin", "Needtobreathe", "drivers", "James `` Jamie '' Dornan", "2012", "Games played", "2013", "Sunday night", "President pro tempore of the Senate", "Jonathan Breck", "electron shells", "Sharecropping", "Madison, Wisconsin, United States", "England and Wales", "the 1994 season", "Kingsford, Michigan", "a sweet alcoholic drink made with rum, fruit juice, and syrup or Grenadine", "head - up", "Austria - Hungary", "Lana Del Rey", "Paracelsus", "George Warren Barnes", "ABC", "If the car is slowed initially by manual use of the automatic gear box", "1958", "1996", "Social Security Act of 1935", "Andr\u00e9 3000", "Celtic", "Latitude", "ended Russia's participation in World War I", "the Lake of cumbria", "a nine of diamonds", "Poland", "\"Appaloosa\"", "enzootic", "lincolnfield", "charlie lloyd", "Bocelli became completely blind at the age of 12", "Pandosia", "Spain", "Ronald Ryan", "2,099", "1969 until 1974", "the fourth President of Pakistan from 1971 to 1973", "Melbourne", "five", "al Fayed", "three", "1998.", "Mediterranean coastline", "opry mignon", "Norman Bates", "Frankfort", "Doc Holliday", "a bug", "the Backstreet Boys", "Muscular Dystrophy", "the stamps", "cilla black", "the Daily Mirror", "yellow"], "metric_results": {"EM": 0.5, "QA-F1": 0.6297085042150832}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.33333333333333337, 0.2857142857142857, 1.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.0, 0.7368421052631579, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7999999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3595", "mrqa_naturalquestions-validation-8272", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-750", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5444", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-2275", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-2818", "mrqa_hotpotqa-validation-1513", "mrqa_hotpotqa-validation-4940", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-10393", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-11512", "mrqa_searchqa-validation-16757"], "SR": 0.5, "CSR": 0.5374177631578947, "EFR": 0.96875, "Overall": 0.727952302631579}, {"timecode": 38, "before_eval_results": {"predictions": ["$2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda,", "lemurs", "Man Ray", "Billy Bob Thornton", "Carson McCullers", "1957", "Charleston", "calcium", "Alpha", "savings", "Meyer Lansky", "Iraq", "William Montagu", "Live Free or Die Hard", "Staden (a short form of Hovedstaden, meaning Capital)", "Mikhail Gorbachev", "Dr. Richard Kimble", "Sisyphus", "INXS", "A Few Good Men", "auf wiedersehen", "Fyodor Dostoevsky", "elizabeth browning", "Quiz Show", "sculpere", "the vikings", "Sylvia Woolf", "Jezebel", "Barbara Walters", "Qualcomm", "Frasier (Gilpin)", "The Brady Bunch", "1789", "King Harold Godwinson", "1984", "on the shore of Lake Erie in downtown", "in southwestern Colorado and northwestern New Mexico", "Gupta Empire", "13", "April 12, 2017", "the Atlantic Ocean", "Dylan Thomas", "denburn", "Mnemosyne", "Dakar, Senegal", "American Waddington Limited", "Gilda", "Kentucky Wildcats", "Eugene Walter", "\"Secrets and Lies\"", "26,000", "drawings and approximately 1 million old master prints,", "A Bug's Life", "SpongeBob SquarePants 4-D", "10 below in Chicago, Illlinois.", "federal officers' bodies", "\"design its own separate strategies for making progress toward achieving this long-term goal.\"", "UNICEF", "26", "state senators who will decide whether to remove him from office", "24 Rezai", "overturned the Plessy v. Ferguson decision of 1896, which allowed state - sponsored segregation, insofar as it applied to public education", "seven", "20 November 1989"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6667510400490877}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, true], "QA-F1": [0.8275862068965517, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.17647058823529413, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8452", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-13646", "mrqa_searchqa-validation-16459", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-13127", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-8357", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-15072", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-2098", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-7574", "mrqa_triviaqa-validation-3226", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-1375", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-4913", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-156", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-3283", "mrqa_naturalquestions-validation-10090"], "SR": 0.5625, "CSR": 0.5380608974358974, "EFR": 1.0, "Overall": 0.7343309294871795}, {"timecode": 39, "before_eval_results": {"predictions": ["Tyndale Bible", "Raimond Gaita", "Fife", "Isabella Hedgeland", "Daniel Andre Sturridge ( ; born 1 September 1989) is an English professional footballer who plays for Premier League club Liverpool and the England national team.", "The 2016 Oklahoma Sooners football team", "Overijssel", "20th Century Fox", "1972", "epic verse", "Schaeffler AG", "1976", "James Burke", "Bishop's Stortford", "The Fault in Our Stars", "Bhushan Patel", "Katherine Virginia \"Kassie\" DePaiva (n\u00e9e Wesley; born March 21, 1961)", "Araminta Ross", "Vernon Charles Kay", "RAF Tangmere, West Sussex", "Saturday Night Live", "downtown Dayton, Ohio, United States", "aeronautical engineer", "1976", "\u00c6thelstan", "three", "KBS2", "The 38th International 500-Mile Sweepstakes", "Houston Rockets", "three", "Henry II", "63 mph", "Maria Szraiber", "whether they wish to collect a jackpot prize in cash or annuity", "Inequality of opportunity was higher in the transition economies of Central and Eastern Europe and Central Asia", "American comedy - drama film about the titular dog, Marley & Me : The Puppy Years", "1981", "exclusive rights granted by a sovereign state or intergovernmental organization to an inventor or assignee for a limited period of time", "in the 1898 Treaty of Paris", "the Columbia River Gorge", "Cambridge", "squash", "Sotheby\u2019s", "scoop", "B\u00e9la Bart\u00f3k", "Niger", "kangaroos", "The New York appeals court", "more than 1.2 million people.", "33-year-old", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "surgical anesthetic propofol", "Salt Lake", "The Mobile County Circuit Judge Herman Thomas", "Dr. Quinn", "the Queen of Sheba", "Johnny Weissmuller", "MUSICAL THEATOR", "a jazz saxophonist", "the Hudson", "twin-lens reflex", "Irsay", "1963", "Tim Rooney"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5427528155324208}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4, 0.0, 0.16666666666666669, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.3157894736842105, 0.3076923076923077, 1.0, 0.2222222222222222, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2114", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-4423", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-487", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-3053", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-3808", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-6370", "mrqa_triviaqa-validation-274", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3593", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-8329", "mrqa_naturalquestions-validation-4169"], "SR": 0.4375, "CSR": 0.535546875, "retrieved_ids": ["mrqa_squad-train-79132", "mrqa_squad-train-31763", "mrqa_squad-train-38148", "mrqa_squad-train-15487", "mrqa_squad-train-26706", "mrqa_squad-train-49178", "mrqa_squad-train-71327", "mrqa_squad-train-68429", "mrqa_squad-train-49806", "mrqa_squad-train-44221", "mrqa_squad-train-43651", "mrqa_squad-train-49968", "mrqa_squad-train-49594", "mrqa_squad-train-34578", "mrqa_squad-train-11261", "mrqa_squad-train-42836", "mrqa_naturalquestions-validation-9383", "mrqa_squad-validation-7087", "mrqa_naturalquestions-validation-328", "mrqa_hotpotqa-validation-4902", "mrqa_searchqa-validation-6517", "mrqa_naturalquestions-validation-9877", "mrqa_hotpotqa-validation-639", "mrqa_naturalquestions-validation-8951", "mrqa_searchqa-validation-14030", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-1178", "mrqa_triviaqa-validation-1966", "mrqa_searchqa-validation-9651", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-2482", "mrqa_triviaqa-validation-2050"], "EFR": 1.0, "Overall": 0.733828125}, {"timecode": 40, "UKR": 0.75390625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1214", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3472", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-402", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4664", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6641", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8868", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-787", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13843", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9057", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_squad-validation-10004", "mrqa_squad-validation-10059", "mrqa_squad-validation-1006", "mrqa_squad-validation-10112", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-10503", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1634", "mrqa_squad-validation-1703", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-2985", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-3269", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3556", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-366", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4044", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-509", "mrqa_squad-validation-5147", "mrqa_squad-validation-5275", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6024", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8033", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-82", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8319", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8370", "mrqa_squad-validation-8374", "mrqa_squad-validation-8452", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8872", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-10", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1528", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5849", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.861328125, "KG": 0.484375, "before_eval_results": {"predictions": ["St. George's United Methodist Church", "in 1891", "Kirstjen Nielsen", "6 - 7 % average GDP growth annually", "source code", "Scots law", "2013", "Fleetwood Mac", "Ric Flair", "the French company R2E Micral CCMC", "October 27, 2017", "31 December 1600", "1608", "1273.6 cm", "the internal reproductive anatomy ( such as the uterus in females )", "December 24, 1836", "sorrow regarding the environment", "in Franklin and Wake counties in the U.S. state of North Carolina", "seven", "byte - level operations", "the nerves and ganglia outside the brain and spinal cord", "Denmark", "Hermann Ebbinghaus", "Sam Waterston", "on the southeastern coast of the Commonwealth of Virginia in the United States", "March 11, 2018", "Stephen A. Douglas", "an optional message body", "in the southwestern part of the island", "a thirty - second call to one of a number of friends ( who provide their phone numbers in advance", "Transvaginal ultrasonography", "Ku - Klip", "Schadenfreude", "Tahrir Square", "Purple Rain", "Phaethon", "Northumberland", "national parliaments", "Analytical Cubism", "Metropolitan Borough of Oldham", "model", "Londonderry", "St. Louis, Missouri", "Chelmsford", "1,462", "Mary Harron", "GE Appliances", "staff sergeant", "Dennis Davern,", "the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures", "Zimbabwe", "Islamic militants", "normal maritime traffic", "Jackson was in good health, contrary to media reports he was diagnosed with skin cancer.", "the Two-Faced Sewing Machine", "John Paul Jones", "sealed-beam", "Edinburgh", "Mrs. Doubtfire", "arsenic", "Detroit", "2004", "Katherine Harris", "Statue of Liberty"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7076270840143838}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.1111111111111111, 0.2222222222222222, 1.0, 1.0, 0.41666666666666663, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6956521739130436, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.9714285714285714, 1.0, 0.6666666666666666, 0.8, 0.4210526315789474, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10030", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-2698", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-7637", "mrqa_triviaqa-validation-6822", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-2452", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-7016", "mrqa_hotpotqa-validation-1870"], "SR": 0.609375, "CSR": 0.5373475609756098, "EFR": 1.0, "Overall": 0.7273913871951219}, {"timecode": 41, "before_eval_results": {"predictions": ["manually suppress the fire", "Outfield", "Kida", "Tom Bower", "Thomas Jefferson", "Krypton", "the duodenum", "left - sided heart failure", "July -- October 2012", "Abbot Suger", "Filipino American History Month", "1997", "9 February 2018", "re-education", "season ten", "administratively part of No. 1 Group RAF, flying out of RAF Coningsby in Lincolnshire", "Louis XV", "20 year - old Kyla Coleman from Lacey, Washington", "the Carthaginians'Phoenician ancestry", "a 1969 plan for a system of reusable spacecraft of which it was the only item funded for development", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "De pictura", "Nicholas Sparks", "Road / Track", "IBM", "1997", "Effy ended up flatting with Naomi", "Phosphorus pentoxide", "a narcissistic ex-lover who did the protagonist wrong", "the theory of T\u0101\u1e47\u1e0dava dance ( Shiva )", "Geraldine Margaret Agnew", "Sir Hugh Beaver", "November 1999", "st paul", "Norman Hartnell", "becoming bald or fear of being around bald people.", "bitter liqueurs", "2", "desert", "gin", "Chrysler", "McComb, Mississippi", "King James II of England (James VII of Scotland)", "Anah\u00ed Giovanna Puente Portilla de Velasco", "in the second half of the third season", "Revolver", "Peterhouse, Cambridge", "shut down every year.", "President Pervez Musharraf", "Graham", "climate change", "Leo Frank,", "2006", "Indian Ocean waters", "Patrick Dempsey", "The Tell-Tale Heart and Other Stories", "Kwai Chang Caine", "an ideal for fishing, canoeing, and kayaking and near Lake Kissimmee State Park", "Franklin Roosevelt", "Anaheim", "blue", "illegal commercial fishing", "for \"security issues\" and not their faith,", "head"], "metric_results": {"EM": 0.375, "QA-F1": 0.49699951329800474}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, true, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, false, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.4, 0.13333333333333336, 0.0, 0.2758620689655173, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.7142857142857143, 0.2857142857142857, 0.9090909090909091, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-8267", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-10252", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-8314", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1529", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-4880", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-236", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2075", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-9753", "mrqa_searchqa-validation-9112", "mrqa_searchqa-validation-15605", "mrqa_searchqa-validation-6175", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-2315"], "SR": 0.375, "CSR": 0.5334821428571428, "EFR": 0.95, "Overall": 0.7166183035714285}, {"timecode": 42, "before_eval_results": {"predictions": ["Magnetic stratigraphers", "other individuals, teams, or entire organizations", "1974", "an explosion and a fire", "Centre of Excellence", "Dominican", "Ministry of European Integration", "writer, essayist, philosopher, historian and playwright", "Patricia Jude Francis Kensit", "1933", "Jonghyun", "by Bandai on November 23, 1996 in Japan and May 1997 in the rest of the world", "musician", "Hans Rosenfeldt", "Dan Lin, Roy Lee, Phil Lord and Christopher Miller", "Danish", "Groundhog Day", "Barnoldswick", "Tsung-Dao Lee", "Mike Pringle", "the 2007 Formula One season", "16,116", "1449", "Flavivirus", "USC Marshall School of Business", "1 million", "Flatbush section of Brooklyn, New York City", "Louis \"Louie\" Zamperini", "quantum mechanics", "October 20, 2017", "eastern Tennessee, United States", "322,421", "Saint Michael, Barbados", "the Way of the World", "in the duodenum by enterocytes of the Duodenal lining", "28 July 1914", "the Italian Campaign", "Kevin Kline", "Bob Dylan", "the anterolateral corner of the spinal cord", "C\u00f4te d'Or", "canterbury", "the Forest of Fontainebleau", "Phil Redmond", "efore C ommon E ra", "James Mason", "\"The Man with One Red Shoe\"", "Milan", "2009", "-- the motherless cub defended by Elphaba in \"Wicked.\"", "a plaque", "Hamas spokesman Sami Abu Zahri", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "Haeftling,", "the afterlife", "the Grateful Dead", "Like Water for Chocolate", "\"Livin' On A Prayer\"", "The World's Best", "San Francisco 49ers", "cricket", "Solidarity", "Mexico", "the University of Pennsylvania"], "metric_results": {"EM": 0.625, "QA-F1": 0.6975260416666667}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.4, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6, 0.0, 1.0, 1.0, 0.25000000000000006, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-1778", "mrqa_hotpotqa-validation-5220", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3719", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-7511", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-1350", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-66", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-1408", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-4679", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-568"], "SR": 0.625, "CSR": 0.5356104651162791, "retrieved_ids": ["mrqa_squad-train-34575", "mrqa_squad-train-66458", "mrqa_squad-train-74279", "mrqa_squad-train-9758", "mrqa_squad-train-35916", "mrqa_squad-train-55489", "mrqa_squad-train-12105", "mrqa_squad-train-56293", "mrqa_squad-train-39312", "mrqa_squad-train-48735", "mrqa_squad-train-60378", "mrqa_squad-train-34067", "mrqa_squad-train-9545", "mrqa_squad-train-78980", "mrqa_squad-train-63444", "mrqa_squad-train-52844", "mrqa_triviaqa-validation-2415", "mrqa_naturalquestions-validation-3022", "mrqa_triviaqa-validation-3437", "mrqa_squad-validation-9578", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-1791", "mrqa_hotpotqa-validation-5473", "mrqa_naturalquestions-validation-9323", "mrqa_squad-validation-3609", "mrqa_searchqa-validation-10638", "mrqa_squad-validation-6962", "mrqa_naturalquestions-validation-988", "mrqa_searchqa-validation-10890", "mrqa_newsqa-validation-3210"], "EFR": 1.0, "Overall": 0.7270439680232558}, {"timecode": 43, "before_eval_results": {"predictions": ["a six membraned chloroplast", "the third season of the television series How I Met Your Mother", "Massillon, Ohio", "Edmund is rescued after his treason", "2006 -- 06 )", "Arnold Schoenberg", "The NFL Scouting combine", "Islamic Community", "Chelsea", "Akshay Kumar", "pepsinogen", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "the victory of good over evil, the arrival of spring, end of winter", "Rafael Nadal", "June 1991", "The Epistle of Paul to the Philippians", "2018", "1999", "Mark Wahlberg", "12 November 2010", "Theodosius I", "Richard Stallman", "Dalveer Bhandari", "November 2016", "A vanishing point", "Covington, Kentucky", "the Ming", "the foreign exchange market", "the European economy had collapsed, and much of the European industrial infrastructure had been destroyed", "the heights", "Ministry of Corporate Affairs", "Peter Andrew Beardsley MBE", "October 12, 2017", "William Boyd", "cormorant", "Florentia", "Norman Mailer", "george gently", "Snickers candy bars", "Daily Herald", "Diamond Rio", "15,000 people for basketball matches and 15,500 for concerts", "Kristian Eivind Espedal", "Anneliese Michel", "Province of Canterbury", "Delaware River", "Hertz", "43 percent", "Nineteen", "July for A Country Christmas", "At least 40 former U.S. Marines or sons of Marines who lived at Camp Lejeune", "Brett Cummins,", "held in a trust fund", "an appearance on CNN's \"Larry King Live\" on March 31.\"", "ice cream", "Walla Walla", "A Million Little pieces", "Linus", "plebeians", "a butterfly", "William Bradford", "the letters of the American alphabet", "the negligence of the employee", "Istanbul"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6470024724597896}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.26666666666666666, 1.0, 0.0, 0.5, 1.0, 0.2222222222222222, 0.0, 0.5, 1.0, 0.0, 1.0, 0.5365853658536585, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.761904761904762, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-6113", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-3236", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6057", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-7778", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5000", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1279", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-6265"], "SR": 0.5625, "CSR": 0.5362215909090908, "EFR": 1.0, "Overall": 0.7271661931818182}, {"timecode": 44, "before_eval_results": {"predictions": ["Times Square", "Kittie", "George Washington Bridge", "Craig William Macneill", "Oracle", "Kevin Spacey", "\"War & Peace\"", "Thomas Mawson", "British", "Jean Erdman", "Rabies", "Electress of Hanover", "mentalfloss.com", "New Jersey", "1995", "Francis Albert Sinatra", "blood pudding", "Esteban Ocon", "Kurt Vonnegut Jr.", "mixed martial arts", "1st Baron Dowding", "9", "16\u201321", "UNLV Rebels", "6,396.", "Division of Cook", "Radcliffe College", "13", "George Harrison", "The Shins", "Captain Matchbox Whoopee", "Thomas Allen", "20th", "the American League ( AL ) champion Cleveland Indians", "Staci Keanan", "the fictional Iron River Ranch in the fictitious small town of Garrison, Colorado", "March 2003", "Harry", "fermenting dietary fiber into short - chain fatty acids ( SCFAs ), such as acetic acid and butyric acid, which are then absorbed by the host", "14 November 2001", "turnip", "great range of Grand Prix cars from the 1950s right up to the present day", "Falkland Islands", "gleneagles", "Dublin", "Neighbours", "Marie", "a satellite.", "Daniel Wozniak,", "voyage", "Iran's President Mahmoud Ahmadinejad", "Sen. Joe Lieberman,", "Naples home.", "Jaime Andrade", "trenchcoat", "Eragon", "Matthew Perry", "Ivan the Terrible", "Kingston Harbour", "Lisa Gherardini", "Cleopatra", "LSD", "exploits", "four"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6821764122315593}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true], "QA-F1": [0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.5, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 1.0, 0.07142857142857142, 1.0, 0.0, 0.9777777777777777, 1.0, 0.0, 0.3529411764705882, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5961", "mrqa_hotpotqa-validation-2862", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-1662", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-1326", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-4505", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2581", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-7393", "mrqa_triviaqa-validation-4732", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1418", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-134", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-5564", "mrqa_triviaqa-validation-2578"], "SR": 0.5625, "CSR": 0.5368055555555555, "EFR": 0.9642857142857143, "Overall": 0.720140128968254}, {"timecode": 45, "before_eval_results": {"predictions": ["the emergence of the new state of Turkey in the Ottoman Anatolian heartland", "May 1, 2011", "Bob Day", "English author", "23 December 1893", "James Edward Franco", "31", "Sun Valley, Idaho", "Grand Harbour", "Dane William De Haan", "girls aged 11 to 18", "Duval County, Florida", "American burlesque", "Captain Cook's Landing Place", "alt-right", "Kentucky Exposition Center", "Belladonna", "former professional basketball player and current assistant coach", "Named in honour of Louis Mountbatten", "Mario Lemieux", "North Greenwich Arena", "Channel 4", "British racing driver", "1921", "Get Him to the Greek", "Interstate 22", "Ronald Lyle \" Ron\" Goldman", "RAF Mount Pleasant", "evangelical Christian", "Mel Blanc", "five", "performed under the mononym Charice until his gender transition to male", "\"Section.80\"", "Kida", "four distinct levels", "commemorating fealty and filial piety", "a blue rectangle in the canton ( referred to specifically as the `` union '' ) bearing fifty small, white, five - pointed stars arranged in nine offset horizontal rows, where rows of six stars ( top and bottom", "the Roman Empire", "Theodore Roosevelt", "early 1988", "Jamaica", "brighton", "900", "The Quatermass Experiment", "Honeybees", "a lion", "Martin Luther King", "Sarah", "2,800", "11", "$273 million", "Iran", "diplomatic relations", "Jenny Sanford,", "Islamabad", "Alexander Calder", "Nautilus", "Danish", "Jabberwocky", "Metamorphic", "ping pong", "California, Texas and Florida,", "Bright Automotive,", "Stuntman"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6396682919621749}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, false, true, false, false, true, false], "QA-F1": [0.11111111111111112, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 1.0, 0.12765957446808512, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9846", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-238", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-1648", "mrqa_hotpotqa-validation-3332", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-3452", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1491", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-4338", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6393", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2591", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-16616", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3858"], "SR": 0.546875, "CSR": 0.5370244565217391, "retrieved_ids": ["mrqa_squad-train-71556", "mrqa_squad-train-61468", "mrqa_squad-train-8075", "mrqa_squad-train-47901", "mrqa_squad-train-67411", "mrqa_squad-train-14752", "mrqa_squad-train-46857", "mrqa_squad-train-57584", "mrqa_squad-train-86023", "mrqa_squad-train-19778", "mrqa_squad-train-45806", "mrqa_squad-train-71744", "mrqa_squad-train-5678", "mrqa_squad-train-52288", "mrqa_squad-train-49544", "mrqa_squad-train-40907", "mrqa_triviaqa-validation-67", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1199", "mrqa_triviaqa-validation-6334", "mrqa_hotpotqa-validation-541", "mrqa_hotpotqa-validation-5000", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-14900", "mrqa_squad-validation-3667", "mrqa_naturalquestions-validation-2297", "mrqa_newsqa-validation-4105", "mrqa_naturalquestions-validation-688", "mrqa_newsqa-validation-2748", "mrqa_hotpotqa-validation-1356", "mrqa_searchqa-validation-12130", "mrqa_triviaqa-validation-4897"], "EFR": 1.0, "Overall": 0.7273267663043479}, {"timecode": 46, "before_eval_results": {"predictions": ["matching white pants", "Nina Stibbe", "December 1993", "Matt Groening", "Karolina Dean", "the world's fourth-largest media group", "1916", "ten", "40 million", "Paul Manafort Jr.", "Wes Archer", "26,000", "Anne of Green Gables", "Michele Bachmann", "teacher", "father", "Acela Express", "2004 Nokia Sugar Bowl", "Univision", "The Light in the Piazza", "400 MW", "Fort Hood", "green and yellow", "My Father", "Juilliard School", "The interview", "Hilo", "Enemy", "Joseph J. Pulitzer", "Lazio", "the European migrant crisis", "National Subscription Television", "2,627", "March 31, 2013", "1977", "the Earth", "Dr. Derek Shepherd", "December 19, 2014", "Waylon Jennings", "1992", "Fluorine", "Nunavut", "Antoine Henri Becquerel", "ledger", "guitar", "Comedy of Errors", "Malaysia", "Egypt", "\"Wolfman,\"", "it was the biggest police operation in Kabul in several months.", "fractured pelvis and sacrum", "1.2 million", "three", "maintain an \"aesthetic environment\" and ensure public safety", "Baltimore", "Marmaduke", "Hanson", "I Love Rock n' Roll", "your goals", "the Air", "Jeopardy", "Scotland", "thebulletin school", "wet Wet Wet"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6969122023809524}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 1.0, 0.4, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-499", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-2412", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4584", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-2079", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-2267", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-5859", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-3165", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-9607", "mrqa_triviaqa-validation-3056"], "SR": 0.609375, "CSR": 0.538563829787234, "EFR": 1.0, "Overall": 0.7276346409574468}, {"timecode": 47, "before_eval_results": {"predictions": ["meningitis", "rockhound", "the (Miami) Dolphins", "John Hersey", "singapore", "Life of Cato the Elder", "the tulip", "an Old Manse", "the earth", "Ross Perot", "capable", "the Earth's ozone layer", "Spiderman", "Ezra Cornell", "Ying", "(John Wilkes) Booth", "Florida", "Johns Hopkins University", "Dobie Gillis", "the Mississippi and Missouri rivers", "London", "Millie", "(Scott) Peterson", "vampire", "John Henry", "singapore", "the choughs", "Vicksburg", "Orion", "Daumier", "\"If Ya Wanna be Bad Ya Gotta Be Good\"", "Troilus", "Puente Hills Mall", "Francisco Pizarro", "members who sit in congressional districts that are allocated to each of the 50 states on a basis of population as measured by the U.S. Census", "beta decay", "St. Louis Cardinals", "June's younger sister", "parthenogenesis", "Kim Basinger", "singapore", "capable of sitting in the", "the Tower of London", "\"San Francisco Bay\".[into the Pacific Ocean", "Frank McCourt", "Daewoo", "Rihanna", "12", "member of ribosomal proteins (r-protein or rProtein)\"", "Political correctness", "\"The Royal Family\".", "the Food and Agriculture Organization", "two or three acts", "Friends", "2007", "the First Balkan War", "He holds a Saudi passport.", "\"your President Bush doesn't like us Muslims.\"", "Tuesday", "\"Rin Tin Tin: The Life and the Legend\"", "the Bronx.", "an \"unnamed international terror group\"", "the 11th anniversary of the September 11, 2001, terror attacks.", "Iran's parliament speaker"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5844207875457874}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-6133", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-13871", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-500", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-14854", "mrqa_searchqa-validation-3872", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-7867", "mrqa_searchqa-validation-2713", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-12962", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-3207", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5807", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2215"], "SR": 0.515625, "CSR": 0.5380859375, "EFR": 1.0, "Overall": 0.7275390625}, {"timecode": 48, "before_eval_results": {"predictions": ["Stella McCartney", "Little Women", "Paavo Berglund", "Injun Joe", "the Rubik's Cube", "the James River", "the minstrel", "salivary", "france", "the Solidarity", "Zen", "Aristotle's", "(Diane) Arbus", "defense", "the Manhattan island", "Martina Navratilova", "Sweden", "molten", "Margaret Spellings", "City of Hope", "Frasier", "Like Water for Chocolate", "a catalog", "Alice in Alice", "a high school football team in the fictional town of Dillon, Texas.", "Signs", "Henry Cisneros", "the Snows of Kilimanjaro", "Nguyen", "William Shakespeare", "Teflon", "Prince", "Thomas Lennon", "Peter Finch", "plant anatomy", "the theory that the environment sets certain constraints or limitations, but culture is otherwise determined by social conditions", "Middle C ( the fourth C key from left on a standard 88 - key piano keyboard )", "Julia Roberts", "the player will encounter jungles, forts, ruins, and small villages and the world is built to allow players much more freedom, such as allowing players to engage, board, and capture passing ships and swimming to nearby beaches", "1980 Summer Olympics", "rabbit", "Ogaden", "france", "Hera", "Sarah", "Martin Clunes", "Aquaman", "Percy thrower", "the Matildas", "5320 km", "She made her film debut in the 1995 teen drama \"Kids\".", "Solace", "People v. Turner", "the village of Sandtoft, near Belton on the Isle of Axholme in the English county of Lincolnshire.", "Sulla", "Jenny Bae", "The man ran out of bullets and blew himself up.", "overturned about 5:15 p.m. Saturday,", "Islamic", "throwing three punching but said only one connected.", "they did not receive a fair trial.", "in cities throughout Canada.", "several weeks,", "Madonna"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6366936831550802}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.9166666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.19999999999999998, 0.33333333333333337, 1.0, 0.36363636363636365, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-13528", "mrqa_searchqa-validation-3153", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-12506", "mrqa_searchqa-validation-14796", "mrqa_searchqa-validation-12851", "mrqa_searchqa-validation-6550", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-1719", "mrqa_searchqa-validation-3033", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-9085", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-3947", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-503", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-1533", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3362"], "SR": 0.546875, "CSR": 0.5382653061224489, "retrieved_ids": ["mrqa_squad-train-38868", "mrqa_squad-train-81698", "mrqa_squad-train-48962", "mrqa_squad-train-23233", "mrqa_squad-train-40690", "mrqa_squad-train-74315", "mrqa_squad-train-50272", "mrqa_squad-train-44", "mrqa_squad-train-12688", "mrqa_squad-train-69196", "mrqa_squad-train-17038", "mrqa_squad-train-78344", "mrqa_squad-train-1855", "mrqa_squad-train-82082", "mrqa_squad-train-20133", "mrqa_squad-train-19926", "mrqa_newsqa-validation-3918", "mrqa_hotpotqa-validation-3755", "mrqa_newsqa-validation-3374", "mrqa_triviaqa-validation-533", "mrqa_searchqa-validation-12504", "mrqa_naturalquestions-validation-10442", "mrqa_triviaqa-validation-5859", "mrqa_searchqa-validation-6554", "mrqa_triviaqa-validation-183", "mrqa_newsqa-validation-2315", "mrqa_searchqa-validation-11366", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-6430", "mrqa_naturalquestions-validation-7250", "mrqa_triviaqa-validation-4442", "mrqa_naturalquestions-validation-2060"], "EFR": 1.0, "Overall": 0.7275749362244899}, {"timecode": 49, "before_eval_results": {"predictions": ["Children of Men", "Fernando Rey", "Rolling Stone", "chutney", "America", "Shia", "Zionism", "Gaelic", "Marvin", "the Robert F. Kennedy Bridge", "Jacob and Wilhelm Grimm", "(Jose de) San Martin", "Elizabeth Edward", "Richard Cory", "Franklin D. Roosevelt", "Perry Mason", "Italy", "Borneo", "Brazil", "a surrogate", "American Bandstand", "Sanrio", "Brazil", "Missouri University of Science and Technology", "Bob Dylan", "Dick Cheney", "vitamin D", "a car", "Rudolf", "Abraham Lincoln", "messenger", "Homeland Security", "people of the United States", "Thomas Jefferson", "Michigan", "the turn of the traditional lunisolar Chinese calendar", "Missouri River", "The 180th meridian or antimeridian", "Teri Garr", "Animals are divided by body plan into vertebrates and invertebrates", "bigflip", "chord", "Great Victoria Desert", "Nicolas cage", "Viral particles", "ASIN", "New York", "Albert Reynolds", "Clive Staples Lewis", "Ford Field in Detroit, Michigan.", "Benedict of Nursia", "The Spiderwick Chronicles", "May 30, 2005", "1945", "You Can be a Star", "thin paper-based card for competitions and plastic to conceal PINs", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "$5.5 billion", "forcibly drugging", "heavy turbulence", "April 22.", "couple's surrogate lost the pregnancy.", "Friday,", "United States"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5481928661616162}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.1111111111111111, 0.2222222222222222, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.3636363636363636, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-16374", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-9677", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-9737", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-12956", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-1673", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-7767", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-2329", "mrqa_triviaqa-validation-3139", "mrqa_triviaqa-validation-5294", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-3420", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1054"], "SR": 0.453125, "CSR": 0.5365625, "EFR": 0.9714285714285714, "Overall": 0.7215200892857143}, {"timecode": 50, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-2548", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4009", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7639", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12278", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14854", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16297", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3410", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6275", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-509", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.83203125, "KG": 0.46640625, "before_eval_results": {"predictions": ["741 weeks", "Baaghi ( English : Rebel )", "the fourth C key from left on a standard 88 - key piano keyboard", "in Rome in 336", "Viceroyalty of New Spain", "1997 ( XXXII )", "San Crist\u00f3bal, Pinar del R\u00edo Province ( now in Artemisa Province ), in western Cuba", "Yuzuru Hanyu", "four", "2001", "Eddie Van Halen", "Ozzie Smith", "the fourth quarter of the preceding year", "1986", "Archduke Franz Ferdinand of Austria", "Jacob Ormond", "William Wyler", "a sociological perspective", "the red - bed country of its watershed", "31", "1770 BC", "President of the United States", "Welch, West Virginia", "the heart's natural pacemaker", "Super Bowl LII", "the President of the United States", "nine", "a bronze statue designed by Thomas Crawford ( 1814 -- 1857 ) that, since 1863, has crowned the dome of the U.S. Capitol building in Washington, D.C.", "September 6, 2019", "Norway", "the London Symphony Orchestra and London Philharmonic", "Marley & Me", "puff", "sahara", "Edinburgh", "john Nash", "sahara", "reims", "sahuddersfield's Peter Ramsden", "Gorky", "Trey Parker and Matt Stone", "The Life of Charlotte Bront\u00eb", "The Future", "Keith Crofford", "Harvey Birdman", "Corendon Dutch Airlines", "\"From Here to Eternity\"", "\"The Spectator\".", "Anil Kapoor", "New York City Mayor Michael Bloomberg", "Unseeded Frenchwoman", "a space for aspiring entrepreneurs to brainstorm with like-minded people.", "March 3,", "September 6, 1918,", "two remaining crew members", "in rural Tennessee.", "Guinness", "(William) Clinton", "Barnard College", "a lion", "Morse code", "Kristine Tsuya Yamaguchi", "Johnson", "One Life to Live"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5766626602564102}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.923076923076923, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.25, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-2063", "mrqa_triviaqa-validation-6342", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4995", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-1084", "mrqa_searchqa-validation-8101", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-1742"], "SR": 0.46875, "CSR": 0.5352328431372548, "EFR": 0.9705882352941176, "Overall": 0.7151485906862745}, {"timecode": 51, "before_eval_results": {"predictions": ["gravitation", "Charles Oscar Waters", "March 1996", "the Rashidun Caliphs", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "Earle Hyman", "1998", "free floating", "Hasmukh Adhia", "the United States Congress declared war ( Public Law 77 - 328, 55 STAT 795 ) on the Empire of Japan", "its absolute temperature", "Joseph M. Scriven", "Strabo", "pit road speed", "5.7 million", "Magnavox Odyssey", "Mike Alstott", "to the right of the dinner plate", "1", "April 2010", "present - day southeastern Texas", "Shenzi", "1871", "the Ramones", "Georgia Nicholson ( Groome ) as she tries to find a boyfriend while also organising her fifteenth birthday party", "Beorn", "8,850 km", "Melvil Dewey in 1876", "Joseph Sherrard Kearns", "one person", "members of the gay ( LGBT ) community", "a single peptide bond or one amino acid with two peptide bonds", "Wat Tyler", "in York", "jons boron, barium, magnesium, iodine, chlorine, and potassium.", "Russia", "Tennessee", "bond", "Prague", "paul Lhote, the amateur art historian, collector, and editor of the Gazette des beaux-Arts", "four", "London", "Jenson button", "2004", "18 December 1975", "Carrefour", "Florida and Oklahoma", "Nelson County", "Pakistan intelligence institutions and its army", "onto the college campus.", "orders immigrants to carry their alien registration documents at all times", "Saturday", "breast cancer.", "Wednesday", "Dean Martin, Katharine Hepburn and Spencer Tracy", "accusations of improper or criminal conduct.", "beta", "William Faulkner", "Consumers Union Reports", "Amsterdam", "Auguste Escoffier", "William Shatner", "double curve", "Anne Rice"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6411668192918192}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.1, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8000000000000002, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6075", "mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-607", "mrqa_triviaqa-validation-1423", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-2330", "mrqa_hotpotqa-validation-4172", "mrqa_hotpotqa-validation-76", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2369", "mrqa_newsqa-validation-2843", "mrqa_searchqa-validation-8345", "mrqa_searchqa-validation-13488", "mrqa_searchqa-validation-11102"], "SR": 0.515625, "CSR": 0.5348557692307692, "retrieved_ids": ["mrqa_squad-train-22480", "mrqa_squad-train-67832", "mrqa_squad-train-55196", "mrqa_squad-train-32134", "mrqa_squad-train-30108", "mrqa_squad-train-62976", "mrqa_squad-train-31851", "mrqa_squad-train-44684", "mrqa_squad-train-27290", "mrqa_squad-train-10300", "mrqa_squad-train-55316", "mrqa_squad-train-45412", "mrqa_squad-train-5762", "mrqa_squad-train-68362", "mrqa_squad-train-1839", "mrqa_squad-train-55305", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-6840", "mrqa_searchqa-validation-11296", "mrqa_hotpotqa-validation-3870", "mrqa_searchqa-validation-14337", "mrqa_hotpotqa-validation-2452", "mrqa_hotpotqa-validation-5598", "mrqa_triviaqa-validation-137", "mrqa_naturalquestions-validation-5497", "mrqa_searchqa-validation-14874", "mrqa_hotpotqa-validation-4180", "mrqa_naturalquestions-validation-1491", "mrqa_newsqa-validation-3394", "mrqa_squad-validation-6670", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-4100"], "EFR": 1.0, "Overall": 0.7209555288461538}, {"timecode": 52, "before_eval_results": {"predictions": ["Martha Coolidge", "bioelectromagnetics", "Serial (Bad) Weddings", "Dizzy Dean", "George Raft (born George Ranft; September 26, 1901 \u2013 November 24, 1980)", "jesse", "Javed Miandad", "the Shriners", "DJ Scotch Egg", "67,575", "Target Corporation", "Gatwick", "12\u201318", "white goat", "9 February 1971", "810", "Wal-Mart Canada Corp.", "the heaviest album of all", "Borwick railway station", "1834", "5.3 million", "1943", "more than 250 million copies worldwide", "7 June 1985", "1983", "Around 200,000 passengers", "orchestrated the first movement piano sketch", "Danny Glover", "Sam Kinison", "Indianapolis Motor Speedway", "Sun Belt Conference", "The cinema of Russia", "erosion", "more than 1,000", "`` Nelson's Sparrow ''", "nearby objects show a larger parallax than farther objects when observed from different positions", "coercivity", "Jimmy Flynn", "H.L. Hunley", "Sally Field", "Homo sapiens", "tartar", "Ithaca", "Andrew Lloyd Webber", "jesse", "caryatid", "Ceylon", "piccadilly", "London and Buenos Aires", "Yusuf Saad Kamel", "SSM Cardinal Glennon Children's Medical Center.", "Patrick McGoohan, the actor who created one of British television's most surreal thrillers,", "Brown-Waite", "re-impose order", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "Mashhad", "synapses", "Hungarian Rhapsody", "Antwerp", "a planet called Ashlar", "blackbirds", "The Sound of Music", "a new Broom", "Angola"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5796502976190476}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.14285714285714288, 0.0, 0.4, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.2857142857142857, 1.0, 1.0, 0.08333333333333333, 0.4, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-88", "mrqa_hotpotqa-validation-5627", "mrqa_hotpotqa-validation-3864", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-3475", "mrqa_hotpotqa-validation-74", "mrqa_hotpotqa-validation-3641", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1837", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-3679", "mrqa_naturalquestions-validation-10618", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-3756", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-1383", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-8370"], "SR": 0.484375, "CSR": 0.5339033018867925, "EFR": 0.9696969696969697, "Overall": 0.7147044293167524}, {"timecode": 53, "before_eval_results": {"predictions": ["1 April 1985", "Park Seo-joon", "Marty Ingels", "Chief Creative Officer", "Margarida", "Norbertine", "Richard II", "Coahuila, Mexico", "Province of Canterbury", "born 2 May 2015", "Eenasul Fateh", "Ladies' Code", "all U.S. territories", "Jung Yun-ho", "Debbie Isitt", "Clive Staples Lewis", "2017", "Adam Karpel", "evangelical", "the Battelle Energy Alliance", "2012", "Sky News", "Bardot", "Steve Carell", "Jesus", "2,615", "mermaid", "private", "General Manager", "Bellagio and The Mirage", "Aksel Sandemose", "January 30, 1930", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "90 \u00b0 N", "1830", "Authority", "1984", "Elizabeth Dean Lail", "Times Square in New York City", "1898", "Nowhere Boy", "oromia", "John Napier", "Watt-hours", "South Africa", "mark", "British Airways", "3", "al Fayed's security team", "testing-fire a long-range missile", "police dogs", "February 2008", "outside his house in Najaf's Adala neighborhood", "she was a pain in the ass,\"", "NATO's Membership Action Plan", "Three", "Agatha Christie", "the union", "Converse", "Trinidad and Tobago", "ego", "Ahab", "Solitude", "The Vision"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5155822425123896}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.19047619047619047, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-2784", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2831", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-1632", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-3031", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4844", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-64", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-811", "mrqa_triviaqa-validation-2698", "mrqa_triviaqa-validation-2441", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-16360", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-12630"], "SR": 0.421875, "CSR": 0.5318287037037037, "EFR": 0.9459459459459459, "Overall": 0.7095393049299299}, {"timecode": 54, "before_eval_results": {"predictions": ["Israel", "The Harvest of a Quiet Eye", "South Africa", "\"A Dangerous Man: Lawrence After Arabia\u2019", "John J. Pershing", "Barbra Streisand", "Australia and England", "You May Also Like", "Ken Purdy", "Switzerland", "an sense of loyalty and dedication to a specific person or persons (e.g. Superman and his loyalty to the people of Earth, Zorro and his respect for his people)", "ethiopian", "mathematician", "cyclops", "La Toya", "Adolf Hitler", "Brits", "a rat", "four", "oakum", "1912", "She and her navigator, Fred Noonan", "13", "B\u00e9la Bart\u00f3k", "Tom Watson", "Libya", "ethiopian", "Norwegian", "radish", "ethiopian", "Amsterdam", "ethiopian", "Lew Brown", "Paracelsus", "a section of the Torah ( Five Books of Moses ) used in Jewish liturgy during a single week", "2014", "Times Square in New York City west to Lincoln Park in San Francisco", "Luther Ingram", "The Constitution of India", "Middle Eastern alchemy", "three", "Russian Ark", "Harry F. Sinclair", "diving duck", "ABC", "four months in jail", "Jimmy Collins ( Jeremy Jordan) and Kyle Bishop ( Andy Mientus)", "1974", "Alexandre Caizergues, of France,", "Buenos Aires.", "Vicente Carrillo Leyva,", "Prague", "your environmental efforts make even more impact than Harrison Ford's chest.", "Judge Sonia Sotomayor,", "269,000", "the Revolutionary Armed Forces of Colombia, better known as FARC,", "Lech Walesa", "outside", "Sweden", "Hiroshima", "Paraguay", "The Tops", "Roger Williams", "James Joyce"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5734100877192982}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.19999999999999998, 0.13333333333333333, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3410", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-338", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-3334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-360", "mrqa_triviaqa-validation-4677", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1394", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-6490", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1037", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-1239"], "SR": 0.46875, "CSR": 0.5306818181818183, "retrieved_ids": ["mrqa_squad-train-74566", "mrqa_squad-train-74659", "mrqa_squad-train-10254", "mrqa_squad-train-72618", "mrqa_squad-train-28919", "mrqa_squad-train-51168", "mrqa_squad-train-4316", "mrqa_squad-train-14652", "mrqa_squad-train-60453", "mrqa_squad-train-11940", "mrqa_squad-train-7000", "mrqa_squad-train-45214", "mrqa_squad-train-80900", "mrqa_squad-train-17917", "mrqa_squad-train-60929", "mrqa_squad-train-86289", "mrqa_triviaqa-validation-1430", "mrqa_newsqa-validation-3854", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-2111", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6247", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-10618", "mrqa_triviaqa-validation-2099", "mrqa_searchqa-validation-568", "mrqa_newsqa-validation-2253", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-5927", "mrqa_triviaqa-validation-6840", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-23"], "EFR": 1.0, "Overall": 0.7201207386363636}, {"timecode": 55, "before_eval_results": {"predictions": ["severe flooding", "Peshawar,", "Australia and New Zealand", "hanged in 1979", "Kurdish group", "the District of Columbia National Guard,", "the simple puzzle video game,", "The flooding was so fast that the thing flipped over,\"", "Elizabeth Birnbaum", "Dubai", "that the four women who Krazy-Glued a cheater's penis to his stomach were way harsh and beyond psycho.", "42 years old", "She's very new and involves repairing my leaky valve using a clip device, without open heart surgery, so that my heart will function better.", "to fritter his cash away on fast cars, drink and celebrity parties.\"", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.\"", "Six members", "Brown-Waite", "tennis", "Alinghi", "Michael Schumacher", "Arnold Drummond", "Hong Kong,", "Roy Foster", "I'm certainly not nearly as good of a speaker as he is.\"", "Liverpool", "Maria of South Carolina,", "Mandi Hamlin,", "to put a lid on the marking of Ashura this year.", "10 municipal police officers", "Herman Cain", "Two", "maintain order and its tactics in doing so.", "Fall 1998", "October 27, 1904", "February 9, 2018", "June 3, 1937", "the Abegweit Passage of Northumberland Strait", "the only common requirement is that the shooter must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "interstate communications by radio, television, wire, satellite, and cable", "starch", "Eva Marie", "1925", "bear", "Bahrain", "triathlon", "Kenya", "Kevin Spacey", "noises Off", "black nationalism", "Great Lakes and Midwestern", "Jarome Iginla", "1980", "MG", "an expeditionary EA-18G Growler squadron of the United States Navy based at Naval Air Station Whidbey Island, Washington", "the Salzburg Festival", "Fiat Group", "Washington Irving", "Jamaica Inn", "I can't walk out", "after the fact", "drums", "Warren Schmidt", "birds", "Big Momma"], "metric_results": {"EM": 0.5, "QA-F1": 0.6075429459064328}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-1499", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-917", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2758", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-602", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-5233", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-7031", "mrqa_searchqa-validation-4167"], "SR": 0.5, "CSR": 0.5301339285714286, "EFR": 1.0, "Overall": 0.7200111607142857}, {"timecode": 56, "before_eval_results": {"predictions": ["Straits of Tiran", "Basil Feldman,", "antelope", "ernest of WTA", "Joseph V. Micallef", "Czech Republic", "Oliver!", "testicles", "driving Miss Daisy", "Oklahoma", "cunard\u00ef\u00bf\u00bds", "Addis Ababa in Ethiopia or Khartoum in Sudan", "Pink Floyd", "Batman", "basketball", "geyser", "Craig Kilborn", "rum Cocktails", "wigan", "tennis", "beetles", "pilot of the Future", "Josh Brolin", "Pearl Slaghoople", "sistan-Baluchistan", "Flybe", "egypt", "tuppence", "Brat Pack", "jujitsu", "weasel", "prince eddy", "Gary Player", "May 2017", "Southport, North Carolina", "Columbia River Gorge", "Doug Pruzan", "October 23, 2017", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "9 February 2018", "Liga MX", "Attorney General and as Lord Chancellor of England", "Cosmopolitan of Las Vegas", "Mel Blanc", "Republican", "Tak and the Power of Juju", "jet-powered tailless delta wing high-altitude strategic bomber", "Phil Spector", "30-minute", "12 million", "free enterprise", "11,", "1,073", "Argentine", "The Da Vinci Code", "breast self-examination.\"", "Port Washington", "caution", "the Fountain of Youth", "John", "Abbeville", "Bangkok", "Typhoid Mary", "a cure or solution for any illness or problem"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5536160714285714}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3118", "mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-5969", "mrqa_triviaqa-validation-6540", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-1424", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-5067", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-899", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-8783", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-510", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-2913", "mrqa_searchqa-validation-7030", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-3732", "mrqa_searchqa-validation-3538", "mrqa_searchqa-validation-7493"], "SR": 0.515625, "CSR": 0.5298793859649122, "EFR": 1.0, "Overall": 0.7199602521929824}, {"timecode": 57, "before_eval_results": {"predictions": ["kinshasa", "lips", "in recent years.", "piscinae", "Richard Marx", "zulu", "k Karachi", "taegeukki", "Vim Tonic", "a\u1e25mad \u1e24asan al-Bakr", "vickers", "anthropocene", "menorah", "cosmos", "Matlock", "persuasion", "Planet of Mars", "suvajit Mustafi", "eye", "8", "feldman", "lung", "wake", "\"Sugar Baby Love\"", "fifteen", "canisius College", "george v", "significant achievement", "ben Whishaw", "points based scoring", "jest", "basil", "4 January 2011", "hairpin turn", "1986", "in cell - mediated, cytotoxic innate immunity", "a child's favourite colour", "Rosalind Bailey", "1839", "22 \u00b0 00 \u2032 N 80 \u00b0 00", "comedy", "hiphop", "Juventus", "Kennebec County", "Magnus Carlsen", "jocelyn", "The Walking Dead", "Pylos and Thebes", "\" Raw Power,\"", "digging at the site", "an impromptu memorial for the late singer", "western United States.", "at the bottom of the hill", "201-262-2800.", "legitimacy of that race.", "along a three-mile stretch on and near the LBJ Freeway,", "New Balance Corporation", "dehydration in the Sahara", "Mystery Science", "\"Baby Got Back In The U.S.S.\"", "Wales", "Deep Purple", "the Glorious Revolution of 1688", "Mexico"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5039930555555556}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 1.0, 0.8, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.8, 0.8000000000000002, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6831", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-930", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-7584", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-4483", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-5451", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5299", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5412", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-4095", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-15364", "mrqa_searchqa-validation-15959", "mrqa_searchqa-validation-11046"], "SR": 0.40625, "CSR": 0.5277478448275862, "retrieved_ids": ["mrqa_squad-train-76881", "mrqa_squad-train-7756", "mrqa_squad-train-37852", "mrqa_squad-train-83040", "mrqa_squad-train-67421", "mrqa_squad-train-41556", "mrqa_squad-train-21365", "mrqa_squad-train-43031", "mrqa_squad-train-10933", "mrqa_squad-train-65586", "mrqa_squad-train-79984", "mrqa_squad-train-13959", "mrqa_squad-train-25931", "mrqa_squad-train-62929", "mrqa_squad-train-64891", "mrqa_squad-train-32510", "mrqa_squad-validation-9846", "mrqa_naturalquestions-validation-2319", "mrqa_newsqa-validation-2112", "mrqa_squad-validation-8278", "mrqa_hotpotqa-validation-1872", "mrqa_squad-validation-6874", "mrqa_triviaqa-validation-183", "mrqa_searchqa-validation-6054", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-3581", "mrqa_naturalquestions-validation-1784", "mrqa_hotpotqa-validation-5535", "mrqa_searchqa-validation-15565", "mrqa_squad-validation-3540", "mrqa_naturalquestions-validation-4664", "mrqa_searchqa-validation-3272"], "EFR": 0.9473684210526315, "Overall": 0.7090076281760436}, {"timecode": 58, "before_eval_results": {"predictions": ["Payaya Indians", "March 11, 2018", "Wisconsin", "1535", "Erica Rivera", "International Baccalaureate", "Tommy James and the Shondells", "alcohol or smoking", "Paul Lynde", "United States, its NATO allies and others", "James Rodr\u00edguez", "Hugh S. Johnson", "The Miracles", "spontaneously", "the buttock and down the lower limb", "Detroit", "Cadillac", "Imperial Japan", "Salman Khan", "`` Far Away ''", "development of electronic computers", "the defendant's negligence was gross, that is, it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "Sri Lanka Podujana Peramuna", "Hold On", "alternative rock", "November 1999", "Hal Derwin", "the government - owned Panama Canal Authority", "Scott Bakula as Dwayne `` King '' Cassius Pride", "round", "William Wyler", "Woody Paige", "William Blake", "Miranda v. Arizona", "massively multiplayer online games", "Buddha", "new German U-Stadtbahns", "geography", "Much Ado About Nothing", "Lancashire", "1926 Paris", "flew solo to Scotland", "Belarus", "Walcha", "James I of England", "Golden Calf", "James A. Garfield", "nearly 80 years", "Kurt Cobain,", "the outdoors, particularly if they have a garden to eat from,", "British capital's other two airports, Stansted and Gatwick,", "Goa", "56,", "notified of the results by a chaplain about 1:45 p.m.", "The Lost Symbol", "Iranian city of Mashhad", "\"And damn'd be him that first cries ( Hold, enough!\"", "out-of-body experience", "Athol Fugard", "Jonah", "Treasure Island", "Death Watch", "resolution", "Boiling Lake"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6125616632871097}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9387755102040817, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.125, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-6821", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-575", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-3854", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-3727", "mrqa_searchqa-validation-8279", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9151"], "SR": 0.53125, "CSR": 0.5278072033898304, "EFR": 0.9666666666666667, "Overall": 0.7128791490112995}, {"timecode": 59, "before_eval_results": {"predictions": ["dismissal.", "Long Island", "Robert Redford's", "Dennis Davern,", "Harry Nicolaides,", "Russia and China", "Pakistan's", "free fixes for the consumer.", "heavy turbulence", "Naples", "outside influences in next month's run-off election", "Miss USA Rima Fakih", "\" Teen Patti\"", "looking at designating the sedative as a \"scheduled\" drug,", "President Obama", "$55.7 million", "to ensure there is no shortage of the drug while patients wait for an approved product to take its place.", "$75 for full-day", "autonomy", "Dick Cheney,", "Jenny Sanford,", "international search team", "Animal Planet", "would have significant public health experience and understand how these processes work, how meat enters the chain of commerce.", "Monday", "used-luxury", "the leader of a drug cartel", "books", "Dr. Jennifer Arnold and husband Bill Klein,", "Sabina Guzzanti", "a bag", "general secretary", "January to May 2014", "Bob Dylan, George Harrison", "long - range patrol planes that were sent out by those navies to scout for enemy warships, cargo ships, and troop ships", "Arunachal Pradesh", "chain elongation", "1926", "since 3, 1, and 4", "Sreejita De", "Il Divo", "sugar cane", "perfumes", "november", "Albert einstein", "Nancy Astor", "mmorpg", "in jury Lane, Haverfordwest", "1,462", "Ben Ainslie", "The Dalton Gang", "HackThis Site.org", "\" Shut Up\"", "2016 U.S. Senate election", "Mark Anthony \"Baz\" Luhrmann", "Two Pi\u00f1a Coladas", "Martin Luther", "Penn State", "Joe Hill", "an eagle", "the National Archives Building", "Hera", "nag", "the East River"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6632427647783251}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.25, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.9714285714285714, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.9333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666665, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-2639", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-1826", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-961", "mrqa_naturalquestions-validation-4028", "mrqa_naturalquestions-validation-388", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-1366", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1501", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2989", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-8933", "mrqa_searchqa-validation-2662"], "SR": 0.515625, "CSR": 0.5276041666666667, "EFR": 1.0, "Overall": 0.7195052083333333}, {"timecode": 60, "UKR": 0.734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-205", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2407", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2684", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3167", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5774", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3947", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4646", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2416", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-515", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-11869", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2381", "mrqa_searchqa-validation-2582", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-289", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3153", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8737", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2376", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-183", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3784", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.822265625, "KG": 0.48046875, "before_eval_results": {"predictions": ["Bronnie", "Bonnie Lipton", "Benzodiazepines", "19 June 2018", "in the central plains", "god Neptune's trident", "ase", "During Hanna's recovery masquerade celebration", "Gibraltar", "1928", "Coordinated Universal Time", "Jeff Fungus", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Beijing", "2017 - 18 season", "summer of 1979", "Johnny Cash", "$2 million in 2011, with a winner's share of $315,600", "Mel Gibson", "assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "on the Fox Ranch in Malibu Creek State Park, northwest of Los Angeles", "rum, fruit juice, and syrup or grenadine", "a Scandinavian patronymic surname, meaning son of Hans", "port of Nueva Espa\u00f1a", "Kelly Reno", "thymine ( T )", "the illegitimate son of Ned Stark", "British Army soldiers shot and killed people while under attack by a mob", "near the city of Cairo, Illinois", "ice giants", "Christopher Columbus", "the Government House at New Delhi", "lake windermere", "trees", "Gremlins", "Audi A4", "tartare", "Babylonian Empire", "black", "newbury", "Taliban's Islamic Emirate of Afghanistan", "2009", "2009", "Humberside Airport", "\"King of Cool\"", "Elizabeth Taylor", "Patricia Jude Francis Kensit", "neuro-orthopaedic Irish veterinary surgeon", "Arthur E. Morgan III,", "\"procedure on her heart,\"", "back at work", "called it the largest and perhaps most sophisticated ring of its kind in U.S. history.", "As mayor of Seoul from 2002 to 2004,", "Russian concerns that the defensive shield could be used for offensive aims.", "the militants are suspected of launching attacks inside Pakistan and in neighboring Afghanistan from their haven in the mountainous tribal region along the northwestern border.", "see my kids graduate from this school district.", "Yellowstone National Park", "the Austrian parliament", "the Democratic Ward", "John Knox", "the Catholic Church", "the pelvis", "the Zapata", "Anderson Cooper"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6108488959711786}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.7272727272727272, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6153846153846153, 1.0, 0.782608695652174, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 0.8, 1.0, 0.1090909090909091, 0.14285714285714288, 1.0, 0.08695652173913045, 0.28571428571428575, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-6763", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-10004", "mrqa_naturalquestions-validation-800", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-4394", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3885", "mrqa_hotpotqa-validation-1219", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2695", "mrqa_searchqa-validation-9270", "mrqa_searchqa-validation-8946", "mrqa_searchqa-validation-12686", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-2333"], "SR": 0.484375, "CSR": 0.5268954918032787, "retrieved_ids": ["mrqa_squad-train-986", "mrqa_squad-train-48418", "mrqa_squad-train-7553", "mrqa_squad-train-64138", "mrqa_squad-train-16942", "mrqa_squad-train-42129", "mrqa_squad-train-42044", "mrqa_squad-train-20366", "mrqa_squad-train-39488", "mrqa_squad-train-40255", "mrqa_squad-train-70203", "mrqa_squad-train-14981", "mrqa_squad-train-9966", "mrqa_squad-train-20737", "mrqa_squad-train-58511", "mrqa_squad-train-8920", "mrqa_searchqa-validation-8370", "mrqa_hotpotqa-validation-1326", "mrqa_newsqa-validation-2078", "mrqa_hotpotqa-validation-1533", "mrqa_triviaqa-validation-1511", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-3357", "mrqa_triviaqa-validation-575", "mrqa_searchqa-validation-2997", "mrqa_newsqa-validation-2723", "mrqa_hotpotqa-validation-1844", "mrqa_searchqa-validation-15986", "mrqa_naturalquestions-validation-922", "mrqa_hotpotqa-validation-4545", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-2856"], "EFR": 0.9393939393939394, "Overall": 0.7006797612394436}, {"timecode": 61, "before_eval_results": {"predictions": ["StubHub Center in Carson, California", "Haiti", "1038", "3.5 mya", "Arkansas", "on the lateral side", "David Motl", "Andrew Lloyd Webber", "Anne Murray", "much of the European industrial infrastructure had been destroyed", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "the fourth C key from left on a standard 88 - key piano keyboard", "Valens", "Nick Faldo", "Tim Allen", "won", "the song was used as the theme song for the Michael Douglas film, The Jewel of the Nile", "Thomas Jefferson", "the symbol \u00d7", "Bobby Darin", "Buffalo Bill", "Puerto Rico", "late January or early February", "marker icon", "Nalini Negi", "Samaria", "Andreas Vesalius", "Shaw", "Southampton ( 1902, then in the Southern League )", "Wisconsin", "a central place in Christian eschatology", "~ 3.5 million years old", "chambodia", "even numbers", "Welcome Stranger", "\"Black Swan\"", "brixham", "Alison Moyet", "Tacoma", "jaws", "Manchester", "black nationalism", "Martin Stadium", "January 28, 2016", "Buckingham Palace", "2013", "chicken", "Isabella", "\"Teen Patti\" (\"Card Game\")", "parents", "Jaime Andrade,", "Facebook", "750", "Scarlett Keeling", "they don't feelMisty Cummings has told them everything she knows.", "April 28,", "Doc Holliday", "Ezra", "The USA Network's original grassroots talent search", "a pacemaker", "the Nationalist Party", "the SAT", "Chuck Berry", "rubles"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5320638562826063}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, false, false, false, false, true, false], "QA-F1": [0.7142857142857143, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 0.0, 1.0, 0.46153846153846156, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5000000000000001, 0.0, 1.0, 0.2222222222222222, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.0, 0.7, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-7027", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-3935", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7920", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-833", "mrqa_triviaqa-validation-797", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-3909", "mrqa_triviaqa-validation-7755", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3335", "mrqa_hotpotqa-validation-1453", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-879", "mrqa_searchqa-validation-4373", "mrqa_searchqa-validation-15666", "mrqa_searchqa-validation-8515", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-3147"], "SR": 0.453125, "CSR": 0.5257056451612903, "EFR": 1.0, "Overall": 0.712563004032258}, {"timecode": 62, "before_eval_results": {"predictions": ["Tuesday", "the legitimacy of that race.", "2-1", "Majid Movahedi,", "women and breast cancer.", "Kris Allen,", "a tanker", "the only goal of the game to ensure Hamburg remain in touch with the top three", "took an anti-doping test after a Serie A game at Roma", "Little Rock Central High School in Arkansas.", "5,600 people every year, and about 10 percent of those cases are hereditary.", "prisoners at the South Dakota State Penitentiary and ultimately delivered in Iraq", "Jacob Zuma,", "Rihanna", "allegedly grabbed a pupil by the throat and threw her against a wall,", "chairman of the House Budget Committee,", "be silent.", "are standing by to provide security as needed.", "peanuts,", "for early detection and helping other women cope with the disease.", "56,", "English", "suicides", "the shelling of the compound", "Stephen Worgu", "four decades", "in legislative elections in Buenos Aires.", "Turkey,", "a bank", "was sexually assaulting a toddler and capturing it on videotape years ago,", "40 militants and six Pakistan soldiers dead,", "Sunday", "Massachusetts", "Emma Watson", "an extension of the Hypertext Transfer Protocol ( HTTP ) for secure communication over a computer network, and is widely used on the Internet", "mid-1980s", "in pilgrimages to Jerusalem", "Meg Optimus", "Ric Flair", "James Martin Lafferty", "Purple Rain", "a\u00e7a\u00ed", "Lundy", "insulin", "Jerusalem", "Captain Jean-Luc Picard", "queen elizabeth of england", "a karst cave", "Nathan Bedford Forrest", "Mark Neveldine and Brian Taylor", "the art of diving", "KXII", "\"Bertram, count of Rousillon\" (an Elisabethan English misspelling for Roussillon)", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "National Basketball Development League", "the Atlanta Braves, and the Tampa Bay Devil Rays", "the London Bridge", "Passover", "Hormel Foods", "a penny", "a dowry", "1849", "beryl", "Bastille Day"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6506580908341744}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [0.5, 1.0, 1.0, 0.0, 0.6, 1.0, 1.0, 0.4444444444444444, 0.0, 0.2222222222222222, 0.14285714285714288, 0.7058823529411764, 1.0, 1.0, 0.0, 0.0, 1.0, 0.7692307692307693, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.1111111111111111, 0.923076923076923, 0.0, 1.0, 0.0, 0.56, 0.0, 0.46153846153846156, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4210526315789474, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-232", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2758", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3875", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6522", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-3010", "mrqa_triviaqa-validation-7232", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5503", "mrqa_searchqa-validation-11056"], "SR": 0.515625, "CSR": 0.5255456349206349, "EFR": 1.0, "Overall": 0.712531001984127}, {"timecode": 63, "before_eval_results": {"predictions": ["Mandi Hamlin", "Dan Parris,", "publicly criticized his father's parenting skills.", "Cameroon,", "martial arts,", "he said Chaudhary's death was warning to management.", "$24.1 million,", "Mokotedi Mpshe,", "political dead-end", "Daniel Wozniak,", "Kgalema Motlanthe,", "Sen. Patty Murray (D-Washington), chairwoman of the Senate Committee on Veterans' Affairs,", "Hundreds of women protest child trafficking and shout anti-French slogans", "September,", "March 22,", "Amanda Knox and her Italian former boyfriend, Raffaele Sollecito,", "The woman", "Hong Kong's Victoria Harbor", "hot and humid and it rains almost every day of the year.", "Garth Brooks", "two-state solution", "more than 1.2 million", "Michelle Obama", "around 3.5 percent of global greenhouse emissions.", "off the coast of Dubai", "death of cardiac arrest on June 25.", "Tuesday night's short program", "the \" Michoacan Family,\"", "The forward's lawyer", "New York", "a growing number of GOP state senators", "stand down.", "bow bridge", "Andy Serkis", "Nicolas Anelka", "the inferior thoracic border", "1987", "helps scientists better understand the spread of pollution around the globe", "to turn our will and our lives over to the care of God as we understood Him", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "the Monkees", "a crow", "Saturday Night Live", "Racing Cars", "Mujib", "Peru", "dogs", "red", "\"Cymbeline\"", "Indiana University", "University of Texas Longhorns", "Galo (], \"Rooster\")", "Vernier, Switzerland", "Patrick Dempsey and Amanda Peterson", "Indian", "A play-by-post role-playing game", "Eleanor Rigby", "an online auction", "La Bohme", "Maude", "Prince Edward Island", "Chief Justice of the Supreme Court", "beef", "Bach"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6825773902821317}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.5714285714285715, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.27586206896551724, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.888888888888889, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2526", "mrqa_newsqa-validation-4055", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-1229", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-7088", "mrqa_searchqa-validation-13505"], "SR": 0.5625, "CSR": 0.526123046875, "retrieved_ids": ["mrqa_squad-train-74325", "mrqa_squad-train-83178", "mrqa_squad-train-43570", "mrqa_squad-train-33824", "mrqa_squad-train-35282", "mrqa_squad-train-36995", "mrqa_squad-train-57931", "mrqa_squad-train-7333", "mrqa_squad-train-8653", "mrqa_squad-train-39817", "mrqa_squad-train-33275", "mrqa_squad-train-65495", "mrqa_squad-train-52053", "mrqa_squad-train-5454", "mrqa_squad-train-1975", "mrqa_squad-train-79794", "mrqa_squad-validation-6962", "mrqa_naturalquestions-validation-8896", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-11296", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-1301", "mrqa_triviaqa-validation-7328", "mrqa_searchqa-validation-9112", "mrqa_hotpotqa-validation-1778", "mrqa_newsqa-validation-3060", "mrqa_hotpotqa-validation-5799", "mrqa_newsqa-validation-3302", "mrqa_naturalquestions-validation-2024", "mrqa_searchqa-validation-743", "mrqa_naturalquestions-validation-8851", "mrqa_newsqa-validation-856"], "EFR": 1.0, "Overall": 0.712646484375}, {"timecode": 64, "before_eval_results": {"predictions": ["July 18, 1994,", "had the surgery December 13 after lumpectomies failed to eradicate her breast cancer.", "\"E! News\"", "1,073", "Security officer Stephen Johns reportedly opened the door for the man police say", "acid attack", "concerned that the legislation will foster racial profiling,", "have", "The same Judaism that produced Martin Buber, Emanuel Levinas, or Primo Levi also produced the Stern Gang, Meir Kahane and Baruch Goldstein.", "July 4.", "different women coping with breast cancer", "co-writing credits", "a Nazi concentration camp,", "The Intertropical Convergence", "heavy flannel or wool", "the pirates", "Phoenix, Arizona,", "His sole reason for being on Flight 253 was to kill all of the passengers and himself.", "Jacob", "Former detainees", "Democrat", "Darrel Mohler", "United States", "The sole survivor of the crash", "12-hour-plus shifts of backbreaking labor", "former CEO of an engineering and construction company", "Madonna", "in the head", "Sen. Barack Obama", "Tuesday,", "MISS YOU! WE LOVE YOU MICHAEL!", "\"Stagecoach\"", "Cadillac", "Speaker of the House of Representatives", "the world's second most populous country after the People's Republic of China", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "a phrase used in elocution teaching to demonstrate rounded vowel sounds", "Eddie Murphy", "non-ferrous", "The pia mater", "Phil Lynott", "Lohengrin", "Mars", "a jumper", "Department of Justice", "Uganda", "aqaba Governorate", "South Dakota", "National Collegiate Athletic Association", "Christopher McCulloch", "Argentinian", "\"Waiting for Guffman\"", "villanelle", "2011", "I Am Furious", "Liam Cunningham", "\"Hey, I heard you missed us, we're back\"", "Donkey Kong", "oxygen", "the Thirty Years' War", "Massachusetts", "Vice Presidents", "Aramaic", "Uvula"], "metric_results": {"EM": 0.5, "QA-F1": 0.5885720819124751}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.1111111111111111, 1.0, 1.0, 0.9565217391304348, 1.0, 0.0, 1.0, 0.08333333333333334, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.11764705882352941, 0.0, 0.8205128205128205, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-3688", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3859", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-2049", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3245", "mrqa_searchqa-validation-6072", "mrqa_searchqa-validation-8827", "mrqa_searchqa-validation-16606"], "SR": 0.5, "CSR": 0.5257211538461539, "EFR": 0.9375, "Overall": 0.7000661057692308}, {"timecode": 65, "before_eval_results": {"predictions": ["three", "two paintings by Pablo Picasso, Bjoern Quellenberg,", "Spc. Megan Lynn Touma,", "The ruling Justicialist Party,", "death squad killings carried out during his rule in the 1990s.", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "She is the Magneto to my Wolverine, the Saruman to my Frodo, the Dr. Octopus to my Spiderman.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Silvan Shalom", "Mexico", "$250,000 for Rivers' charity: God's Love We Deliver.", "Negotiators aboard a U.S. Navy warship are trying to secure the release of an American freighter captain who is being held by pirates on a lifeboat off the coast of Somalia,", "Muslim", "fractured pelvis and sacrum -- the triangular bone within the pelvis.", "capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "800,000", "Three", "Nineteen", "Paul Schlesselman of West Helena, Arkansas,", "Timothy Masters,", "Cash for Clunkers", "immediately appeal the ruling and seek a stay of the order with the U.S. Court of Appeals for the District of Columbia.", "Deputy Treasury Secretary", "The federal officers' bodies", "the content of the speech, not just the delivery.", "My family comes from a Muslim background, and we're not defined by religion,\"", "volatile zone along the equator between South America and Africa.", "Anil Kapoor's", "former general secretary of the Communist Party,", "Dr. Albert Reiter,", "extremists", "Robert Barnett,", "near major hotels and in the parking areas of major Chinese supermarkets", "Marcus Aurelius", "2018", "23 September 1889", "November 17, 1800", "nearby", "March 26, 1973", "100,000", "Alan B'Stard", "zulu warriors", "ad nausea", "Something In The Air", "Calvors", "gold", "charlie biggers", "\"The Woodentops\"", "2005", "British Bristol Olympus turbojet", "Republic of Ireland national team", "Democratic", "paracyclist", "pubs, bars and restaurants", "Venice", "Albert Park", "\"legal tort\"", "Willy Ronis", "Maine", "Toronto", "(Andrew) Johnson", "&", "Anne", "ear"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5675436600436601}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.22222222222222224, 1.0, 0.5, 0.4615384615384615, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.07407407407407407, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 0.30303030303030304, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07999999999999999, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-1988", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-2424", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-2330", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2414", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-6304", "mrqa_triviaqa-validation-5378", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-3053", "mrqa_hotpotqa-validation-3608", "mrqa_searchqa-validation-3309", "mrqa_searchqa-validation-11525", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4774", "mrqa_searchqa-validation-11877", "mrqa_searchqa-validation-3776"], "SR": 0.453125, "CSR": 0.5246212121212122, "EFR": 0.9714285714285714, "Overall": 0.7066318317099567}, {"timecode": 66, "before_eval_results": {"predictions": ["Bill Ponsford", "Algonquian tribes", "The southernmost large city in Europe,", "a large portion of rural Maine, published six days per week in Bangor, Maine.", "the Northern Wars", "\u00c6thelwald Moll", "Glam metal", "Mitsubishi", "\"Lend a hand \u2014 care for the land!\"", "Marc Bolan", "Mineola", "\"lo Stivale\" (the Boot)", "Arizona State University.", "Jack Thomas Chick", "Milk Barn Animation", "\"Green Chair\"", "The Apple iPod+HP", "Nero", "1692", "London", "duck", "Jay Chou", "July 14, 2009", "1790", "2009", "coal town in McDowell County", "Tianhe Stadium", "Louis \"Louie\" Zamperini", "$26 billion", "five", "Alain Robbe-Grillet", "Winter Haven", "2007", "Dan Aykroyd", "a flash music video featuring an animated dancing banana was created", "a Celtic people living in northern Asia Minor", "the S - stage of interphase", "Orwell", "6 P", "Austria - Hungary", "Prince Edward Island", "Usain Bolt", "york", "john masefield", "cherries", "Philippines", "george vi", "paddington bear", "1981", "The lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "Dolgorsuren Dagvadorj,", "July", "Ronaldinho", "Anne Frank,", "a review of state government practices completed in 100 days.", "the abduction of minors.", "(Jose de) San Martin", "a pantaloon", "(Henry) Puccini", "a plus word", "Cervantes", "Versailles", "the Orioles", "a"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6553104575163398}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-3893", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-9670", "mrqa_naturalquestions-validation-10330", "mrqa_triviaqa-validation-5110", "mrqa_triviaqa-validation-2310", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-1941", "mrqa_searchqa-validation-7536", "mrqa_searchqa-validation-3284", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-8137"], "SR": 0.578125, "CSR": 0.525419776119403, "retrieved_ids": ["mrqa_squad-train-27613", "mrqa_squad-train-73945", "mrqa_squad-train-82613", "mrqa_squad-train-63609", "mrqa_squad-train-4609", "mrqa_squad-train-12485", "mrqa_squad-train-82166", "mrqa_squad-train-61165", "mrqa_squad-train-20826", "mrqa_squad-train-43696", "mrqa_squad-train-62904", "mrqa_squad-train-44336", "mrqa_squad-train-85533", "mrqa_squad-train-54537", "mrqa_squad-train-45249", "mrqa_squad-train-74869", "mrqa_hotpotqa-validation-3332", "mrqa_newsqa-validation-2552", "mrqa_triviaqa-validation-6608", "mrqa_naturalquestions-validation-335", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-14874", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-7593", "mrqa_newsqa-validation-3519", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-659", "mrqa_hotpotqa-validation-68", "mrqa_newsqa-validation-3702", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-798", "mrqa_searchqa-validation-5787"], "EFR": 1.0, "Overall": 0.7125058302238806}, {"timecode": 67, "before_eval_results": {"predictions": ["Sicily", "Diego Maradona", "Mariah Carey", "Rear-Admiral of the Navy", "Bangladesh", "Mayflower", "Union of Post Office Workers", "Minder", "pongo", "River Wensum", "France", "Taiwan", "a hand", "rhinos", "Evelyn Glennie", "New Zealand", "carousel", "capital of Algeria", "land between two rivers", "narcolepsy", "Carl Smith", "bing.com", "jump jump", "Cubism", "Pink Floyd", "Brian Deane", "Kinshasa, Zaire", "Dublin", "Battle of Agincourt", "weir weir", "The Good Life", "javelin throw", "volcanic activity and then gradually moves away from the ridge", "a maritime signal, indicating that the vessel flying it is about to leave", "in 2003", "primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Mumbai Rajdhani Express", "Stephen Curry of Davidson", "the bank's own funds and signed by a cashier", "Robyn", "Delphine Software of France", "Taeko Ikeda", "MTV Russia", "1874", "2006", "11 Grands Prix wins and 68 podiums", "sarod", "National Hockey League", "Jaime Andrade", "to kill members of the Zetas cartel", "Patrick McGoohan,", "30", "her father's", "Friday", "images of the small girl being sexually assaulted.", "the Nazi war crimes suspect", "Johns Hopkins", "Cardiff", "(Norah) Jones", "The Haflinger", "Elsinore", "The X-Files", "Rita Mae Brown", "Civil War"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6165449134199135}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.3636363636363636, 1.0, 0.6666666666666666, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4641", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6927", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-2644", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-2674", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-4538", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5587", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-3774", "mrqa_newsqa-validation-829", "mrqa_searchqa-validation-1152"], "SR": 0.515625, "CSR": 0.5252757352941176, "EFR": 1.0, "Overall": 0.7124770220588236}, {"timecode": 68, "before_eval_results": {"predictions": ["Martin \"Al\" Culhane,", "sincerity", "Indonesian", "mammoths", "Lisa's parents never mentioned anyone wanting to harm them.", "people give the United States abysmal approval ratings.", "anonymous.", "Felipe Massa,", "Matthew Fisher", "Dogpatch Labs", "San Diego,", "Technological Institute of Higher Learning of Monterrey,", "10 years", "\"Dancing With the Stars.\"", "a one-shot victory in the Bob Hope Classic", "3-2", "78,000 parents of children ages 3 to 17.iReport.com:", "machine guns and two silencers", "\"The Screening Room\"", "death of a pregnant soldier", "16 times.", "Anil Kapoor.", "2-1", "CNN", "Joan Rivers", "on the bench", "actress", "one", "college campus.", "Elisabeth", "1979", "Phillip A. Myers.", "difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "205 lb ( 93 kg ) and under", "The Natya Shastra", "American Indian allies", "Bulgaria", "Derek Pastula", "New Jersey, United States", "priam", "leeds", "Utah", "alpen", "south africa", "vote", "salmon", "Hungarian Horntail", "StubHub Center", "Jaguar Land Rover Limited", "Sutton Hoo", "Eminem", "Stuart Price", "Buffalo", "FAI Junior Cup", "2013", "a walrus", "Inamona", "a hand", "F", "salaam", "hip", "Japan", "Alive"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6708333333333333}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-2166", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-522", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2906", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-805", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-1628", "mrqa_triviaqa-validation-6728", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-4036", "mrqa_searchqa-validation-16356", "mrqa_searchqa-validation-8405", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-12454"], "SR": 0.578125, "CSR": 0.5260416666666667, "EFR": 1.0, "Overall": 0.7126302083333333}, {"timecode": 69, "before_eval_results": {"predictions": ["10 Afghan police officers", "Molotov cocktails, rocks and glass.", "rwanda", "more than 100", "a cross-country sojourn in search of work.", "India", "Hanin Zoabi,", "Six", "Immigration Minister Eric Besson", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "Iran", "education", "hit and slashed open his left eyebrows.", "review their emergency plans and consider additional security measures", "a female soldier,", "Burhanuddin Rabbani,", "African-Americans", "a place for another non-European Union player in Frank Rijkaard's squad.", "1979", "Congress", "Venus Williams", "a month", "Pfc. Bowe Bergdahl to \"stand tall, stand firm.\"", "Alan Kardec", "hundreds", "they're very or somewhat scared about the way things are going in the country today.", "Royal Air Force helicopter", "$1.5 million.", "about 75 miles east of Yakima", "Brian David Mitchell,", "Will Smith.", "allegedly faking a doctor's note", "November 2, 2016", "Nepal", "mind your manners", "the name America", "the fourth quarter of the preceding year", "a substance that fully activates the receptor that it binds to )", "for the 1994 season", "Tbilisi, Georgia", "a leaf", "New Zealand", "john Seddon", "the fallopian tube", "Grayson Perry", "france", "\"Holiday Inn\"", "Salvatore Ferragamo", "University of Texas at Austin", "Robbie Gould", "Patrick Swayze", "Roy Spencer", "Netrobalane canopus", "string musical", "32", "Jocelyn Moorhouse", "Hawaii", "beethoven", "aardwolf", "a ceviche", "Peter Bogdanovich", "cunning", "Alexander Popov", "Milton Glaser"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6807830624236875}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, true], "QA-F1": [0.4, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.375, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-1997", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-8928", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-4913", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-3761", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-2060", "mrqa_searchqa-validation-8460"], "SR": 0.609375, "CSR": 0.5272321428571429, "retrieved_ids": ["mrqa_squad-train-25961", "mrqa_squad-train-68189", "mrqa_squad-train-20085", "mrqa_squad-train-13529", "mrqa_squad-train-1995", "mrqa_squad-train-68214", "mrqa_squad-train-58774", "mrqa_squad-train-68221", "mrqa_squad-train-23509", "mrqa_squad-train-83200", "mrqa_squad-train-51321", "mrqa_squad-train-70681", "mrqa_squad-train-83604", "mrqa_squad-train-62527", "mrqa_squad-train-20550", "mrqa_squad-train-85990", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-8515", "mrqa_newsqa-validation-4011", "mrqa_naturalquestions-validation-5298", "mrqa_newsqa-validation-232", "mrqa_naturalquestions-validation-10419", "mrqa_triviaqa-validation-338", "mrqa_newsqa-validation-3703", "mrqa_newsqa-validation-3194", "mrqa_naturalquestions-validation-8555", "mrqa_triviaqa-validation-5849", "mrqa_hotpotqa-validation-3641", "mrqa_naturalquestions-validation-5538", "mrqa_triviaqa-validation-3742", "mrqa_searchqa-validation-16149", "mrqa_naturalquestions-validation-1491"], "EFR": 1.0, "Overall": 0.7128683035714285}, {"timecode": 70, "UKR": 0.771484375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-487", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-816", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4033", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-12126", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10068", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-2591", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-3723", "mrqa_squad-validation-3745", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-486", "mrqa_squad-validation-512", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5348", "mrqa_squad-validation-5362", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6595", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-689", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-816", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9408", "mrqa_squad-validation-96", "mrqa_squad-validation-9845", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7217", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.865234375, "KG": 0.49765625, "before_eval_results": {"predictions": ["April 9, 2012", "Hank Williams", "September 2, 1945", "Lord Banquo", "king Louie", "Waylon Jennings", "Robert Irsay", "18", "Katherine Kiernan Maria `` Kate '' Mulgrew", "Castleford", "October 2012", "Sammi Smith", "July 20, 2017", "Lake Powell", "Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton, Tewksbury, and Salem", "South Dakota", "five", "Billy Gibbons of ZZ Top", "William Shakespeare's As You Like It", "2", "the southeastern United States", "the sperm and one from the egg", "committed suicide", "2002", "the problems and / or goals", "a moral tale", "1956", "the west coast of Central America", "either in front or on top of the brainstem", "semi-automatic, but not fully automatic,", "V\u1e5bksayurveda", "1990", "john hurt", "Cyprus", "George W. Bush", "south africa", "snakes", "weetabix", "Hague", "Nigeria", "Harry F. Sinclair", "Russian", "1903", "motor vehicles", "13 May 2018", "pastels", "Saint Petersburg Conservatory", "middleweight", "\"Don't treat the girls the way she likes,\"", "CNN", "The FBI's Baltimore field office", "dismissed all charges", "\"Dancing With the Stars.\"", "228", "19-12", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "(Peter) Rubens", "Neptune", "(\" Bugsy\") Siegel", "Albert", "Everybody Wang Chung", "(George) Calder", "France", "a C-note"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6371612762237762}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.7692307692307692, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-7651", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-6718", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-6050", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-5334", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-5809", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-680", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-1228", "mrqa_searchqa-validation-14226", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12794"], "SR": 0.546875, "CSR": 0.5275088028169015, "EFR": 0.9310344827586207, "Overall": 0.7185836571151044}, {"timecode": 71, "before_eval_results": {"predictions": ["Norway", "Morocco", "Casablanca", "Geneva Conventions", "Classics", "New Jersey", "Lake Baikal", "salt", "a badger", "the Kuiper Belt", "a cat", "Mimi Bobeck", "winter", "nag", "Ned", "Boston Red Sox", "J! Archive - Show #3589", "Ohio", "pantomime", "halfpipe", "Israel", "Apple", "Moscow", "a bus", "shiatsu", "Columbo", "William McKinley", "Hannibal", "Ankara", "The Deep", "orangutan", "\"Death, be not\"", "Anatomy", "16", "8 bytes", "China", "1,000", "Gary Lineker", "October 29 - 30, 2012", "Andreas Vesalius", "Hawaii", "bird", "neurons", "work", "minder", "potatoes", "hickory", "antelopes", "281", "44", "Rockbridge County", "Freeform", "1965", "200,167", "Harvard", "Martin Ingerman", "a body", "Elena Kagan", "designer Tom Ford", "A staff sergeant in the U.S. Air Force", "1913.", "Kim Il Sung", "Ricardo Valles de la Rosa,", "Tuesday"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7786458333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2447", "mrqa_searchqa-validation-14509", "mrqa_searchqa-validation-7372", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-7819", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-4240", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-6169", "mrqa_naturalquestions-validation-6671", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-1374", "mrqa_hotpotqa-validation-4321", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-3848"], "SR": 0.703125, "CSR": 0.5299479166666667, "EFR": 1.0, "Overall": 0.7328645833333334}, {"timecode": 72, "before_eval_results": {"predictions": ["alloys", "Defending Your Life", "Amadeus", "\"Spanish Ladies\"", "Charley", "Microsoft", "Beethoven", "Latin", "high jump", "Nicole Kidman", "Cork", "Fort Sumter", "Bucharest", "the Sahara", "Happy Days", "Mentor", "Vanna White", "Morris West", "trod", "Frank Sinatra", "Green Lantern", "infrared", "barbecue", "the Department of Natural Resources", "\"J Jungle Jim\"", "Christ Church", "Paul Williams", "Jean-Paul Sartre", "Michelle Kwan", "a Little Diner flauta", "Baal", "Westinghouse", "gastrocnemius", "prophets", "Blue laws", "the interplay of supply and demand", "Bonnie Plunkett", "charbagh", "Shirley Partridge", "Barry Bonds", "colonel", "george vi", "three", "Mad Hatter", "Barbarella", "pentecost", "Nissan", "Brazil", "nuclear weapons", "rocket", "William Bradford", "Tim Allen", "Rochdale", "postmodern schools of thought", "Netherlands", "Romeo Montague", "Fullerton, California,", "misdemeanor assault charges", "education", "12-1", "to fill a million sandbags", "Kris Allen,", "north-south highway", "a one-shot victory"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6950114121989122}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.46153846153846156, 1.0, 0.0, 0.3636363636363636]}}, "before_error_ids": ["mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-9180", "mrqa_searchqa-validation-12784", "mrqa_searchqa-validation-3016", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-3178", "mrqa_searchqa-validation-15982", "mrqa_searchqa-validation-16034", "mrqa_searchqa-validation-613", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-5481", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-3640", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-212", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-2858"], "SR": 0.59375, "CSR": 0.5308219178082192, "retrieved_ids": ["mrqa_squad-train-16382", "mrqa_squad-train-12722", "mrqa_squad-train-44659", "mrqa_squad-train-75220", "mrqa_squad-train-73955", "mrqa_squad-train-45454", "mrqa_squad-train-85495", "mrqa_squad-train-7302", "mrqa_squad-train-47212", "mrqa_squad-train-18463", "mrqa_squad-train-7620", "mrqa_squad-train-10972", "mrqa_squad-train-26240", "mrqa_squad-train-38698", "mrqa_squad-train-69853", "mrqa_squad-train-85870", "mrqa_triviaqa-validation-1420", "mrqa_newsqa-validation-3415", "mrqa_hotpotqa-validation-4001", "mrqa_naturalquestions-validation-2169", "mrqa_triviaqa-validation-430", "mrqa_squad-validation-7096", "mrqa_newsqa-validation-2865", "mrqa_squad-validation-3667", "mrqa_searchqa-validation-14070", "mrqa_triviaqa-validation-6338", "mrqa_naturalquestions-validation-9753", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-917", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2456", "mrqa_triviaqa-validation-3207"], "EFR": 1.0, "Overall": 0.7330393835616438}, {"timecode": 73, "before_eval_results": {"predictions": ["Clint McAuliffe", "an armadillo", "The Daniel Boone National Forest", "Bigfoot", "the rupee", "Cold Mountain", "Frisbee", "nosy neighbor", "the pommel horse", "Queen Victoria", "a basement", "Newton", "Aramayana", "a girl and a beast", "a buttered cookie sheet", "Clint Yeoh", "a Whiskey Cocktails", "The French Line", "A-C", "an Old Manse", "the Caribbean", "frogs", "Fermium", "a corrboration of an accomplice", "an aerie", "Woody Guthrie", "the Arawak Indians", "the Clydesdales", "fontanels", "Winston Churchill", "the French military", "Herman Melville", "upon a military service member's retirement, separation, or discharge from active duty", "1924", "1926", "Veterans Committee", "Dumont d'Urville Station", "Gwendoline Christie", "Cambridge May Ball scene", "The British colonial government", "Formosa", "\"His Holiness.\"", "east coast", "mccartney", "richard peter", "bodhidharma", "lincolnshire", "\u00c9dith Piaf", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "England", "the villanelle poetic form", "late 12th Century", "1966", "Paper", "New York Shakespeare Festival", "Ribhu Dasgupta", "12 hours in jail.", "The Islamic republic's alleged efforts to acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "potential revenues from oil and gas", "Stephen Tyrone Johns", "around 3.5 percent of global greenhouse emissions.", "Elin Nordegren,", "Tuesday", "Ben Roethlisberger"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6074514521039305}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6896551724137931, 1.0, 1.0, 0.3076923076923077, 1.0, 1.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5142857142857142, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-7951", "mrqa_searchqa-validation-11850", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-15440", "mrqa_searchqa-validation-14992", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-4408", "mrqa_searchqa-validation-3770", "mrqa_searchqa-validation-12722", "mrqa_searchqa-validation-1486", "mrqa_searchqa-validation-15210", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-3524", "mrqa_naturalquestions-validation-923", "mrqa_triviaqa-validation-3177", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6231", "mrqa_triviaqa-validation-3438", "mrqa_hotpotqa-validation-2905", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3887"], "SR": 0.53125, "CSR": 0.5308277027027026, "EFR": 0.9333333333333333, "Overall": 0.7197072072072073}, {"timecode": 74, "before_eval_results": {"predictions": ["Pacific Grove", "Phillip Paley", "during `` Seeing Red '', Warren Mears, one of the Trio, arrives at Slayer's house with a gun", "classical architecture", "Darlene Cates", "James Long", "most of Sweden's political energy in the international arena had been directed towards the preservation of the League of Nations", "16 November 2001", "September 19 - 22, 2017", "November 1999", "on the inner wall of the pedestal", "mining", "April 1, 2016", "after World War II", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Spanish missionaries, ranchers and troops", "Gare du Nord", "John Travolta", "1960", "along Interstate 20, while the real town is nowhere near any interstate", "Butter Island off North Haven, Maine in the Penobscot Bay", "December 1886", "8 January 1999", "speech", "Nalini Negi", "for operations, personnel, equipment, and activities", "American singer Lesley Gore's version hit # 1 on the pop and rhythm and blues charts in the United States", "Hitler's personal refusal to authorise it, leading Heydrich and the faction demanding pre-emptive war to plot the F\u00fchrer's assassination", "September 8, 2017", "The Intolerable Acts", "American production duo The Chainsmoker", "British rock band Procol Harum", "tahrir Square", "Heather Stanning and Helen Glover", "charlie", "sally mccartney", "ely", "Castor", "canadensis", "gasoline", "Hanford Nuclear Reservation", "Ty Cobb", "over 1.6 million", "Paul W. S. Anderson", "Roscoe Lee Browne", "dice", "Clara Petacci", "Charice", "Citizens", "that Iran could be secretly working on a nuclear weapon", "off Somalia's coast.", "Daniel Radcliffe", "militants", "gas pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "his mother,", "intention to set up headquarters in Dublin.", "Winston Churchill", "Ricky Martin", "Mark Twain", "San Salvador", "Amelia Earhart", "ethanol", "Knott's Berry Farm", "the pituitary gland"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6480093878161313}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.25, 0.6666666666666666, 1.0, 0.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.08333333333333334, 1.0, 1.0, 1.0, 0.0, 1.0, 0.08, 0.10526315789473684, 0.0, 1.0, 1.0, 0.75, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8750000000000001, 0.0, 1.0, 0.0, 0.16666666666666669, 0.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-138", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-255", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-2653", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-4162", "mrqa_triviaqa-validation-283", "mrqa_triviaqa-validation-3294", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-5521", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-1950", "mrqa_newsqa-validation-192"], "SR": 0.53125, "CSR": 0.5308333333333333, "EFR": 0.9666666666666667, "Overall": 0.726375}, {"timecode": 75, "before_eval_results": {"predictions": ["North Atlantic Ocean", "Dmitri Mendeleev", "Ramanaa", "charbagh", "Amerigo Vespucci", "2018", "at the head of Lituya Bay in Alaska", "0.30 in ( 7.6 mm )", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "the breast or lower chest of beef or veal", "May 2017", "Brian Steele", "2011", "19 June 2018", "Ancient Greek terms", "Harishchandra", "sport utility vehicles", "If a vehicle towing a trailer skids", "Cheap Trick", "Julie Adams", "FIGG Bridge Engineers, a Tallahassee - based firm", "Turducken", "synovial joint", "Kent", "Billy Idol", "2017", "2020", "August Darnell", "The International System of Units ( SI )", "Rufus and Chaka Khan", "a political ideology", "New York City", "Trimdon", "zelle", "green", "49", "stohart", "eros", "Robinson", "argentina", "1875", "Pearl Jam", "33 of the 100 seats", "Manchester", "Mitsubishi Motors", "John McClane", "Adelaide", "Exit 82", "attracted some U.S. senators who couldn't resist taking the vehicles for a spin.", "41,280 pounds", "Spc. Megan Lynn Touma,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "Lindsey Vonn", "it was important to provide alternative work for poor Afghan farmers to encourage them to give up opium production.", "Bill", "comments he made last night at the Annual Caddy Awards dinner in Shanghai,\"", "Byron", "a snakes", "Odin", "Abstract expressionism", "Wall Street", "the blubber", "Kinsey Millhone", "Romanov"], "metric_results": {"EM": 0.5, "QA-F1": 0.6066017316017316}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.42857142857142855, 0.6666666666666666, 1.0, 0.0, 1.0, 0.06666666666666667, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4659", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-8183", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-8845", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6288", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2197", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-404", "mrqa_hotpotqa-validation-620", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-4241", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-9741", "mrqa_searchqa-validation-2728", "mrqa_searchqa-validation-15046"], "SR": 0.5, "CSR": 0.5304276315789473, "retrieved_ids": ["mrqa_squad-train-78547", "mrqa_squad-train-86451", "mrqa_squad-train-45174", "mrqa_squad-train-51188", "mrqa_squad-train-29453", "mrqa_squad-train-34988", "mrqa_squad-train-73879", "mrqa_squad-train-56125", "mrqa_squad-train-15517", "mrqa_squad-train-83818", "mrqa_squad-train-4526", "mrqa_squad-train-4270", "mrqa_squad-train-27841", "mrqa_squad-train-52418", "mrqa_squad-train-17545", "mrqa_squad-train-42210", "mrqa_hotpotqa-validation-156", "mrqa_searchqa-validation-6072", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-3637", "mrqa_searchqa-validation-12536", "mrqa_newsqa-validation-3191", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-6452", "mrqa_squad-validation-6776", "mrqa_hotpotqa-validation-4617", "mrqa_naturalquestions-validation-8782", "mrqa_triviaqa-validation-4764", "mrqa_naturalquestions-validation-9142", "mrqa_newsqa-validation-1053", "mrqa_squad-validation-7162", "mrqa_hotpotqa-validation-5513"], "EFR": 0.9375, "Overall": 0.7204605263157895}, {"timecode": 76, "before_eval_results": {"predictions": ["several weeks,", "involvement during World War II in killings at a Nazi German death camp in Poland.", "the Ku Klux Klan,", "Filippo Inzaghi", "pesos", "Silicon Valley.", "Booches Billiard Hall,", "Liverpool,\"", "Expedia", "the punishment for the player", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "Afghan National Security Forces", "Nechirvan Barzani,", "planned attacks", "$273 million", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "actress", "Brian Smith,", "Brazil", "she supports the two-state solution to the Mideast conflict,", "$3 billion,", "\"Three Little Beers,\"", "Tuesday the reality he has seen is \"terrifying.\"", "Dan Brown's", "gathering information about the rebels to give to the Colombian military.", "mental health", "Annie Duke", "570 billion pesos ($42 billion)", "India", "response to a civil disturbance call,", "November 17, 1800", "Acts passed by the Congress of the United States and its predecessor, the Continental Congress, that were either signed into law by the President or passed by Congress after a presidential veto", "concerned with all legal affairs", "Giancarlo Stanton", "A status line", "David Ben - Gurion", "18", "TC", "antelopes", "Gorbachev", "36", "Ravi Shankar", "Baton Rouge", "Abe Reles", "Floor Exercise", "Mussolini", "sa\u00f0reyjar", "August 21, 1995", "1966", "Algernod Lanier Washington", "Highwayman", "Michael Rispoli", "Middlesbrough", "Brenton Thwaites", "adhesion", "crumpets", "the Potomac", "dinoflagellates", "Double Indemnity", "Glengarry", "The Piano", "a penny"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7463205632053882}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.923076923076923, 1.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.06896551724137931, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1711", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-1452", "mrqa_triviaqa-validation-6079", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-4623", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-2003", "mrqa_searchqa-validation-12186"], "SR": 0.6875, "CSR": 0.5324675324675325, "EFR": 0.9, "Overall": 0.7133685064935065}, {"timecode": 77, "before_eval_results": {"predictions": ["Dr. Jennifer Arnold and husband Bill Klein,", "David Beckham", "Alexandre Caizergues,", "jobs", "sedate", "an American al Qaeda member", "debris", "1959,", "lower house of parliament,", "Six members", "East Java", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "psychotropic drugs", "3 to 17", "NASCAR.", "executive director of the Americas Division of Human Rights Watch,", "jazz", "Mark Fields", "Alicia Keys", "FBI Special Agent Daniel Cain,", "military trials", "home in Peshawar", "Daytime Emmy Lifetime Achievement Award.", "Jewish", "President Obama's", "Woosuk Ken Choi,", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "Almost all British troops", "Filippo Inzaghi", "Greeley, Colorado,", "building bombs,", "\"E! News\"", "Sophia Akuffo", "North Carolina", "Los Lonely Boys", "LED illuminated display", "pagan custom, namely, the winter solstice which in Europe occurs in December", "1966", "1985 -- 1993", "sedimentary rock", "chiltern", "chiropractic", "polish", "To Kill a Mockingbird", "thanksgiving", "samuel Johnson", "peter", "just two years", "the Knight Company", "Ferdinand Magellan", "23 March 1991", "34th", "154 days", "seven members", "Snowball II is killed off", "Scottish", "Ali", "Captain Nemo", "Man Ray", "the \"Hans & Franz\" \"SNL\" skit", "James Watt", "Mercury and Venus", "snakes", "The Bee"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7162715935559006}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.125, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6875000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.5217391304347826, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.09999999999999999, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-2933", "mrqa_newsqa-validation-292", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-3749", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7203", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2281", "mrqa_triviaqa-validation-1043", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-3504", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12187"], "SR": 0.59375, "CSR": 0.5332532051282051, "EFR": 0.9615384615384616, "Overall": 0.7258333333333333}, {"timecode": 78, "before_eval_results": {"predictions": ["a micronutrient-rich diet", "Jamie Fraser", "August 14, 1848", "American Horror Story", "Daniel Craig", "The Zebras", "Captain Marvel", "\"Nebo Zovyot\"", "Gainsborough Trinity F.C.", "Carlos Santana", "DreamWorks Animation", "40 Acres and a Mule Filmworks", "Fort Bragg", "Division I Football Bowl Subdivision", "Robert A. Iger", "The Gang", "A play-by-post role-playing game", "Newcastle upon Tyne, England", "Stephen James Ireland", "Rooster", "27 January 1974", "second cousin once removed", "James Dean", "Jennifer Taylor", "a Christian evangelist", "Illinois", "Twitch Interactive, a subsidiary of Amazon.com", "1853", "Coalwood", "Sarajevo", "Gareth Barry", "The Ryukyuan people", "in the west by the east coast of Queensland, in the east by Vanuatu ( formerly the New Hebrides ) and by New Caledonia, and in the northeast approximately by the southern extremity of the Solomon Islands", "Samantha Jo \ufffd Mandy \ufffd Moore ( born March 28, 1976 in St. Louis, Missouri and raised in Breckenridge, Colorado )", "a patronymic surname, which arose separately in England and Wales", "Duisburg", "in the middle of the 15th century, in Yemen's Sufi monasteries", "the 4th century", "between 1765 and 1783", "Deputy Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "Another Day in Paradise", "Ronnie Carroll", "North by Northwest", "a lightweight, folding version", "diijon", "stomach", "may 22, 1907, Dorking, Surrey, Eng.\u2014died July 11, 1989,", "arcelorMittal Orbit", "Afghanistan", "82", "appealed against the punishment", "South Africa", "then-Sen. Obama", "what caused the collapse", "in the Bronx and grew up in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "environmental", "cosmetology", "the Knickerbocker", "Tim Russert", "Louisville, Kentucky", "Tigger", "Giovanni Bertati", "Ariel Sharon", "GILBERT & SullIVAN"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6647691981931112}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.26666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.34782608695652173, 0.8, 1.0, 0.3636363636363636, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0909090909090909, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5873", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-4909", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-1435", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4791", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-1965", "mrqa_naturalquestions-validation-3483", "mrqa_triviaqa-validation-106", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-2138", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-6", "mrqa_searchqa-validation-9708", "mrqa_searchqa-validation-8670"], "SR": 0.578125, "CSR": 0.5338212025316456, "retrieved_ids": ["mrqa_squad-train-60804", "mrqa_squad-train-54495", "mrqa_squad-train-52425", "mrqa_squad-train-17942", "mrqa_squad-train-76169", "mrqa_squad-train-12377", "mrqa_squad-train-23790", "mrqa_squad-train-48071", "mrqa_squad-train-67855", "mrqa_squad-train-1358", "mrqa_squad-train-77986", "mrqa_squad-train-73793", "mrqa_squad-train-5818", "mrqa_squad-train-66964", "mrqa_squad-train-81264", "mrqa_squad-train-24505", "mrqa_hotpotqa-validation-156", "mrqa_naturalquestions-validation-335", "mrqa_searchqa-validation-14782", "mrqa_naturalquestions-validation-800", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-6840", "mrqa_searchqa-validation-16856", "mrqa_newsqa-validation-2656", "mrqa_searchqa-validation-12956", "mrqa_naturalquestions-validation-5571", "mrqa_newsqa-validation-1529", "mrqa_hotpotqa-validation-1378", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-2060", "mrqa_searchqa-validation-10017", "mrqa_triviaqa-validation-1924"], "EFR": 0.9629629629629629, "Overall": 0.7262318330989217}, {"timecode": 79, "before_eval_results": {"predictions": ["St. Louis", "Pensacola", "Pope John X.", "Apalachee", "The Emperor of Japan", "English", "Carlos Alberto", "Dana Fox", "7 January 1936", "The Onion", "Portsea", "Lapland", "1698", "Cuban descent", "2010", "California", "Gr\u00e1inne Mhaol", "Ouse and Foss", "Thrushcross Grange", "Ten Walls", "My Boss, My Hero", "York County", "Grave Digger", "Ariels", "Europop", "Humberside", "Viaport Rotterdam", "Yasir Hussain", "566", "Cookstown", "YIVO", "Kairi", "Network - Protocol driver", "head coach", "Jacqueline MacInnes Wood", "Malayalam", "126", "Lyndon B. Johnson", "a maritime signal, indicating that the vessel flying it is about to leave", "18 September", "alligators", "frottage", "polo", "Mrs Merton", "green", "12", "drake", "Octavian", "At least 38", "hanged in 1979 for the murder of a political opponent", "Karen Floyd", "Canada.", "Michael Arrington,", "South Carolina Republican Party Chairwoman Karen Floyd", "usion teams", "The remaining 240 patients will be taken to hospitals in other provinces", "out-of-body", "air dominance", "Wyoming", "creditable", "the Hudson River", "Nicholas", "the Taliban", "James Naismith"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6757970328282827}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.4444444444444444, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5454545454545454, 1.0, 1.0, 0.0, 0.4444444444444445, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-3109", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-2012", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8603", "mrqa_triviaqa-validation-5498", "mrqa_triviaqa-validation-3825", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1210", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-1570", "mrqa_searchqa-validation-10199"], "SR": 0.59375, "CSR": 0.5345703125, "EFR": 1.0, "Overall": 0.7337890625}, {"timecode": 80, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-794", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9073", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3317", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.8359375, "KG": 0.46875, "before_eval_results": {"predictions": ["Parthenon", "Dennis Potter", "6", "Botticelli", "avocadoe", "Venice", "architect", "Bruce Springsteen and Jon Bon Jovi", "Chief Inspector of Prisons", "Some Like It Hot", "havre", "The Gunpowder Plot of 1605", "Poland", "New York City", "Wee Jimmy Krankie", "Norway", "le pneu Michelin boit l'obstacle", "1890", "William Shatner", "Phil Mickelson", "ha", "lenny ha", "Lady Bracknell", "chai", "binky", "geology", "Bolivia", "gold", "polyhedron", "below sea level", "tea", "Houyhnhnm", "Frank Langella", "Greenbriar Boys", "Gloria ( Lisa Stelly )", "an instant messaging client that was first developed and popularized by the Israeli company Mirabilis in 1996", "two", "pigs", "Janie Crawford, an African - American woman in her early forties", "in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "$60 million", "Alan David Sokal", "Crystal Dynamics", "1955", "Alexandre Dumas", "Tom Wolfe", "Little Dixie", "8-track cartridge", "$17,000", "there is not a process to ensure that auto owners comply with recalls.", "think we have to rely on having a clinical breast exam once a year at a health care provider and doing your self-breast exam on a monthly basis.", "forgery and flying without a valid license,", "700", "first or second week in April.", "five", "nine newly-purchased bicycles", "ethanol", "Florida", "the Massachusetts Bay Colony", "a flying saucer", "Voltaire", "the Jordan River", "aperire", "Prince William"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6442091762404263}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8, 0.33333333333333337, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.15384615384615385, 0.07142857142857144, 0.2857142857142857, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-5545", "mrqa_triviaqa-validation-2530", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-3793", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-2658", "mrqa_triviaqa-validation-302", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-3857", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-1801", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-34", "mrqa_hotpotqa-validation-1142", "mrqa_newsqa-validation-2358", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1611", "mrqa_searchqa-validation-6294", "mrqa_searchqa-validation-4617", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-15708"], "SR": 0.53125, "CSR": 0.5345293209876543, "EFR": 0.9666666666666667, "Overall": 0.7107860725308642}, {"timecode": 81, "before_eval_results": {"predictions": ["catalyst", "coco chanel", "Clara wieck", "Socrates", "Molly Brown", "Portugal", "the Warren Commission", "perfume", "brighton", "a righteous man", "throw", "Wars of the Roses", "norman Brookes", "Albert Reynolds", "al jazeera", "bruce", "Bayern M\u00fcnchen", "superficial thrombophlebitis", "Lorelei", "Mickey spillane", "Sir Walter Scott", "Shayne Ward", "French Guiana", "Vienna", "Amsterdam", "pickled peppers", "the troposphere", "city of london", "Spain", "Arizona Diamondbacks", "little jack Horner", "springflower", "Freedom Day", "Have I Told You Lately", "in the New Testament", "Woody Paige", "1661", "2015", "the amino acids glycine and arginine", "the right", "University of Southern California Trojans", "January 2004", "The More", "Adelaide Lightning", "Idaho", "Donald Sterling", "Richard Arthur", "Retina display", "the giant mega-yacht 'Wally Island'", "an \"unnamed international terror group\"", "her decades-long portrayal of Alice Horton on", "the death of a pregnant soldier", "The iCloud service", "nearly $162 billion in war funding", "Mad Men", "$273 million", "artillery", "heart", "Brownsville", "the kings", "Frank Sinatra", "Tasmania", "the Bodleian", "Ivy Dickens"], "metric_results": {"EM": 0.625, "QA-F1": 0.6877367424242424}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-3990", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-3302", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-5561", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-7319", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-686", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-370", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-162", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-6131", "mrqa_searchqa-validation-8274", "mrqa_searchqa-validation-9043"], "SR": 0.625, "CSR": 0.5356326219512195, "retrieved_ids": ["mrqa_squad-train-77135", "mrqa_squad-train-58048", "mrqa_squad-train-70829", "mrqa_squad-train-17545", "mrqa_squad-train-22296", "mrqa_squad-train-26384", "mrqa_squad-train-13577", "mrqa_squad-train-23110", "mrqa_squad-train-71561", "mrqa_squad-train-45424", "mrqa_squad-train-9060", "mrqa_squad-train-37097", "mrqa_squad-train-45091", "mrqa_squad-train-58382", "mrqa_squad-train-29527", "mrqa_squad-train-64800", "mrqa_searchqa-validation-14874", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5670", "mrqa_newsqa-validation-3261", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-10249", "mrqa_triviaqa-validation-5567", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-1072", "mrqa_searchqa-validation-6680", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-2555", "mrqa_hotpotqa-validation-1727", "mrqa_newsqa-validation-3096", "mrqa_triviaqa-validation-5992"], "EFR": 1.0, "Overall": 0.7176733993902439}, {"timecode": 82, "before_eval_results": {"predictions": ["buxton", "mikado", "Yellowstone", "wilder romeo", "maryhold ernay", "bitter almond", "The Salvation Army", "bali", "0", "genesis", "john of Gaunt", "mountain tunes", "the Benedictine Order", "Trinity College", "evolution", "Cuba", "10", "phosphorus", "1927", "Tennessee Williams", "architecture", "Hindu", "Howard Keel", "austral", "USS Missouri", "mayhmi", "lithium", "paul eld", "Colleen McCullough", "nastase", "Adrian Chiles", "para handy", "Johannes Gutenberg", "an edible tuber", "Amanda Leighton", "copper ( Cu )", "the lumbar cistern", "Nigeria", "30 months", "Shannen Doherty", "Charles Otto Puth Jr.", "Alfred Joel Horford Reynoso", "Gal Gadot", "Greek Revival", "\"Castle on the Hill\"", "2013 Cannes Film Festival", "Dominican", "World War I", "health care reform", "resources", "The Stooges", "Kandi Burruss,", "\"We must find ways to relieve some of this stress,\"", "Somali", "power-sharing talks", "Stanford", "blimps", "the House", "the South Beach diet", "the root", "the Southern Cross", "Indonesia", "Homo erectus", "Athol Fugard"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6247801677489178}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.1818181818181818, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-5460", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-2816", "mrqa_triviaqa-validation-3640", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-5081", "mrqa_triviaqa-validation-4668", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-989", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-4649", "mrqa_hotpotqa-validation-5077", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-655", "mrqa_searchqa-validation-10249"], "SR": 0.578125, "CSR": 0.536144578313253, "EFR": 0.9629629629629629, "Overall": 0.7103683832552432}, {"timecode": 83, "before_eval_results": {"predictions": ["Luigi Pirandello", "lew Deighton", "cotton", "tartarus", "leopold I", "tartan", "Mark Darcy", "alpha", "mulhac\u00e9n", "nasdaq", "The Great Gatsby", "Poland", "geoffrey", "Operation (game)Operation is a battery-operated game of physical skill that tests players' hand-eye coordination and fine motor skills.", "is our children learning?\"", "mike Gatting", "australian", "Massachusetts", "eldorado", "geoffrey raft", "Armageddon", "purple rain", "Sinclair Lewis", "h Herman Wouk", "northamptonshire", "k Keswick", "a toad", "Runic", "blue", "john Nash,", "robert plant", "Pocahontas", "during the 1890s Klondike Gold Rush", "a 5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "St. Mary's County is part of the Baltimore -- Washington metropolitan area", "Massillon, Ohio", "Robin", "while studying All My Sons by Arthur Miller, a play about a man whose choice to send out faulty airplane parts for the good of his business and family caused the death of twenty one pilots during World War II", "1976", "photon energy E", "June 2, 2008", "Ronnie Schell", "Estadio de L\u00f3pez Cort\u00e1zar", "George Adamski", "Bangor International Airport", "actress and model", "Soci\u00e9t\u00e9 de Micro\u00e9lectronique et d' Horlogerie", "Campbellsville", "British oil companies' efforts to drill off the northern coast of the islands.", "Alejandro Peralta Alvarez,", "charlie Darwin", "1-1", "Mindanao.", "three out of four questioned say that things are going well for them personally.", "surgical anesthetic propofol", "armed robbery and kidnapping of another victim,", "Confucian", "Aquitaine", "Niels Bohr", "Sicily", "the ulnar nerve", "a large chart for family trees", "Kilmer", "the Tigris River"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5891095235945588}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.9859154929577464, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 0.6666666666666666, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-5704", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4943", "mrqa_triviaqa-validation-6595", "mrqa_triviaqa-validation-2603", "mrqa_triviaqa-validation-4160", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-7670", "mrqa_naturalquestions-validation-5798", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-2782", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-1554", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-1903", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-3509", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-837", "mrqa_searchqa-validation-4860", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-12343", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-9720"], "SR": 0.484375, "CSR": 0.5355282738095238, "EFR": 0.9696969696969697, "Overall": 0.7115919237012986}, {"timecode": 84, "before_eval_results": {"predictions": ["tony blair", "green", "sam Mendes", "operation Dynamo", "an American Western series", "john murray", "spain", "w", "Messenger", "Hungary", "granada", "rugby", "olivier Twist", "chai", "alien", "big dipper", "apes", "Luigi Pirandello", "hillsborough", "horseshoes", "Annie Leibovitz", "atomic kitten", "thalia", "tommy Roe", "cuticle", "spain", "rings", "john mEnroe", "Festival of Britain", "Sherlock Holmes", "maxilla", "suez canal", "Presley Smith", "in the blood to the liver", "The Impalas", "in southern Turkey, dividing the Mediterranean coastal region of southern Turkey from the central Anatolian Plateau", "Henry Selick", "Frankel", "Tulsa, Oklahoma", "full '' sexual intercourse", "Linda Ronstadt", "British romantic comedy", "Bonkyll Castle", "American", "Outstanding Lighting Design", "North Atlantic Conference", "12", "George Michael Cohan", "\"Larry King Live.\"", "Taliban", "\"Public Enemies,\"", "Tim Clark, Matt Kuchar and Bubba Watson", "over the", "early detection and helping other women cope with the disease.\"", "Michelle Rounds", "28", "the 400th anniversary", "London", "the liver", "Big Brother", "Tom Cruise", "a short circuit", "Katherine Heigl", "behind the eight ball"], "metric_results": {"EM": 0.625, "QA-F1": 0.7166193181818181}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5562", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-6077", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-5458", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-8950", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4065", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-2915", "mrqa_searchqa-validation-15106"], "SR": 0.625, "CSR": 0.5365808823529412, "retrieved_ids": ["mrqa_squad-train-52532", "mrqa_squad-train-57204", "mrqa_squad-train-17218", "mrqa_squad-train-82710", "mrqa_squad-train-78855", "mrqa_squad-train-84175", "mrqa_squad-train-48006", "mrqa_squad-train-22488", "mrqa_squad-train-60007", "mrqa_squad-train-59097", "mrqa_squad-train-49650", "mrqa_squad-train-82577", "mrqa_squad-train-55732", "mrqa_squad-train-24746", "mrqa_squad-train-73986", "mrqa_squad-train-45620", "mrqa_hotpotqa-validation-2328", "mrqa_naturalquestions-validation-9703", "mrqa_squad-validation-6931", "mrqa_newsqa-validation-522", "mrqa_triviaqa-validation-3437", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-4944", "mrqa_triviaqa-validation-4", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-1325", "mrqa_hotpotqa-validation-3395", "mrqa_triviaqa-validation-3591", "mrqa_naturalquestions-validation-3303", "mrqa_hotpotqa-validation-1217", "mrqa_naturalquestions-validation-9953", "mrqa_triviaqa-validation-5538"], "EFR": 0.9583333333333334, "Overall": 0.709529718137255}, {"timecode": 85, "before_eval_results": {"predictions": ["Los Ticos", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "\"I think she's wacko.\"", "Chevron", "Monday night", "citizenship", "The Bronx County District Attorneys Office", "the 1950s", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "flooding and debris", "tens of thousands of new voters became the key to his Iowa win", "Lucky Dube,", "45 minutes, five days a week.", "Nigeria, Africa's largest producer.", "Los Alamitos Joint Forces Training Base", "10 below", "Mad Men", "Fargo, North Dakota,", "Asashoryu's", "her captors released her,", "checkposts and military camps in the Mohmand agency,", "China", "President Obama", "Mashhad", "Washington State's decommissioned Hanford nuclear site,", "Bollywood superstar", "16", "prisoners at the South Dakota State Penitentiary", "Thursday and Friday", "iPods", "3-0", "you know what is important in life, and it's not your car.\"", "AMX - 30", "The International System of Units ( SI )", "Nashville, Tennessee", "Chelsea ( 2009 -- 10 )", "her castle", "BC Jean and Toby Gad", "Brenda", "China", "Emily Davison", "Pompey", "new york history", "streptococcus", "\u201clone wolf\u201d", "a business cycle", "the Spanish", "stop motion effects", "international football", "gamecock", "Alex Song", "40 million", "26,000", "1911", "Singha (Thai: \u0e2a\u0e34\u0e07\u0e2b\u0e4c ) is a 5% abv pale lager", "Dirk Werner Nowitzki", "Austin Powers", "Louisiana", "the iris", "actress", "Bob Dylan", "Princess Diana", "the crossword", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5859749264695511}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true], "QA-F1": [0.0, 0.07692307692307693, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.04878048780487805, 0.0, 0.5714285714285715, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473685, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-341", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2395", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-9436", "mrqa_triviaqa-validation-2820", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-1773", "mrqa_triviaqa-validation-2511", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-4680", "mrqa_hotpotqa-validation-257", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-5087", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-1467", "mrqa_searchqa-validation-5024", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-4284"], "SR": 0.515625, "CSR": 0.5363372093023255, "EFR": 1.0, "Overall": 0.717814316860465}, {"timecode": 86, "before_eval_results": {"predictions": ["australia", "heel", "pink", "Jessica", "johnny kes cupid", "bette davis", "faversham", "coke", "spain", "australia", "Mediterranean", "looking glass", "golf", "will carling", "Jonathan Swift", "sigmundFreud", "nyasaland", "ken Russell", "crow", "Enrico Caruso", "tara", "Morgan Spurlock", "spain", "spain", "Saturn", "spain", "e", "1879", "lizards", "cosmos", "time", "The West Wing", "Kimberlin Brown", "6 - 6", "it activates a relay which will handle the higher current load", "Andy Warhol", "Johnny Darrell", "Organisms in the domains of Archaea and Bacteria reproduce with binary fission", "hair / fur ( including wool ) and feathers", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "President Barack Obama", "1937", "bioelectromagnetics", "Len Wiseman", "Dutch", "Nathan Bedford Forrest", "locations", "Bruce Almighty", "15", "Mashhad", "issued his first military orders as leader of North Korea", "200.", "quality of teaching and learning in American schools", "Nothing happened in 1994 when Kim Il Sung died", "several weeks,", "digging", "The Beverly Hillbillies", "Airbus", "Kenya", "frank", "Paris Saint-Germain", "The Mousetrap", "a guardian angel", "Herodotus"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6168938700188701}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.4444444444444445, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.5, 0.15384615384615385, 0.6153846153846153, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-7767", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-1587", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-2548", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5531", "mrqa_naturalquestions-validation-243", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-2670", "mrqa_hotpotqa-validation-3506", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1614", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-77", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-12949"], "SR": 0.5625, "CSR": 0.5366379310344828, "EFR": 0.9642857142857143, "Overall": 0.7107316040640395}, {"timecode": 87, "before_eval_results": {"predictions": ["CBS", "The Los Angeles Dance Theater", "Northern Ireland", "Richard Street", "1983 Summer Universiade", "The Missouri Tigers football program", "fourth-largest", "Buddha's delight", "Natalie Chandler", "Toshi Ichiyanagi", "1988", "Reinhard Heydrich", "A bass", "2004 Paris Motor Show", "Jeffrey Chiang", "Hanford Site", "December 19, 1998", "Mickey's PhilharMagic", "Arizona Health Care Cost Containment System", "Donald Sterling", "Sada Carolyn Thompson", "Matt Groening", "All That", "glee", "Danish", "Gateways", "\"Invader (Invasor)\"", "July 25 to August 4", "2015", "810", "San Diego County Fair", "Blackpool Football Club", "electron donors", "Iowa", "The sacroiliac joint", "Stefanie Scott", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W", "erosion", "Kerris Lilla Dorsey", "Joseph Sherrard Kearns", "spark-ignition", "Jack Lemmon", "Mexico", "suez canal", "mmorpgs", "The Big Bopper", "plymouth", "Tina Turner", "Roger Federer", "five", "public toilets and playgrounds.", "Haiti's", "Michael Schumacher", "books", "Les Bleus", "228", "Frederick II", "Burping", "Vietnam", "centigrade", "Boston", "New Orleans", "Steve Wynn", "The Curse of the Black Pearl"], "metric_results": {"EM": 0.71875, "QA-F1": 0.773139880952381}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-3545", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-5266", "mrqa_naturalquestions-validation-8417", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-761", "mrqa_searchqa-validation-1818", "mrqa_searchqa-validation-256", "mrqa_searchqa-validation-280", "mrqa_searchqa-validation-11596", "mrqa_searchqa-validation-10796"], "SR": 0.71875, "CSR": 0.5387073863636364, "retrieved_ids": ["mrqa_squad-train-50963", "mrqa_squad-train-70674", "mrqa_squad-train-44652", "mrqa_squad-train-16088", "mrqa_squad-train-729", "mrqa_squad-train-56959", "mrqa_squad-train-22332", "mrqa_squad-train-76508", "mrqa_squad-train-39596", "mrqa_squad-train-28285", "mrqa_squad-train-450", "mrqa_squad-train-65916", "mrqa_squad-train-15901", "mrqa_squad-train-74072", "mrqa_squad-train-61693", "mrqa_squad-train-4214", "mrqa_hotpotqa-validation-1220", "mrqa_searchqa-validation-15959", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-992", "mrqa_hotpotqa-validation-1837", "mrqa_triviaqa-validation-599", "mrqa_newsqa-validation-646", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-826", "mrqa_naturalquestions-validation-5143", "mrqa_searchqa-validation-6504", "mrqa_triviaqa-validation-4913", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-5476", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-388"], "EFR": 0.9444444444444444, "Overall": 0.7071772411616162}, {"timecode": 88, "before_eval_results": {"predictions": ["Sunday", "severe flooding", "southern city of Naples", "Hundreds of contraband cell phones", "the rig's captain, Curt Kutcha, told him he had tried to activate a \"kill switch\" that would cut off the well before abandoning the structure.", "be silent.", "body bags", "Chinese", "Taliban", "served in the military", "Prague was \"wow.\"", "wounded 46 others,", "The Intertropical Convergence Zone", "Gordon Brown", "\"Quiet Nights,\"", "17-month", "Asashoryu", "managing his time", "Hungary", "Bush-era Justice Department", "London", "Ewan McGregor", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "September 11, 2001,", "her boyfriend, Dodi Fayed,", "four", "Operation Pipeline Express.", "District Attorney Larry Abrahamson", "piano lessons.", "have no more oil and we'll have to find another way to live,\"", "Michael Partain,", "Nigeria,", "Abid Ali Neemuchwala", "the Election Commission of India", "Buddhism", "Waylon Jennings", "Bart Cummings", "You are a puzzle", "Lulu", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "Barack Obama", "end of March 1939", "a centaur", "Mj\u00f6llnir", "Truro", "Swindon Town", "Sisyphus", "calcium sulfate", "Andrea Ch\u00e9nier", "La Familia Michoacana", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Central Avenue", "Quentin Coldwater", "the Corps of Discovery", "Nippon Professional Baseball", "punk rock", "James", "John Donne", "big tooth", "Joan of Arc", "lilac", "the Library of Congress", "Smacks", "R.E.M."], "metric_results": {"EM": 0.5, "QA-F1": 0.5485446338513108}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, true, false, false, false, true, true, false, true, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5714285714285715, 0.04081632653061224, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 0.16666666666666666, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6153846153846153, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-695", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-338", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3978", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-5125", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-191", "mrqa_hotpotqa-validation-5355", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-9973", "mrqa_searchqa-validation-12633"], "SR": 0.5, "CSR": 0.5382724719101124, "EFR": 0.96875, "Overall": 0.7119513693820225}, {"timecode": 89, "before_eval_results": {"predictions": ["38,", "anyone wanting to harm them.", "Pakistan's", "education about rainforests.", "we should help the auto industry and not other industries in this same way.", "\"extremely weak\" and said he weighs barely 100 pounds in a court document filed this week, but he walked on his own during the 45 minutes he was at the ceremony.", "Africa", "Bollywood superstar", "NATO's Membership Action Plan, or MAP,", "Wigan", "Vivek Wadhwa,", "Africa", "Osama", "between Pyongyang and Seoul", "Saturday's Hungarian Grand Prix.", "British author J.G. Ballard,", "Schalke", "Hu and other top Chinese officials", "cars", "more than 4,000", "1 million", "Tomas Olsson,", "documents to recognize the legal right to freedom from tyranny,", "64,", "jazz", "the last surviving British soldier from World War I", "Pixar's", "Miami Beach, Florida,", "some of the best stunt ever pulled off", "near the village of Dara Bazar in the Bajaur Agency,", "his business dealings for possible securities violations", "five", "12.9 - kilometre ( 8 mi )", "Pakistan", "The early modern period began approximately in the early 16th century", "Wisconsin", "water can flow from the sink into the faucet without modifying the system", "10 national ( significant ) numbers after the `` 0 '' trunk code", "British Columbia", "the sidewalk between Division Street and East Broadway", "robert laver", "france", "hopper", "Kenneth MacDonald", "meryl Streep", "potatoes", "australia Crosbie", "transition elements", "Knoxville, Tennessee", "Coalwood", "Sun Records founder Sam Phillips", "Westminster system", "Allan A. Goldstein", "\"Europop\".", "Black pudding", "northeastern", "T. S. Eliot", "Elie Wiesel", "Juliana", "Parkinson's disease", "shalom", "Nixon", "Kublai Khan", "a packer"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5626279047489154}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 0.1702127659574468, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.8571428571428571, 0.5714285714285715, 0.0, 1.0, 0.4, 1.0, 0.5, 0.0, 0.5714285714285715, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 0.0, 0.2222222222222222, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-852", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-3350", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-2682", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-4166", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-350", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-5601", "mrqa_triviaqa-validation-5269", "mrqa_triviaqa-validation-3605", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-3687", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-8966", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-13619", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-16575"], "SR": 0.390625, "CSR": 0.5366319444444445, "EFR": 0.9487179487179487, "Overall": 0.7076168536324786}, {"timecode": 90, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12633", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7012", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.865234375, "KG": 0.5046875, "before_eval_results": {"predictions": ["Apple Inc.", "second", "\"Avatar,\"", "using recreational drugs", "during the short time in office, from Vice President Joe Biden's verbose tendencies to an unfortunate Air Force One photo op that frightened New Yorkers -- playfully pointing his finger at his young daughters.", "CNN", "Rwanda", "Another high tide", "scored a hat-trick", "Apple employees", "80,", "at least seven", "one of their loved one's kidneys be transplanted to Ms. Cole if they were a match,\"", "Kenneth Cole", "the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "drugs against their will.", "to stop Noriko Savoie from being able to travel to Japan for summer vacation.", "strangulation and asphyxiation and had two broken bones in his neck,", "learn in safer surroundings.", "southern city of Naples", "July", "abducting each other for ransoms or retribution.", "Kim", "Colorado prosecutor", "Citizens are picking members of the lower house of parliament,", "Georgia Aquarium", "Saturday", "Kenneth Cole", "Republicans", "dismissed all charges", "NATO fighters", "Airbus A330-200", "14 \u00b0 41 \u2032 34 '' N 17 \u00b0 26 \u2032 48 '' W", "The Live - Stock Dealers'and Butchers'Association of New Orleans", "two", "the government", "Lenin", "eight", "Kenny Anderson", "Amenhotep IV", "Sherlock Holmes", "france", "in the north west", "The Hague", "Tombstone", "bactrian", "Grail", "Kim Smith", "T. R. M. Howard", "County Armagh", "The Bye Bye Man", "Eugene", "35,124", "Phil Collins", "Valhalla Highlands Historic District", "Can't Be Tamed", "numbness", "Howard Hughes", "the tapir", "pep", "Meyer Lansky", "Russians", "Brave New World", "Paul Revere"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7242083072263994}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5263157894736842, 0.3333333333333333, 1.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-4500", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-3386", "mrqa_triviaqa-validation-4041", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-3080", "mrqa_searchqa-validation-11572", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-1049"], "SR": 0.65625, "CSR": 0.5379464285714286, "retrieved_ids": ["mrqa_squad-train-27218", "mrqa_squad-train-77516", "mrqa_squad-train-66226", "mrqa_squad-train-24022", "mrqa_squad-train-25908", "mrqa_squad-train-45871", "mrqa_squad-train-27288", "mrqa_squad-train-15221", "mrqa_squad-train-7672", "mrqa_squad-train-9323", "mrqa_squad-train-1440", "mrqa_squad-train-39323", "mrqa_squad-train-41305", "mrqa_squad-train-45307", "mrqa_squad-train-55752", "mrqa_squad-train-5806", "mrqa_triviaqa-validation-4199", "mrqa_naturalquestions-validation-9064", "mrqa_searchqa-validation-4456", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-237", "mrqa_newsqa-validation-1281", "mrqa_naturalquestions-validation-9726", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-2925", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-7834", "mrqa_hotpotqa-validation-751", "mrqa_triviaqa-validation-804", "mrqa_triviaqa-validation-4960", "mrqa_hotpotqa-validation-5503", "mrqa_newsqa-validation-1440"], "EFR": 0.9545454545454546, "Overall": 0.7252171266233767}, {"timecode": 91, "before_eval_results": {"predictions": ["francs", "tulle", "Count Basie", "English", "Dog", "The Naked Gun", "the Clean Air Act", "Mercury & Venus", "\"The Wings of the Dove\"", "Stephen King", "Dunkin' Donuts", "the Viking Ship Museum", "Carmen", "Raven Symone", "the Clark bar", "Aladdin and the Wonderful Lamp", "The Devil's Advocate", "The Big Red One", "Charles Dickens", "Harry S. Truman", "echinacea", "peripheral vision", "The Police", "Halley's comet", "\"Just say no\"", "Namibia", "Kilimanjaro", "Ruth", "Dresden", "Magnolia acuminata", "Tennessee", "Big Ben", "Edward Furlong", "any cardiac rhythm where depolarization of the cardiac muscle begins at the sinus node", "the Naturalization Act of 1790", "Tim McGraw and Kenny Chesney", "seawater pearls", "May 2010", "a liquid crystal on silicon (LCoS ) ( based on an LCoS chip from Himax ), field - sequential color system, LED illuminated display", "Jesse McCartney", "Kentucky Derby", "campania", "la boh\u00e8me", "surtsey", "vevey, vaud, Switzerland", "six-pocket", "mmorpgs", "Herman G\u00f6ring", "Robert Digges Wimberly Connor", "about 3,000 inhabitants scattered in a dozen fishing villages", "1901", "member of the Executive Council of the General Anthroposophical Society at the Goetheanum in Dornach, Switzerland", "Dunlop", "Charles Hastings Judd", "1963", "Washington", "Wigan Athletic", "Democrats", "did not go into further detail about her heart condition or the medical procedure.", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "this week,\"", "Jacob Zuma,", "U.S. Secretary of State Hillary Clinton", "Lt. Holley Wimunc."], "metric_results": {"EM": 0.625, "QA-F1": 0.7547436299081036}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.1111111111111111, 0.28571428571428575, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.761904761904762, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.4210526315789474, 1.0, 1.0, 0.2857142857142857, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-13135", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-5681", "mrqa_searchqa-validation-5935", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-2265", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-4204", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-169", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1392"], "SR": 0.625, "CSR": 0.5388926630434783, "EFR": 0.9583333333333334, "Overall": 0.7261639492753623}, {"timecode": 92, "before_eval_results": {"predictions": ["severnton-on-trent", "sash", "moon landing", "queen", "Les Invalides", "The White House", "prince harry", "September 19", "h. h. Asquith", "john bbirolli", "clover", "george carlin", "testicles", "dumbo", "chromium", "staple Singers", "Baffin Island", "lesser antilles", "tragedy", "kia", "hard Times", "spain", "aikido", "folklorist", "four", "operation", "niece", "brook", "pyrotechnic", "at last", "france", "qu", "Afghanistan", "more of one good could be produced only by diverting resources from the other good, resulting in less production of it", "August 15, 1971", "December 24, 1836", "November 27, 2013", "a vertebral column ( spine )", "Rachel Kelly Tucker", "23 February", "\"boundary river\"", "Piper", "Westland", "Dennis Potter", "Ben Savage", "Prospero", "Southern State Parkway", "David Pajo", "40", "give detainees greater latitude in selecting legal representation", "Lexus, Lincoln, Infiniti or Porsche", "BBC's central London offices", "in a motel,", "Eintracht Frankfurt", "100 to 150", "March 24,", "Ted", "the piano", "governor of yore", "anthrax", "table tennis", "jade", "Tunisia", "Don Quixote"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5994791666666667}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-7661", "mrqa_triviaqa-validation-5934", "mrqa_triviaqa-validation-6725", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-6764", "mrqa_triviaqa-validation-3296", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-5112", "mrqa_triviaqa-validation-7367", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-3208", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-9400", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1003", "mrqa_searchqa-validation-802"], "SR": 0.5625, "CSR": 0.539146505376344, "EFR": 0.9642857142857143, "Overall": 0.7274051939324118}, {"timecode": 93, "before_eval_results": {"predictions": ["Colombia", "barbarians", "Ty Hardin", "dakota", "Thames", "Nadia Comaneci", "John Masefield", "olivier dahan", "egypt", "sesame Street", "groundhogs", "joule", "paul mkey", "m65", "britishtennis.com", "paul anka", "keeper of the Longstone (Fame Islands) lighthouse", "romania", "de quincey", "romania", "anton", "cauliflower", "Ambassador Bridge", "hrogate", "seven wonders", "Sony Interactive Entertainment", "beetle", "bauxite", "peter of jollity", "jeszebel", "p\u00e1linka", "sandstone Trail", "Session Initiation Protocol", "piety", "Part 2", "2013", "The Blind Boys of Alabama", "Etienne de Mestre", "1933", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house", "19th", "Atomic Kitten", "Johnny D. Bright", "saloon-keeper", "5320 km", "Hordaland", "Bancroft Shed", "Flushed Away", "women.", "18", "American Civil Liberties Union", "Michelle Obama", "Egypt", "People Against Switching Sides", "$8.8 million", "Manmohan Singh's", "the Etruscans", "Judi Dench", "Westminster Abbey", "runoff", "a pikas", "Richard Nixon", "Mexico", "Nebraska"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6066919191919191}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0909090909090909, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3773", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-432", "mrqa_triviaqa-validation-5097", "mrqa_triviaqa-validation-4954", "mrqa_triviaqa-validation-5647", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1718", "mrqa_triviaqa-validation-4200", "mrqa_triviaqa-validation-4827", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-1312", "mrqa_hotpotqa-validation-2746", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-5718", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-11414"], "SR": 0.5625, "CSR": 0.5393949468085106, "retrieved_ids": ["mrqa_squad-train-79425", "mrqa_squad-train-76858", "mrqa_squad-train-3268", "mrqa_squad-train-86436", "mrqa_squad-train-17431", "mrqa_squad-train-70787", "mrqa_squad-train-24388", "mrqa_squad-train-46324", "mrqa_squad-train-27753", "mrqa_squad-train-16191", "mrqa_squad-train-85018", "mrqa_squad-train-10803", "mrqa_squad-train-53043", "mrqa_squad-train-49508", "mrqa_squad-train-40784", "mrqa_squad-train-41428", "mrqa_hotpotqa-validation-4080", "mrqa_searchqa-validation-3449", "mrqa_squad-validation-2095", "mrqa_hotpotqa-validation-1902", "mrqa_newsqa-validation-2414", "mrqa_naturalquestions-validation-4821", "mrqa_hotpotqa-validation-2079", "mrqa_newsqa-validation-3265", "mrqa_hotpotqa-validation-5787", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-9973", "mrqa_newsqa-validation-1988", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-1529", "mrqa_triviaqa-validation-3787", "mrqa_hotpotqa-validation-68"], "EFR": 1.0, "Overall": 0.7345977393617021}, {"timecode": 94, "before_eval_results": {"predictions": ["the Atlantic", "a computer", "korky the Cat", "tintoretto", "Machu Picchu", "hawkley", "Blofeld", "massive stars", "romania", "Sir Hardy Amies", "the Central line", "bERLIOZ", "edmund", "mccartney", "tintoretto", "9", "sony Potter", "12", "Schengen Area", "Femoral", "Lilacs", "peter sampras", "gerry lloyd", "cosmos", "no 9", "eton College", "Richard Curtis", "south Carolina", "croquet", "john Donne", "coffee", "the Boston Pops Orchestra", "for the red - bed country of its watershed", "February 25, 2003", "the northern bluegrass band the Greenbriar Boys", "CBS", "Taiwan", "Abigail Hawk", "Lewis Carroll", "Asia", "1895", "The conversation", "five times", "26,000", "London", "Democratic", "Belladonna", "Matt Groening", "citizenship", "Cash for Clunkers", "\"A Lion Among Men,\"", "South Korea's new president", "prisoners at the South Dakota State Penitentiary", "1,500 Marines", "Pakistani officials,", "Steven Green", "Chevy Chase", "the Statue of Liberty", "Hugh Laurie", "fog", "the Prado Museum", "William M. Tweed", "a repo man", "Elvis Presley"], "metric_results": {"EM": 0.625, "QA-F1": 0.6927412714097496}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.08695652173913042, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-164", "mrqa_triviaqa-validation-947", "mrqa_triviaqa-validation-5293", "mrqa_triviaqa-validation-4476", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-4359", "mrqa_triviaqa-validation-6498", "mrqa_triviaqa-validation-1541", "mrqa_triviaqa-validation-4473", "mrqa_triviaqa-validation-3442", "mrqa_naturalquestions-validation-5006", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-4007", "mrqa_newsqa-validation-3681", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-10824", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-14018"], "SR": 0.625, "CSR": 0.540296052631579, "EFR": 0.9166666666666666, "Overall": 0.7181112938596492}, {"timecode": 95, "before_eval_results": {"predictions": ["Hollywood", "18th", "last week,", "\"Rin Tin Tin: The Life and the Legend\"", "Afghan lawmakers", "human rights abuses against ethnic Somalis by rebels and Ethiopian troops are rampant.", "2,000 euros ($2,963)", "Camp Lejeune, North Carolina", "prostate cancer,", "Karthik Rajaram,", "kill then-Sen. Obama on October 23, 2008,", "Robert Barnett,", "finance", "Samoa", "Jacob Zuma,", "severe famine", "WTA Tour titles", "A huge man-made island shaped like a date palm tree", "an \"unnamed international terror group\"", "a U.S. soldier", "Bhola for the Muslim festival of Eid al-Adha.", "38,", "U.S. State Department and British", "Jaipur", "$40 billion during the operations phase.", "$55.7 million", "11:30 p.m. Tuesday,", "a student", "9 a.m.", "40", "Afghanistan and India", "Frank Ricci,", "16 June", "Lisa Stelly", "Procol Harum", "the largest financial inflows to developing countries", "Mahatma Gandhi", "twice", "Levi and Bosco", "Sri Lanka Podujana Peramuna", "antoine de caunes", "romania", "stare Miasto", "anton leibovitz", "copper", "jubielka", "technetium", "methanol", "\"Histoires ou contes du temps pass\u00e9\"", "Lord Chancellor of England", "Lauren Lane", "Easy", "1932", "Centennial Olympic Stadium", "February 14, 1859", "Protestant Christian", "Woodland Hills", "black", "the gold rush", "a jabs", "a bluefin", "a lampoon", "Tommy Franks", "Liechtenstein"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6473958333333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 0.8333333333333333, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-880", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-10510", "mrqa_triviaqa-validation-1612", "mrqa_triviaqa-validation-4463", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-4510", "mrqa_triviaqa-validation-216", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-3718", "mrqa_hotpotqa-validation-4588", "mrqa_hotpotqa-validation-4007", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-7128", "mrqa_searchqa-validation-12584", "mrqa_searchqa-validation-8240"], "SR": 0.5625, "CSR": 0.54052734375, "EFR": 1.0, "Overall": 0.73482421875}, {"timecode": 96, "before_eval_results": {"predictions": ["the surge,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "in an artificial coma after she was admitted to a hospital in April for multiple organ failure,", "genocide, crimes against humanity, and war crimes.", "Rodong Sinmun", "two satellites", "near the Somali coast", "James Whitehouse,", "Hundreds", "Eleven", "near his home in Peshawar", "12-1 on aggregate.", "Aung San Suu Kyi", "July", "\"wider relationship\"", "bartering", "civilians,", "41,280", "The Tinkler", "suicides", "Charlotte Gainsbourg and Willem Dafoe", "urgently to be rescued,", "1940's Japan.", "\"a striking blow to due process and the rule of law.\"", "September 23,", "the first five Potter films have been held in a trust fund which he has not been able to touch.", "Pastor Paula White", "people who want, or need,", "television network.", "AbdulMutallab", "71 percent of Americans consider China an economic threat to the United States,", "R.E.M.", "Achal Kumar Jyoti", "Kenny Anderson", "7000301604928199000 \u2660 3.016 049 281 99 ( 23 ) u", "September 2, 1945", "President", "Washington metropolitan area", "Divyanka Tripathi", "tiffany and co", "heath Ledger", "Elizabeth Taylor", "jesse", "dodo", "kunagawa", "percyus", "piers", "Nicolas Vanier", "two", "Karen O", "North African Arab", "Brad Wilk", "MGM Resorts International", "4,530", "Krypto Report", "Hawaii", "satellites", "the Danube", "Fiddler", "Cue", "Redcliffe", "Grand Central Station", "shrewd"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6800629058441559}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 0.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2996", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-769", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-4063", "mrqa_naturalquestions-validation-2182", "mrqa_naturalquestions-validation-9450", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-7343", "mrqa_triviaqa-validation-5730", "mrqa_hotpotqa-validation-3428", "mrqa_searchqa-validation-4231", "mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-13697"], "SR": 0.609375, "CSR": 0.5412371134020619, "retrieved_ids": ["mrqa_squad-train-65247", "mrqa_squad-train-17773", "mrqa_squad-train-47127", "mrqa_squad-train-12908", "mrqa_squad-train-83071", "mrqa_squad-train-19368", "mrqa_squad-train-81490", "mrqa_squad-train-37762", "mrqa_squad-train-63339", "mrqa_squad-train-67129", "mrqa_squad-train-23111", "mrqa_squad-train-51635", "mrqa_squad-train-20501", "mrqa_squad-train-35591", "mrqa_squad-train-60422", "mrqa_squad-train-33504", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-1314", "mrqa_naturalquestions-validation-2207", "mrqa_hotpotqa-validation-5285", "mrqa_searchqa-validation-1941", "mrqa_searchqa-validation-9483", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-1390", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5466", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-7312", "mrqa_triviaqa-validation-7394", "mrqa_searchqa-validation-16459", "mrqa_searchqa-validation-9741", "mrqa_searchqa-validation-13871"], "EFR": 1.0, "Overall": 0.7349661726804124}, {"timecode": 97, "before_eval_results": {"predictions": ["Kindle Fire", "President Obama and Britain's Prince Charles", "that students often know ahead of time when and where violence will flare up on campus.", "the eradication of the Zetas cartel from the state of Veracruz, Mexico,", "women.", "that the FDA is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "the iconic Hollywood headquarters of Capitol Records,", "Trevor Rees,", "next year", "defaulted on the mortgage and the house fell into foreclosure.", "Filippo Inzaghi", "that", "al-Moayad", "he was not certain where the information that sparked the FAA notification came from, but it was \"probably from NORAD,\" or the North American Aerospace Defense Command,", "Illness", "her apartment near Fort Bragg in North Carolina.", "Free laundry", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "18th", "the government.", "15,000", "in Japan: the IV cafe.", "Top Gun", "al Qaeda.", "a Prius", "likening one American diplomat to a \"prostitute\" and threatening to oust another from his country.", "a delegation of American Muslim and Christian leaders", "Barack Obama", "autonomy", "Jeddah, Saudi Arabia,", "1831", "a leaky valve", "September 1980", "1905", "Amerigo Vespucci", "the foreign exchange market ( FX )", "Chinese immigration", "position", "The 111th edition", "Rory McIlroy", "Lake Nicaragua", "Macbeth", "From Russia with Love", "Santiago", "y Yahoo!", "1961", "rounders", "cars", "University of Southern California", "Lucille Ball", "14", "Cheshire County, New Hampshire", "the Atlantic Ocean", "tomato", "odd-eyed", "Magnate", "a spoonful", "Qwerty", "Austria", "garlic", "Argentina", "Repent", "C Daryl Chessman", "Paul"], "metric_results": {"EM": 0.625, "QA-F1": 0.743206272893773}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.8, 1.0, 0.9444444444444444, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.7692307692307693, 0.5, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 1.0, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-2696", "mrqa_newsqa-validation-813", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-3943", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5000", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-115", "mrqa_triviaqa-validation-6942", "mrqa_hotpotqa-validation-3139", "mrqa_hotpotqa-validation-1273", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15672"], "SR": 0.625, "CSR": 0.5420918367346939, "EFR": 0.9583333333333334, "Overall": 0.7268037840136055}, {"timecode": 98, "before_eval_results": {"predictions": ["downslope wind", "James Blunt", "queen katherine Parr", "air", "Lancashire", "whale", "mELBA", "hannibal heyes and kid Curry", "ubba", "lighthouse keeper", "al Yeganeh", "ukraine", "trombone", "egypt", "marriage", "Adrian Cronauer", "Thomas Jefferson", "macau", "Eric Coates", "Dublin", "Welcome Stranger", "brown", "The Violin", "nippon Sangyo", "north Carolina", "Time Machine", "jack Nicholson", "nounA", "new Hampshire", "ukraine wrayburn", "an arrowhead", "hand gun", "Schadenfreude", "Kelly Reno", "2018", "Ed Sheeran", "Dr. Derek Shepherd ( Patrick Dempsey )", "local organization of businesses whose goal is to further the interests of businesses", "Andrea Brooks", "The Impalas", "Cleveland Browns", "Edith Cavell", "Rick and Morty", "Tom Shadyac", "Jena Malone", "Red and Assiniboine Rivers", "Illinois", "Field of Dreams", "Steven Green", "prisoners at the South Dakota State Penitentiary", "surgical anesthetic propofol", "70,000", "his fleet of trucks used to pick up cargo.", "$250,000 for Rivers' charity: God's Love We Deliver.", "antihistamine and an epinephrine auto-injector for emergencies,", "more and more suspicious of the way their business books were being handled.", "War of 1812", "Valium", "The Wright Brothers", "the pie", "Waterloo", "Pan", "Qantas", "Anna and the King of Siam"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6557725694444445}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 0.3, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_triviaqa-validation-6214", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-1456", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-3614", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-6862", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-2342", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-2267", "mrqa_naturalquestions-validation-5509", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-1700", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2853", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-4208", "mrqa_searchqa-validation-15766", "mrqa_searchqa-validation-3710"], "SR": 0.578125, "CSR": 0.5424558080808081, "EFR": 1.0, "Overall": 0.7352099116161617}, {"timecode": 99, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3209", "mrqa_hotpotqa-validation-3351", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-38", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5235", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7499", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13135", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13632", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2816", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9772", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2269", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3442", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4041", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4204", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6359", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.83984375, "KG": 0.4953125, "before_eval_results": {"predictions": ["caracas", "Richard Marx", "cat", "lodges", "indiana jones", "moon River", "Malcolm Turnbull", "a fish", "charlie cairoli", "Gene Autry", "Addis Ababa", "jupiter", "Donald Sutherland", "alligators", "\"Holiday Inn\"", "equatorial Guinea", "mEXICO cartel", "tassel", "London Underground Piccadilly Line", "hominins", "paul Gauguin", "Michael Caine", "a transitional point between feathered dinosaurs and modern birds", "wordsworth", "Queens Park Rangers", "Sindh\u016b", "reel life", "yellow", "cedars", "cycling", "24", "Helen Clark", "Australia", "more than a million members ( including 195,000 youth members )", "1994", "the foreign exchange market ( FX )", "March 11, 2016", "precipitation is any product of the condensation of atmospheric water vapor that falls under gravity", "31 January 1934", "Ben Faulks", "Clarence Nash", "Cheshire", "September 21, 2014", "\"Kitty Hawk\"", "Kew Gardens", "Roger Jason Stone Jr.", "\"Si Da Ming Bu\"", "James William McCutcheon", "1975", "Hundreds of Palestinians have been killed, including many civilians,", "the Nazi war crimes suspect", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "MEND", "Swiss art heist", "South Africa,", "Oxbow,", "(St.) Louis", "Auguste Rodin", "Red Button", "Colonial Williamsburg", "Walt Whitman", "Time", "a kiwi", "quarks"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6233901515151514}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-4056", "mrqa_triviaqa-validation-4768", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-4464", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1340", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-1026", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-4911", "mrqa_hotpotqa-validation-5146", "mrqa_hotpotqa-validation-1083", "mrqa_newsqa-validation-60", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-4030", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-3358", "mrqa_searchqa-validation-14625"], "SR": 0.546875, "CSR": 0.5425, "retrieved_ids": ["mrqa_squad-train-75426", "mrqa_squad-train-57770", "mrqa_squad-train-4218", "mrqa_squad-train-56673", "mrqa_squad-train-53289", "mrqa_squad-train-3914", "mrqa_squad-train-60112", "mrqa_squad-train-64950", "mrqa_squad-train-54733", "mrqa_squad-train-9330", "mrqa_squad-train-2429", "mrqa_squad-train-23632", "mrqa_squad-train-61509", "mrqa_squad-train-16105", "mrqa_squad-train-78194", "mrqa_squad-train-69772", "mrqa_triviaqa-validation-487", "mrqa_naturalquestions-validation-2467", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-4160", "mrqa_hotpotqa-validation-5760", "mrqa_naturalquestions-validation-2732", "mrqa_newsqa-validation-3566", "mrqa_naturalquestions-validation-5809", "mrqa_newsqa-validation-973", "mrqa_hotpotqa-validation-5753", "mrqa_naturalquestions-validation-2323", "mrqa_triviaqa-validation-4483", "mrqa_triviaqa-validation-575", "mrqa_naturalquestions-validation-6806", "mrqa_hotpotqa-validation-1692", "mrqa_triviaqa-validation-2092"], "EFR": 1.0, "Overall": 0.7278749999999999}]}