{"method_class": "er", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5', diff_loss_weight=0.0, gradient_accumulation_steps=1, inference_query_size=1, init_memory_cache_path='na', kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, local_adapt_lr=1e-05, max_grad_norm=0.1, memory_key_encoder='facebook/bart-base', memory_path='experiments/ckpt_dirs/qa/er/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5/memory_dict.pkl', memory_store_rate=1.0, num_adapt_epochs=0, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, replay_candidate_size=8, replay_frequency=3, replay_size=32, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, upstream_sample_ratio=0.5, use_replay_mix=True, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=16, result_file='experiments/results/qa/qa_er_lr=3e-5_ep=10_l2w=0_rs=32_rf=3_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.5.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 5410, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["1978", "587,000 square kilometres", "itinerant farmers", "Gamal Abdul Nasser", "39", "Philo of Byzantium", "cnidarians", "the dukes", "Christopher Eccleston", "Ed Lee", "Keck and Mithouard", "Word and Image Department", "Water (H2O) and carbon dioxide (CO2)", "BBC Radio 5 Live", "Baptism", "achievement-oriented motivations (\"pull\")", "2 million", "inferior", "until 1796", "dummy upper stages filled with water", "Variable lymphocyte receptors", "progressive folk-rock", "$32 billion", "Derek Wolfe", "Basel", "1937", "tourism", "white", "Midsummer\u2019s Night", "installed electrical arc light based illumination systems", "2016", "all large cases of the problem are hard", "photooxidative damage", "five", "iteratively", "the Sun", "Climate fluctuations during the last 34 million years", "Bible translation", "Hayri Abaza", "a better understanding of the Mau Mau command structure", "Turnagain Lane", "monophyletic", "adaptive immune system", "The Dornbirner Ach", "water flow through the body cavity", "CBSE", "a cubic interpolation formula", "Writers Guild of America", "education", "five", "14th to the 19th century", "extended structure", "the Lisbon Treaty", "the Romantic Rhine", "2012", "No Child Left Behind", "The Deadly Assassin and Mawdryn undead", "higher than normal O2 exposure for a fee", "three to five", "TFEU article 294", "The Northern Chinese were ranked higher", "Tracy Wolfson", "local building authority regulations and codes of practice", "The WB"], "metric_results": {"EM": 0.859375, "QA-F1": 0.8807426948051948}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4257", "mrqa_squad-validation-486", "mrqa_squad-validation-5362", "mrqa_squad-validation-291", "mrqa_squad-validation-1862", "mrqa_squad-validation-9520", "mrqa_squad-validation-3104", "mrqa_squad-validation-3609", "mrqa_squad-validation-8256"], "SR": 0.859375, "CSR": 0.859375, "EFR": 0.8888888888888888, "Overall": 0.8741319444444444}, {"timecode": 1, "before_eval_results": {"predictions": ["euphoric", "Khorasan", "higher", "bigamy", "pathogens", "the geographical area it covers", "ships", "12 January", "7.5%", "Ren\u00e9 Lalique", "The Hoppings", "the Scots", "double or triple", "wage or salary", "Each packet is labeled with a destination address, source address, and port numbers", "Maria Fold and thrust Belt", "Yinchuan", "between AD 0\u20131250", "Chivas", "26", "Amazonia: Man and Culture in a Counterfeit Paradise", "1550", "two", "1850", "Greg Brady", "colonialism", "22 October 2006", "10,000", "Foreign Protestants Naturalization Act", "art posters", "Americans", "its unpaired electrons", "Los Angeles", "over 100,000", "wealth", "chromoplasts", "1,548", "The upper Rhine and upper Danube are easily crossed", "Peter Pratt and Geoffrey Beevers", "2010", "Germany and Austria", "Immunoproteomics", "solid economic growth", "ditch digger", "1754\u20131763", "Danny Trevathan", "Jochi", "1999", "rubisco", "August 1914", "public", "affordable housing", "seven", "John Mearsheimer and Robert Pape", "Business Connect", "1550 to 1900", "rules that conflict with morality", "Bauhaus", "2015", "French", "EBSCO", "The official vegetable of Washington State is a sweet onion", "I've been exiled to Siberia!", "The definition of a cloister is a secluded monastery or any... a place of religious seclusion"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8233840811965812}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.15384615384615385]}}, "before_error_ids": ["mrqa_squad-validation-3190", "mrqa_squad-validation-4677", "mrqa_squad-validation-360", "mrqa_squad-validation-9802", "mrqa_squad-validation-9372", "mrqa_squad-validation-7701", "mrqa_squad-validation-6891", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5763", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-206"], "SR": 0.796875, "CSR": 0.828125, "EFR": 1.0, "Overall": 0.9140625}, {"timecode": 2, "before_eval_results": {"predictions": ["2016", "expansion", "Shirley and Johnson", "Miller", "1892 to 1894", "26", "99.4", "St Thomas Becket", "they are homebound", "two", "Moscone Center", "Germany", "January", "about thirty", "10", "Greg Olsen", "Prince of P\u0142ock", "electron microscopy", "computability theory", "1275", "Fred Silverman", "liquid nitrogen", "Quasiturbine", "UK", "Daniel 8:9\u201312, 23\u201325", "Ex post facto laws", "cameras", "time and space hierarchy theorems", "$105 billion", "six", "55 mph", "The Bachelor", "child-killers", "Warfare and the long occupation", "Cretaceous\u2013Paleogene extinction", "almost a month", "George B. Storer", "CD40", "94", "their dispersed population and distance from the Scottish Parliament in Edinburgh", "difference in potential energy", "George Westinghouse", "Geneva", "Court of Justice", "Lake \u00dcberlingen", "Budapest", "Genoese traders", "the \"simple people\"", "Stanford University", "formal language", "systematic economic inequalities", "stealing", "lunar new year", "Steve McQueen", "alcohol", "Sedgefield", "Sirhan Sirhan", "a wooden comb", "Asia", "People!  and The Carnabeats", "a Yemeni cleric and his personal assistant", "the 3rd Sun", "Xherdan Shaqiri", "Potomac River"], "metric_results": {"EM": 0.734375, "QA-F1": 0.765625}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6337", "mrqa_squad-validation-7165", "mrqa_squad-validation-1703", "mrqa_squad-validation-5921", "mrqa_squad-validation-3540", "mrqa_squad-validation-4096", "mrqa_squad-validation-8534", "mrqa_squad-validation-4206", "mrqa_squad-validation-3945", "mrqa_squad-validation-4771", "mrqa_squad-validation-1509", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4028", "mrqa_newsqa-validation-817", "mrqa_searchqa-validation-13857", "mrqa_hotpotqa-validation-1902"], "SR": 0.734375, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 3, "before_eval_results": {"predictions": ["Religious Coalition for Reproductive Choice", "Waal", "malaria parasite", "three", "over $40 million", "SyFy", "Ollie Treiz", "five", "younger", "nearly three hundred years", "1 July 1851", "the world's economy", "nine", "the property owner", "1916", "twice", "its unpaired electrons", "Golovin", "how or whether this connection is relevant on microscales", "Matt Smith", "Baltimore", "noble", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "CBS and NBC", "mediaeval music", "southern California", "26", "Masovian Primeval Forest", "Apollo 1 backup crew", "2008", "a hemicycle", "the Connectional Table", "waldzither", "a bishop", "performance", "destruction of the forest", "10.0%", "Outlaws", "Africa", "an occupancy permit", "one", "Dignity Health", "Battle of Jumonville Glen", "Kony Ealy", "a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "Napoleon", "2016", "kennedy", "kennedy", "Bones", "The History Book Club", "kennedy", "California's 33rd congressional district", "yerevan", "The Vampire Armand", "The Solar System is located within the disk", "Hugh S. Johnson", "The Five Stages of Sleep", "a person who smuggles what across the U.S. border", "various", "1896", "Carrousel du Louvre", "last year's Gaza campaign", "Odense"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7113935291858678}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8387096774193548, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19047619047619044, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-6008", "mrqa_squad-validation-10427", "mrqa_squad-validation-438", "mrqa_squad-validation-6118", "mrqa_squad-validation-2612", "mrqa_squad-validation-10068", "mrqa_squad-validation-4360", "mrqa_squad-validation-6426", "mrqa_searchqa-validation-9423", "mrqa_searchqa-validation-14645", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-6097", "mrqa_searchqa-validation-4289", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_naturalquestions-validation-808", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-7616", "mrqa_hotpotqa-validation-3780", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-1749", "mrqa_hotpotqa-validation-5584"], "SR": 0.65625, "CSR": 0.76171875, "retrieved_ids": ["mrqa_squad-train-18592", "mrqa_squad-train-28195", "mrqa_squad-train-38972", "mrqa_squad-train-54614", "mrqa_squad-train-23146", "mrqa_squad-train-40088", "mrqa_squad-train-76521", "mrqa_squad-train-31904", "mrqa_squad-train-8258", "mrqa_squad-train-73969", "mrqa_squad-train-64043", "mrqa_squad-train-22632", "mrqa_squad-train-20792", "mrqa_squad-train-33368", "mrqa_squad-train-41237", "mrqa_squad-train-58182", "mrqa_squad-validation-3609", "mrqa_searchqa-validation-206", "mrqa_squad-validation-5921", "mrqa_searchqa-validation-3272", "mrqa_squad-validation-7701", "mrqa_triviaqa-validation-3751", "mrqa_squad-validation-5362", "mrqa_squad-validation-4677", "mrqa_triviaqa-validation-4028", "mrqa_squad-validation-4206", "mrqa_hotpotqa-validation-5763", "mrqa_squad-validation-486", "mrqa_squad-validation-1703", "mrqa_newsqa-validation-817", "mrqa_hotpotqa-validation-1902", "mrqa_squad-validation-8256"], "EFR": 1.0, "Overall": 0.880859375}, {"timecode": 4, "before_eval_results": {"predictions": ["13", "The affair caused lasting damage to Luther's reputation.", "178", "1st century BC", "The Five Doctors", "Combined Statistical Area", "Bryan Davies", "1060s", "Fresno", "they finally captured Ticonderoga", "Utopia", "John Debney", "BBC Dead Ringers", "July 11, 1962", "Light", "49\u201315", "pancake-shaped circular disks about 300\u2013600 nanometers in diameter", "The energy crisis", "nine", "very low tuition fees", "increase its bulk and decrease its density", "paying his rent at the Hotel New Yorker", "achieving crime control via incapacitation and deterrence", "John Debney", "symbiotic", "Larry Roberts", "CBS", "leftist/communist/nationalist insurgents/opposition", "RSA", "the wisdom and prudence of certain decisions of procurement", "France's claim to the region was superior to that of the British", "male sex hormones such as testosterone seem to be immunosuppressive", "very weak", "private citizen", "Florida", "August 1992", "71%", "Samuel Phillips", "1962", "Newcastle Diamonds", "five", "demographics and economic ties", "UNICEF", "Morgan Tsvangirai", "civilians", "maintain an \"aesthetic environment\" and ensure public safety", "Pakistani territory", "four months ago", "right-wing extremist groups.", "Argentina lays claim not just to the islands, but to any resources that could be found there.", "led authorities to a $13 million global crime ring", "Don Draper", "it was beautiful. The rain held off wherever Muhammad Ali went.", "there is one body and one Spirit just as you were called to the one hope that belongs to your call one Lord", "1979", "a large primate species", "John Denver", "1641", "House of Commons", "david Klum", "The ballot", "plants", "white", "Montezuma"], "metric_results": {"EM": 0.671875, "QA-F1": 0.6961097461097461}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 0.6, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 0.05405405405405406, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2591", "mrqa_squad-validation-7792", "mrqa_squad-validation-2717", "mrqa_squad-validation-10269", "mrqa_squad-validation-7715", "mrqa_squad-validation-236", "mrqa_squad-validation-8811", "mrqa_squad-validation-6874", "mrqa_squad-validation-7713", "mrqa_squad-validation-6595", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_naturalquestions-validation-7901", "mrqa_triviaqa-validation-6976", "mrqa_hotpotqa-validation-2418", "mrqa_searchqa-validation-8027", "mrqa_triviaqa-validation-1088"], "SR": 0.671875, "CSR": 0.74375, "EFR": 1.0, "Overall": 0.871875}, {"timecode": 5, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "Air Force missile projects", "The Entertainment Channel", "from home viewers who made tape recordings of the show", "cattle and citrus", "quantum electrodynamics", "through confirmation and sometimes the profession of faith", "75th birthday", "a computational problem where a single output (of a total function) is expected for every input", "Plasmodium falciparum", "civil disobedience", "Masovian gothic", "more wealth and income", "polynomial-time", "2012", "European Parliament and the Council of the European Union", "antithetical", "polynomial time", "Fred Singer", "William of Volpiano and John of Ravenna", "water in equilibrium with air", "Arizona Cardinals", "Seventy percent", "co-chair of TAR WGI", "July 24", "nearly three hundred years", "Encoded Archival description (EAD)", "the Privy Council", "a citizen's relation to the state and its laws", "Golden Gate Bridge", "2011", "respiration", "environmental degradation", "Fred Pierce", "2016", "390", "1 million", "in the chloroplasts of C4 plants", "When the reaction occurs in a liquid solution, the solid formed is called the'precipitate '", "Jim Capaldi, Paul Carrack, and Peter Vale,", "the five closer together in trying to figure out who their tormentor is", "in order to halt it following brake failure", "2008", "boy", "1998", "is the study of the record of the Earth's magnetic field in rocks, sediment, or archeological materials", "the road is travelled by funeral convoys for fallen Canadian Forces personnel from CFB Trenton to the coroner's office in Toronto", "April 25 -- 30 in Park Avenue, just outside the Waldorf - Astoria Hotel", "Tsetse fold their wings completely when they are resting so that one wing rests directly on top of the other over their abdomens", "December 1, 2009", "occur in sequence with each heartbeat", "the Colonel Bogey March", "Richard John Seddon", "Switzerland", "Black Abbots", "1919", "Subha", "Roger Federer", "Bobby Jindal", "July 4", "\"Personal Jesus\"", "the Thorn Birds", "cuttlefish", "Australia"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6374790244809909}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, true], "QA-F1": [0.4, 1.0, 1.0, 0.9411764705882353, 0.5, 1.0, 1.0, 1.0, 0.2666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.7058823529411764, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 0.9767441860465117, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-7647", "mrqa_squad-validation-2705", "mrqa_squad-validation-1600", "mrqa_squad-validation-6670", "mrqa_squad-validation-2160", "mrqa_squad-validation-3556", "mrqa_squad-validation-4260", "mrqa_squad-validation-8189", "mrqa_squad-validation-7629", "mrqa_squad-validation-8671", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2555", "mrqa_hotpotqa-validation-3223", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-2328", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-10685"], "SR": 0.546875, "CSR": 0.7109375, "EFR": 0.9655172413793104, "Overall": 0.8382273706896552}, {"timecode": 6, "before_eval_results": {"predictions": ["2007", "1973", "Siegfried", "June", "27 September 2001", "teachers who are friendly and supportive", "500", "northern China", "18 February 1546", "tree growth stages", "Karl von Miltitz", "a supervisory church body", "via the ballast tanks of ships", "1279", "volcanic", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "(at the opposite end from the mouth)", "models", "river Deabolis", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts", "October", "three", "\"chameleon circuit\"", "sea level", "by using net wealth (adding up assets and subtracting debts), the Oxfam report, for instance, finds that there are more poor people in the United States and Western Europe than in China", "geographic scholars under colonizing empires", "Lippe", "colonialism", "10,000", "layered basaltic lava flows", "political parties", "$5,000,000", "a second Gleichschaltung or similar event in the future", "a transient or ongoing role", "11 February 2012", "Sets heart in mediastinum and limits its motion", "The selected to vote include academics, journalists, producers, and others with music industry experience", "the country was known as Santo Domingo -- the name of its present capital and patron saint, Saint Dominic", "O'Meara", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "The novena should begin on Good Friday", "When all the numbers required to win a prize have been marked off", "9.0 -- 9.1 ( M )", "English author Rudyard Kipling", "Roanoke", "David Joseph Madden", "college football at Baylor, where he won the 2011 Heisman Trophy", "minor key symphonies", "in Christianity", "British statesman and prime minister Benjamin Disraeli, 1st Earl of Beaconsfield", "Belfast", "St Paul's Cathedral", "Sydney", "Australian", "Battle of Britain and the Battle of Malta", "a \"stressed and tired force\" made vulnerable by multiple deployments", "Casalesi Camorra", "Microsoft", "bonds", "Queen Wilhelmina", "the Mole", "Paolo di Dono", "Crete", "Selfie"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6691954746642247}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.05714285714285715, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 0.0, 1.0, 0.07692307692307693, 0.4, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.4, 0.25, 0.42857142857142855, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2095", "mrqa_squad-validation-4506", "mrqa_squad-validation-2468", "mrqa_squad-validation-3620", "mrqa_squad-validation-9408", "mrqa_squad-validation-7554", "mrqa_squad-validation-9865", "mrqa_squad-validation-6962", "mrqa_squad-validation-1866", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-4942", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-1058", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-3918", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-4084", "mrqa_triviaqa-validation-5713"], "SR": 0.578125, "CSR": 0.6919642857142857, "retrieved_ids": ["mrqa_squad-train-68524", "mrqa_squad-train-70424", "mrqa_squad-train-58673", "mrqa_squad-train-23165", "mrqa_squad-train-77852", "mrqa_squad-train-71007", "mrqa_squad-train-78498", "mrqa_squad-train-19393", "mrqa_squad-train-58283", "mrqa_squad-train-64936", "mrqa_squad-train-12764", "mrqa_squad-train-57272", "mrqa_squad-train-29941", "mrqa_squad-train-20081", "mrqa_squad-train-80116", "mrqa_squad-train-83137", "mrqa_naturalquestions-validation-6857", "mrqa_squad-validation-3104", "mrqa_squad-validation-4677", "mrqa_triviaqa-validation-4028", "mrqa_squad-validation-1600", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-1749", "mrqa_hotpotqa-validation-4950", "mrqa_squad-validation-4260", "mrqa_searchqa-validation-7793", "mrqa_squad-validation-10269", "mrqa_hotpotqa-validation-4259", "mrqa_squad-validation-9802", "mrqa_squad-validation-9372", "mrqa_newsqa-validation-462", "mrqa_squad-validation-6118"], "EFR": 1.0, "Overall": 0.8459821428571428}, {"timecode": 7, "before_eval_results": {"predictions": ["annual NFL Experience", "over three days", "second and third run movies, along with classic films", "radiography", "the government and the National Assembly and the Senate. The Judiciary", "lipophilic alkaloid toxins through their flesh", "the Compromise of 1850", "a multi-party system", "phagosomal", "1969", "preparation and approval process", "Community law", "indirectly", "Downtown San Diego", "1954", "Bermuda 419 turf", "The Judiciary", "they are judged \" wrong\" by an individual conscience, or as part of an effort to render certain laws ineffective, to cause their repeal, or to exert pressure to get one's political wishes on some other issue", "\"The Big Bang\"", "only a minority of the genetic material is kept in circular chromosomes while the rest is in branched, linear, or other complex structures", "The Hoppings funfair", "Decision problems", "seven", "Miami", "a person violates a law in order to create a test case as to the constitutionality of a law, or by fleeing the jurisdiction", "56.2%", "Financial crisis of 2007\u201308", "William Hartnell", "Percy Shelley", "\"ctenes\" or \"comb plates\"", "American Broadcasting-Paramount Theatres, Inc.", "Ray Henderson", "rises 735 feet ( 224 m )", "B.R. Ambedkar, the chairman of the Drafting Committee, is widely considered to be its chief architect", "Kim Basinger", "Secretary of Homeland Security is Kirstjen Nielsen", "2001 -- 2002 season", "the rez is characterized by lack of opportunity and poor education, the solution to which appears to lie in the Western world", "two", "1973", "Bachendri Pal", "line the cavities and surfaces of blood vessels and organs throughout the body", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, breast, lung, salivary glands, eye, and skin", "Plank", "Ron Harper", "Dmitri Mendeleev", "sacroiliac", "the Potteries", "South African", "John Chilcot", "the Eagle has landed", "Kind Hearts and Coronets", "\" Engirundho Vandhaal\"", "2018 Unibet Premier League Darts", "2004", "Buenos Aires", "Evan Bayh of Indiana and Virginia Gov. Tim McAuliffe are considered to be among the top tier of VP contenders.", "after Wood went missing off Catalina Island, near the California coast, following an argument the couple had.", "Juarez drug cartel", "a chemical reaction to speed up but is not used up", "achthus", "1876", "\"Something to Talk About\"", "between the three towns of Doncaster, Scunthorpe and Gainsborough"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6082080402561294}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.923076923076923, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.8, 1.0, 0.34146341463414637, 0.0, 0.09523809523809523, 1.0, 1.0, 1.0, 0.0, 0.15384615384615383, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.4444444444444445, 1.0, 0.09523809523809525, 1.0, 0.0, 1.0, 0.9523809523809523, 0.9302325581395349, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.09090909090909091, 0.0, 0.5, 0.16666666666666669, 0.0, 1.0, 0.0, 0.18181818181818182]}}, "before_error_ids": ["mrqa_squad-validation-8319", "mrqa_squad-validation-4186", "mrqa_squad-validation-8316", "mrqa_squad-validation-8496", "mrqa_squad-validation-4589", "mrqa_squad-validation-10444", "mrqa_squad-validation-436", "mrqa_squad-validation-6787", "mrqa_squad-validation-7774", "mrqa_squad-validation-8732", "mrqa_squad-validation-10140", "mrqa_squad-validation-6776", "mrqa_squad-validation-7773", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-2940", "mrqa_triviaqa-validation-3602", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-861", "mrqa_newsqa-validation-3889", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-493", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-6517", "mrqa_hotpotqa-validation-1533"], "SR": 0.46875, "CSR": 0.6640625, "EFR": 1.0, "Overall": 0.83203125}, {"timecode": 8, "before_eval_results": {"predictions": ["Episcopal Areas", "\"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "scoil phr\u00edobh\u00e1ideach", "1993", "Chinatown", "state, relative cost of living, and grade taught", "it stimulated his brain cells", "home viewers who made tape recordings", "the chosen machine model", "captive import policy", "the 1855 colonial constitution, passed by the United Kingdom Parliament", "Rhine Gorge", "savanna or desert", "1967", "Vince Lombardi Trophy", "2003", "Kurt Vonnegut", "10", "to be compensated for their patient care skills", "ten", "12", "1995\u201396", "San Diego International Airport", "hydrogen and helium", "two", "Golden Gate Bridge", "Elders", "1331", "rapid destruction of the donor red blood cells by host antibodies ( IgG, IgM )", "Casey Beane", "Spain had more than compensated by recovering Menorca", "Eurasian Plate", "1947", "John Dalton", "they have also won the competition the most times in a row", "it was on this day in 1930", "was a naval battle fought between an alliance of Greek city - states under Themistocles and the Persian Empire", "Road / Track", "Pradyumna", "Phillipa Soo", "May 18, 2010", "St. John's, Newfoundland and Labrador", "Mangal Pandey", "Asuka", "all of those elements have the same number of electron shells", "the pressure is assumed to be 1 atm ( 101.325 kPa )", "Laughing my great *a* off", "beetles", "Malawi", "50", "an Anglo-Saxon saint", "Jena Malone", "French", "1993", "civilians", "if huge hunks of ice -- such as parts of Greenland and the western shelf of Antarctica", "orders immigrants to carry their alien registration documents at all times and requires police to question people if there's reason to suspect they're in the United States illegally", "Piedad Cordoba", "the Treaty of Paris", "a classic red mani", "Robert", "congruent", "The Dark Tower", "Ministry of European Integration"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6418205697810961}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, false, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.0, 0.0, 1.0, 1.0, 0.5454545454545454, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.2222222222222222, 0.0, 0.0, 1.0, 0.0, 0.3888888888888889, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 0.4, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5789473684210525, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9578", "mrqa_squad-validation-2236", "mrqa_squad-validation-1445", "mrqa_squad-validation-7643", "mrqa_squad-validation-2902", "mrqa_squad-validation-8984", "mrqa_squad-validation-6404", "mrqa_squad-validation-3667", "mrqa_naturalquestions-validation-2210", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-1119", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-672", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-2631", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-3581", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-13484"], "SR": 0.515625, "CSR": 0.6475694444444444, "EFR": 0.967741935483871, "Overall": 0.8076556899641577}, {"timecode": 9, "before_eval_results": {"predictions": ["Sports Night", "c1750", "Newton", "Ford", "Political Islam", "51.6%", "public policy goals", "the superior and the norm", "1985", "standard", "Theory of the Earth", "New Jersey, Rhode Island and Delaware", "2,249", "AS-205 would have been devoted to space experiments and contribute no new engineering knowledge about the spacecraft", "Earth", "prime elements", "T. J. Ward", "5", "many individuals in the LDS Church, often a trusted friend, who may hold any office, from Elder to Bishop, or no office at all", "primes", "1995", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "applied mathematics to the construction", "spy network", "Elizabeth", "1861\u20131865", "1968", "Daimler-Benz", "25", "Baldwin", "May 1801", "John Schlesinger", "hamburgers", "Ellie Kemper", "Ministro Pistarini International Airport", "leg injury", "life insurance", "bioelectromagnetics", "The State of Franklin", "Innsbruck", "Andes", "Vishal Bhardwaj", "Lincoln Memorial University", "five", "Lily Hampton", "Wyatt and Dylan Walters", "Justin Bieber", "Nitty Gritty Dirt Band", "April", "Lucky the Leprechaun", "(abydus)", "one", "river", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Intertropical Convergence Zone", "\"Quiet Nights\"", "Crandon, Wisconsin", "the Lone Ranger", "(CRUX)", "(born November 8, 1965) is an American computer scientist and entrepreneur.", "Cheryl of the Clue Crews", "1922", "support, movement, protection, production of blood cells, storage of minerals, and endocrine regulation", "John F. Kennedy"], "metric_results": {"EM": 0.53125, "QA-F1": 0.609733893557423}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 0.38095238095238093, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.47058823529411764, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-361", "mrqa_squad-validation-9608", "mrqa_squad-validation-7017", "mrqa_squad-validation-10506", "mrqa_squad-validation-3954", "mrqa_squad-validation-2315", "mrqa_squad-validation-6806", "mrqa_squad-validation-8231", "mrqa_squad-validation-6154", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-162", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-1567", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-4050", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-4272", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2324", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-2892", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5876"], "SR": 0.53125, "CSR": 0.6359375, "retrieved_ids": ["mrqa_squad-train-1155", "mrqa_squad-train-83055", "mrqa_squad-train-23492", "mrqa_squad-train-80952", "mrqa_squad-train-40788", "mrqa_squad-train-77977", "mrqa_squad-train-301", "mrqa_squad-train-52282", "mrqa_squad-train-42921", "mrqa_squad-train-9603", "mrqa_squad-train-5304", "mrqa_squad-train-70965", "mrqa_squad-train-49029", "mrqa_squad-train-67014", "mrqa_squad-train-26061", "mrqa_squad-train-22320", "mrqa_squad-validation-2717", "mrqa_squad-validation-9802", "mrqa_squad-validation-7773", "mrqa_naturalquestions-validation-1038", "mrqa_squad-validation-7554", "mrqa_triviaqa-validation-7616", "mrqa_squad-validation-10140", "mrqa_newsqa-validation-3889", "mrqa_triviaqa-validation-2099", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-2095", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2555", "mrqa_squad-validation-3104", "mrqa_squad-validation-6787", "mrqa_naturalquestions-validation-2837"], "EFR": 0.9666666666666667, "Overall": 0.8013020833333333}, {"timecode": 10, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1086", "mrqa_hotpotqa-validation-1327", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1966", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2418", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2804", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3486", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3750", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4617", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-861", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2901", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-3538", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5355", "mrqa_naturalquestions-validation-5510", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5687", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-5896", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-710", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-7464", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7754", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8151", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8896", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9963", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1364", "mrqa_newsqa-validation-1636", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3197", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-787", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14030", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-206", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8027", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10004", "mrqa_squad-validation-10010", "mrqa_squad-validation-10024", "mrqa_squad-validation-10038", "mrqa_squad-validation-10059", "mrqa_squad-validation-10068", "mrqa_squad-validation-10072", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10115", "mrqa_squad-validation-10124", "mrqa_squad-validation-10140", "mrqa_squad-validation-10232", "mrqa_squad-validation-10340", "mrqa_squad-validation-10340", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1172", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1311", "mrqa_squad-validation-1409", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1541", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1634", "mrqa_squad-validation-1637", "mrqa_squad-validation-1651", "mrqa_squad-validation-1703", "mrqa_squad-validation-1762", "mrqa_squad-validation-1817", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2095", "mrqa_squad-validation-2108", "mrqa_squad-validation-2160", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2315", "mrqa_squad-validation-2325", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2403", "mrqa_squad-validation-2461", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2612", "mrqa_squad-validation-2678", "mrqa_squad-validation-2711", "mrqa_squad-validation-2717", "mrqa_squad-validation-2752", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2861", "mrqa_squad-validation-2869", "mrqa_squad-validation-2902", "mrqa_squad-validation-291", "mrqa_squad-validation-2916", "mrqa_squad-validation-2934", "mrqa_squad-validation-2952", "mrqa_squad-validation-2985", "mrqa_squad-validation-3049", "mrqa_squad-validation-3104", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-322", "mrqa_squad-validation-3222", "mrqa_squad-validation-3223", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-334", "mrqa_squad-validation-3347", "mrqa_squad-validation-3416", "mrqa_squad-validation-343", "mrqa_squad-validation-3440", "mrqa_squad-validation-3524", "mrqa_squad-validation-3540", "mrqa_squad-validation-3556", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3609", "mrqa_squad-validation-361", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-3620", "mrqa_squad-validation-3660", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3713", "mrqa_squad-validation-3745", "mrqa_squad-validation-3751", "mrqa_squad-validation-3752", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3866", "mrqa_squad-validation-3871", "mrqa_squad-validation-3873", "mrqa_squad-validation-3954", "mrqa_squad-validation-3957", "mrqa_squad-validation-3962", "mrqa_squad-validation-3986", "mrqa_squad-validation-4026", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-418", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4206", "mrqa_squad-validation-4242", "mrqa_squad-validation-4246", "mrqa_squad-validation-4257", "mrqa_squad-validation-4260", "mrqa_squad-validation-4305", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4447", "mrqa_squad-validation-4451", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-45", "mrqa_squad-validation-453", "mrqa_squad-validation-4533", "mrqa_squad-validation-4547", "mrqa_squad-validation-4575", "mrqa_squad-validation-4589", "mrqa_squad-validation-4630", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-47", "mrqa_squad-validation-4707", "mrqa_squad-validation-4730", "mrqa_squad-validation-4771", "mrqa_squad-validation-4775", "mrqa_squad-validation-4832", "mrqa_squad-validation-486", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-5052", "mrqa_squad-validation-5099", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-519", "mrqa_squad-validation-5230", "mrqa_squad-validation-524", "mrqa_squad-validation-5250", "mrqa_squad-validation-5329", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5362", "mrqa_squad-validation-5364", "mrqa_squad-validation-539", "mrqa_squad-validation-5434", "mrqa_squad-validation-5440", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5558", "mrqa_squad-validation-5562", "mrqa_squad-validation-5597", "mrqa_squad-validation-5650", "mrqa_squad-validation-5671", "mrqa_squad-validation-5693", "mrqa_squad-validation-57", "mrqa_squad-validation-5753", "mrqa_squad-validation-5772", "mrqa_squad-validation-5783", "mrqa_squad-validation-5791", "mrqa_squad-validation-5881", "mrqa_squad-validation-5921", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-5999", "mrqa_squad-validation-6013", "mrqa_squad-validation-6042", "mrqa_squad-validation-6118", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6288", "mrqa_squad-validation-6291", "mrqa_squad-validation-6421", "mrqa_squad-validation-6426", "mrqa_squad-validation-6491", "mrqa_squad-validation-6552", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6776", "mrqa_squad-validation-6787", "mrqa_squad-validation-6801", "mrqa_squad-validation-6805", "mrqa_squad-validation-6806", "mrqa_squad-validation-6852", "mrqa_squad-validation-6861", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6948", "mrqa_squad-validation-6958", "mrqa_squad-validation-6962", "mrqa_squad-validation-6996", "mrqa_squad-validation-7017", "mrqa_squad-validation-7026", "mrqa_squad-validation-7030", "mrqa_squad-validation-7035", "mrqa_squad-validation-71", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7173", "mrqa_squad-validation-7328", "mrqa_squad-validation-7331", "mrqa_squad-validation-734", "mrqa_squad-validation-7347", "mrqa_squad-validation-7372", "mrqa_squad-validation-7380", "mrqa_squad-validation-7384", "mrqa_squad-validation-7395", "mrqa_squad-validation-742", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-758", "mrqa_squad-validation-7628", "mrqa_squad-validation-7629", "mrqa_squad-validation-764", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7713", "mrqa_squad-validation-7715", "mrqa_squad-validation-7723", "mrqa_squad-validation-7747", "mrqa_squad-validation-7774", "mrqa_squad-validation-7792", "mrqa_squad-validation-7793", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-7976", "mrqa_squad-validation-7993", "mrqa_squad-validation-8002", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-82", "mrqa_squad-validation-8232", "mrqa_squad-validation-8256", "mrqa_squad-validation-828", "mrqa_squad-validation-8319", "mrqa_squad-validation-8320", "mrqa_squad-validation-8338", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-847", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8558", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8671", "mrqa_squad-validation-8679", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-8728", "mrqa_squad-validation-8732", "mrqa_squad-validation-8796", "mrqa_squad-validation-8811", "mrqa_squad-validation-8839", "mrqa_squad-validation-8862", "mrqa_squad-validation-8872", "mrqa_squad-validation-8920", "mrqa_squad-validation-893", "mrqa_squad-validation-8930", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-916", "mrqa_squad-validation-9178", "mrqa_squad-validation-9240", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9331", "mrqa_squad-validation-9351", "mrqa_squad-validation-9408", "mrqa_squad-validation-9413", "mrqa_squad-validation-9470", "mrqa_squad-validation-9520", "mrqa_squad-validation-9532", "mrqa_squad-validation-959", "mrqa_squad-validation-96", "mrqa_squad-validation-9608", "mrqa_squad-validation-9647", "mrqa_squad-validation-9777", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-9865", "mrqa_squad-validation-988", "mrqa_squad-validation-9984", "mrqa_squad-validation-999", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1140", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1644", "mrqa_triviaqa-validation-2050", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3602", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-5871", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6160", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-6513", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6903", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7616"], "OKR": 0.91796875, "KG": 0.44453125, "before_eval_results": {"predictions": ["November 1979", "Pittsburgh", "William Rainey Harper", "the classical element fire", "the Mughal state", "photosynthesis", "10th and 11th centuries", "53,000", "animosity toward each other", "giving her brother Polynices a proper burial", "lion, leopard, buffalo, rhinoceros, and elephant", "1998 NFL draft", "Widener Library", "a fee per unit of connection time, even when no data is transferred", "Nobel Prize", "Bart Starr", "UK", "Australia", "7.8%", "Mars", "2005", "27", "Louis King", "Boston", "Argentina", "Coronation Street", "funding by the pharmaceutical companies due to the Prescription Drug User Fee Act", "Art Deco-style skyscraper", "Amundsen Sea", "a male-dominated industry", "Brig Gen Augustine Warner Robins", "the flags of dependent territories and other areas of special sovereignty", "Russian film industry", "Thriller", "Restoration Hardware", "Kent Hovind", "Iron Man 3", "Tomasz Adamek", "Waylon Smithers", "Ordos City China Science Flying Universe Science and Technology Co., Ltd.", "The Vanishing", "Wendell Erdman Berry", "Che Guevara", "Indians", "gastrocnemius muscle", "1,228 km / h ( 763 mph )", "cells", "a compiler can derive machine code -- a form consisting of instructions that the computer can directly execute", "typhoid fever", "William Boyd", "Octopussy", "A4", "Labor Day", "a social networking site that allows users to publish what they are doing using 140 characters or less", "The supplemental spending bill provides nearly $162 billion in war funding without the restrictions congressional Democrats vowed to put into place since they took control of Congress nearly two years ago", "genocide", "off the coast of Dubai", "murder", "Stalin", "Karl Marx", "tin", "blackbeard", "cereal-&-milk", "Tintin"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7769814387001888}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444444, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 0.4, 0.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.3076923076923077, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.34285714285714286, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-366", "mrqa_squad-validation-4750", "mrqa_squad-validation-7724", "mrqa_hotpotqa-validation-148", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-201", "mrqa_hotpotqa-validation-173", "mrqa_naturalquestions-validation-9885", "mrqa_triviaqa-validation-4960", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-163", "mrqa_searchqa-validation-1856", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-14014"], "SR": 0.703125, "CSR": 0.6420454545454546, "EFR": 1.0, "Overall": 0.7532528409090908}, {"timecode": 11, "before_eval_results": {"predictions": ["the difference in potential energy", "Many words", "accessory pigments that override the chlorophylls' green colors", "62", "$20,000", "the Autons with the Nestene Consciousness and Daleks in series 1", "1080i HD", "income inequality", "104 \u00b0F (40 \u00b0C)", "a long-term conspiracy against Islam by the Western governments.", "road engines", "religious", "Larry Ellison", "consultant", "Derek Wolfe and Malik Jackson", "luxurious parks and royal gardens", "the Establishment Clause of the First Amendment or individual state Blaine Amendments", "Centrum", "20%", "win an acquittal and avoid imprisonment or a fine", "commemorating fealty and filial piety", "Melissa Disney", "Theodore Roosevelt", "Andy Cole", "13 February", "200 to 500 mg up to 7 mg", "1807", "The US Army tape, personal nametapes, and rank insignia could be sewn - on at the wearers preference", "Monk's Caf\u00e9", "Director of National Intelligence", "Francisco Pizarro", "a Czech word, robota, meaning `` forced labor ''", "Kyla Pratt", "The three - point line was first tested at the collegiate level in a 1945 NCAA game between Columbia and Fordham", "Michael Crawford", "a young husband and wife and how they deal with the challenge of buying secret Christmas gifts for each other with very little money", "2017", "New Croton Reservoir", "550 quadrillion Imperial gallons", "Kepner", "to increase the quality of a herd, or to introduce an outcross of bloodlines", "Annette Strean", "Anwar Sadat", "Alberich", "The Battle of Kasserine Pass", "Richard Wagner", "Muriel Spark", "Islamic philosophy", "Minnesota", "Seventeen", "Scottish", "a large stein mug", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "nearly $162 billion in war funding", "The most important thing is that our family is still together", "Samuel Herr, 26, and Juri Kibuishi, 23, of Irvine", "Old Trafford", "The Truman Show", "Jack McCall", "diamond", "ice caps", "Pakistan Rupee", "taking any and all appropriate personnel actions including termination, discipline and referral of any wrongdoing for criminal prosecution", "a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal to defame, insult or threaten the crown"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6016855719711323}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.0, 0.4, 0.0, 1.0, 0.14285714285714288, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.06896551724137931, 0.42857142857142855]}}, "before_error_ids": ["mrqa_squad-validation-7729", "mrqa_squad-validation-7577", "mrqa_squad-validation-7162", "mrqa_squad-validation-9632", "mrqa_squad-validation-7087", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-3609", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-100", "mrqa_triviaqa-validation-5538", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-4664", "mrqa_newsqa-validation-3914", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-1265", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-2716", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-702"], "SR": 0.515625, "CSR": 0.6315104166666667, "EFR": 0.9354838709677419, "Overall": 0.7382426075268816}, {"timecode": 12, "before_eval_results": {"predictions": ["San Joaquin Light & Power Building", "2003", "British", "Doritos", "live", "23.9%", "Daewoo", "1777", "34\u201319", "John D. Rockefeller", "Pole Mokotowskie", "Newcastle College", "63%", "Boulton", "can produce both eggs and sperm at the same time", "the laws of physics", "buoyancy", "international drug suppliers", "Oregon", "tuberculosis", "The Young Men's Christian Association", "tunisia", "Charles Springall", "the Iron Duke", "John Glenn", "28", "The Golden Child", "Scarborough", "Il Trovatore", "tun Frank Burns", "The Kentucky Derby", "Calvin Coolidge", "Hindi", "tunisia", "Nowhere Boy", "the Isle of Wight Festival", "Lancashire", "the recorder", "Michael J. Fox", "Japanese", "Poland", "October 2, 2017", "In the early 1900s", "Ravi Shastri", "Bohrium", "Anthony Hopkins", "Ella Jane Fitzgerald", "Bill Cosby", "Anna Clyne", "aluminum foil", "2014", "Hong Kong's Victoria Harbor", "school", "Herman Thomas", "Sunday", "anesthetic and sedative", "the conifers", "tunisia", "South Africa", "the dachshund", "the First Charter ones and twos", "The Stamp", "the tunisia", "the runt pig"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6456845238095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.8571428571428571, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4645", "mrqa_squad-validation-3492", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4", "mrqa_triviaqa-validation-4591", "mrqa_triviaqa-validation-3160", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-7132", "mrqa_triviaqa-validation-298", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-444", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-134", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-13604", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-16660", "mrqa_searchqa-validation-16664", "mrqa_searchqa-validation-13542"], "SR": 0.59375, "CSR": 0.6286057692307692, "retrieved_ids": ["mrqa_squad-train-73595", "mrqa_squad-train-67943", "mrqa_squad-train-24885", "mrqa_squad-train-62189", "mrqa_squad-train-8295", "mrqa_squad-train-52676", "mrqa_squad-train-54072", "mrqa_squad-train-70499", "mrqa_squad-train-49305", "mrqa_squad-train-58901", "mrqa_squad-train-84578", "mrqa_squad-train-77828", "mrqa_squad-train-81778", "mrqa_squad-train-67572", "mrqa_squad-train-59397", "mrqa_squad-train-26481", "mrqa_hotpotqa-validation-2946", "mrqa_squad-validation-9865", "mrqa_naturalquestions-validation-3300", "mrqa_squad-validation-6891", "mrqa_triviaqa-validation-659", "mrqa_squad-validation-2902", "mrqa_naturalquestions-validation-4942", "mrqa_newsqa-validation-817", "mrqa_naturalquestions-validation-5510", "mrqa_hotpotqa-validation-1039", "mrqa_squad-validation-8316", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-2837", "mrqa_squad-validation-7715", "mrqa_newsqa-validation-163", "mrqa_squad-validation-8811"], "EFR": 1.0, "Overall": 0.7505649038461538}, {"timecode": 13, "before_eval_results": {"predictions": ["Elders", "WatchESPN", "permanent pulmonary fibrosis", "the first two series", "February 1, 2016", "Antigone", "1972", "over fifty", "A Norman named Oursel", "\u00a341,004", "1806-07", "Executive Vice President of Football Operations and General Manager", "British", "1920s", "poet", "the number of social services that people can access wherever they move", "John Dryden", "21", "Fresh Fields", "feisty", "Ecuador", "Brussels", "Hitler", "Egypt", "Barrow, Carlisle, Whitehaven and Workington", "photographer", "johnson", "Tanzania", "a large, rugged dog for sledding, carting, weight-pulling, or other outdoor-related activities", "hot and cold beverages", "leicestershire", "Bill johnson", "johnson", "gold", "Istanbul", "wale", "Albert Einstein", "Los Angeles", "L. Pasteur", "Vito Corleone", "White Christmas", "the nucleus", "12 November 2010", "November 1975", "Afonso IV", "southern Turkey", "Barney Miller", "Flavivirus", "Nye County", "\"Traumnovelle\" (\"Dream Story\")", "the \"Pour le M\u00e9rite\"", "San Francisco, California", "Robert Barnett", "1-0", "not for sale", "June 6, 1944", "19", "backbreaking labor", "Earl Louis \"Curly\" Lambeau", "A New English Dictionary on Historical", "Jimmy Carter", "Jimmy Lee", "Jezebel", "cheese"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6720172443977591}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.16666666666666669, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.25, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.23529411764705882, 0.6666666666666666, 0.5, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1135", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-5352", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-67", "mrqa_triviaqa-validation-3272", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-1966", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-1349", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-804", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-9939", "mrqa_naturalquestions-validation-9672", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-2852", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2096", "mrqa_newsqa-validation-4067", "mrqa_searchqa-validation-7204", "mrqa_searchqa-validation-3905", "mrqa_searchqa-validation-7749"], "SR": 0.578125, "CSR": 0.625, "EFR": 1.0, "Overall": 0.7498437499999999}, {"timecode": 14, "before_eval_results": {"predictions": ["Xbox One", "tyrosinase", "reduced wages", "2005", "Art Deco style in painting and art", "The Prospect Studios", "Advanced Steam movement", "three", "the Moscone Center in San Francisco", "since 2001", "identified change orders or project changes that increased costs", "comedies and family-oriented series", "case law by the Court of Justice, international law and general principles of European Union law", "oxide compounds such as silicon dioxide, making up almost half of the crust's mass", "278", "bacterial genera, which are gram-positive and have the same spherical shaped cells called cocci.", "earplugs", "Melvil Dewey", "slide trumpet", "St Jude", "7 wives", "China", "feet", "classic building toy, Lincoln Logs", "March 19", "Diptera", "mumbawa", "kelp", "apples", "a multiple telegraph, using Morse code to convey several messages simultaneously, each at a different pitch.", "Erik Thorvaldson", "tungsten", "Gulliver's Travels", "Black September", "warblers", "jodie Foster", "geodetics", "comets", "the sense of smell", "Niveditha, Diwakar, Shruti", "Tagalog or English", "2013", "1861", "President James Madison", "Lou Rawls", "Southern Illinois University Carbondale", "Marco Hietala", "New Orleans, Louisiana", "Worcester, Massachusetts", "North Carolina", "July 23, 1971", "March 22", "August 19, 2007", "comets", "Ike", "95", "J. Crew", "John Harvard", "Montserrat in Catalonia (from Latin Mons Serratus Saw-Toothed Mountain)", "lymphoma and blood disease", "joplin", "the Riviera Casino", "comets", "Valeri Vladimirovich \"Val\" Bure"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5586061507936508}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.2666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.8, 1.0, 0.28571428571428575, 1.0, 0.5, 0.5, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.19999999999999998, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7534", "mrqa_squad-validation-512", "mrqa_squad-validation-3670", "mrqa_squad-validation-1546", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-992", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-857", "mrqa_triviaqa-validation-6032", "mrqa_triviaqa-validation-776", "mrqa_triviaqa-validation-3008", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-5801", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-2189", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1418", "mrqa_hotpotqa-validation-3366", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-4545", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-3783", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-14099", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-12049"], "SR": 0.46875, "CSR": 0.6145833333333333, "EFR": 1.0, "Overall": 0.7477604166666666}, {"timecode": 15, "before_eval_results": {"predictions": ["the Treaties establishing the European Union", "Isaac Newton", "draftsman", "mesoglea", "Associate Membership", "his own men", "the same franchise he went to the Super Bowl with as a player", "through sponsors", "61.1%", "Baden-W\u00fcrttemberg", "Most Western countries, and some others, have now banned it, but it remains lawful in the United States", "The \"Big Five\"", "between 25-minute episodes", "Shakespeare", "Terrence Malick", "Ceefax", "Ireland", "William Wakefield", "\u0130skenderun", "red deer", "roca", "tondere", "otters", "jennifer kiang Kai-shek", "macroalgal", "macroalgal", "macroalgal", "euston", "Spice Girls", "a record of the debits and credits relating to the person, business, etc.", "feet", "South Dakota", "17", "Moldova", "AFC Wimbledon", "Old Sparky", "Northern Ireland", "roger lemens", "the Golden Age of Science fiction", "mass of alcohol per mass of blood", "Thomas Jefferson", "Ben Willis", "Rafael Barba", "The Lykan", "they play pivotal roles in diverse cellular activities including growth ( by signaling neurotrophins ), differentiation, metabolism, adhesion, motility, death", "commercial", "44", "model, actress and television host", "John Murray", "Jeff Meldrum", "Canadian", "her landlord", "Afghanistan,", "the Kurdish Workers' Party,", "Saturday", "Jaime Andrade", "his father", "tritonic", "the infield", "Labor Day", "macroalgal", "Pennsylvania", "fuel-producing", "anil kapoor"], "metric_results": {"EM": 0.40625, "QA-F1": 0.47003432765151515}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6363636363636364, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0625, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6128", "mrqa_squad-validation-980", "mrqa_squad-validation-2086", "mrqa_squad-validation-7708", "mrqa_triviaqa-validation-3742", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-239", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2434", "mrqa_triviaqa-validation-1709", "mrqa_triviaqa-validation-533", "mrqa_triviaqa-validation-3956", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-6990", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-3940", "mrqa_triviaqa-validation-487", "mrqa_triviaqa-validation-1998", "mrqa_naturalquestions-validation-4523", "mrqa_naturalquestions-validation-10202", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-9271", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-2555", "mrqa_hotpotqa-validation-1116", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2857", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-1443", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-3979"], "SR": 0.40625, "CSR": 0.6015625, "retrieved_ids": ["mrqa_squad-train-1274", "mrqa_squad-train-43555", "mrqa_squad-train-11189", "mrqa_squad-train-66180", "mrqa_squad-train-22804", "mrqa_squad-train-38807", "mrqa_squad-train-1885", "mrqa_squad-train-23228", "mrqa_squad-train-9864", "mrqa_squad-train-80051", "mrqa_squad-train-568", "mrqa_squad-train-45537", "mrqa_squad-train-42989", "mrqa_squad-train-39658", "mrqa_squad-train-23161", "mrqa_squad-train-59507", "mrqa_squad-validation-9632", "mrqa_squad-validation-4360", "mrqa_squad-validation-1546", "mrqa_squad-validation-8189", "mrqa_hotpotqa-validation-5363", "mrqa_searchqa-validation-5440", "mrqa_squad-validation-5362", "mrqa_searchqa-validation-11539", "mrqa_triviaqa-validation-356", "mrqa_naturalquestions-validation-2297", "mrqa_naturalquestions-validation-553", "mrqa_newsqa-validation-702", "mrqa_naturalquestions-validation-8782", "mrqa_hotpotqa-validation-2523", "mrqa_squad-validation-2315", "mrqa_newsqa-validation-2552"], "EFR": 0.9736842105263158, "Overall": 0.7398930921052631}, {"timecode": 16, "before_eval_results": {"predictions": ["12 December 1964", "seawater", "a protest", "around 5 million", "Van de Graaff generator", "bounding", "between 1835 and 1842", "Vivienne Westwood", "absolution", "France", "megaprojects", "demand for higher quality housing increased", "1960s to the mid-1970s", "W. Edwards Deming", "Irsay", "1947, 1956, 1975, 2015 and 2017", "Paul Lynde", "Kevin Kline", "a computer maintenance utility", "Saint Alphonsa", "the President of the United States and confirmed by the Senate for staggered 14 - year terms", "Jesse Triplett", "Kristy Swanson", "in a compact layout to combine keys which are usually kept separate", "Missouri River", "in desperation, with only a small chance of success and time running out on the clock", "Lex Luger and Rick Rude", "in florida it is illegal to sell alcohol before 1 pm on any sunday", "in contemporary Earth, where the sudden appearance of a worldwide storm causes 98 % of the world's population to disappear, and zombie - like creatures rise to attack the remainder", "1923", "in the United Kingdom, the town of Carcassonne in Aude, France, for the portrayal of Nottingham and its castle", "California and South Carolina", "Christy Plunkett ( Anna Faris )", "Sir Hugh Beaver", "23 September 1889", "October 22, 2017", "Tokyo for the 2020 Summer Olympics", "1980", "beef patties", "St. Louis", "Australia and Ireland", "Hitler", "Llandudno", "albinism", "in round five of the 2017 season in an eighteen-point win.", "John Schlesinger", "Clitheroe Football Club", "1992", "Neighbours", "$7.3 billion", "France's famous Louvre museum", "Kenneth Cole", "attacks that started in April 1994, Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing", "10 years", "citizens", "her decades-long portrayal of Alice Horton", "khamsin", "saprophytes", "two", "al-Wahhab", "barry", "copper", "The Ansonia Hotel", "Chiltern Hills"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6561152248996918}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.2857142857142857, 1.0, 0.47058823529411764, 0.0, 1.0, 0.1142857142857143, 1.0, 0.060606060606060615, 0.0, 0.0, 0.0625, 1.0, 0.2222222222222222, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.923076923076923, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8369", "mrqa_squad-validation-2297", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-7889", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-2207", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-7635", "mrqa_triviaqa-validation-7696", "mrqa_hotpotqa-validation-2793", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-15859", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-14243"], "SR": 0.578125, "CSR": 0.6001838235294117, "EFR": 0.9259259259259259, "Overall": 0.7300656998910675}, {"timecode": 17, "before_eval_results": {"predictions": ["Henry Plitt", "charging their students tuition", "lysozyme and phospholipase A2", "the issue of laity having a voice and vote in the administration of the church, insisting that clergy should not be the only ones to have any determination in how the church was to be operated", "ABC Sunday Night Movie", "Thomas Edison and Nikola Tesla", "the established Church", "any terrestrial distance", "greenhouse gas", "the \"scariest TV show of all time\"", "The Scottish Parliament", "beer", "Arabic", "lunar module", "the PowerPlay", "the Sidecar", "the Eiffel Tower", "snake", "King Edward", "the human body", "the yellow fever", "the Greek for \"earth measure\"", "the Prestige", "the Latin post scriptum", "Denver", "Petroleum", "Maria Full of Grace", "the Pro-Jig Clamp Set", "Arby's", "Governor Adlai Stevenson", "the retina", "Albright", "Union Pacific & the Central Pacific", "the Lord of the Rings", "tarantulas", "Andrew Marvell", "Mars", "six", "Sean O' Neal", "his influential uncle Abu Talib", "Baker, California", "April 1979", "the International Border", "America", "Missouri", "anahuac", "British European Airways", "Tahrir Square", "Albert", "14 December 1990", "Clara Petacci", "first freshman to finish as the runner-up", "a series of bilateral treaties whereby the Swiss Confederation has adopted various provisions of European Union law in order to participate in the Union's single market", "a music television network originating from Germany", "1868", "The Da Vinci Code", "Paul McCartney", "buckling under pressure from the ruling party.", "Chesley \"Sully\" Sullenberger", "All You Need is Love", "tennis", "In Time", "Milira", "Pangaea"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5563200243301694}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.1951219512195122, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.13793103448275862, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9878", "mrqa_squad-validation-6024", "mrqa_searchqa-validation-2979", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-16347", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-7518", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-2953", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-3951", "mrqa_searchqa-validation-16905", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-4902", "mrqa_triviaqa-validation-4135", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-5401", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2093"], "SR": 0.515625, "CSR": 0.5954861111111112, "EFR": 0.967741935483871, "Overall": 0.7374893593189964}, {"timecode": 18, "before_eval_results": {"predictions": ["1960", "the Saudi monarchy", "in Smiljan", "Muqali", "2016", "by technique", "2008", "41", "the edge railed rack and pinion Middleton Railway", "increasing access to education", "Penance", "Intermezzo", "white", "Bangladesh", "Warren Gamaliel Harding", "Joy Division", "Athol Fugard", "a percussion instrument", "the Constitution", "a Tibetan antelope", "glaciers", "Charles Earl Bowles", "canticle", "sanguine", "Maine", "a Fokker", "Pilgrim's Progress", "a Grail", "Macbeth", "Chocolate Factory", "Mountain Dew", "Engelbert Humperdinck", "Pearl Jam", "(National) PATTERNS", "Smokey Robinson", "Lhasa", "hercules", "from statute or the Constitution itself", "Kerry Shale", "2028", "Patrick Walshe", "Massachusetts", "Wisconsin", "Marc Brunel", "Czech Republic", "Donegal", "Sir Edwin Landseer", "one person", "a spa town", "Academy Award for Best Animated Feature", "Aircraft", "Minnesota", "Lisburn Distillery Football Club", "Viaport Rotterdam", "2002\u201303", "9 a.m.", "a civil disturbance call", "2008", "Ralph Cifaretto", "Lance Cpl. Maria Lauterbach and her fetus", "on the Ohio River near Warsaw, Kentucky,", "Percy Sledge", "Dublin", "her"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6852430555555555}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.4444444444444445, 0.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9516", "mrqa_squad-validation-1227", "mrqa_squad-validation-3179", "mrqa_searchqa-validation-15758", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-7430", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-3814", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-2639", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9637", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9975", "mrqa_naturalquestions-validation-10653", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-4528", "mrqa_hotpotqa-validation-218", "mrqa_hotpotqa-validation-541", "mrqa_newsqa-validation-4197", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2573", "mrqa_triviaqa-validation-6931"], "SR": 0.59375, "CSR": 0.5953947368421053, "retrieved_ids": ["mrqa_squad-train-10052", "mrqa_squad-train-85777", "mrqa_squad-train-71193", "mrqa_squad-train-74547", "mrqa_squad-train-72062", "mrqa_squad-train-2210", "mrqa_squad-train-44463", "mrqa_squad-train-50160", "mrqa_squad-train-45623", "mrqa_squad-train-19861", "mrqa_squad-train-35983", "mrqa_squad-train-32695", "mrqa_squad-train-36004", "mrqa_squad-train-49885", "mrqa_squad-train-65152", "mrqa_squad-train-62929", "mrqa_naturalquestions-validation-710", "mrqa_triviaqa-validation-4591", "mrqa_squad-validation-438", "mrqa_triviaqa-validation-5713", "mrqa_hotpotqa-validation-2555", "mrqa_naturalquestions-validation-3052", "mrqa_hotpotqa-validation-1533", "mrqa_searchqa-validation-2179", "mrqa_triviaqa-validation-1966", "mrqa_squad-validation-9608", "mrqa_newsqa-validation-2955", "mrqa_naturalquestions-validation-5538", "mrqa_triviaqa-validation-3272", "mrqa_squad-validation-7773", "mrqa_hotpotqa-validation-2793", "mrqa_squad-validation-3954"], "EFR": 0.9615384615384616, "Overall": 0.7362303896761133}, {"timecode": 19, "before_eval_results": {"predictions": ["Ice Ages", "Oligocene", "byker", "Fridays", "refuse to sign bail", "to encourage investment", "*R\u012bnaz", "Beyonc\u00e9 and Bruno Mars", "\"zip\" the mouth shut when the animal is not feeding", "J\u00f3zsef Pulitzer", "Steve Coogan", "left fielder", "Ellesmere Port", "Punjabi/Pashtun", "KXII", "Who's That Girl", "Manor of the More", "Europe", "Melbourne", "Woodsy owl", "\"The Snowman\"", "1994\u201395", "Satchmo, Satch or Pops", "Philip Livingston", "hamburgers", "C. J. Cherryh", "Holston River", "1939", "Tabasco", "8/7c", "Iceal Hambleton", "Cleveland Cleveland", "\"Catch Me If You Can\"", "racehorse breeder and owner", "a vegetarian dish called Buddha's delight", "The Washington Post", "\"Secrets and Lies\"", "eagles", "endocrine ( hormonal )", "one - point perspective, and their vanishing point corresponds to the oculus, or `` eye point '', from which the image should be viewed for correct perspective geometry", "The Wizard", "Anthony Mayfield", "Wabanaki Confederacy members Abenaki and Mi'kmaq, and Algonquin, Lenape, Ojibwa, Ottawa, Shawnee, and Wyandot", "Cape Town", "the French Revolution", "geometry", "Rajasthan", "an Italian liquor characterized by a bittersweet taste with a twist of almond flavor", "Diogenes", "einstein", "(Dr. Jennifer Arnold and husband Bill Klein,", "American", "Gary Player", "full health-care coverage", "made 109 as Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets on the opening day.", "Donald Trump", "Ovid", "Martin Van Buren", "einstein", "The Pentagon", "a Beanie Baby", "Wigan Athletic", "Iowa", "on your social networking sites"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5504910714285713}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.5, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.13333333333333333, 0.16, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-5456", "mrqa_squad-validation-114", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-5448", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-3528", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-5163", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-4864", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-3491", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-5075", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-2898", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-15311", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1182"], "SR": 0.46875, "CSR": 0.5890625, "EFR": 1.0, "Overall": 0.7426562499999999}, {"timecode": 20, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1051", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-1617", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1902", "mrqa_hotpotqa-validation-1968", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2555", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2676", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3302", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3760", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4545", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5249", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-5401", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5763", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-798", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10396", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1448", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2169", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2688", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-3369", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4193", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4942", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9175", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9672", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1329", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-163", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-560", "mrqa_searchqa-validation-10685", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-13604", "mrqa_searchqa-validation-13784", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-15636", "mrqa_searchqa-validation-15676", "mrqa_searchqa-validation-15727", "mrqa_searchqa-validation-15900", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-16905", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2639", "mrqa_searchqa-validation-2642", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3131", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3461", "mrqa_searchqa-validation-3894", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5125", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6517", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-6942", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9419", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9812", "mrqa_searchqa-validation-9998", "mrqa_squad-validation-10004", "mrqa_squad-validation-10013", "mrqa_squad-validation-10024", "mrqa_squad-validation-1006", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10112", "mrqa_squad-validation-10199", "mrqa_squad-validation-10395", "mrqa_squad-validation-10412", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10493", "mrqa_squad-validation-10506", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1462", "mrqa_squad-validation-1496", "mrqa_squad-validation-1512", "mrqa_squad-validation-1527", "mrqa_squad-validation-1541", "mrqa_squad-validation-1546", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1600", "mrqa_squad-validation-1637", "mrqa_squad-validation-1684", "mrqa_squad-validation-1762", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1866", "mrqa_squad-validation-199", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-236", "mrqa_squad-validation-2376", "mrqa_squad-validation-2468", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-2591", "mrqa_squad-validation-2602", "mrqa_squad-validation-2705", "mrqa_squad-validation-2723", "mrqa_squad-validation-276", "mrqa_squad-validation-2834", "mrqa_squad-validation-2869", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-302", "mrqa_squad-validation-3049", "mrqa_squad-validation-3063", "mrqa_squad-validation-3092", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3398", "mrqa_squad-validation-3416", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3540", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-3610", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-3871", "mrqa_squad-validation-3925", "mrqa_squad-validation-3950", "mrqa_squad-validation-3986", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4127", "mrqa_squad-validation-4179", "mrqa_squad-validation-4186", "mrqa_squad-validation-419", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-4246", "mrqa_squad-validation-436", "mrqa_squad-validation-4360", "mrqa_squad-validation-4376", "mrqa_squad-validation-438", "mrqa_squad-validation-4403", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4497", "mrqa_squad-validation-4506", "mrqa_squad-validation-4533", "mrqa_squad-validation-4649", "mrqa_squad-validation-466", "mrqa_squad-validation-4677", "mrqa_squad-validation-4707", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-4980", "mrqa_squad-validation-500", "mrqa_squad-validation-510", "mrqa_squad-validation-516", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5193", "mrqa_squad-validation-5230", "mrqa_squad-validation-5334", "mrqa_squad-validation-5362", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5562", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5866", "mrqa_squad-validation-5921", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-599", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6154", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-641", "mrqa_squad-validation-6595", "mrqa_squad-validation-6653", "mrqa_squad-validation-6670", "mrqa_squad-validation-6676", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6891", "mrqa_squad-validation-6942", "mrqa_squad-validation-6996", "mrqa_squad-validation-7096", "mrqa_squad-validation-7105", "mrqa_squad-validation-7137", "mrqa_squad-validation-715", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7380", "mrqa_squad-validation-7534", "mrqa_squad-validation-7554", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7701", "mrqa_squad-validation-7708", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7792", "mrqa_squad-validation-7850", "mrqa_squad-validation-7956", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-8231", "mrqa_squad-validation-8287", "mrqa_squad-validation-8362", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8496", "mrqa_squad-validation-8534", "mrqa_squad-validation-8566", "mrqa_squad-validation-8613", "mrqa_squad-validation-8657", "mrqa_squad-validation-8667", "mrqa_squad-validation-8687", "mrqa_squad-validation-8699", "mrqa_squad-validation-8732", "mrqa_squad-validation-878", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-8984", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9331", "mrqa_squad-validation-9578", "mrqa_squad-validation-96", "mrqa_squad-validation-9606", "mrqa_squad-validation-9608", "mrqa_squad-validation-9632", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-980", "mrqa_squad-validation-9802", "mrqa_squad-validation-9845", "mrqa_squad-validation-9849", "mrqa_squad-validation-988", "mrqa_squad-validation-9966", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1038", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1080", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1201", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-15", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1719", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1924", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2377", "mrqa_triviaqa-validation-2452", "mrqa_triviaqa-validation-2731", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2967", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4135", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4469", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4586", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-4801", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5107", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5525", "mrqa_triviaqa-validation-5538", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5676", "mrqa_triviaqa-validation-5777", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6661", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6799", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6840", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7132", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-804"], "OKR": 0.900390625, "KG": 0.45625, "before_eval_results": {"predictions": ["America's Funniest Home Videos", "\"the Romantic Rhine\"", "philanthropy", "1986", "Republic of Kenya", "PNU and ODM camps", "pseudorandom number generators", "\u20ac5,000", "a baffle plate", "a pound", "Spain", "Havana", "Harriet Beecher Stowe", "21", "her new movie", "\"The Andy Griffith Show\",", "The Bladder", "Pulsed Laser", "Richard E. Byrd", "Resident Evil", "Lake Mead", "James Earl Ray", "President Johnson", "The Complete Poems", "a trumpet", "South African Boer War", "Nancy Reagan", "Agatha Christie", "Cleveland", "Oscar Wilde", "Thesaurus", "calamity", "Lake Alakol", "The Starland Vocal Band", "kerosene", "a Silver Necklace", "U.S. service members who have died without their remains being identified", "Carol Ann Susi", "Upstate New York", "Las Vegas", "Missouri, during the Kirtland period of Latter Day Saint history, circa 1834", "Dirk Benedict", "Akshay Kumar", "algal-growth processes", "Japan", "Jody Patton", "Nigeria", "Francis Scott Key", "The Truman Show", "George Gently", "Leslie Knope", "1978", "University of Missouri", "Nicolas Winding Refn", "Tokyo's Narita International Airport", "$26 billion", "2013\u201314 Premier League", "Pixar's", "Inter Milan", "\"It's been decided by society that a 15-year-old can't vote, can't join the armed forces and cannot buy alcohol,", "If  your ex's loved ones ask why you broke up", "two Israeli soldiers, Ehud \"Udi\" Goldwasser and Eldad Regev.", "two years ago", "1973"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6841517857142857}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 0.08333333333333333, 0.7499999999999999, 0.19999999999999998, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9161", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-14614", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-4655", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-15737", "mrqa_searchqa-validation-11604", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-11905", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5780", "mrqa_triviaqa-validation-173", "mrqa_triviaqa-validation-7625", "mrqa_triviaqa-validation-7392", "mrqa_hotpotqa-validation-1182", "mrqa_newsqa-validation-2753", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-1184", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1034"], "SR": 0.578125, "CSR": 0.5885416666666667, "EFR": 1.0, "Overall": 0.7382552083333334}, {"timecode": 21, "before_eval_results": {"predictions": ["punts", "arrest", "Confucian propriety and ancestor veneration", "5,000", "the Barnett Center", "immunomodulators", "the RSA algorithm", "harpoons", "Francis Scott Key", "San Francisco earthquake", "piccolo", "the Panther", "Sri Lanka", "the bums", "Marlon Brando", "the Lone Ranger", "tears", "the Old Curiosity Shop", "pearls", "Australia", "defence", "Europa", "Pope John XXIII", "the sun", "the Mercury program", "m&M Mars", "defence", "John Edwards", "Hamas", "the defence", "the slide", "the publication of resolutions against Boris Pasternak in the Literaturnaya", "the cotton textile industry", "Rome", "lymphatic", "Bed and breakfast", "1038", "19 June 2018", "Mahatma Gandhi", "Virginia Dare", "Lionel Hardcastle", "iOS, watchOS, and tvOS", "The Shard", "The Blue Boy", "Allende", "Sue Ryder", "Adrian Edmondson", "the sense of taste (ageusia)", "Tony Curtis", "the lead roles of Timmy Sanders and Jack", "\"good\" by Ofsted", "The Prodigy", "Miss Mulatto", "New Journalism", "Boyd Gaming", "Barbara Niven", "Sunday", "eight", "April 13", "Del Potro.", "the Gaslight Theater", "Anil Kapoor", "soldiers have been shot over the last day in Somalia, two of them fatally, Somali media reports said.", "1964"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6026785714285715}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5714285714285715, 1.0, 0.4444444444444445, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12130", "mrqa_searchqa-validation-5408", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-14559", "mrqa_searchqa-validation-14313", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-14337", "mrqa_searchqa-validation-14613", "mrqa_searchqa-validation-743", "mrqa_searchqa-validation-16149", "mrqa_searchqa-validation-3608", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-8957", "mrqa_searchqa-validation-16301", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2748", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7058", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-639", "mrqa_hotpotqa-validation-325", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-926", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-3501"], "SR": 0.546875, "CSR": 0.5866477272727273, "retrieved_ids": ["mrqa_squad-train-2904", "mrqa_squad-train-2013", "mrqa_squad-train-67760", "mrqa_squad-train-46484", "mrqa_squad-train-893", "mrqa_squad-train-64453", "mrqa_squad-train-53987", "mrqa_squad-train-30458", "mrqa_squad-train-20746", "mrqa_squad-train-6358", "mrqa_squad-train-28579", "mrqa_squad-train-8010", "mrqa_squad-train-68063", "mrqa_squad-train-69092", "mrqa_squad-train-56694", "mrqa_squad-train-5566", "mrqa_triviaqa-validation-173", "mrqa_squad-validation-6891", "mrqa_searchqa-validation-11905", "mrqa_newsqa-validation-462", "mrqa_squad-validation-6128", "mrqa_naturalquestions-validation-4523", "mrqa_triviaqa-validation-5380", "mrqa_hotpotqa-validation-5363", "mrqa_squad-validation-9578", "mrqa_naturalquestions-validation-3651", "mrqa_newsqa-validation-1364", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2169", "mrqa_squad-validation-6154", "mrqa_searchqa-validation-9038", "mrqa_squad-validation-2591"], "EFR": 1.0, "Overall": 0.7378764204545455}, {"timecode": 22, "before_eval_results": {"predictions": ["Mike Carey", "Rankine cycle", "May 21, 2013", "iger team", "11:28", "1321 to 1323", "AFC", "Laurence Olivier Award", "Rafael Palmeiro", "nine", "Central Park", "Enkare Nairobi", "PewDiePie", "Thrushcross Grange", "Linda McCartney's Life in Photography", "10 June 1921", "A Bug's Life", "Michelle Anne Sinclair", "Angel Parrish", "\"Advanced Dragons\"", "Bundesliga", "\"Apprendi v. New Jersey\"", "cleaning services, support services, property services, catering services, security services and facility management services", "4,972", "Captain", "Mount Everest", "119", "British", "Red Dead Redemption", "Las Vegas Outlaws", "neo-v\u00f6lkisch movement", "Sugar Ray Robinson", "seal hunting", "Jeffrey Adam \"Duff\" Goldman", "Parachutes", "a political ideology", "Mendel", "John Adams", "Toto", "Jakkur, Bangalore, India", "2009", "James Arthur", "14", "Vanity Fair", "Route 66", "Welcome Stranger", "Pour Moi", "aardvark", "Jeremy Bates", "a police patrol car", "Six", "Mexico", "Dr. Cade", "standing next to a story", "a broken pelvis", "Brown and her family", "Rocco Ritchie", "love", "Armistice Day", "double-headed eagle", "Federer", "Hannibal Lecter", "Venezuela", "Marie Fredriksson"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6457465277777777}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.8, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-136", "mrqa_squad-validation-3926", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-3629", "mrqa_hotpotqa-validation-1019", "mrqa_hotpotqa-validation-4272", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-939", "mrqa_hotpotqa-validation-3964", "mrqa_hotpotqa-validation-1386", "mrqa_hotpotqa-validation-3237", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-5103", "mrqa_naturalquestions-validation-6786", "mrqa_naturalquestions-validation-4033", "mrqa_newsqa-validation-120", "mrqa_newsqa-validation-1541", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-1496", "mrqa_searchqa-validation-9888", "mrqa_searchqa-validation-7840", "mrqa_searchqa-validation-6337", "mrqa_searchqa-validation-472", "mrqa_naturalquestions-validation-9195"], "SR": 0.5625, "CSR": 0.5855978260869565, "EFR": 1.0, "Overall": 0.7376664402173913}, {"timecode": 23, "before_eval_results": {"predictions": ["trans-Atlantic wireless telecommunications facility known as Wardenclyffe", "beerhouse and whorehouse", "Pitt", "the meeting of the Church's General Assembly", "vocational subjects", "ESPN Deportes", "the Greek Goddess of Revenge", "smell", "Honshu", "a \u201cbitter almond\u201d smell", "RF", "(Charles) Ritchie (died 1790)", "8.8/10 (55 votes)", "a barred, spiral galaxy", "high jump", "milk", "Derbyshire, England", "James Cameron", "Emma Chambers", "eenezer Scrooge", "Spain", "RFK", "the Guardian", "North by Northwest", "oxygen", "(Albert) Juantorena", "(William) Hearst", "Henry III", "Cream", "Korea", "The Live Read of Space Jam", "(Sinn Fein)", "ambergris", "The Time Machine", "The Lion King", "5 September 1666", "pneumonoultramicroscopicsilicovolcanoconiosis", "William Strauss and Neil Howe", "26 \u00b0 37 \u2032 N 81 \u00b0 50 \u2032 W \ufeff / \ufef7 26.617 \u00b0 N 81.617", "Norway", "at the fictional elite conservative Vermont boarding school Welton Academy, it tells the story of an English teacher who inspires his students through his teaching of poetry", "1978", "July", "Ghana Technology University College", "Charles Quinton Murphy", "12", "Pakistan", "Washington, D.C.", "Michael Jordan", "Arnoldo Rueda Medina,", "\" waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "a snare and temptation and illusion", "a bank", "at Eintracht Frankfurt to close the gap to two points thanks to goals from Joel Matip, Benedikt Howedes Ivan Rakitic and Kevin Kuranyi.", "(Khaled) al-Islamiyya,", "\"pattern matching.\"", "jazz", "RF", "Canada", "Olivia Newton-John", "soup", "(Jean) Lacoste", "a grill", "Alien Intelligences"], "metric_results": {"EM": 0.34375, "QA-F1": 0.47333263405937087}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [0.25, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.4, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.823529411764706, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.4, 0.2222222222222222, 1.0, 1.0, 0.0, 0.0, 0.06896551724137931, 0.0, 1.0, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1384", "mrqa_squad-validation-1965", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-226", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-6120", "mrqa_triviaqa-validation-181", "mrqa_triviaqa-validation-7071", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-6597", "mrqa_triviaqa-validation-5027", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6657", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-6688", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-4852", "mrqa_naturalquestions-validation-590", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-5791", "mrqa_naturalquestions-validation-10442", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-751", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-2807", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2144", "mrqa_newsqa-validation-914", "mrqa_newsqa-validation-3302", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-303", "mrqa_hotpotqa-validation-1516"], "SR": 0.34375, "CSR": 0.5755208333333333, "EFR": 0.9761904761904762, "Overall": 0.7308891369047619}, {"timecode": 24, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "40,000", "Geordie", "plantar fasciitis in his left foot", "there was nothing which would constitute a hazard in unfriendly hands", "Virgil", "Queen Victoria", "Tokyo", "February", "Laufeyi", "Samuel Johnson", "6 Litres", "West Point", "33", "Caernarfon station", "curling", "Avro Lancaster", "boxing", "in 1994", "Joe Willie Kirk", "protects and holds the lungs, heart, trachea, esophagus, endocrine glands, thoracic aorta and the pulmonary artery", "\"Ain't No Mountain High Enough\"", "ambulance driver", "Charles Dickens", "Hampshire", "New Zealand", "u2", "right ventricle", "Christian Wulff", "soup made by The soup Dragon and blue string milky mice", "milky-purple berry", "The Jacaranda", "Jesse Garon Presley", "My Bologna Has a First Name", "Epernay", "its population, serving staggered terms of six years", "Buddhism", "the Yuan palaces be razed", "65,535 bytes", "A turlough, or turlach", "Health or vitality", "George Warren Barnes", "The Grandmaster", "Nanna Popham Britton", "The Nick Cannon Show", "Marvel Comics", "Taylor Swift", "Sierra Leone", "September 8, 2017", "CNN's Max Foster", "seven", "Russian air force", "At least 38 people", "about 3,000 kilometers (1,900 miles)", "Paul McCartney and Ringo Starr", "The first official week of summer, marks \"Lightning Safety awareness Week\" for the National Oceanic and Atmospheric Administration (NOAA)", "Clifford Odets", "degaussing", "Fargo", "because it wasn't meaty enough", "Mahayana", "the Headless Horseman", "a grizzly bear", "Carrie Underwood"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5716483443457128}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, false, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.5, 0.0, 0.5, 0.0, 0.0, 0.14285714285714288, 1.0, 0.0, 0.5, 0.5, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-629", "mrqa_squad-validation-1598", "mrqa_triviaqa-validation-5317", "mrqa_triviaqa-validation-6092", "mrqa_triviaqa-validation-5958", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-3363", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-3844", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-2092", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-3963", "mrqa_triviaqa-validation-183", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-1787", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-2205", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-4092", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-2340", "mrqa_searchqa-validation-3156", "mrqa_searchqa-validation-7", "mrqa_searchqa-validation-14881", "mrqa_searchqa-validation-9185"], "SR": 0.453125, "CSR": 0.5706249999999999, "retrieved_ids": ["mrqa_squad-train-3371", "mrqa_squad-train-17207", "mrqa_squad-train-50109", "mrqa_squad-train-41647", "mrqa_squad-train-52910", "mrqa_squad-train-79341", "mrqa_squad-train-41070", "mrqa_squad-train-64664", "mrqa_squad-train-60920", "mrqa_squad-train-52990", "mrqa_squad-train-42343", "mrqa_squad-train-8278", "mrqa_squad-train-28566", "mrqa_squad-train-82847", "mrqa_squad-train-57348", "mrqa_squad-train-48624", "mrqa_searchqa-validation-15737", "mrqa_squad-validation-2705", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-9056", "mrqa_searchqa-validation-4367", "mrqa_naturalquestions-validation-5154", "mrqa_squad-validation-6962", "mrqa_naturalquestions-validation-5865", "mrqa_triviaqa-validation-3742", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-1998", "mrqa_squad-validation-7729", "mrqa_triviaqa-validation-6848", "mrqa_searchqa-validation-5440", "mrqa_hotpotqa-validation-2748"], "EFR": 1.0, "Overall": 0.7346718749999999}, {"timecode": 25, "before_eval_results": {"predictions": ["Gaelic", "punish Christians by God", "The Mask of Anarchy", "Japanese", "given x and y in R such that p divides the product xy, then p divides x or y", "juliet", "7", "Italy", "rounders", "al bundy", "The Daily Mirror", "Fort Sumter", "pindar", "Giuseppe Verdi", "Jordan", "juliet", "Downton Abbey", "groucho", "raclette", "lie detector", "Daedalus", "Una Stubbs", "cardiac", "sea otter", "Fringillidae", "paul", "May", "juliet", "Paul C\u00e9zanne", "puffer", "A-ha", "a high-speed car crash,", "paul", "All Stars", "juliet", "Noel Kahn", "Mitch Murray", "Rajendra Prasad", "U + 2234", "Alamodome and city of San Antonio", "Chris Rea", "retinal ganglion cell axons and glial cells", "Restoration Hardware", "Khalifa International Stadium", "James Harrison", "Canada's first train robbery", "Lindsey Islands", "Bardot", "E22", "Booches Billiard Hall", "1998.", "Umar Farouk AbdulMutallab", "Lebanese", "the two remaining crew members from the helicopter", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "Tehran,", "Taiwan", "Wallachia", "Flamin' Hot", "vodka", "larry jabbar", "chafing", "Ukraine", "angels"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5485615079365078}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6864", "mrqa_squad-validation-9048", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-948", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-4411", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7606", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-3506", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-5483", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-3316", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3008", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3097", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-4640", "mrqa_searchqa-validation-9293"], "SR": 0.484375, "CSR": 0.5673076923076923, "EFR": 1.0, "Overall": 0.7340084134615384}, {"timecode": 26, "before_eval_results": {"predictions": ["defensins", "the Earth must be much older than had previously been supposed in order to allow enough time for mountains to be eroded and for sediments to form new rocks", "5 to 15 years", "the pre-game and halftime coverage", "sternum", "new Zealand", "carbon", "albert", "one of the Clarks,", "a Nor'easter", "Rajasthan", "ha", "tony adkins", "Argentina", "a docked yacht", "kia", "power station", "the troposphere", "2nd Battalion Parachute Regiment", "percy deGeneres", "charlie drake", "Spain", "Richard Noble", "richmond", "tintoretto", "the north", "120", "Hans Lippershey", "Gryffindor", "Andrew Lloyd Webber", "Wilkie Collins", "yunte Huang", "Canada", "Baton Rouge", "Laura Jane Haddock", "a man called Lysander", "The Inn at Newport Ranch", "Woodrow Wilson", "Mishri", "King Xavier", "Max Martin", "Sean Yseult", "Mount Everest", "18.7 miles", "actor and filmmaker", "a coaxial cable with RCA connectors or a fibre optic cable with TOSLINK connectors", "Tony Stewart", "Stage Stores", "at least $20 million to $30 million,", "the amount of fuel used at high altitude.", "40", "Gen. Stanley McChrystal", "Natalie Cole.", "$627,", "CNN affiliate WFTV.", "cricket", "The Man Without A Country", "deimos", "Benazir Bhutto", "Dracula", "Ipanema", "a violin", "the band's logo in gold lettering over black sleeve", "John von Neumann"], "metric_results": {"EM": 0.5, "QA-F1": 0.5858550397830584}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.38095238095238093, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0909090909090909, 0.0, 1.0, 0.4444444444444445, 0.14285714285714285, 0.2857142857142857, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.823529411764706, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5054", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-3022", "mrqa_triviaqa-validation-2514", "mrqa_triviaqa-validation-6873", "mrqa_triviaqa-validation-2", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7148", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-5619", "mrqa_triviaqa-validation-4242", "mrqa_triviaqa-validation-6043", "mrqa_triviaqa-validation-4663", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-2839", "mrqa_hotpotqa-validation-2428", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-5513", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-469", "mrqa_searchqa-validation-2806", "mrqa_searchqa-validation-2896", "mrqa_searchqa-validation-3888", "mrqa_searchqa-validation-6489", "mrqa_naturalquestions-validation-3556"], "SR": 0.5, "CSR": 0.5648148148148149, "EFR": 0.96875, "Overall": 0.7272598379629629}, {"timecode": 27, "before_eval_results": {"predictions": ["three hours", "Cosgrove Hall", "American Civil Rights Movement", "principal role of committees in the Scottish Parliament is to take evidence from witnesses, conduct inquiries and scrutinise legislation", "Armin Meiwes ( ; born 1 December 1961) is a German computer repair technician who achieved international notoriety for killing and eating a voluntary victim whom he had found via the Internet", "Katherine Harris", "The Longest Yard", "WAMC", "Nikhil Banerjee", "Croatian", "Food and Agriculture Organization", "Kolkata", "2010 to 2012", "Dorothy", "Bonobo", "Lake County, Illinois", "the Harris Museum, Harris Institute or Art School, Harris Technical School and the Harris Orphanage", "Mark Dayton", "Black Panther Party", "The One (film)  As One (; lit.  \"Korea\") is a 2012 South Korean sports drama film starring Ha Ji-won and Bae Doona.", "Denmark", "The Sun", "University of Missouri-Kansas City", "October 3, 2017", "Walt Disney and Ub Iwerks", "River Clyde", "Chicago", "Moonstruck", "Fulgencio Batista", "Seventeen", "Kim Sung-su", "ten years of probation", "Outside", "Paul Teutul Jr.", "bypasses, to cross major bridges, and to provide direct intercity connections", "Herbert Hoover", "Charles Carson", "Chris Sarandon", "Tianying, who reciprocates his attention, but another boy Cheng, a rebellious kung fu prodigy whose family is close to Meiying's, attempts to keep them apart by violently attacking Dre", "fourth season", "Schadenfreude", "Poland", "John Mortimer", "nor\u00f0rvegr", "Japanese silvergrass", "The Daily Mail", "national feeling or national pride", "red", "The Hutus were considered inferior, prompting resentment that was passed on through the generations.", "Saturday", "The Book", "Iowa's critical presidential caucuses", "drinking straight from a bottle of Grey Goose.", "12.3 million people worldwide", "why do genocides and mass atrocities happen?", "Doom 3", "Benjamin Franklin", "global village", "Forrest Gump", "leachman", "WD-40 L lubricant", "Vermont", "hunting", "lemon"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5516343390804598}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.19999999999999998, 0.13793103448275862, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 0.1, 0.33333333333333337, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5285", "mrqa_squad-validation-9479", "mrqa_hotpotqa-validation-4651", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-1140", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-1249", "mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-5227", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-8975", "mrqa_naturalquestions-validation-8404", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-225", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-1891", "mrqa_newsqa-validation-3574", "mrqa_newsqa-validation-2723", "mrqa_searchqa-validation-10221", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-6504", "mrqa_searchqa-validation-1507", "mrqa_searchqa-validation-13505", "mrqa_triviaqa-validation-647"], "SR": 0.46875, "CSR": 0.5613839285714286, "retrieved_ids": ["mrqa_squad-train-1621", "mrqa_squad-train-26816", "mrqa_squad-train-34881", "mrqa_squad-train-40744", "mrqa_squad-train-5438", "mrqa_squad-train-36402", "mrqa_squad-train-40313", "mrqa_squad-train-4758", "mrqa_squad-train-36951", "mrqa_squad-train-14192", "mrqa_squad-train-69453", "mrqa_squad-train-78554", "mrqa_squad-train-29895", "mrqa_squad-train-17644", "mrqa_squad-train-37587", "mrqa_squad-train-75233", "mrqa_naturalquestions-validation-7598", "mrqa_searchqa-validation-15727", "mrqa_naturalquestions-validation-7549", "mrqa_searchqa-validation-1443", "mrqa_triviaqa-validation-7643", "mrqa_newsqa-validation-1034", "mrqa_hotpotqa-validation-4664", "mrqa_naturalquestions-validation-9064", "mrqa_naturalquestions-validation-10396", "mrqa_hotpotqa-validation-1617", "mrqa_searchqa-validation-1429", "mrqa_searchqa-validation-3449", "mrqa_newsqa-validation-4197", "mrqa_triviaqa-validation-3751", "mrqa_hotpotqa-validation-2019", "mrqa_triviaqa-validation-446"], "EFR": 0.9705882352941176, "Overall": 0.7269413077731092}, {"timecode": 28, "before_eval_results": {"predictions": ["Lucas Horenbout", "Barnett Center", "UNESCO's World Heritage list", "punk rock", "Texas", "Argentine cuisine", "Valley Falls", "1942", "Sylvia Pankhurst", "Rhode Island School of Design", "\"Battleship\"", "Arthur Miller", "July 1, 1916", "Abigail", "unidentified flying objects", "Dame Eileen June Atkins", "Toshi Ichiyanagi", "Africa", "a jersey", "Catwoman", "Nassau County", "Duncan Kenworthy", "Mr. Church", "Ed Robert Martin Jr.", "23", "Innviertel", "World War II", "Minette Walters", "\"Le Divorce\"", "Princeton University", "three's Company", "Rungrado 1st", "Linux Format", "1881", "August 8, 1945", "Guy Carawan", "presidential representative democratic republic", "Sir Henry Cole", "the Corinthian and Saronic Gulfs", "Mel Gibson", "Barack and Michelle Obama", "Austria", "randolph", "silicon", "9", "the solar system", "the Word", "rabbit-ear antennas", "Felipe Calderon", "Prince George's County", "Manmohan Singh", "a face transplant", "Fernando Caceres", "Veracruz, Mexico", "Gettysburg", "Vermont", "sheep", "compost", "the Abbey Theatre", "the Nile", "an out-of-body experience", "United States, NATO member states, Russia and India", "Russian air company Vertikal-T", "Ricardo Urbina"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6553943452380953}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.3, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-1126", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-2009", "mrqa_hotpotqa-validation-4298", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4642", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-2064", "mrqa_triviaqa-validation-1987", "mrqa_triviaqa-validation-167", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-2288", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-3461", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-2797", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-6054", "mrqa_searchqa-validation-1484", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-1563"], "SR": 0.53125, "CSR": 0.5603448275862069, "EFR": 0.9666666666666667, "Overall": 0.7259491738505747}, {"timecode": 29, "before_eval_results": {"predictions": ["over the winter of 1973\u201374", "Xbox One", "three hundred sixty", "Franois Truffaut", "Final Cut Pro", "swiss", "The chiaroscuro", "a American Tragedy", "brood", "Dairy Queen", "Roger Bacon", "Gene Autry", "voice pitch", "swoven polypropylene", "Sydney", "offbeat", "Alexander Graham Bell", "the Gulf War", "Colorado River", "Sing Sing", "the South Beach diet", "the integument", "the Intihuatana pyramid", "Phnom Penh", "Fairbanks", "swiss", "The Enterprise", "wives", "Kevin Costner", "Rhode Island", "a mysterious old mansion", "Gunsmoke", "The World's Largest Hotel", "Spacewar", "The optic nerve carries the ganglion cell axons to the brain, and the blood vessels that supply the retina", "a normally inaccessible mini-game in the 2004 video game Grand Theft Auto : San Andreas, developed by Rockstar North", "Massachusetts", "flawed democracy", "24", "The Lightning thief", "mathematics", "Dead Poets Society", "Diana Ross", "9", "Taipei", "Sinclair Lewis", "Moulin Rouge", "Fort Bragg", "September 23, 1935", "$10.5 million (USD 8 million)", "\"Naked\"", "Colonel Patrick John Mercer,", "Alien Resurrection", "the arrival of the first Spanish conquistadors", "Apple CEO Tim Cook also announced a handful of tweaks", "543 elected members", "the hunt for Nazi Gold and possibly the legendary Amber Room", "caused sections of the roof to collapse.", "seven", "Jeffrey Jamaleldine", "30,000", "four", "a solitary figure who is not understood by others, but is actually wise", "November 1961"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6526165674603175}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.25, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.22222222222222224, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-9525", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-1424", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-4657", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-9950", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-14336", "mrqa_searchqa-validation-1722", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-11919", "mrqa_searchqa-validation-11578", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-9142", "mrqa_triviaqa-validation-7195", "mrqa_hotpotqa-validation-157", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-2296", "mrqa_hotpotqa-validation-4754", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-1068", "mrqa_newsqa-validation-2766", "mrqa_naturalquestions-validation-8728"], "SR": 0.578125, "CSR": 0.5609375, "EFR": 1.0, "Overall": 0.7327343749999999}, {"timecode": 30, "UKR": 0.732421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1123", "mrqa_hotpotqa-validation-1356", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1408", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1601", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1727", "mrqa_hotpotqa-validation-173", "mrqa_hotpotqa-validation-1740", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1913", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-2050", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-2216", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2313", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2563", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2631", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2691", "mrqa_hotpotqa-validation-2748", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-281", "mrqa_hotpotqa-validation-2852", "mrqa_hotpotqa-validation-2918", "mrqa_hotpotqa-validation-2927", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3069", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-3755", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-443", "mrqa_hotpotqa-validation-4465", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-468", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4757", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4874", "mrqa_hotpotqa-validation-4932", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-5036", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5273", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5760", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5883", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-926", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1139", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1462", "mrqa_naturalquestions-validation-1537", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-2518", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2679", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3267", "mrqa_naturalquestions-validation-3316", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3536", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4695", "mrqa_naturalquestions-validation-4746", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-5394", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5804", "mrqa_naturalquestions-validation-6011", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6199", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-7549", "mrqa_naturalquestions-validation-7598", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-8153", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-834", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8650", "mrqa_naturalquestions-validation-9039", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-9248", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9271", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9953", "mrqa_naturalquestions-validation-9997", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-1597", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2093", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-2536", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3103", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-3374", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-4114", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-462", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-857", "mrqa_newsqa-validation-926", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10275", "mrqa_searchqa-validation-10638", "mrqa_searchqa-validation-10875", "mrqa_searchqa-validation-10931", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-1164", "mrqa_searchqa-validation-11905", "mrqa_searchqa-validation-11919", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12331", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12504", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12632", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-13542", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-14398", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15193", "mrqa_searchqa-validation-1584", "mrqa_searchqa-validation-15976", "mrqa_searchqa-validation-16301", "mrqa_searchqa-validation-16562", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1825", "mrqa_searchqa-validation-1857", "mrqa_searchqa-validation-1958", "mrqa_searchqa-validation-2179", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-2892", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3159", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3376", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-3449", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-4410", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-472", "mrqa_searchqa-validation-4755", "mrqa_searchqa-validation-4879", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-5787", "mrqa_searchqa-validation-6030", "mrqa_searchqa-validation-6247", "mrqa_searchqa-validation-6334", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6489", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7050", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7481", "mrqa_searchqa-validation-7749", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8444", "mrqa_searchqa-validation-8865", "mrqa_searchqa-validation-8992", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9313", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9612", "mrqa_searchqa-validation-9637", "mrqa_searchqa-validation-9651", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10078", "mrqa_squad-validation-10097", "mrqa_squad-validation-10395", "mrqa_squad-validation-1042", "mrqa_squad-validation-10427", "mrqa_squad-validation-10433", "mrqa_squad-validation-10444", "mrqa_squad-validation-10471", "mrqa_squad-validation-10493", "mrqa_squad-validation-1052", "mrqa_squad-validation-1078", "mrqa_squad-validation-1138", "mrqa_squad-validation-1304", "mrqa_squad-validation-1445", "mrqa_squad-validation-1496", "mrqa_squad-validation-1541", "mrqa_squad-validation-1598", "mrqa_squad-validation-1637", "mrqa_squad-validation-1833", "mrqa_squad-validation-1850", "mrqa_squad-validation-1862", "mrqa_squad-validation-1975", "mrqa_squad-validation-199", "mrqa_squad-validation-2108", "mrqa_squad-validation-2236", "mrqa_squad-validation-2247", "mrqa_squad-validation-2297", "mrqa_squad-validation-2376", "mrqa_squad-validation-2545", "mrqa_squad-validation-2576", "mrqa_squad-validation-276", "mrqa_squad-validation-2810", "mrqa_squad-validation-2952", "mrqa_squad-validation-3004", "mrqa_squad-validation-3190", "mrqa_squad-validation-3194", "mrqa_squad-validation-3302", "mrqa_squad-validation-3309", "mrqa_squad-validation-332", "mrqa_squad-validation-3398", "mrqa_squad-validation-3436", "mrqa_squad-validation-3524", "mrqa_squad-validation-3525", "mrqa_squad-validation-3577", "mrqa_squad-validation-358", "mrqa_squad-validation-360", "mrqa_squad-validation-3616", "mrqa_squad-validation-3620", "mrqa_squad-validation-3640", "mrqa_squad-validation-3660", "mrqa_squad-validation-3670", "mrqa_squad-validation-3715", "mrqa_squad-validation-3800", "mrqa_squad-validation-3820", "mrqa_squad-validation-3851", "mrqa_squad-validation-3865", "mrqa_squad-validation-387", "mrqa_squad-validation-3871", "mrqa_squad-validation-3926", "mrqa_squad-validation-3957", "mrqa_squad-validation-402", "mrqa_squad-validation-4044", "mrqa_squad-validation-4186", "mrqa_squad-validation-4194", "mrqa_squad-validation-4201", "mrqa_squad-validation-424", "mrqa_squad-validation-4332", "mrqa_squad-validation-4360", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4506", "mrqa_squad-validation-4547", "mrqa_squad-validation-4649", "mrqa_squad-validation-4677", "mrqa_squad-validation-4775", "mrqa_squad-validation-487", "mrqa_squad-validation-4927", "mrqa_squad-validation-4935", "mrqa_squad-validation-494", "mrqa_squad-validation-5054", "mrqa_squad-validation-510", "mrqa_squad-validation-5172", "mrqa_squad-validation-5173", "mrqa_squad-validation-5185", "mrqa_squad-validation-5334", "mrqa_squad-validation-5348", "mrqa_squad-validation-5366", "mrqa_squad-validation-5434", "mrqa_squad-validation-5448", "mrqa_squad-validation-5455", "mrqa_squad-validation-5581", "mrqa_squad-validation-5650", "mrqa_squad-validation-5791", "mrqa_squad-validation-5809", "mrqa_squad-validation-585", "mrqa_squad-validation-5951", "mrqa_squad-validation-5980", "mrqa_squad-validation-6013", "mrqa_squad-validation-6015", "mrqa_squad-validation-6024", "mrqa_squad-validation-6118", "mrqa_squad-validation-6193", "mrqa_squad-validation-6217", "mrqa_squad-validation-6238", "mrqa_squad-validation-629", "mrqa_squad-validation-6337", "mrqa_squad-validation-6382", "mrqa_squad-validation-6638", "mrqa_squad-validation-6677", "mrqa_squad-validation-6698", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6805", "mrqa_squad-validation-6833", "mrqa_squad-validation-6874", "mrqa_squad-validation-6891", "mrqa_squad-validation-6996", "mrqa_squad-validation-703", "mrqa_squad-validation-7162", "mrqa_squad-validation-7165", "mrqa_squad-validation-7347", "mrqa_squad-validation-737", "mrqa_squad-validation-7575", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-7647", "mrqa_squad-validation-7653", "mrqa_squad-validation-7670", "mrqa_squad-validation-7715", "mrqa_squad-validation-7724", "mrqa_squad-validation-7747", "mrqa_squad-validation-7850", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-816", "mrqa_squad-validation-817", "mrqa_squad-validation-8189", "mrqa_squad-validation-8196", "mrqa_squad-validation-824", "mrqa_squad-validation-8374", "mrqa_squad-validation-8416", "mrqa_squad-validation-8534", "mrqa_squad-validation-8687", "mrqa_squad-validation-8732", "mrqa_squad-validation-879", "mrqa_squad-validation-8839", "mrqa_squad-validation-8939", "mrqa_squad-validation-90", "mrqa_squad-validation-9040", "mrqa_squad-validation-9074", "mrqa_squad-validation-9249", "mrqa_squad-validation-9265", "mrqa_squad-validation-9413", "mrqa_squad-validation-9451", "mrqa_squad-validation-9783", "mrqa_squad-validation-9798", "mrqa_squad-validation-9802", "mrqa_squad-validation-9849", "mrqa_squad-validation-9994", "mrqa_triviaqa-validation-1029", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-1433", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1620", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1998", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-2387", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-3001", "mrqa_triviaqa-validation-3155", "mrqa_triviaqa-validation-328", "mrqa_triviaqa-validation-3339", "mrqa_triviaqa-validation-3441", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-356", "mrqa_triviaqa-validation-362", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3901", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4132", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4192", "mrqa_triviaqa-validation-4193", "mrqa_triviaqa-validation-4222", "mrqa_triviaqa-validation-4272", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4528", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4910", "mrqa_triviaqa-validation-4916", "mrqa_triviaqa-validation-5017", "mrqa_triviaqa-validation-5113", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-5203", "mrqa_triviaqa-validation-5380", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5544", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-5614", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-5992", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-603", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6115", "mrqa_triviaqa-validation-6133", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-636", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-6688", "mrqa_triviaqa-validation-6700", "mrqa_triviaqa-validation-6803", "mrqa_triviaqa-validation-6804", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6976", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7080", "mrqa_triviaqa-validation-7087", "mrqa_triviaqa-validation-7187", "mrqa_triviaqa-validation-7207", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-746", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-804", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-922"], "OKR": 0.8671875, "KG": 0.49609375, "before_eval_results": {"predictions": ["increasing access to education", "Arab", "Chester", "Veracruz", "safety issues", "Raymond Thomas,", "Teresa Hairston", "The Swiss art heist", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags", "U.S.-Mexico border", "a music video", "Monterrey,", "head", "file papers shortly with an appeals court seeking an emergency stay", "Long troop deployments", "dogs who walk on ice in Alaska.", "the home,", "27,", "Amanda Knox's", "falling space debris,", "Chinese tourists", "Cameron-Ritchie,", "The Transportation Security Administration", "the Gulf", "Booches Billiard Hall,", "apartment building", "Remains", "At least 40", "the Democratic VP candidate", "1831", "Daniel Radcliffe", "Conway", "Nick Adenhart,", "a ranking used in combat sports, such as boxing or mixed martial arts, of who the better fighters are relative to their weight", "the nucleus", "the date on which the Constitution of India came into effect on 26 January 1950", "the cavities and surfaces of blood vessels and organs throughout the body", "The mixing of sea water and fresh water", "driver's license", "currently starring as Phyllis Summers on The Young and the Restless", "Dilbert", "Johnny Mathis", "Dim sum", "George III", "CHICAGO", "algebra", "Rosslyn Chapel", "Acid house", "Oahu", "Thrushcross Grange", "Rigoletto", "Lost and Found", "The Hertz Corporation", "Elliot Fletcher", "Lovecraft", "Berkeley", "Peter Kropotkin", "the Wessex", "Wyoming", "Mall of America", "Mayo", "Orson Welles", "Bangkok", "Jimmy Ellis"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6122957238828503}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.90625, 0.0, 1.0, 0.25, 0.0, 0.0, 0.7499999999999999, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8695652173913044, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-247", "mrqa_newsqa-validation-1965", "mrqa_newsqa-validation-2271", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-1440", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-1261", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-419", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-3729", "mrqa_newsqa-validation-3721", "mrqa_newsqa-validation-2042", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-8056", "mrqa_naturalquestions-validation-2556", "mrqa_triviaqa-validation-285", "mrqa_hotpotqa-validation-4625", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-3678", "mrqa_hotpotqa-validation-4972", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-9692"], "SR": 0.515625, "CSR": 0.5594758064516129, "retrieved_ids": ["mrqa_squad-train-7960", "mrqa_squad-train-65245", "mrqa_squad-train-54852", "mrqa_squad-train-69743", "mrqa_squad-train-14565", "mrqa_squad-train-26002", "mrqa_squad-train-24797", "mrqa_squad-train-26133", "mrqa_squad-train-71116", "mrqa_squad-train-11479", "mrqa_squad-train-37456", "mrqa_squad-train-52099", "mrqa_squad-train-52108", "mrqa_squad-train-75207", "mrqa_squad-train-58041", "mrqa_squad-train-53662", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-4664", "mrqa_hotpotqa-validation-3395", "mrqa_naturalquestions-validation-3344", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-3814", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2064", "mrqa_searchqa-validation-472", "mrqa_triviaqa-validation-5538", "mrqa_squad-validation-1965", "mrqa_naturalquestions-validation-4097", "mrqa_triviaqa-validation-4050", "mrqa_searchqa-validation-5570", "mrqa_naturalquestions-validation-6786", "mrqa_squad-validation-3492"], "EFR": 1.0, "Overall": 0.7310357862903226}, {"timecode": 31, "before_eval_results": {"predictions": ["1969", "Highly combustible materials", "the Intertropical Convergence Zone ( ITCZ )", "2012", "Joanne Wheatley", "Clarence Williams", "cuttlefish", "Isla Grande de Tierra del Fuego", "Stephen Graham", "Radiotelegraphy", "The Drew Las Vegas", "1957", "Kentucky", "Invertebrates", "a tradeable entity used to avoid the inconvenienceiences of a pure barter system", "1648 - 51 war", "the breast or lower chest of beef or veal", "1933", "Tony Rydinger", "southern Anatolia", "halogenated paraffin hydrocarbons", "Professor Eobard Thawne", "Dr. Lexie Grey", "Number 4, Privet Drive, Little Whinging in Surrey, England", "1939", "Alex Skuby", "the University of Oxford", "15,000 BC", "Bacon", "November 27, 2013", "In the television series's fourth season", "one of the most common words in scripture", "Action Jackson", "the giraffe", "1883", "Dyfed-Powys Police", "Massachusetts", "Indonesia", "crow", "Poldek Pfefferberg", "2013 Cannes Film Festival", "Conservative Party", "1967", "Hjernevask", "corn", "3,680", "1966", "Knox's parents,", "\"The most affecting thing about this whole wheelchair for children is when the parents realize the gift that is being given to their children and they reach out to hug you.\"", "possible victims of physical and sexual abuse.", "bipartisan", "early detection and helping other women cope", "rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds", "16", "Hector Berlioz", "Omaha", "Communist", "ice hockey", "Hillary Clinton,", "Cock Robin", "Halloween", "trans-Pacific flight", "2 June 1961", "Phil Collins"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5589157973489982}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.9, 0.8, 0.6666666666666666, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.7272727272727272, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8750000000000001, 0.19354838709677416, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3491", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-1133", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-5939", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-9323", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-6374", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5473", "mrqa_hotpotqa-validation-5", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-491", "mrqa_searchqa-validation-11366", "mrqa_searchqa-validation-14694", "mrqa_searchqa-validation-6195", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-6554", "mrqa_hotpotqa-validation-3464"], "SR": 0.4375, "CSR": 0.5556640625, "EFR": 0.8888888888888888, "Overall": 0.7080512152777778}, {"timecode": 32, "before_eval_results": {"predictions": ["worst", "via pores in the epidermis", "Louis XIII's successor, Louis XIV, had a great interest in Versailles", "Ajay Tyagi", "the northernmost point on the Earth", "28 %", "West Norse sailors", "a vasectomy can cause blockages or a break in the vas deferens or the childbirthidymis", "plant", "Andrew Lloyd Webber", "New England Patriots", "John J. Flanagan", "the minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Ra\u00fal Eduardo Esparza", "In 1945", "Milira", "In 1989", "Ledger", "XXXVI", "USS Chesapeake", "Christopher Lloyd", "Pashto and Persian", "Pakistan", "a combination of genetics and the male hormone dihydrotestosterone", "Fascist Italy", "a fluorapatite - like remineralized veneer is formed over the remaining surface of the enamel", "Rocinante", "Genesis 3 : 19", "Alastair Cook", "Brad Dourif", "Robert Irsay", "a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "New York Giants quarterback Phil Simms", "a till all be fulfilled", "Fred", "Haute", "Otto von Bismarck", "a moustache", "Tiffany and Co.", "Sid Vicious", "Kids", "Alyson Stoner", "Free Range Films", "Gambaga", "a pioneer in watch design, manufacturing and distribution", "in their home country", "Draco Youlanda Ruby Clinton-Baddeley", "can I say to you?", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "avere Zinna", "on-loan David Beckham claimed his first goal in Italian football.", "barter -- trading goods and services without exchanging money", "the area was sealed off, so they did not know casualty figures.", "Bob Bogle,", "(Count von) Zeppelin", "(Henry) Tudor,", "Tsingtao", "incognito", "a bats", "Galileo", "A hard rock and...   uncle tom's cabin", "Charlotte Gainsbourg and Willem Dafoe", "2,000 euros ($2,963)", "Seasons of My Heart"], "metric_results": {"EM": 0.5, "QA-F1": 0.5776920995670995}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, true, true, false, true, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.42857142857142855, 1.0, 0.0, 1.0, 1.0, 1.0, 0.1904761904761905, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1700", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-7217", "mrqa_naturalquestions-validation-5152", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-6232", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-3353", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-3593", "mrqa_triviaqa-validation-2415", "mrqa_triviaqa-validation-2033", "mrqa_triviaqa-validation-331", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-329", "mrqa_hotpotqa-validation-1178", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-461", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-1914", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-1778", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-9195", "mrqa_searchqa-validation-16579"], "SR": 0.5, "CSR": 0.5539772727272727, "EFR": 1.0, "Overall": 0.7299360795454545}, {"timecode": 33, "before_eval_results": {"predictions": ["Thomas Girtin", "ten", "Charles Perrault", "2", "meditation", "seven", "Bachendri Pal", "7 July", "usually in May", "Arthur Chung", "The Outback", "Andy Serkis", "in France, shipped overseas in crate, and assembled on the completed pedestal on what was then called Bedloe's Island", "Sean O'Neal", "Himadri Station", "257,083", "ensure party discipline in a legislature", "Empiricism", "beneath the liver", "George II", "1920s", "Under normal conditions", "Divyanka Tripathi", "May 26, 2017", "in the basic curriculum -- the enkuklios paideia or `` education in a circle ''", "Mason Alan Dinehart III", "federal law intended to check the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "Buffalo Lookout", "Lykan Hypersport", "Geoffrey Dyson Palmer", "ice giants", "March 26, 1973", "1991", "Pete Seeger", "Sweden", "1915", "bankside power station", "onion", "trombone", "12", "Theodore Roosevelt Mason", "Route 37 East", "Robert John Day", "Do Kyung-soo", "Mexico City", "Travis County", "1937", "Polo", "Arsene Wenger", "Derek Mears", "Bob Bogle,", "Miguel Cotto.", "recite her poetry.", "Pakistani officials,", "the Nile", "Pushing Daisies", "Dirty Diana", "polish", "Isaac Newton", "Queen Nefertiti", "nonfat", "400 years", "This is not a project for commercial gain.", "Zhanar Tokhtabayeba,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.667716165413534}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5714285714285715, 0.0, 0.0, 0.8571428571428571, 0.7999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5520", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-3396", "mrqa_naturalquestions-validation-7312", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-6897", "mrqa_naturalquestions-validation-9450", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-7201", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-7403", "mrqa_triviaqa-validation-7748", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-3294", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5451", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-3076", "mrqa_searchqa-validation-15655", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-13384", "mrqa_searchqa-validation-6943", "mrqa_searchqa-validation-15769", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-3415"], "SR": 0.5625, "CSR": 0.5542279411764706, "retrieved_ids": ["mrqa_squad-train-30249", "mrqa_squad-train-58386", "mrqa_squad-train-35427", "mrqa_squad-train-67432", "mrqa_squad-train-67891", "mrqa_squad-train-33900", "mrqa_squad-train-29294", "mrqa_squad-train-21377", "mrqa_squad-train-25780", "mrqa_squad-train-82067", "mrqa_squad-train-10010", "mrqa_squad-train-34346", "mrqa_squad-train-17433", "mrqa_squad-train-52339", "mrqa_squad-train-66530", "mrqa_squad-train-54916", "mrqa_searchqa-validation-6054", "mrqa_squad-validation-436", "mrqa_naturalquestions-validation-5040", "mrqa_triviaqa-validation-7616", "mrqa_naturalquestions-validation-9885", "mrqa_newsqa-validation-3066", "mrqa_searchqa-validation-5570", "mrqa_naturalquestions-validation-1890", "mrqa_hotpotqa-validation-5584", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-2212", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2129", "mrqa_searchqa-validation-4640", "mrqa_squad-validation-438", "mrqa_triviaqa-validation-2092"], "EFR": 0.9642857142857143, "Overall": 0.722843356092437}, {"timecode": 34, "before_eval_results": {"predictions": ["The city was blockaded by Union forces, who gained control of the nearby Fort Clinch", "high", "1986", "Confederate", "Walter Mondale", "Vienna", "humid subtropical climate", "A footling breech", "Gaelic \u00d3 Brad\u00e1in", "John McConnell", "times sign or the dimension sign", "the liver and kidneys", "`` If These Dolls Could Talk ''", "1992", "the lateral side", "the university", "Baez", "Malware", "Konakuppakatil Gopinathan Balakrishnan", "May 5, 1904", "IB", "Jack Barry", "Catherine Tramell", "In the year 2026", "18,000 volunteers", "March 23, 2018", "subduction zone", "Jacques Cousteau", "1960", "Fix You", "minced meat", "help batterers work to change their attitudes and personal behavior so they would learn to be nonviolent in any relationship", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Imola", "Wigan", "a person trained to pilot, navigate, or otherwise participate as a crew member of a spacecraft", "Dante", "hockney", "li'l Abner", "Tokyo", "crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide", "Irish Chekhov", "Cody Miller", "Mary Elizabeth Hartman", "ten", "Tallahassee City Commission", "Roosevelt Island", "air support.", "a national telephone survey of more than 78,000 parents of children ages 3 to 17.", "Sabina Guzzanti", "release a notorious killer Sunday in order to bring closure to the families of three missing military men.", "north Georgia mountains", "The American Civil Liberties Union", "the Zimbabwean government", "a pirate captain of the brig Jolly Roger", "baldness", "(Karabakh)", "Archer Daniels Midland", "a short, thick, underground stem with stored starches and...", "The History of the World", "Leyden jar", "14", "Aniston, Demi Moore and Alicia Keys", "Festival Foods"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5852672453389958}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.8387096774193548, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.35294117647058826, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7096", "mrqa_naturalquestions-validation-7143", "mrqa_naturalquestions-validation-3340", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-9961", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-10356", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-1248", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-10353", "mrqa_triviaqa-validation-2357", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-3689", "mrqa_hotpotqa-validation-3633", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1018", "mrqa_newsqa-validation-664", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-9057", "mrqa_searchqa-validation-14616", "mrqa_newsqa-validation-4177"], "SR": 0.515625, "CSR": 0.553125, "EFR": 0.9354838709677419, "Overall": 0.7168623991935483}, {"timecode": 35, "before_eval_results": {"predictions": ["algorithms have been written", "1964", "A severe famine swept the nation in 1991-1993,", "the Internet", "further reconciliation among Sunnis, Shiites and tribes of different sects and bring some former members of Saddam Hussein's Baath party into the political fold.", "more than 4,000", "Nasser Medical Institute", "an annual road trip,", "the frequency of waterboarding was among the operational details that had not been declassified.", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "the two-hour finale.", "brilliant, powerful novels, which have been published all over the world.\"", "new Touch,", "gun charges,", "UNICEF", "dismissed all charges", "shooting himself three times in the head with a.40-caliber pistol,", "Kris Allen,", "Janet and La Toya,", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "\"fusion teams,\"", "too many glass shards left by beer drinkers in the city center,", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "one", "NATO fighters", "750", "one", "black, red or white,", "Phay Siphan,", "October 29 and November 5.", "energy-efficient light-emitting diodes", "two and a half hours.", "on Anjuna beach in Goa", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "personal and artistic freedom from the Beatles", "Tokyo", "Madison's proposed Virginia Plan", "Judith Cynthia Aline Keppel", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "The Convention's first act, on 10 August 1792, was to establish the French First Republic and officially strip the king of all political powers", "Fu\u00dfball-Bundes", "Elvis Presley", "a Spitfire undercarriage", "Marcus Brody", "Nadia Comaneci", "Florida", "a man (Pauline Phillips, aka Abigail Van Buren)", "1912", "World War I", "Meghan Markle", "the acid house and later rave scenes", "The 133d Air Refueling Squadron", "South African", "1754", "a chiffon", "Artemis", "rodeo", "a psalm or canticle", "the Sun Also Rises", "a beaver", "Tequila Sunrise", "(George) Spilsbury", "Vladivostok", "Clive Cussler"], "metric_results": {"EM": 0.4375, "QA-F1": 0.546209435478737}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.2857142857142857, 1.0, 0.0, 0.5, 0.7499999999999999, 1.0, 0.125, 0.47619047619047616, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.823529411764706, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.33333333333333337, 0.3636363636363636, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2481", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-856", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-91", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-2980", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-6307", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-10311", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-4688", "mrqa_triviaqa-validation-68", "mrqa_triviaqa-validation-4582", "mrqa_triviaqa-validation-7594", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-3028", "mrqa_hotpotqa-validation-1034", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-6780", "mrqa_searchqa-validation-9366", "mrqa_triviaqa-validation-138"], "SR": 0.4375, "CSR": 0.5499131944444444, "EFR": 0.9166666666666666, "Overall": 0.7124565972222222}, {"timecode": 36, "before_eval_results": {"predictions": ["trespassing at a nuclear-missile installation", "2014 Winter Olympics in Sochi, Russia", "Sleeping with the Past", "Laodicea", "Matt Monro", "on 19 June 2018", "2020 National Football League ( NFL ) season", "a Norwegian town circa 1879", "in the 18th century", "before the first year begins", "Vicente Fox", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "the Speaker or, in his absence, by the Deputy Speaker of the Lok Sabha or in his presence, the Deputy - Chairman of the Rajya Sabha", "Supplemental oxygen", "1961", "45 %", "his cousin D\u00e1in", "1933", "Russell Huxtable", "alpha efferent neurons", "runoff", "chain elongation", "1", "the team that lost the pre-game coin toss", "Fred Ott", "In the television series's fourth season", "adenine ( A ), uracil ( U ), guanine ( G ), thymine ( T ), and cytosine ( C )", "The President pro tempore", "September 4, 2000 to February 25, 2003", "somatic cell nuclear transfer", "a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms", "Eriq La Salle", "Payson, Lauren, and Kaylie", "Bonnie", "F. Lee Bailey and Alan Dershowitz.", "a pieman", "Singapore", "Malta", "bambi II", "potash", "Rob Reiner", "Medgar Evers", "Lombardy", "Yasiin Bey", "The FIFA Women's World Cup", "University of Texas Longhorns", "Carol Ann Duffy", "Sunday.", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "putting a personal and human face on the issue", "the family's blog", "Port-au-Prince,", "more than 200.", "1959.", "Luxor Las Vegas", "Tennessee", "the Vatican City", "(Thomas) Edison", "Gulf of Tonkin", "Amor", "gladiators", "Fareed Zakaria", "three", "engaging with the Taliban in Pakistan and Afghanistan."], "metric_results": {"EM": 0.4375, "QA-F1": 0.5839701361440491}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false], "QA-F1": [0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.8571428571428571, 0.4444444444444445, 0.6666666666666666, 0.8, 0.0, 1.0, 0.7878787878787877, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.7272727272727272, 0.33333333333333337, 0.0, 0.6, 1.0, 0.8405797101449275, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.1, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.17391304347826086]}}, "before_error_ids": ["mrqa_squad-validation-7014", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-9371", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-5465", "mrqa_naturalquestions-validation-5831", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-10631", "mrqa_naturalquestions-validation-5006", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-3187", "mrqa_triviaqa-validation-7164", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1420", "mrqa_triviaqa-validation-7394", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-3547", "mrqa_hotpotqa-validation-257", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-705", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15986", "mrqa_searchqa-validation-9778", "mrqa_newsqa-validation-4175"], "SR": 0.4375, "CSR": 0.546875, "retrieved_ids": ["mrqa_squad-train-8087", "mrqa_squad-train-37771", "mrqa_squad-train-28826", "mrqa_squad-train-85961", "mrqa_squad-train-85209", "mrqa_squad-train-44028", "mrqa_squad-train-12288", "mrqa_squad-train-17148", "mrqa_squad-train-31945", "mrqa_squad-train-32011", "mrqa_squad-train-84359", "mrqa_squad-train-18433", "mrqa_squad-train-37322", "mrqa_squad-train-38041", "mrqa_squad-train-70621", "mrqa_squad-train-26508", "mrqa_triviaqa-validation-14", "mrqa_searchqa-validation-9419", "mrqa_triviaqa-validation-4199", "mrqa_hotpotqa-validation-5584", "mrqa_newsqa-validation-1092", "mrqa_searchqa-validation-4367", "mrqa_searchqa-validation-16905", "mrqa_triviaqa-validation-6826", "mrqa_hotpotqa-validation-5513", "mrqa_naturalquestions-validation-8450", "mrqa_newsqa-validation-3735", "mrqa_newsqa-validation-3574", "mrqa_hotpotqa-validation-204", "mrqa_newsqa-validation-1034", "mrqa_squad-validation-7643", "mrqa_naturalquestions-validation-8975"], "EFR": 0.8888888888888888, "Overall": 0.7062934027777777}, {"timecode": 37, "before_eval_results": {"predictions": ["warmer", "Brooke Wexler", "Guy Berryman", "Needtobreathe", "drivers who meet more exclusive criteria", "James `` Jamie '' Dornan", "2012", "Games played", "2013", "15 May 2004", "President pro tempore of the Senate", "Gina Philips", "electron shells", "landowner", "Madison, Wisconsin, United States", "the England and Wales Cricket Board ( ECB ) in 2003", "1994", "Kingsford, Michigan", "a sweet alcoholic drink made with rum, fruit juice, and syrup or grenadine", "head - up", "Austria - Hungary", "Lana Del Rey", "Paracelsus", "George Warren Barnes", "ABC", "If the car is slowed initially by manual use of the automatic gear box", "1958", "1996", "the Social Security Act of 1935", "an appearance from Fonzworth Bentley and another from actress Ki Toy Johnson", "Celtic", "Latitude", "Soviet Russia defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the Lake", "a nine of diamonds playing card", "Poland", "Appaloosa", "the black death", "Lichfield Cathedral", "Charlie Sheen", "Bocelli became completely blind at the age of 12", "Pandosia", "Spain", "Ronald Ryan", "2,099", "1969 until 1974", "President of Pakistan", "Melbourne", "18th", "Trevor Rees", "three", "1998.", "an international search team", "opryland", "Norman Bates", "Frankfort", "(Henry) Fonda", "DEET", "the backstreet Boys", "the Muscular Dystrophy Association", "Easter Seal", "cilla black", "the Daily Mirror", "yellow"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6317534415081822}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, true, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.33333333333333337, 0.3333333333333333, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.20000000000000004, 0.0, 1.0, 0.0, 1.0, 0.5882352941176471, 0.5, 0.0, 0.7368421052631579, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 0.09523809523809525, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3595", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-8272", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-8287", "mrqa_naturalquestions-validation-998", "mrqa_naturalquestions-validation-10016", "mrqa_naturalquestions-validation-73", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-750", "mrqa_naturalquestions-validation-2319", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-7138", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5444", "mrqa_triviaqa-validation-2011", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-5476", "mrqa_hotpotqa-validation-1513", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-10393", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-16757"], "SR": 0.515625, "CSR": 0.5460526315789473, "EFR": 0.967741935483871, "Overall": 0.7218995384125637}, {"timecode": 38, "before_eval_results": {"predictions": ["gain support from China for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda, as well as a nearly $1.8 billion dam\"", "lemur", "(Man) Ray", "Billy Bob Thornton", "( Carson) McCullers", "(Little Rock Crisis)", "Italy", "calcium", "Alpha", "savings", "Meyer Lansky", "Iraq", "(Henry) Crok ( Bowles, xxii)", "Live Free or Die Hard", "Staden (a short form of Hovedstaden, meaning Capital)", "( Mikhail) Gorbachev", "Dr. Richard Kimble", "Sisyphus", "INXS", "A Few Good Men", "auf wiedersehen", "(Adolescent) Faust", "Elizabeth Barrett Browning", "Quiz Show", "sculpere", "Norse seafarers", "(William) Inge", "Jezebel", "Barbara Walters", "QUALCOMM,", "(Gilpin)", "The Brady Bunch", "1789", "Norman victory", "in 1984 in Dover, New Hampshire", "on the shore of Lake Erie in downtown", "in the Rocky Mountains in southwestern Colorado and northwestern New Mexico", "Gupta Empire was an ancient Indian empire, which existed at its zenith from approximately 319 to 485 CE and covered much of the Indian subcontinent", "13", "April 12, 2017", "Atlantic Ocean", "Dylan Thomas", "Aberdeen", "Mnemosyne", "Dakarz", "go (not the Chinese game of Weiqi, but based on world travel)", "gilda", "Kentucky Wildcats", "William Shakespeare", "ABC series \"Secrets and Lies\"", "26,000", "drawings", "A Bug's Life", "SpongeBob SquarePants 4-D", "10 below in Chicago, Illlinois.", "federal officers' bodies", "\"design its own separate strategies for making progress toward achieving this long-term goal.\"", "UNICEF", "26", "state senators who will decide whether to remove him from office", "24 Rezai -- who had only claimed WTA Tour titles at Strasbourg and Bali prior to Madrid -- continued her remarkable week with a 6-2 7-5 victory,", "the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "at the age of seven, survived the massacre of his clan perpetrated by his brother, Itachi, who spared Sasuke's life because he did not consider him worth killing", "20 November 1989 ( the 30th anniversary of its Declaration of the Rights of the Child )"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6659538559185737}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [0.8095238095238095, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 0.4615384615384615, 0.16, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.7499999999999999, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.967741935483871, 0.07407407407407407, 0.4]}}, "before_error_ids": ["mrqa_squad-validation-8452", "mrqa_searchqa-validation-12126", "mrqa_searchqa-validation-16459", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-13127", "mrqa_searchqa-validation-4907", "mrqa_searchqa-validation-8357", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-2098", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-3931", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-720", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-1375", "mrqa_hotpotqa-validation-5799", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-532", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-3283", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-6641", "mrqa_naturalquestions-validation-4462"], "SR": 0.5625, "CSR": 0.546474358974359, "EFR": 1.0, "Overall": 0.7284354967948719}, {"timecode": 39, "before_eval_results": {"predictions": ["the Bible", "Raimond Gaita", "Fife", "Isabella Kelly", "Daniel Andre Sturridge", "The 2016 Oklahoma Sooners football team", "Overijssel, Netherlands", "20th Century Fox", "1971", "epic verse", "Continental AG", "1976", "James Burke", "Bishop's Stortford", "The Fault in Our Stars", "Vikram Bhatt", "Kassie DePaiva", "Harriet Tubman", "Vernon Charles Kay", "RAF Tangmere, West Sussex", "Saturday Night Live", "The Dayton Memorial Hall", "aeronautical engineer", "1976", "\u00c6thelstan", "three", "KBS2", "The Indianapolis Motor Speedway", "Houston Rockets", "three", "1176", "63 mph", "Maria Szraiber", "annuity", "Inequality of opportunity", "American comedy - drama film about the titular dog, Marley", "1981", "a limited period of time", "in the 1898 Treaty of Paris", "Columbia River Gorge", "Cambridge", "squash", "Picasso", "carousel", "Bart\u00f3k", "Niger", "joey", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "more than 1.2 million people.", "33-year-old", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "propofol", "Hartsfield-Jackson International", "Judge Herman Thomas", "Dr. Quinn", "Reine de Saba", "Johnny Weissmuller", "MUSICAL THEATER", "Charles Parker", "the Hudson River", "Twin lens reflex", "Maryland Senate's actions", "1963", "Tim Rooney"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5541876526251526}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 1.0, 0.4, 0.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.16666666666666669, 1.0, 0.2, 1.0, 0.30769230769230765, 0.33333333333333337, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.5714285714285715, 1.0, 1.0, 0.5, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-4423", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-859", "mrqa_hotpotqa-validation-4691", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1622", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-487", "mrqa_hotpotqa-validation-3017", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-3053", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-1632", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-3808", "mrqa_triviaqa-validation-1314", "mrqa_newsqa-validation-814", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-3593", "mrqa_searchqa-validation-10066", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-4457", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-4169"], "SR": 0.453125, "CSR": 0.544140625, "retrieved_ids": ["mrqa_squad-train-24851", "mrqa_squad-train-48511", "mrqa_squad-train-49561", "mrqa_squad-train-68180", "mrqa_squad-train-68718", "mrqa_squad-train-48455", "mrqa_squad-train-73225", "mrqa_squad-train-40664", "mrqa_squad-train-66649", "mrqa_squad-train-34931", "mrqa_squad-train-7130", "mrqa_squad-train-19389", "mrqa_squad-train-5360", "mrqa_squad-train-44659", "mrqa_squad-train-76852", "mrqa_squad-train-41624", "mrqa_hotpotqa-validation-2769", "mrqa_squad-validation-1600", "mrqa_searchqa-validation-13857", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-1636", "mrqa_hotpotqa-validation-2762", "mrqa_newsqa-validation-2552", "mrqa_naturalquestions-validation-9387", "mrqa_triviaqa-validation-5317", "mrqa_hotpotqa-validation-5103", "mrqa_newsqa-validation-2251", "mrqa_searchqa-validation-9664", "mrqa_searchqa-validation-14645", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3585", "mrqa_triviaqa-validation-6688"], "EFR": 1.0, "Overall": 0.7279687499999999}, {"timecode": 40, "UKR": 0.74609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1164", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1214", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-216", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3472", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-402", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4664", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5157", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5584", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-763", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-104", "mrqa_naturalquestions-validation-10496", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1418", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3789", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4302", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-5109", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5792", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6641", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-8868", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1209", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2107", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2613", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2960", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3355", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4123", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-702", "mrqa_newsqa-validation-787", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10015", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10963", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13843", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14874", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15743", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2860", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-301", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-5578", "mrqa_searchqa-validation-639", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7634", "mrqa_searchqa-validation-7652", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-900", "mrqa_searchqa-validation-9057", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_squad-validation-10004", "mrqa_squad-validation-10059", "mrqa_squad-validation-1006", "mrqa_squad-validation-10112", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-10503", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1570", "mrqa_squad-validation-158", "mrqa_squad-validation-1634", "mrqa_squad-validation-1703", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-2985", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-3269", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3556", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-3611", "mrqa_squad-validation-366", "mrqa_squad-validation-3660", "mrqa_squad-validation-3667", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4044", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4421", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-509", "mrqa_squad-validation-5147", "mrqa_squad-validation-5275", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5456", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6024", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8033", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-816", "mrqa_squad-validation-82", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8319", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8370", "mrqa_squad-validation-8374", "mrqa_squad-validation-8452", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-8723", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8872", "mrqa_squad-validation-8984", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9311", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-10", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-1210", "mrqa_triviaqa-validation-1314", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1528", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-2413", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2772", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3786", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-430", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4542", "mrqa_triviaqa-validation-4560", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5383", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5554", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5849", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-647", "mrqa_triviaqa-validation-659", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.849609375, "KG": 0.5015625, "before_eval_results": {"predictions": ["St. George's United Methodist Church", "in San Francisco", "Kirstjen Nielsen", "6 - 7 % average GDP growth annually", "source code", "three legal systems, each of which applies to a particular geographical area", "2013", "Fleetwood Mac", "Ric Flair", "R2E Micral", "October 27, 2017", "31 December 1600", "1608", "1273.6 cm", "the internal reproductive anatomy ( such as the uterus in females )", "December 24, 1836", "sorrow regarding the environment", "Franklin and Wake counties", "seven", "byte - level", "the nerves and ganglia outside the brain and spinal cord", "Scandinavian patronymic surname, meaning son of Hans", "Hermann Ebbinghaus", "Sam Waterston", "on the southeastern coast of the Commonwealth of Virginia", "April 7, 2016", "Stephen A. Douglas", "HTTP / 1.1", "the southwestern part of the island", "the contestant", "Transvaginal ultrasonography", "Ku - Klip", "self - satisfaction", "Tahrir Square", "Purple Rain", "Phaethon", "Northumberland", "Eric Blair", "Cubism", "Metropolitan Borough of Oldham", "model", "Londonderry", "St. Louis, Missouri", "Chelmsford", "1,462", "Mary Harron", "GE Appliances", "staff sergeant", "Dennis Davern,", "ongoing work to update the labeling of the fluoroquinolone", "Zimbabwe,", "Islamic militants", "ferry vessels", "Jackson was in good health, contrary to media reports he was diagnosed with skin cancer.", "Joe Ranft", "John Paul Jones", "sealed-beam", "Edinburgh", "Mrs. Doubtfire", "arsenic", "Detroit", "2004", "Katherine Harris", "Liberty"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6642199775708246}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.1111111111111111, 0.2222222222222222, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8235294117647058, 0.0, 1.0, 0.0, 0.0, 0.06451612903225806, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 0.6666666666666666, 0.0, 0.4210526315789474, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10030", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-937", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-5474", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-1884", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-6822", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-2452", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1219", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-3655", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-7016", "mrqa_hotpotqa-validation-1870"], "SR": 0.578125, "CSR": 0.5449695121951219, "EFR": 1.0, "Overall": 0.7284470274390243}, {"timecode": 41, "before_eval_results": {"predictions": ["manually suppress the fire", "Outfield", "Kida", "Tom Bower", "John Adams", "Krypton", "duodenum by enterocytes of the duodenal lining", "left - sided heart failure", "July -- October 2012", "Abbot Suger", "Filipino American History Month", "1997", "9 February 2018", "re-education", "season ten", "out of RAF Coningsby in Lincolnshire", "Louis XV", "20 year - old Kyla Coleman from Lacey, Washington", "they were some of the largest wars that had ever taken place", "Kennedy Space Center ( KSC ) in Florida", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "De pictura", "Nicholas Sparks", "performance marker", "IBM", "1997", "Michelle", "Phosphorus pentoxide", "a narcissistic ex-lover who did the protagonist wrong", "Natya Shastra", "Geraldine Margaret Agnew - Somerville ( born 19 May 1967 )", "Sir Hugh Beaver", "November 1999", "st Pauls", "Norman Hartnell", "becoming bald or fear of being around bald people", "bitter liqueurs", "2 1/2", "flower", "gin", "Chrysler Automobile N.V.", "McComb, Mississippi", "King James II of England ( James VII of Scotland)", "Anah\u00ed Giovanna Puente Portilla de Velasco", "second half of the third season", "Revolver", "The Dragon School", "bankruptcies", "Benazir Bhutto,", "Rihanna", "climatecare,", "Leo Frank,", "2006", "Indian Ocean", "Patrick Dempsey", "The Tell-Tale Heart and Other Stories", "Kwai Chang Caine", "a lake", "F.D.R", "Anaheim", "red Cliff", "illegal commercial fishing.", "Iran", "in the head with a.40-caliber pistol,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5707558784606198}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, false, false, false, false], "QA-F1": [1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4, 0.0, 1.0, 0.2758620689655173, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.7142857142857143, 0.2857142857142857, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-4761", "mrqa_naturalquestions-validation-4737", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-8267", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-6326", "mrqa_naturalquestions-validation-8314", "mrqa_triviaqa-validation-6300", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1529", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-4880", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-236", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2075", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-9753", "mrqa_searchqa-validation-9112", "mrqa_searchqa-validation-6175", "mrqa_newsqa-validation-1053", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-2315"], "SR": 0.4375, "CSR": 0.5424107142857143, "EFR": 0.9444444444444444, "Overall": 0.7168241567460317}, {"timecode": 42, "before_eval_results": {"predictions": ["Magnetic stratigraphers", "other individuals, teams, or entire organizations", "1974", "an explosion and a fire", "Centre of Excellence", "Dominican", "Ministry of European Integration", "writer, essayist, philosopher, historian and playwright", "Patricia Jude Francis Kensit", "1933", "Kim Jong-hyun", "Japan", "musician", "Hans Rosenfeldt", "United States, Australia and Denmark", "Danish", "Groundhog Day", "Barnoldswick", "Tsung-Dao Lee", "Mike Pringle", "2007", "16,116", "1501", "Flavivirus", "USC Marshall School of Business", "over 1 million", "Flatbush Zombies", "William Nicholson", "quantum mechanics", "October 20, 2017", "Tennessee", "322,421", "Saint Michael, Barbados", "Earth, Wind & Fire", "in the duodenum by enterocytes of the Duodenal lining", "28 July 1914", "Italy", "Kevin Kline", "Bob Dylan", "the anterolateral corner of the spinal cord", "Dijon", "Nottingham", "the French school of landscape painters", "Colin Cant", "the Greek goddess of the wilderness", "James Mason", "\"The Ghost Writer\"", "Milan", "2009", "the motherless cub defended by Elphaba in \"Wicked.\"", "a plaque at the home of his great-grandfather", "al-Maqdessi,", "soldiers,", "prison inmates.", "Heaven", "\"Touch of Grey\"", "El Salvador", "Gina", "\"A thing of beauty is a joy for ever\"", "San Francisco 49ers", "Other Sports", "Solidarity", "Mexico", "medicine"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6151537698412699}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.25000000000000006, 1.0, 1.0, 0.28571428571428575, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-2358", "mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-297", "mrqa_hotpotqa-validation-1778", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-4426", "mrqa_hotpotqa-validation-3719", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-7511", "mrqa_triviaqa-validation-6334", "mrqa_triviaqa-validation-2097", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-66", "mrqa_newsqa-validation-3320", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2255", "mrqa_searchqa-validation-8520", "mrqa_searchqa-validation-6621", "mrqa_searchqa-validation-5984", "mrqa_searchqa-validation-4679", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-7641", "mrqa_searchqa-validation-568"], "SR": 0.53125, "CSR": 0.5421511627906976, "retrieved_ids": ["mrqa_squad-train-1422", "mrqa_squad-train-82962", "mrqa_squad-train-17507", "mrqa_squad-train-9633", "mrqa_squad-train-24526", "mrqa_squad-train-25461", "mrqa_squad-train-3569", "mrqa_squad-train-49225", "mrqa_squad-train-79740", "mrqa_squad-train-263", "mrqa_squad-train-75726", "mrqa_squad-train-39871", "mrqa_squad-train-35935", "mrqa_squad-train-80434", "mrqa_squad-train-44768", "mrqa_squad-train-72021", "mrqa_naturalquestions-validation-73", "mrqa_squad-validation-8811", "mrqa_searchqa-validation-10221", "mrqa_squad-validation-4096", "mrqa_newsqa-validation-1965", "mrqa_squad-validation-7713", "mrqa_hotpotqa-validation-751", "mrqa_newsqa-validation-1034", "mrqa_naturalquestions-validation-3267", "mrqa_hotpotqa-validation-2555", "mrqa_triviaqa-validation-5483", "mrqa_newsqa-validation-2573", "mrqa_searchqa-validation-170", "mrqa_searchqa-validation-3449", "mrqa_squad-validation-10427", "mrqa_naturalquestions-validation-5636"], "EFR": 0.9666666666666667, "Overall": 0.7212166908914728}, {"timecode": 43, "before_eval_results": {"predictions": ["six membraned chloroplast", "the third season of the television series How I Met Your Mother", "Massillon, Ohio", "Father Christmas", "2002 -- 03 and 2006 -- 06 )", "Arnold Schoenberg", "The NFL Scouting combine", "ummat al - Islamiyah", "Chelsea", "Akshay Kumar", "pepsinogen", "66 \u00b0 33 \u2032 47.0 '' north of the Equator", "signifies the victory of good over evil, the arrival of spring, end of winter, and for many a festive day to meet others, play and laugh, forget and forgive, and repair broken relationships", "Rafael Nadal", "June 1991", "Paul", "2018", "1999 to 2001", "Mark Wahlberg", "12 November 2010", "Theodosius I died", "Richard Stallman", "Dalveer Bhandari", "November 2016", "A vanishing point", "Covington, Kentucky", "The Han", "foreign exchange market (FX )", "the European economy had collapsed", "northwest of Bemis Heights", "Indian government ministry", "Peter Andrew Beardsley MBE", "May 17, 2018", "William Boyd", "raven", "Florentia", "Norman Mailer", "George Gently", "Snickers candy bars", "Daily Herald", "Diamond Rio", "15,000 people for basketball matches and 15,500 for concerts (with standing public ramp)", "Kristian Eivind Espedal", "Erich Schmidt-Leichner", "North East Lincolnshire, England", "Delaware River", "Hertz", "43 percent", "Nineteen", "July", "Michael Partain,", "Brett Cummins,", "held in a trust fund", "a kidney transplant", "ice cream", "Walla Walla", "A Million Little pieces", "Charlie Brown", "plebeians", "a butterfly effect", "a disease", "Fingerspelling", "Medical Malpractice", "the Islamic Republic of Iran"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6256448412698413}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.26666666666666666, 1.0, 0.0, 0.28571428571428575, 1.0, 0.2222222222222222, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.5333333333333333, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9737", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-9987", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-4860", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-10148", "mrqa_triviaqa-validation-2593", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-7778", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-4522", "mrqa_newsqa-validation-3835", "mrqa_newsqa-validation-1279", "mrqa_searchqa-validation-1161", "mrqa_searchqa-validation-2166", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-11574"], "SR": 0.53125, "CSR": 0.5419034090909092, "EFR": 0.9666666666666667, "Overall": 0.7211671401515152}, {"timecode": 44, "before_eval_results": {"predictions": ["Times Square Studios", "Kittie", "George Washington Bridge", "Craig William Macneill", "Oracle Corporation", "Annette Bening", "War & Peace", "Thomas Mawson", "British", "Jean Erdman", "Rabies", "Electress of Hanover", "mentalfloss.com", "New Jersey", "1995", "Frank Sinatra", "pork fat or beef suet, pork blood and a relatively high proportion of o oats", "Esteban Ocon", "Kurt Vonnegut Jr.", "mixed martial arts", "Hugh Dowding", "9", "16\u201321", "UNLV Rebels", "6,396", "Australian Electoral Division", "Radcliffe College", "13", "George Harrison", "The Shins", "The Captain Matchbox Whoopee Band", "Thomas Allen", "19th", "National League ( NL )", "Staci Keanan", "fictional Iron River Ranch in the fictitious small town of Garrison, Colorado", "March 2003", "Harry", "metabolizing bile acids, sterols, and xenobiotics", "14 November 2001", "turnip", "Williams F1", "Falkland", "gleneagles", "Dublin", "Neighbours", "Margate", "a long-range missile", "Daniel Wozniak,", "The Pilgrims", "American Muslim and Christian leaders", "ICE chief Julie Myers.", "Naples home.", "Jaime Andrade", "trenchcoat", "Eragon", "Matthew Perry", "Ivan the Terrible", "Kingston", "Lisa Gherardini", "Cleopatra", "lysergS\u00e4ureDiethylamid", "the Island", "four"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6718501984126983}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 0.07142857142857142, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-167", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3092", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-5660", "mrqa_hotpotqa-validation-1662", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-4811", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-1948", "mrqa_hotpotqa-validation-2581", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-7393", "mrqa_triviaqa-validation-4732", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1406", "mrqa_triviaqa-validation-1418", "mrqa_newsqa-validation-472", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-134", "mrqa_searchqa-validation-5564", "mrqa_triviaqa-validation-2318", "mrqa_triviaqa-validation-2578"], "SR": 0.59375, "CSR": 0.5430555555555556, "EFR": 0.9230769230769231, "Overall": 0.7126796207264957}, {"timecode": 45, "before_eval_results": {"predictions": ["the emergence of the new state of Turkey in the Ottoman Anatolian heartland, as well as the creation of modern Balkan and Middle Eastern states", "May 1, 2011", "Bob Day", "English", "23 December 1893", "James Franco", "31", "Sun Valley, Idaho", "Grand Harbour", "\"Valerian and the City of a Thousand Planets\"", "girls aged 11 to 18", "Duval County", "American burlesque", "Captain Cook's Landing Place", "alt-right", "Kentucky, Kentucky", "Belladonna", "16th season", "Prince Louis of Battenberg", "Mario Lemieux", "North Greenwich Arena", "Channel 4", "British racing driver", "1921", "Get Him to the Greek", "Interstate 22", "Ronald Lyle \" Ronnie\" Goldman", "RAF Mount Pleasant", "Azusa Pacific University", "Mel Blanc", "five", "performed under the mononym Charice until his gender transition to male", "\"Section.80\"", "Kida", "four distinct levels", "a traditional holiday originating in China, occurring near the summer solstice", "a blue rectangle in the canton ( referred to specifically as the `` union '' ) bearing fifty small, white, five - pointed stars arranged in nine offset horizontal rows", "the Roman Empire", "Theodore Roosevelt", "1987", "Jamaica", "brighton", "300", "The Quatermass Experiment", "Honeybees", "a lion", "Martin Luther King", "Sarah", "2,800", "11", "$273 million", "Iran", "diplomatic relations", "\"tan lines\"", "Jakarta", "Alexander Calder", "Nautilus", "New York City Ballet", "Jabberwocky", "Metamorphic", "ping pong", "California, Texas and Florida,", "Bright Automotive,", "an open window"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6516094717064544}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, false, true, false, false, false, true, false, false, true, true], "QA-F1": [0.5517241379310345, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.0, 0.05128205128205128, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9846", "mrqa_hotpotqa-validation-3618", "mrqa_hotpotqa-validation-3332", "mrqa_hotpotqa-validation-4362", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-3452", "mrqa_naturalquestions-validation-5943", "mrqa_naturalquestions-validation-94", "mrqa_naturalquestions-validation-3918", "mrqa_triviaqa-validation-1953", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-6393", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-2591", "mrqa_newsqa-validation-3375", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-7787", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-16616", "mrqa_newsqa-validation-2338"], "SR": 0.59375, "CSR": 0.5441576086956521, "retrieved_ids": ["mrqa_squad-train-42657", "mrqa_squad-train-70162", "mrqa_squad-train-29525", "mrqa_squad-train-5462", "mrqa_squad-train-21910", "mrqa_squad-train-40291", "mrqa_squad-train-42226", "mrqa_squad-train-8968", "mrqa_squad-train-59224", "mrqa_squad-train-27335", "mrqa_squad-train-51051", "mrqa_squad-train-48635", "mrqa_squad-train-37", "mrqa_squad-train-72979", "mrqa_squad-train-1860", "mrqa_squad-train-80247", "mrqa_naturalquestions-validation-1884", "mrqa_squad-validation-9608", "mrqa_naturalquestions-validation-6991", "mrqa_searchqa-validation-14178", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-6556", "mrqa_triviaqa-validation-1620", "mrqa_naturalquestions-validation-5876", "mrqa_searchqa-validation-14099", "mrqa_triviaqa-validation-1375", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-7250", "mrqa_naturalquestions-validation-9323", "mrqa_newsqa-validation-2797", "mrqa_squad-validation-1700", "mrqa_hotpotqa-validation-2631"], "EFR": 0.9615384615384616, "Overall": 0.7205923390468227}, {"timecode": 46, "before_eval_results": {"predictions": ["XLVIII", "Mary-Kay Wilmers", "December 1993", "Matt Groening", "the Runaways", "The original News Corporation or News Corp.", "1916", "ten", "40 million", "Paul Mueller", "Wes Archer", "26,000", "Anne of Green Gables", "Michele Marie Bachmann", "Goetheanum", "brother-in-law", "Acela Express", "The 2004 Nokia Sugar Bowl", "Univision", "The Light in the Piazza", "400 MW", "Fort Hood", "green and yellow", "My Father", "Juilliard School", "The interview", "Hilo", "Enemy", "Joseph J. Pulitzer", "Lazio", "European migrant crisis", "National Subscription Television", "2,627", "March 31, 2013", "mid-1980s", "terrestrial", "Dr. Derek Shepherd", "December 19, 2014", "Waylon Jennings", "1992", "Fluorine", "Northwest Territories", "Antoine Henri Becquerel", "ledger", "guitar", "The Comedy of Errors", "Malaysia", "Egypt", "\"Wolfman,\"", "Security was tightened in and around Kabul on Sunday", "fractured pelvis and sacrum -- the triangular bone within the pelvis.", "1.2 million", "three", "maintain an \"aesthetic environment\" and ensure public safety", "Baltimore", "Marmaduke", "Hanson", "\"I Love Rock n' roll\"", "your Dreams", "the Defense Intelligence Agency", "Jeopardy", "Scotland", "The Bulletin", "Wet Wet Wet"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7414062499999999}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-499", "mrqa_hotpotqa-validation-5093", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-2876", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-5369", "mrqa_hotpotqa-validation-4382", "mrqa_hotpotqa-validation-4584", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-2079", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-8502", "mrqa_naturalquestions-validation-2267", "mrqa_naturalquestions-validation-4028", "mrqa_triviaqa-validation-5859", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-3165", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-9607"], "SR": 0.65625, "CSR": 0.5465425531914894, "EFR": 1.0, "Overall": 0.7287616356382979}, {"timecode": 47, "before_eval_results": {"predictions": ["meningitis", "talc", "the (Miami) Dolphins", "John Hersey", "cloves", "Lysistrata", "the tulip", "an Old Manse", "the salt", "Ross Perot", "capable", "the ozone layer", "Spider-Man", "Ezra Cornell", "Arthur Miller", "(John Wilkes) Booth", "Florida", "Johns Hopkins", "Lurch", "the Missouri", "a large, mainly Gothic abbey", "Millie", "(Scott) Peterson", "\"The vampire who said he was you)\"", "John Henry", "Wall Street", "Guernsey", "Fort Sumter", "Orion", "Notre-Dame de Paris", "\"Un regalo que te dio la vida)\"", "Troilus", "Puente Hills Mall", "conquistador Francisco Pizarro", "the U.S. Census", "beta decay", "the fifth season as an American League team", "Victor Dhar", "parthenogenesis exists as a reproductive strategy in several species of whiptail lizard within the Cnemidophorus genus to which the New Mexico Whiptail belongs", "Kim Basinger", "bison", "the Grail", "in singapore", "20 January 1995", "McCourt", "Daewoo", "Rihanna", "5", "a polypeptide chain", "political correctness", "The Royal Family", "the Food and Agriculture Organization", "two or three acts", "Friends", "2007", "First Balkan War", "in Britain.", "\"your President Bush doesn't like us Muslims.\"", "Tuesday", "\"Rin Tin Tin: The Life and the Legend\"", "the Bronx.", "an \"unnamed international terror group\"", "the 11th anniversary of the September 11, 2001, terror attacks.", "Iran's parliament"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6494505494505494}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.42857142857142855, 0.8, 0.0, 1.0, 0.0, 0.0, 0.09523809523809523, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.8]}}, "before_error_ids": ["mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-13871", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-3872", "mrqa_searchqa-validation-12586", "mrqa_searchqa-validation-11180", "mrqa_searchqa-validation-7867", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-8979", "mrqa_searchqa-validation-12962", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-2102", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-1224", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-1213", "mrqa_triviaqa-validation-3207", "mrqa_triviaqa-validation-736", "mrqa_triviaqa-validation-2269", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-1378", "mrqa_hotpotqa-validation-5807", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-2563", "mrqa_newsqa-validation-2215", "mrqa_newsqa-validation-46"], "SR": 0.5625, "CSR": 0.546875, "EFR": 0.9642857142857143, "Overall": 0.7216852678571428}, {"timecode": 48, "before_eval_results": {"predictions": ["Stella McCartney", "Little Women", "the Violin", "Injun Joe", "the Rubik's cube", "the James River", "the Minstrels", "salivary glands", "Chinese", "the Solidarity", "monastery of Our Lady of the Desert", "Aristotle", "(Diane) Arbus", "defense", "Manhattan", "Martina Navratilova", "the Scandinavian country", "molten rock", "Margaret Spellings", "City of Hope", "Frasier", "Like Water for Chocolate", "a catalog", "croquet", "a high school football team in the fictional town of Dillon, Texas.", "Signs", "Henry Cisneros", "Kilimanjaro", "Nguyen", "William Arden", "Teflon Carpet", "Prince", "Thomas Lennon", "Peter Finch", "plant anatomy", "Strabo", "Middle C ( the fourth C key from left on a standard 88 - key piano keyboard )", "Julia Roberts", "Spanish, British, and pirate influence, respectively", "1980 Summer Olympics boycott was one part of a number of actions initiated by the United States to protest the Soviet invasion of Afghanistan", "rabbit", "ogaden", "london", "london", "Sarah", "Timothy Carroll", "Aquaman", "percy thrower", "Australian women's national soccer team", "about 5320 km", "She made her film debut in the 1995 teen drama \"Kids\".", "Solace", "People v. Turner", "Isle of Axholme", "Sulla", "Jenny Bae", "blew himself up.", "overturned about 5:15 p.m.", "Islamic", "not guilty of affray", "they did not receive a fair trial.", "in cities throughout Canada.", "several weeks,", "Madonna"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6580492424242425}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.9166666666666666, 0.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-13528", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-1708", "mrqa_searchqa-validation-12851", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-12278", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-9085", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-6285", "mrqa_naturalquestions-validation-3947", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-4650", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-452", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-5670", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-35", "mrqa_newsqa-validation-3362"], "SR": 0.59375, "CSR": 0.5478316326530612, "retrieved_ids": ["mrqa_squad-train-59605", "mrqa_squad-train-34685", "mrqa_squad-train-32549", "mrqa_squad-train-76790", "mrqa_squad-train-50017", "mrqa_squad-train-66404", "mrqa_squad-train-32531", "mrqa_squad-train-13601", "mrqa_squad-train-5291", "mrqa_squad-train-81284", "mrqa_squad-train-12801", "mrqa_squad-train-52034", "mrqa_squad-train-14632", "mrqa_squad-train-38505", "mrqa_squad-train-82308", "mrqa_squad-train-25308", "mrqa_hotpotqa-validation-861", "mrqa_searchqa-validation-6621", "mrqa_naturalquestions-validation-3300", "mrqa_newsqa-validation-1434", "mrqa_searchqa-validation-7050", "mrqa_hotpotqa-validation-3008", "mrqa_squad-validation-7534", "mrqa_naturalquestions-validation-922", "mrqa_searchqa-validation-12049", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6688", "mrqa_naturalquestions-validation-1884", "mrqa_hotpotqa-validation-506", "mrqa_squad-validation-4677", "mrqa_searchqa-validation-303", "mrqa_hotpotqa-validation-4001"], "EFR": 1.0, "Overall": 0.7290194515306122}, {"timecode": 49, "before_eval_results": {"predictions": ["Children of Men", "Fernando Rey", "Carpenters", "chutney", "Massachusetts", "Sunni", "Zionism", "Gaelic", "Marvin", "the Harlem River", "Jacob", "(Jose) San Martin", "Elizabeth Edward", "Richard Cory", "Franklin D. Roosevelt", "(Perry) Mason", "Italy", "Borneo", "Brazil", "a surrogate", "American Bandstand", "Tokyo", "Brazil", "Missouri", "(Victor) Dylan", "Dick Cheney", "vitamin D", "comics", "the diesel", "(Victor) Gardner", "messenger", "Homeland Security", "people of France", "Thomas Jefferson", "Michigan", "the turn of the traditional lunisolar Chinese calendar", "the Missouri River", "the meridian 180 \u00b0 east or west of the Prime Meridian", "Teri Garr", "Animals are divided by body plan into vertebrates and invertebrates", "teflie", "teflon", "Great Victoria Desert", "teflon", "stomach", "teflon", "Lake Placid", "Albert Reynolds", "C. S. Lewis", "Ford Field in Detroit, Michigan.", "Benedict of Nursia", "The Spiderwick Chronicles", "May 30, 2005", "1945", "You Can be a Star", "of thin paper-based card for competitions and plastic to conceal PINs", "Dean Martin, Katharine Hepburn and Spencer Tracy", "$5.5 billion", "forcibly drugging", "heavy turbulence", "April 22.", "the surrogate,", "Friday,", "the United States"], "metric_results": {"EM": 0.390625, "QA-F1": 0.49972830988455985}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, false, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8571428571428571, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.1111111111111111, 0.2222222222222222, 1.0, 0.1818181818181818, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-2089", "mrqa_searchqa-validation-8686", "mrqa_searchqa-validation-5970", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-9677", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-15416", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-9783", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-12119", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-9737", "mrqa_searchqa-validation-2941", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-9947", "mrqa_searchqa-validation-12956", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-1673", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-7767", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-3139", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-3420", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-129", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-1054"], "SR": 0.390625, "CSR": 0.5446875, "EFR": 1.0, "Overall": 0.7283906250000001}, {"timecode": 50, "UKR": 0.744140625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1332", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2482", "mrqa_hotpotqa-validation-2548", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3026", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3279", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4001", "mrqa_hotpotqa-validation-4009", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5310", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5593", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10092", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-10419", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1224", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2870", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5299", "mrqa_naturalquestions-validation-5497", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7639", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9936", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9975", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-2028", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2914", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3268", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3874", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4009", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-493", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11583", "mrqa_searchqa-validation-12097", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-12278", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12528", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13484", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14775", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14854", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16297", "mrqa_searchqa-validation-16390", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16850", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2268", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3410", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4159", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-4784", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-5570", "mrqa_searchqa-validation-6275", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8312", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9069", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10232", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2315", "mrqa_squad-validation-2376", "mrqa_squad-validation-2591", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4179", "mrqa_squad-validation-4360", "mrqa_squad-validation-4403", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-509", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7047", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7683", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8278", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8476", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8796", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_squad-validation-9798", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1124", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5812", "mrqa_triviaqa-validation-5941", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7172", "mrqa_triviaqa-validation-7392", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92"], "OKR": 0.8515625, "KG": 0.4828125, "before_eval_results": {"predictions": ["741 weeks", "Baaghi ( English : Rebel )", "the fourth C key from left on a standard 88 - key piano keyboard", "in Rome in 336", "Nueva Vizcaya", "1997 ( XXXII )", "western Cuba", "Yuzuru Hanyu", "four", "2001", "Eddie Van Halen", "Ozzie Smith", "the fourth quarter of the preceding year", "1986", "Archduke Franz Ferdinand of Austria", "Brad Pitt", "William Wyler", "a sociological perspective", "the red - bed country of its watershed", "31", "By 1770 BC", "President of the United States", "Welch, West Virginia", "the heart's natural pacemaker", "Super Bowl LII", "the President of the United States", "16", "a bronze statue designed by Thomas Crawford ( 1814 -- 1857 ) that, since 1863", "September 6, 2019", "Russia", "Mike Higham", "Marley & Me", "puff the Magic Dragon", "Son of Sam", "Edinburgh", "John Nash", "rochdale", "reims", "Paul Wellens", "Gorky", "Trey Parker and Matt Stone", "The Life of Charlotte Bront\u00eb", "The Future", "Keith Crofford", "the superhero Birdman", "Corendon Dutch Airlines", "\"From Here to Eternity\"", "the full fascist conspiracy theory of supernatural Jewish power", "Anil Kapoor", "Frank Ricci,", "one of the shocks of the year", "space for aspiring entrepreneurs to brainstorm with like-minded people.", "March 3, 2008.", "1918-1919.", "three", "in rural Tennessee.", "Faxe", "Mikhail Saakashvili", "Barnard College", "the lion", "Morse code", "Kristine Tsuya Yamaguchi", "\"Willie, Willie, Stee, Harry, Dick, John, Harry Three\"", "One Life to Live"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5908739697802197}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.2666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 0.25, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4428", "mrqa_naturalquestions-validation-8126", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-1450", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-599", "mrqa_naturalquestions-validation-10166", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-2063", "mrqa_hotpotqa-validation-1549", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-1640", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4995", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-3288", "mrqa_newsqa-validation-4210", "mrqa_newsqa-validation-3590", "mrqa_newsqa-validation-1084", "mrqa_searchqa-validation-8101", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-4278", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-2624", "mrqa_searchqa-validation-8489", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-1742"], "SR": 0.46875, "CSR": 0.5431985294117647, "EFR": 0.9411764705882353, "Overall": 0.7125781250000001}, {"timecode": 51, "before_eval_results": {"predictions": ["gravity", "Charles Oscar Waters", "March 1996", "the Rashidun Caliphs", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "Earle Hyman", "1998", "free floating", "Hasmukh Adhia", "the United States, bringing the United United States fully into World War II", "its absolute temperature", "Joseph M. Scriven", "Marshall Sahlins", "pit road speed", "5.7 million", "the Magnavox Odyssey", "Mike Alstott", "the right of the dinner plate", "1", "14 -- 20 April", "present - day southeastern Texas", "Ed", "1871", "American punk rock band the Ramones", "Full - Frontal Snogged", "Beorn", "8,850", "Melvil Dewey", "Joseph Sherrard Kearns", "one person", "members of the gay ( LGBT ) community", "a single peptide bond or one amino acid with two peptide bonds", "Wat Tyler", "Tyburn", "Sir Humphry Davy", "Russia", "Tennessee", "bond", "Prague", "antoine Watteau", "four", "London", "Jenson button", "1974", "18 December 1975", "Carrefour", "the town of El Nacimiento in M\u00fazquiz Municipality", "Nelson County", "Pakistan", "onto the college campus.", "orders immigrants to carry their alien registration documents at all times", "Saturday", "breast cancer.", "Wednesday", "Dean Martin, Katharine Hepburn and Spencer Tracy", "sexual harassment", "beta", "William Faulkner", "Consumers Union Reports", "Amsterdam", "Auguste Escoffier", "Rene Auberjonois", "the Bengali alphabet", "Anne Rice"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6147035256410256}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666665, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.33333333333333337, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.8, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5055", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-5585", "mrqa_naturalquestions-validation-4821", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-6481", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-607", "mrqa_triviaqa-validation-1423", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-4172", "mrqa_hotpotqa-validation-76", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-2369", "mrqa_searchqa-validation-8345", "mrqa_searchqa-validation-13488", "mrqa_searchqa-validation-14938", "mrqa_searchqa-validation-11102"], "SR": 0.515625, "CSR": 0.5426682692307692, "retrieved_ids": ["mrqa_squad-train-75194", "mrqa_squad-train-12704", "mrqa_squad-train-42211", "mrqa_squad-train-72926", "mrqa_squad-train-23459", "mrqa_squad-train-5468", "mrqa_squad-train-80940", "mrqa_squad-train-43045", "mrqa_squad-train-10738", "mrqa_squad-train-60307", "mrqa_squad-train-86384", "mrqa_squad-train-66678", "mrqa_squad-train-48395", "mrqa_squad-train-17998", "mrqa_squad-train-82885", "mrqa_squad-train-76918", "mrqa_naturalquestions-validation-7991", "mrqa_newsqa-validation-3261", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-8707", "mrqa_triviaqa-validation-92", "mrqa_hotpotqa-validation-751", "mrqa_triviaqa-validation-2736", "mrqa_hotpotqa-validation-4101", "mrqa_squad-validation-3620", "mrqa_naturalquestions-validation-3782", "mrqa_newsqa-validation-1507", "mrqa_naturalquestions-validation-1133", "mrqa_searchqa-validation-7430", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-5107", "mrqa_squad-validation-6787"], "EFR": 1.0, "Overall": 0.7242367788461539}, {"timecode": 52, "before_eval_results": {"predictions": ["Martha Coolidge", "bioelectromagnetics", "Serial (Bad) Weddings", "Dizzy", "George Raft", "Josh", "Ken Rutherford", "the Shriners", "DJ Scotch Egg", "67,575", "Target Corporation", "Gatwick", "15", "white goat", "nine", "810", "Wal-Mart Canada Corp.", "heaviest album of all", "Borwick railway station", "1834", "5.3 million", "1943", "250 million", "7 June 1985", "1983", "200,000", "orchestrated the first movement piano sketch", "Danny Glover", "Sam Kinison", "Indianapolis Motor Speedway", "Sun Belt Conference", "The cinema of Russia", "erosion", "more than 1,000", "`` Nelson's Sparrow ''", "nearby objects show a larger parallax than farther objects when observed from different positions", "iron", "James P. Flynn", "Horace Lawson Hunley", "Sally Field", "Homo sapiens", "tartar", "Ithaca", "Andrew Lloyd Webber", "pOTUS", "caryatid", "Ceylon", "Piccadilly", "London and Buenos Aires", "Yusuf Saad Kamel", "SSM Cardinal Glennon Children's Medical Center.", "two", "Brown-Waite", "re-impose order", "domestic disturbance calls to police", "an airport runway", "synapses", "Hungarian", "Antwerp", "the Apes", "blackbirds", "The Sound of Music", "a Broom", "Angola"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6053571428571429}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.4, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-2209", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3864", "mrqa_hotpotqa-validation-726", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-4768", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2506", "mrqa_hotpotqa-validation-5354", "mrqa_hotpotqa-validation-3475", "mrqa_hotpotqa-validation-3641", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1837", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-9101", "mrqa_naturalquestions-validation-5927", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-3756", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-1383", "mrqa_searchqa-validation-11862", "mrqa_searchqa-validation-8370", "mrqa_searchqa-validation-5254"], "SR": 0.53125, "CSR": 0.5424528301886793, "EFR": 0.9666666666666667, "Overall": 0.7175270243710692}, {"timecode": 53, "before_eval_results": {"predictions": ["25 October 1921", "Park Seo-joon", "Marty Ingels", "\"Complex\" magazine", "margarida", "Norbertine", "Richard III", "Coahuila, Mexico", "Province of Canterbury", "born 2 May 2015", "Eenasul Fateh", "Ladies' Code", "all U.S. territories except American Samoa", "Jung Yun-ho", "Debbie Isitt", "Clive Staples Lewis", "2017", "Adam Karpel", "evangelical Christian", "Battelle Energy Alliance", "bronze", "BBC News", "Bardot", "Steve Carell", "Jesus", "2,615", "mermaids", "Toxics Release Inventory", "General Manager", "Bellagio and The Mirage", "Dano-Northodox author Aksel Sandemose", "January 30, 1930", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "90 \u00b0 N 0 \u00b0 W", "1830", "Authority", "1984", "Elizabeth Dean Lail", "New York, New Jersey, Pennsylvania, Ohio, Indiana, Illinois, Iowa, Nebraska, Colorado, Wyoming, Utah, Nevada, and California", "1898", "Nowhere Boy", "s Somali", "John Napier", "watt hours", "sambals", "bill", "British Airways", "-30", "al Fayed's", "test-fire a long-range missile", "police dogs", "February 2008", "outside his house in Najaf's Adala neighborhood", "she was a pain in the ass.", "NATO's Membership Action Plan,", "Three", "Agatha Christie", "the union", "Converse", "Trinidad and Tobago", "\"Cogito,ego sum\"", "Ahab", "\"Solitude\"", "\"The Vision\""], "metric_results": {"EM": 0.40625, "QA-F1": 0.522998195878575}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.37499999999999994, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5245901639344263, 0.6, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.19047619047619047, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2784", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-2190", "mrqa_hotpotqa-validation-5549", "mrqa_hotpotqa-validation-2767", "mrqa_hotpotqa-validation-2831", "mrqa_hotpotqa-validation-661", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1434", "mrqa_hotpotqa-validation-3031", "mrqa_hotpotqa-validation-4446", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-1777", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-2690", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-64", "mrqa_triviaqa-validation-6295", "mrqa_triviaqa-validation-811", "mrqa_triviaqa-validation-1289", "mrqa_triviaqa-validation-2698", "mrqa_newsqa-validation-2957", "mrqa_newsqa-validation-1657", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-16360", "mrqa_searchqa-validation-14283", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-12630"], "SR": 0.40625, "CSR": 0.5399305555555556, "EFR": 0.9473684210526315, "Overall": 0.7131629203216374}, {"timecode": 54, "before_eval_results": {"predictions": ["Israel", "Jean Baptiste Le Roy", "south africa", "A Dangerous Man: Lawrence After Arabia", "General John J. Pershing", "Barbra Streisand", "Australia and England", "Numb3rs", "Ken Purdy", "Switzerland", "a sense of loyalty and dedication to a specific person or persons", "roan Atkinson", "Catholic", "cyclops", "Tommy Sheridan", "Adolf Hitler", "Jonathon Bell's Farm", "a rat", "four", "oakum", "1912", "Amelia Earhart", "13", "B\u00e9la Bart\u00f3k", "Tom Watson", "Libya", "ethiopian", "Norwegian", "radishes", "ethiopian", "Amsterdam", "professor", "Lew Brown", "Paracelsus", "The weekly Torah portion", "2014", "Times Square in New York City", "Luther Ingram", "The Constitution of India", "Middle Eastern alchemy", "three members", "The Russian Ark", "Harry F. Sinclair", "seabird", "ABC", "perjury and obstruction of justice", "Justin Paul", "1974", "Alexandre Caizergues, of France,", "The ruling Justicialist Party, or PJ by its Spanish acronym, lost its majority in the Chamber of Deputies after being defeated in 18 of 60 races,", "Vicente Carrillo Leyva", "Prague", "your environmental efforts make even more impact than Harrison Ford's chest.", "Judge Sonia Sotomayor,", "269,000", "FARC rebels.", "Lech Walesa", "outside", "Sweden", "Hiroshima", "Paraguay", "the Miami Dolphins", "William Penn", "James Joyce"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5516894257703082}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.0, 0.19999999999999998, 0.13333333333333333, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3410", "mrqa_triviaqa-validation-5698", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-6411", "mrqa_triviaqa-validation-4934", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-3334", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-360", "mrqa_triviaqa-validation-4677", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-5761", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-3563", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1394", "mrqa_naturalquestions-validation-3546", "mrqa_naturalquestions-validation-6490", "mrqa_hotpotqa-validation-3611", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-410", "mrqa_hotpotqa-validation-1058", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3554", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-1", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-1037", "mrqa_searchqa-validation-13638", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-6670"], "SR": 0.453125, "CSR": 0.5383522727272727, "retrieved_ids": ["mrqa_squad-train-44622", "mrqa_squad-train-4323", "mrqa_squad-train-2457", "mrqa_squad-train-69431", "mrqa_squad-train-70991", "mrqa_squad-train-78208", "mrqa_squad-train-73556", "mrqa_squad-train-22552", "mrqa_squad-train-7342", "mrqa_squad-train-72982", "mrqa_squad-train-58916", "mrqa_squad-train-7804", "mrqa_squad-train-41240", "mrqa_squad-train-28347", "mrqa_squad-train-19420", "mrqa_squad-train-79347", "mrqa_newsqa-validation-340", "mrqa_triviaqa-validation-7393", "mrqa_naturalquestions-validation-4319", "mrqa_hotpotqa-validation-1870", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-10202", "mrqa_searchqa-validation-16301", "mrqa_naturalquestions-validation-1015", "mrqa_triviaqa-validation-3576", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-2434", "mrqa_newsqa-validation-1364", "mrqa_naturalquestions-validation-2119", "mrqa_hotpotqa-validation-2793", "mrqa_squad-validation-7724", "mrqa_searchqa-validation-2896"], "EFR": 0.9714285714285714, "Overall": 0.7176592938311688}, {"timecode": 55, "before_eval_results": {"predictions": ["severe flooding", "Peshawar,", "Australia and New Zealand", "suicide bombing", "Kurdistan Workers' Party,", "the District of Columbia National Guard,", "the simple puzzle video game,", "the flooding was so fast that the thing flipped over,\"", "Elizabeth Birnbaum", "Gulf of Aden,", "why you broke up", "42 years old", "actress", "no plans to fritter his cash away on fast cars, drink and celebrity parties.", "when daughter Sasha exhibited signs of potentially deadly meningitis when she was 4 months old.", "Six", "Brown-Waite", "tennis", "Alinghi", "Michael Schumacher", "the late 1970s to the mid-1980s.", "Hong Kong,", "Roy Foster", "I'm certainly not nearly as good of a speaker as he is.\"", "Liverpool.", "romantic", "Mandi Hamlin", "to put a lid on the marking of Ashura this year.", "10 municipal police officers", "Herman Cain", "Two", "is sending thousands of troops to assist in relief efforts,", "Fall 1998", "October 27, 1904", "February 9, 2018", "June 3, 1937", "12.9 - kilometre ( 8 mi )", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present ), and must sign a waiver prior to shooting", "interstate communications by radio, television, wire, satellite, and cable", "starch", "Eva Cassidy", "1983", "benjamin", "Bahrain", "triathlon", "Kenya", "Kevin Spacey", "noises Off", "black nationalism", "Great Lakes and Midwestern", "Jarome Iginla", "1980", "MG Cars", "an expeditionary EA-18G Growler squadron of the United States Navy based at Naval Air Station Whidbey Island, Washington", "the Salzburg Festival", "Fiat Automobiles NV", "Washington Irving", "Jamaica Inn", "Suspicious Minds", "after the fact", "conga drums", "Warren Schmidt", "birds", "Big Momma"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6736887792397661}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.21052631578947367, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1603", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2667", "mrqa_newsqa-validation-637", "mrqa_newsqa-validation-1180", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3932", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-3788", "mrqa_newsqa-validation-1058", "mrqa_newsqa-validation-2758", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-8617", "mrqa_naturalquestions-validation-9726", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-45", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-739", "mrqa_hotpotqa-validation-5233", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-4167"], "SR": 0.59375, "CSR": 0.5393415178571428, "EFR": 1.0, "Overall": 0.7235714285714285}, {"timecode": 56, "before_eval_results": {"predictions": ["suez Canal", "Basil Feldman", "antelope", "Caroline Garcia", "Roger Casement", "Czech Republic", "Oliver!", "testicles", "driving Miss Daisy", "Oklahoma", "QM2", "Djibouti and Yemen", "animals", "ernathan Crane", "playoff basketball", "geyser", "mention of jumanji", "rum", "wigan", "tennis", "beetles", "Dan Dare", "Josh Brolin", "Pearl Slaghoople", "buchistan", "Flybe", "egypt", "Francesca Annis", "Brat Pack", "jujitsu", "business", "king eddy", "Gary Player", "May 2017", "Southport, North Carolina", "Columbia River Gorge in the U.S. states of Oregon and Washington", "Doug Pruzan", "2019", "eliminate or reduce the trade barriers among all countries in the Americas, excluding Cuba", "9 February 2018", "Liga MX", "Attorney General and as Lord Chancellor of England", "The Cosmopolitan of Las Vegas", "Mel Blanc", "Democratic", "Tak and the Power of Juju", "jet-powered tailless", "Phil Spector", "30-minute", "12 million", "it should stay that way.", "11,", "1,073", "Corinthians,", "mention of a secret long hidden by the brotherhood.", "education about rainforests.", "Foynes, Ireland", "caution", "the Fountain of Youth", "Jones", "Rouen", "Bangkok", "Typhoid Mary", "a cure or solution for any illness or problem"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6204630265567765}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, true, true, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 1.0, 0.4615384615384615, 1.0, 1.0, 0.9600000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4276", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-5969", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-131", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-5067", "mrqa_triviaqa-validation-942", "mrqa_triviaqa-validation-5682", "mrqa_triviaqa-validation-4767", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5736", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-190", "mrqa_naturalquestions-validation-8465", "mrqa_naturalquestions-validation-3697", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-510", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3639", "mrqa_searchqa-validation-7030", "mrqa_searchqa-validation-5987", "mrqa_searchqa-validation-3732", "mrqa_searchqa-validation-3538", "mrqa_searchqa-validation-7493"], "SR": 0.546875, "CSR": 0.5394736842105263, "EFR": 1.0, "Overall": 0.7235978618421053}, {"timecode": 57, "before_eval_results": {"predictions": ["kinshasa, Zaire", "lips", "public baseball analytics", "piscinae", "Richard Marx", "george", "chiba", "korea", "Vimto vimto", "Saddam Hussein", "vickers", "anthropocene", "shabbat", "Cosmos: A spacetime Odyssey", "Matlock", "Persuasion", "aliens", "Talbot Road", "uveitis", "9", "Dickens", "a pleural cavity", "a wake", "sugar baby Love", "15", "canisius", "george v", "significant achievement", "marty Jacobi", "points based scoring system", "JK", "basil", "4 January 2011", "hairpin turn", "1986", "a vertebrate's immune system", "a child's favourite colour,", "William Fox", "1850", "22 \u00b0 00 \u2032 N 80 \u00b0 00", "comedy", "hiphop", "Juventus of Italy", "Kennebec County", "Magnus Carlsen", "marty jon stewart", "The Walking Dead", "Pylos and Thebes", "david bowie,", "the mine was shut down in 1882,", "an impromptu memorial for the late singer", "North Korea", "at the bottom of the hill", "201-262-2800.", "legitimacy of that race.", "Brian Smith.", "New Balance Athletic Shoe,", "dehydration", "Mystery Science", "\"Baby Got Back In The U.S.R.\"", "Wales", "Deep Purple", "James II", "Mexico"], "metric_results": {"EM": 0.359375, "QA-F1": 0.49409722222222224}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.0, 0.5, 0.8, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.8000000000000002, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-6831", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6622", "mrqa_triviaqa-validation-2362", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-4022", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-6964", "mrqa_triviaqa-validation-7584", "mrqa_triviaqa-validation-3450", "mrqa_triviaqa-validation-1095", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-4921", "mrqa_triviaqa-validation-4483", "mrqa_naturalquestions-validation-9979", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-9081", "mrqa_naturalquestions-validation-5451", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5472", "mrqa_hotpotqa-validation-5299", "mrqa_hotpotqa-validation-865", "mrqa_hotpotqa-validation-5412", "mrqa_newsqa-validation-1067", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-4095", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-15364", "mrqa_searchqa-validation-15959"], "SR": 0.359375, "CSR": 0.5363685344827587, "retrieved_ids": ["mrqa_squad-train-59801", "mrqa_squad-train-77102", "mrqa_squad-train-67255", "mrqa_squad-train-4816", "mrqa_squad-train-21826", "mrqa_squad-train-3797", "mrqa_squad-train-85059", "mrqa_squad-train-16708", "mrqa_squad-train-78895", "mrqa_squad-train-4145", "mrqa_squad-train-48758", "mrqa_squad-train-47986", "mrqa_squad-train-51783", "mrqa_squad-train-71490", "mrqa_squad-train-494", "mrqa_squad-train-11622", "mrqa_squad-validation-10506", "mrqa_naturalquestions-validation-9064", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-7071", "mrqa_triviaqa-validation-6588", "mrqa_squad-validation-5362", "mrqa_triviaqa-validation-6867", "mrqa_hotpotqa-validation-3629", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-9963", "mrqa_hotpotqa-validation-2949", "mrqa_hotpotqa-validation-5401", "mrqa_triviaqa-validation-1529", "mrqa_triviaqa-validation-5544", "mrqa_hotpotqa-validation-4300", "mrqa_searchqa-validation-7641"], "EFR": 0.9512195121951219, "Overall": 0.7132207343355762}, {"timecode": 58, "before_eval_results": {"predictions": ["Payaya Indians", "March 11, 2018", "Wisconsin", "1535", "Erica Rivera", "International Baccalaureate", "Tommy James and the Shondells", "alcohol or smoking", "Paul Lynde", "United States, its NATO allies and others", "James Rodr\u00edguez", "Hugh S. Johnson", "The Miracles", "spontaneously", "through the buttock and down the lower limb", "Beijing", "Cadillac", "the United States", "Salman Khan", "`` Deadman's Gun ''", "development of electronic computers in the 1950s", "the defendant owed a duty to the deceased to take care", "Sri Lanka Podujana Peramuna, led by former president Mahinda Rajapaksa", "Hold On", "rock music subgenres", "November 1999", "Hal Derwin", "the government - owned Panama Canal Authority", "Scott Bakula as Dwayne `` King '' Cassius Pride", "from the `` round '', the rear leg of the cow", "William Wyler", "Woody Paige", "William Blake", "Miranda v. Arizona", "multi-user dungeon", "Buddha", "railway", "climates", "Much Ado about Nothing", "Lancashire", "1926 Paris", "flew solo to Scotland in an attempt to negotiate peace with the United Kingdom", "Belarus", "Walcha", "James I of England", "Golden Calf", "Edward Anthony Spitzka", "nearly 80 years", "Kurt Cobain,", "active athletes,\"", "London's Heathrow airport", "Anjuna beach in Goa", "56,", "the results by a chaplain about 1:45 p.m.", "The Lost Symbol", "Mashhad, Iran.", "SHAKESPEARE", "out of body", "Athol Fugard", "Jonah", "Treasure Island", "Death Watch", "ink", "Montego Bay"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6459277701465203}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.4, 1.0, 1.0, 0.3846153846153846, 0.4444444444444445, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.4444444444444445, 0.0, 0.4, 1.0, 0.25, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4038", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-10378", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-6821", "mrqa_triviaqa-validation-3242", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-3854", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-142", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3727", "mrqa_newsqa-validation-1775", "mrqa_searchqa-validation-8279", "mrqa_searchqa-validation-16773", "mrqa_searchqa-validation-33", "mrqa_searchqa-validation-9721"], "SR": 0.546875, "CSR": 0.5365466101694916, "EFR": 1.0, "Overall": 0.7230124470338983}, {"timecode": 59, "before_eval_results": {"predictions": ["dismissal", "Long Island convenience store", "Kenneth Cole", "Dennis Davern,", "Harry Nicolaides,", "Russia", "Pakistan territory", "free fixes for the consumer.", "heavy turbulence", "the southern city of Naples", "outside influences", "Rima Fakih", "Teen Patti", "for the past two years.", "Cybersecurity Concerns", "$55.7 million", "alleviation of their pain", "$3,200 per week", "autonomy", "\"How to Shoot Friends and Interrogate People.\"", "Jenny Sanford,", "\"An international search team was combing Lebanon's Mediterranean coastline for signs of life Tuesday", "Animal Planet", "\"an ongoing investigation into 24 illnesses in multiple states,\"", "Monday", "old car", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "fast cars, drink and celebrity parties.\"", "Dr. Jennifer Arnold and husband Bill Klein,", "Sabina Guzzanti", "a bag", "general secretary", "British Columbia, Canada", "Bob Dylan", "warplanes", "Arunachal Pradesh", "chain elongation", "1926", "3, 1, and 4", "Sreejita De", "Il Divo", "honey", "perfumes", "november", "einstein", "Nancy Astor", "mmorpg", "in the Counter Terrorism Intranet Referral Unit", "public square", "Ben Ainslie", "The Dalton Gang", "HackThis Site.org", "Shut Up", "the 2016 U.S. Senate election", "Mark Anthony \"Baz\" Luhrmann", "Two Pi\u00f1a Coladas", "Martin Luther", "Penn State", "Joe Hill", "turkey vultures", "The National Archives Museum", "Hera", "nag", "the River"], "metric_results": {"EM": 0.5, "QA-F1": 0.6070349702380953}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.09523809523809523, 0.0, 1.0, 0.0, 1.0, 0.375, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.07999999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666665, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-689", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-2416", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3060", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-1826", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-961", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-3028", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-4997", "mrqa_triviaqa-validation-5109", "mrqa_triviaqa-validation-1501", "mrqa_hotpotqa-validation-37", "mrqa_hotpotqa-validation-1714", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2989", "mrqa_searchqa-validation-5556", "mrqa_searchqa-validation-8933", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-10078"], "SR": 0.5, "CSR": 0.5359375, "EFR": 1.0, "Overall": 0.722890625}, {"timecode": 60, "UKR": 0.716796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1155", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1414", "mrqa_hotpotqa-validation-156", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1654", "mrqa_hotpotqa-validation-1655", "mrqa_hotpotqa-validation-1754", "mrqa_hotpotqa-validation-1837", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-205", "mrqa_hotpotqa-validation-2186", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-232", "mrqa_hotpotqa-validation-2333", "mrqa_hotpotqa-validation-236", "mrqa_hotpotqa-validation-2375", "mrqa_hotpotqa-validation-2407", "mrqa_hotpotqa-validation-2410", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-2613", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2684", "mrqa_hotpotqa-validation-2687", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2777", "mrqa_hotpotqa-validation-2781", "mrqa_hotpotqa-validation-2799", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2902", "mrqa_hotpotqa-validation-2946", "mrqa_hotpotqa-validation-2989", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3000", "mrqa_hotpotqa-validation-3008", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-3167", "mrqa_hotpotqa-validation-3271", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3569", "mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3837", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-4300", "mrqa_hotpotqa-validation-4492", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4535", "mrqa_hotpotqa-validation-4656", "mrqa_hotpotqa-validation-4714", "mrqa_hotpotqa-validation-4794", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4900", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5004", "mrqa_hotpotqa-validation-5015", "mrqa_hotpotqa-validation-502", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5090", "mrqa_hotpotqa-validation-5112", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-5367", "mrqa_hotpotqa-validation-5397", "mrqa_hotpotqa-validation-5403", "mrqa_hotpotqa-validation-5412", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5513", "mrqa_hotpotqa-validation-5570", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5753", "mrqa_hotpotqa-validation-5774", "mrqa_hotpotqa-validation-5852", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-876", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-963", "mrqa_naturalquestions-validation-10295", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-1134", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1309", "mrqa_naturalquestions-validation-1437", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1655", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-1791", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1911", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-2060", "mrqa_naturalquestions-validation-2323", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-2761", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2903", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-2906", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3491", "mrqa_naturalquestions-validation-366", "mrqa_naturalquestions-validation-3666", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3808", "mrqa_naturalquestions-validation-3822", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3947", "mrqa_naturalquestions-validation-3969", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4137", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4169", "mrqa_naturalquestions-validation-4316", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4646", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-4775", "mrqa_naturalquestions-validation-4829", "mrqa_naturalquestions-validation-4940", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5300", "mrqa_naturalquestions-validation-5602", "mrqa_naturalquestions-validation-5709", "mrqa_naturalquestions-validation-5780", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-5826", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-594", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6057", "mrqa_naturalquestions-validation-6083", "mrqa_naturalquestions-validation-6093", "mrqa_naturalquestions-validation-613", "mrqa_naturalquestions-validation-6206", "mrqa_naturalquestions-validation-6258", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6650", "mrqa_naturalquestions-validation-6857", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7310", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-7488", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8404", "mrqa_naturalquestions-validation-8450", "mrqa_naturalquestions-validation-8541", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8599", "mrqa_naturalquestions-validation-8707", "mrqa_naturalquestions-validation-8759", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-8990", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-9249", "mrqa_naturalquestions-validation-9327", "mrqa_naturalquestions-validation-9340", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-9435", "mrqa_naturalquestions-validation-9553", "mrqa_naturalquestions-validation-9578", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-988", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-9979", "mrqa_newsqa-validation-1007", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1062", "mrqa_newsqa-validation-1109", "mrqa_newsqa-validation-1145", "mrqa_newsqa-validation-1167", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1240", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1409", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-2112", "mrqa_newsqa-validation-2251", "mrqa_newsqa-validation-2416", "mrqa_newsqa-validation-2466", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-2661", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2955", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-3024", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-3211", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3279", "mrqa_newsqa-validation-3339", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-3884", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-4177", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-515", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-726", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10143", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-10688", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-10987", "mrqa_searchqa-validation-11051", "mrqa_searchqa-validation-11489", "mrqa_searchqa-validation-11539", "mrqa_searchqa-validation-11544", "mrqa_searchqa-validation-11578", "mrqa_searchqa-validation-11815", "mrqa_searchqa-validation-11869", "mrqa_searchqa-validation-12154", "mrqa_searchqa-validation-12249", "mrqa_searchqa-validation-1226", "mrqa_searchqa-validation-1239", "mrqa_searchqa-validation-12426", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-12544", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-13895", "mrqa_searchqa-validation-14178", "mrqa_searchqa-validation-1419", "mrqa_searchqa-validation-14243", "mrqa_searchqa-validation-14642", "mrqa_searchqa-validation-14790", "mrqa_searchqa-validation-1484", "mrqa_searchqa-validation-14891", "mrqa_searchqa-validation-15538", "mrqa_searchqa-validation-15815", "mrqa_searchqa-validation-16038", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16616", "mrqa_searchqa-validation-16856", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-2079", "mrqa_searchqa-validation-2315", "mrqa_searchqa-validation-2381", "mrqa_searchqa-validation-2582", "mrqa_searchqa-validation-2627", "mrqa_searchqa-validation-289", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3041", "mrqa_searchqa-validation-3110", "mrqa_searchqa-validation-3153", "mrqa_searchqa-validation-3305", "mrqa_searchqa-validation-3607", "mrqa_searchqa-validation-3637", "mrqa_searchqa-validation-4258", "mrqa_searchqa-validation-430", "mrqa_searchqa-validation-4456", "mrqa_searchqa-validation-4457", "mrqa_searchqa-validation-45", "mrqa_searchqa-validation-4683", "mrqa_searchqa-validation-4746", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5030", "mrqa_searchqa-validation-6430", "mrqa_searchqa-validation-6852", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7159", "mrqa_searchqa-validation-7459", "mrqa_searchqa-validation-7596", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7793", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8737", "mrqa_searchqa-validation-8838", "mrqa_searchqa-validation-8964", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9233", "mrqa_searchqa-validation-9289", "mrqa_searchqa-validation-9293", "mrqa_searchqa-validation-9324", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-1006", "mrqa_squad-validation-10140", "mrqa_squad-validation-1016", "mrqa_squad-validation-10433", "mrqa_squad-validation-1270", "mrqa_squad-validation-1277", "mrqa_squad-validation-1312", "mrqa_squad-validation-1462", "mrqa_squad-validation-1509", "mrqa_squad-validation-1634", "mrqa_squad-validation-1965", "mrqa_squad-validation-199", "mrqa_squad-validation-2086", "mrqa_squad-validation-2160", "mrqa_squad-validation-2251", "mrqa_squad-validation-2376", "mrqa_squad-validation-2752", "mrqa_squad-validation-2916", "mrqa_squad-validation-3223", "mrqa_squad-validation-3230", "mrqa_squad-validation-34", "mrqa_squad-validation-3416", "mrqa_squad-validation-3492", "mrqa_squad-validation-3581", "mrqa_squad-validation-360", "mrqa_squad-validation-3610", "mrqa_squad-validation-366", "mrqa_squad-validation-3670", "mrqa_squad-validation-3678", "mrqa_squad-validation-3693", "mrqa_squad-validation-3711", "mrqa_squad-validation-3851", "mrqa_squad-validation-3957", "mrqa_squad-validation-3986", "mrqa_squad-validation-4750", "mrqa_squad-validation-494", "mrqa_squad-validation-5035", "mrqa_squad-validation-5375", "mrqa_squad-validation-545", "mrqa_squad-validation-5455", "mrqa_squad-validation-5502", "mrqa_squad-validation-5581", "mrqa_squad-validation-5753", "mrqa_squad-validation-6034", "mrqa_squad-validation-6382", "mrqa_squad-validation-6565", "mrqa_squad-validation-6653", "mrqa_squad-validation-6703", "mrqa_squad-validation-6787", "mrqa_squad-validation-6852", "mrqa_squad-validation-703", "mrqa_squad-validation-7037", "mrqa_squad-validation-7096", "mrqa_squad-validation-7125", "mrqa_squad-validation-7137", "mrqa_squad-validation-7252", "mrqa_squad-validation-7276", "mrqa_squad-validation-7347", "mrqa_squad-validation-7577", "mrqa_squad-validation-7577", "mrqa_squad-validation-758", "mrqa_squad-validation-764", "mrqa_squad-validation-7701", "mrqa_squad-validation-7715", "mrqa_squad-validation-7850", "mrqa_squad-validation-7976", "mrqa_squad-validation-8002", "mrqa_squad-validation-8068", "mrqa_squad-validation-8134", "mrqa_squad-validation-8231", "mrqa_squad-validation-8332", "mrqa_squad-validation-8338", "mrqa_squad-validation-8699", "mrqa_squad-validation-878", "mrqa_squad-validation-8987", "mrqa_squad-validation-9074", "mrqa_squad-validation-9304", "mrqa_squad-validation-9372", "mrqa_squad-validation-9516", "mrqa_squad-validation-9606", "mrqa_triviaqa-validation-1074", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-14", "mrqa_triviaqa-validation-1430", "mrqa_triviaqa-validation-1501", "mrqa_triviaqa-validation-1510", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-1753", "mrqa_triviaqa-validation-180", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-183", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2189", "mrqa_triviaqa-validation-2288", "mrqa_triviaqa-validation-2351", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2382", "mrqa_triviaqa-validation-2524", "mrqa_triviaqa-validation-2596", "mrqa_triviaqa-validation-2763", "mrqa_triviaqa-validation-2830", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2931", "mrqa_triviaqa-validation-298", "mrqa_triviaqa-validation-3031", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3209", "mrqa_triviaqa-validation-347", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3530", "mrqa_triviaqa-validation-3757", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-3784", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3905", "mrqa_triviaqa-validation-3926", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4199", "mrqa_triviaqa-validation-4237", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4607", "mrqa_triviaqa-validation-4897", "mrqa_triviaqa-validation-4937", "mrqa_triviaqa-validation-5133", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-5789", "mrqa_triviaqa-validation-5868", "mrqa_triviaqa-validation-5982", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6098", "mrqa_triviaqa-validation-6128", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-6197", "mrqa_triviaqa-validation-627", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6368", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-6499", "mrqa_triviaqa-validation-6608", "mrqa_triviaqa-validation-6705", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-6867", "mrqa_triviaqa-validation-6920", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7312", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-7643", "mrqa_triviaqa-validation-892", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.810546875, "KG": 0.45390625, "before_eval_results": {"predictions": ["The Vamps", "Bonnie Lipton", "Benzodiazepines", "19 June 2018", "in the central plains", "Uranus", "ase", "During Hanna's recovery masquerade celebration", "Gibraltar", "1928", "UTC \u2212 08 : 00", "Bob Peterson", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "Beijing", "2017 - 18", "1979", "Johnny Cash", "$2 million", "Mel Gibson", "The virion must assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "the Fox Ranch in Malibu Creek State Park", "rum", "a Scandinavian patronymic surname, meaning son of Hans", "the port of Nueva Espa\u00f1a", "Kelly Reno", "cytosine ( T )", "Jon", "to encourage rebellion against the British authorities", "the southern borders of Ohio, Indiana and Illinois", "ice giants", "Christopher Columbus", "the Agra garden", "Orrest Head", "S. molloyi", "Gremlins", "Audi", "tartare", "Babylonian", "black", "newbury", "the Taliban's", "2009", "August 21, 1995", "Humberside Airport", "\"King of Cool\"", "Debbie Reynolds", "Patricia Jude Francis Kensit", "neuro-orthopaedic", "Arthur E. Morgan III,", "\"procedure on her heart,\"", "back at work", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "former CEO of an engineering and construction company with a vast personal fortune.", "Russian concerns that the defensive shield could be used for offensive aims.", "suicide bomber in volatile northwestern Pakistan", "see my kids graduate from this school district.", "Yellowstone", "\"Crazy Foot\"", "Cermak to Bilandic", "John Knox", "the Catholic Church", "the urinary inlet", "Emiliano Zapata", "Anderson Cooper"], "metric_results": {"EM": 0.46875, "QA-F1": 0.603078361371705}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.8, 0.0, 1.0, 0.7272727272727272, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.72, 0.7368421052631579, 0.25, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.8, 1.0, 0.8918918918918919, 0.23529411764705885, 1.0, 0.0, 0.28571428571428575, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4309", "mrqa_naturalquestions-validation-7661", "mrqa_naturalquestions-validation-5155", "mrqa_naturalquestions-validation-9222", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-10536", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-9908", "mrqa_naturalquestions-validation-7484", "mrqa_naturalquestions-validation-1439", "mrqa_naturalquestions-validation-5580", "mrqa_naturalquestions-validation-3413", "mrqa_naturalquestions-validation-800", "mrqa_triviaqa-validation-4525", "mrqa_triviaqa-validation-4205", "mrqa_triviaqa-validation-6207", "mrqa_triviaqa-validation-4394", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3918", "mrqa_newsqa-validation-867", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3686", "mrqa_newsqa-validation-1531", "mrqa_newsqa-validation-2695", "mrqa_searchqa-validation-8946", "mrqa_searchqa-validation-12686", "mrqa_searchqa-validation-14359", "mrqa_searchqa-validation-13513"], "SR": 0.46875, "CSR": 0.5348360655737705, "retrieved_ids": ["mrqa_squad-train-11090", "mrqa_squad-train-40686", "mrqa_squad-train-71724", "mrqa_squad-train-12598", "mrqa_squad-train-5998", "mrqa_squad-train-57937", "mrqa_squad-train-10411", "mrqa_squad-train-29358", "mrqa_squad-train-20976", "mrqa_squad-train-22872", "mrqa_squad-train-49802", "mrqa_squad-train-64620", "mrqa_squad-train-73305", "mrqa_squad-train-30506", "mrqa_squad-train-46001", "mrqa_squad-train-22176", "mrqa_searchqa-validation-1239", "mrqa_hotpotqa-validation-167", "mrqa_squad-validation-7014", "mrqa_newsqa-validation-1775", "mrqa_squad-validation-3945", "mrqa_naturalquestions-validation-4435", "mrqa_newsqa-validation-2215", "mrqa_hotpotqa-validation-5852", "mrqa_squad-validation-7534", "mrqa_squad-validation-6337", "mrqa_triviaqa-validation-607", "mrqa_searchqa-validation-1780", "mrqa_hotpotqa-validation-5103", "mrqa_naturalquestions-validation-1085", "mrqa_squad-validation-5362", "mrqa_newsqa-validation-2563"], "EFR": 0.9411764705882353, "Overall": 0.6914525072324011}, {"timecode": 61, "before_eval_results": {"predictions": ["StubHub Center", "Haiti", "1038", "3.5 mya", "Arkansas", "on the lateral side", "David Motl", "Andrew Lloyd Webber", "Gene MacLellan", "much of the European industrial infrastructure had been destroyed", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "the fourth C key from left on a standard 88 - key piano keyboard", "everyone on board", "Nicklaus", "Tim Allen", "won", "the song was used as the theme song for the Michael Douglas film, The Jewel of the Nile", "Thomas Jefferson of Virginia, Robert R. Livingston of New York, and Roger Sherman of Connecticut", "the symbol \u00d7", "Bobby Darin", "Buffalo Bill", "Puerto Rico", "late January or early February", "marks locations in Google Maps", "Nalini Negi", "the ruling city of the Northern Kingdom of Israel, Samaria", "Andreas Vesalius", "Shaw", "Notts County ( 1894 )", "Wisconsin", "a central place in Christian eschatology", "3.5 million years old", "Cambodia", "8", "Welcome Stranger", "Black Swan", "brixham", "Alison Moyet", "Seattle", "jaws", "Manchester", "black nationalism", "Texas Tech Red Raiders football team", "January 28, 2016", "Buckingham Palace", "2013", "chicken", "Isabella (Belle) Baumfree", "\"Teen Patti\" (\"Card Game\")", "more than 78,000 parents of children ages 3 to 17.", "Jaime Andrade", "Facebook", "750", "\"local Anjuna boy\"", "they don't feelMisty Cummings has told them everything she knows.", "on China, Taiwan, Hong Kong and Mongolia,", "Doc Holliday", "Ezra", "\"Nashville Star\"", "a cardioverter defibrillator", "the Communist Party", "mathematics", "Chuck Berry", "rubles"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6683566688621836}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, true, false], "QA-F1": [0.3636363636363636, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 0.46153846153846156, 0.23529411764705882, 1.0, 0.0, 1.0, 1.0, 0.5000000000000001, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.7777777777777777, 1.0, 1.0, 1.0, 0.0, 0.7, 0.26666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-7027", "mrqa_naturalquestions-validation-9218", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-7939", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-8558", "mrqa_naturalquestions-validation-833", "mrqa_triviaqa-validation-3351", "mrqa_triviaqa-validation-2462", "mrqa_triviaqa-validation-7755", "mrqa_hotpotqa-validation-5285", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3335", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-2988", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-879", "mrqa_searchqa-validation-15666", "mrqa_searchqa-validation-3147"], "SR": 0.578125, "CSR": 0.5355342741935484, "EFR": 1.0, "Overall": 0.7033568548387097}, {"timecode": 62, "before_eval_results": {"predictions": ["20 miles off the Mexican coast,", "the legitimacy of that race.", "twice", "Majid Movahedi,", "women coping with breast cancer", "Kris Allen,", "a tanker that sailed under a Saudi flag,", "the only goal of the game to ensure Hamburg remain in touch with the top three", "took an anti-doping test after a Serie A game at Roma", "soldiers from the 101st Airborne Division,", "5,600", "prisoners", "Jacob Zuma", "Pop superstar Rihanna", "a dorm parent mistreated students", "Paul Ryan", "be silent.", "are standing by to provide security as needed.", "peanuts.", "early detection and helping other women cope with the disease.", "56,", "English", "suicides", "Israeli forces were responding to militant fire near the Gaza border.", "His lawyer", "four decades", "Buenos Aires.", "Turkey,", "a bank", "a nationwide manhunt and search for the girl", "40", "Sunday", "Massachusetts", "Evan Spiliotopoulos", "is an extension of the Hypertext Transfer Protocol ( HTTP ) for secure communication over a computer network, and is widely used on the Internet", "September 19, 1977", "in pilgrimages to Jerusalem", "Starscream", "Ric Flair", "James Martin Lafferty", "Purple Rain", "a\u00e7a\u00ed", "puffin", "insulin", "Jerusalem Hymn", "Captain Jean-Luc Picard", "king elizabeth", "a karst cave", "Nathan Bedford Forrest", "Mark Neveldine and Brian Taylor", "the 10-metre platform event", "KXII", "\"Bertram, count of Rousillon\" (an Elisabethan English misspelling for Roussillon)", "IT", "National Basketball Development League", "the Boston Red Sox", "the London Bridge", "Passover", "Hormel Foods", "the penny", "a dowry", "1849", "beryl", "Bastille Day"], "metric_results": {"EM": 0.515625, "QA-F1": 0.633339923964924}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.9090909090909091, 1.0, 0.2857142857142857, 0.4444444444444444, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.5, 0.0, 1.0, 1.0, 0.7692307692307693, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.15384615384615383, 0.2857142857142857, 0.0, 1.0, 0.0, 0.5384615384615384, 1.0, 0.46153846153846156, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-3227", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-441", "mrqa_newsqa-validation-2556", "mrqa_newsqa-validation-2141", "mrqa_newsqa-validation-1654", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-3803", "mrqa_newsqa-validation-2758", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-1756", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-3875", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-6522", "mrqa_triviaqa-validation-3637", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-3010", "mrqa_triviaqa-validation-7232", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5503", "mrqa_searchqa-validation-11056"], "SR": 0.515625, "CSR": 0.535218253968254, "EFR": 1.0, "Overall": 0.7032936507936508}, {"timecode": 63, "before_eval_results": {"predictions": ["Mandi Hamlin", "Dan Parris, 25, and Rob Lehr, 26,", "publicly criticized his father's parenting skills.", "in Africa", "martial arts,", "saying Chaudhary's death was warning to management.", "$24.1 million,", "Mokotedi Mpshe,", "peace with Israel", "Daniel Wozniak,", "His replacement, African National Congress Deputy President Kgalema Motlanthe,", "Senate Democrats", "protest child trafficking and shout anti-French slogans", "June 2004", "March 22,", "Amanda Knox's aunt Janet Huff", "The woman", "in Hong Kong's Victoria Harbor", "always hot and humid and it rains almost every day of the year.", "Garth Brooks", "two-state solution", "more than 1.2 million", "Michelle Obama and his wife, Michelle Brown,", "around 3.5 percent of global greenhouse emissions.", "off the coast of Dubai", "death of cardiac arrest", "Olympic medal", "Michoacan Family,", "The forward's lawyer", "New York", "South Carolina Republican Party Chairwoman Karen Floyd", "stand down.", "a bow bridge with 16 arches shielded by ice guards", "Andy Serkis", "Andy Cole and Shearer", "the abdominal anatomy of the rib cage, lungs and heart as well as the inferior thoracic border", "1987", "to examine trends in global carbon monoxide and inhalol pollution", "Admitted to God, to ourselves, and to another human being the exact nature of our wrongs", "Hanna Alstr\u00f6m and Bj\u00f8rn Floberg", "Gary Puckett", "a crow family", "Saturday Night Live", "ladee-Lo", "Mujibur Rahman", "Peru", "dogs", "pomegranate", "The Deep Blue Sea", "Indiana University", "University of Texas Longhorns football team", "Galo (], \"Rooster\"", "Vernier, Switzerland", "Patrick Dempsey and Amanda Peterson", "Indian", "A play-by-post role-playing game (or sim)", "Eleanor Rigby", "an online auction web site", "Giacomo Puccini", "Maude", "Prince Edward Island", "Chief Justice of the Supreme Court", "an apple", "Schubert"], "metric_results": {"EM": 0.421875, "QA-F1": 0.595735493409884}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8750000000000001, 0.0, 0.18181818181818182, 0.0, 1.0, 0.5714285714285715, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.35294117647058826, 1.0, 0.10526315789473685, 0.14285714285714288, 0.5714285714285715, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.5714285714285715, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1670", "mrqa_newsqa-validation-3566", "mrqa_newsqa-validation-1529", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1376", "mrqa_newsqa-validation-1550", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-3974", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-2526", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-6447", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-4382", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-4353", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-1909", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-2390", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-638", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-7088", "mrqa_searchqa-validation-13505", "mrqa_searchqa-validation-11692"], "SR": 0.421875, "CSR": 0.533447265625, "retrieved_ids": ["mrqa_squad-train-32787", "mrqa_squad-train-70207", "mrqa_squad-train-40784", "mrqa_squad-train-21190", "mrqa_squad-train-71776", "mrqa_squad-train-26695", "mrqa_squad-train-50673", "mrqa_squad-train-50741", "mrqa_squad-train-26076", "mrqa_squad-train-30099", "mrqa_squad-train-30170", "mrqa_squad-train-21403", "mrqa_squad-train-48685", "mrqa_squad-train-10497", "mrqa_squad-train-84297", "mrqa_squad-train-18298", "mrqa_hotpotqa-validation-1837", "mrqa_naturalquestions-validation-720", "mrqa_hotpotqa-validation-2949", "mrqa_naturalquestions-validation-9195", "mrqa_naturalquestions-validation-10554", "mrqa_searchqa-validation-15758", "mrqa_newsqa-validation-2070", "mrqa_naturalquestions-validation-1479", "mrqa_squad-validation-7165", "mrqa_hotpotqa-validation-2375", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-3751", "mrqa_hotpotqa-validation-532", "mrqa_newsqa-validation-540", "mrqa_squad-validation-291", "mrqa_hotpotqa-validation-2784"], "EFR": 1.0, "Overall": 0.7029394531250001}, {"timecode": 64, "before_eval_results": {"predictions": ["July 18, 1994,", "had the surgery December 13", "\"E! News\"", "1,073", "an 88-year-old white supremacists who stepped into the museum with a rifle and began firing.", "acid", "the legislation will foster racial profiling,", "have", "Jewish civil rights activists", "July 4.", "different women coping with breast cancer", "\"Whiter Shade of Pale\"", "concentration camps,", "Atlantic Ocean.", "covered the entire body from neck to toe.", "The ship", "Phoenix, Arizona,", "was in the bathroom for about 15 to 20 minutes,", "Jacob", "Raymond Soeoth,", "Republican", "Darrel Mohler", "United States, NATO member states, Russia", "her boyfriend,", "12-hour-plus shifts of backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry", "CEO of an engineering and construction company", "Madonna", "in the head", "Sen. Barack Obama", "Tuesday,", "shared of an impromptu memorial for the late singer at the \"Stone Circle,\"", "Stuntman: Yakima Canutt", "Cadillac", "Speaker of the House of Representatives", "second most populous country after the People's Republic of China", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "in elocution teaching", "Eddie Murphy", "non-ferrous", "The pia mater", "Phil Lynott", "Here Comes the bride", "Neptune", "a jumper", "Department of Justice", "Uganda", "Arabah", "South Dakota", "National Collegiate Athletic Association", "Christopher McCulloch", "Argentino Americanos", "\"Waiting for Guffman\"", "villanelle", "2011", "I Am Furious", "Liam Cunningham", "Van Halen", "Donkey Kong", "oxygen", "The Thirty Years' War", "Massachusetts", "the Veep", "Aramaic", "Uvula"], "metric_results": {"EM": 0.515625, "QA-F1": 0.576877546312047}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true], "QA-F1": [0.0, 0.2, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.11764705882352941, 0.0, 0.25, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5333333333333333, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-67", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-1645", "mrqa_newsqa-validation-3584", "mrqa_newsqa-validation-1309", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-3859", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-3605", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-5386", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-3245", "mrqa_searchqa-validation-8827", "mrqa_searchqa-validation-16606"], "SR": 0.515625, "CSR": 0.5331730769230769, "EFR": 0.9354838709677419, "Overall": 0.6899813895781638}, {"timecode": 65, "before_eval_results": {"predictions": ["three", "the 1962 \"Tete de Cheval\" (\"Horse's Head\") and the 1944 \"Verre et Pichet\" (\"Glass and Pitcher\") by Picasso.", "Spc. Megan Lynn Touma,", "The ruling Justicialist Party, or PJ by its Spanish acronym,", "death squad killings", "The lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "The Tinkler.", "$250,000 for Rivers' charity: God's Love We Deliver.", "Silvan Shalom", "Mexico", "$250,000 for Rivers' charity: God's Love We Deliver.", "pirates on a lifeboat off the coast of Somalia,", "Muslim", "experiencing \"persistent pain.\"", "capital murder and three counts of attempted murder", "800,000", "Two other boys ages 13 and 15", "Nineteen political prisoners", "Paul Schlesselman of West Helena, Arkansas,", "Timothy Masters,", "Cash for Clunkers program", "immediate release", "Deputy Treasury Secretary", "The federal officers' bodies", "the content of the speech, not just the delivery.", "a Muslim with Lebanese heritage, but her family is \"not defined by religion,\"", "Intertropical Convergence Zone,", "Actor and producer Anil Kapoor", "former general secretary of the Communist Party,", "Dr. Albert Reiter,", "an endless war in Afghanistan.", "Robert Barnett,", "near major hotels and in the parking areas of major Chinese supermarkets", "Spencer Treat Clark", "2018", "23 September 1889", "November 17, 1800", "nearby", "March 26, 1973", "around 100,000 writes", "roger twat", "the Zulu War", "adverb", "Something In The Air", "Calvors Brewery", "gold", "Charlie Chan", "The Woodentops", "2005", "British Bristol Olympus turbojet", "Indian club", "Republican", "paracyclist", "pubs, bars and restaurants", "Venice", "Albert Park", "a tort", "Thomas Eakins", "Maine", "Toronto", "President Abraham Lincoln", "&", "Anne", "The Auricle"], "metric_results": {"EM": 0.5, "QA-F1": 0.610947992979243}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.5, 0.5, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.8, 0.5714285714285715, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4032", "mrqa_newsqa-validation-998", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-2424", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-2717", "mrqa_newsqa-validation-734", "mrqa_newsqa-validation-3265", "mrqa_newsqa-validation-55", "mrqa_newsqa-validation-4040", "mrqa_newsqa-validation-1559", "mrqa_newsqa-validation-2330", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-2787", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-2901", "mrqa_newsqa-validation-2414", "mrqa_naturalquestions-validation-6949", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-2146", "mrqa_triviaqa-validation-1284", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-6304", "mrqa_triviaqa-validation-5378", "mrqa_triviaqa-validation-4778", "mrqa_hotpotqa-validation-4977", "mrqa_searchqa-validation-11525", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4774", "mrqa_searchqa-validation-11877", "mrqa_searchqa-validation-3776"], "SR": 0.5, "CSR": 0.5326704545454546, "EFR": 1.0, "Overall": 0.702784090909091}, {"timecode": 66, "before_eval_results": {"predictions": ["William Harold \"Bill\" Ponsford", "British traders", "Costa del Sol", "a large portion of rural Maine, published six days per week in Bangor, Maine.", "TheScanian War", "\u00c6thelwald Moll", "Glam metal", "Mitsubishi Jid\u014dsha K\u014dgy\u014d KK", "\"Lend a hand \u2014 care for the land!\"", "Marc Bolan", "Mineola", "\"lo Stivale\"", "former quarterback and punter", "Jack Chick", "Milk Barn Animation", "Green Chair", "The Apple iPod+HP", "Nero", "1692", "Fulham", "duck", "Jay Chou", "July 14, 2009", "1837", "2009", "October Sky", "Tianhe Stadium", "Louis \"Louie\" Zamperini", "$26 billion", "five", "Alain Robbe-Grillet", "Winter Haven", "2007", "Dan Aykroyd", "a flash music video featuring an animated dancing banana was created", "the churches of Galatia '' ( Galatians 1 : 2 )", "a cell, DNA replication begins at specific locations, or origins of replication, in the genome", "Orwell", "Angola", "Austria - Hungary", "Prince Edward Island", "(Robert) Lewis", "york", "john masefield", "clambroth", "Philippines", "king george vi", "pabbingdon", "1981", "claimed since her October indictment that the child might still be alive, even claiming witnesses spotted Caylee since her disappearance.", "Dolgorsuren Dagvadorj,", "July", "Ronaldinho", "Anne Frank,", "a review of state government practices completed in 100 days.", "the abduction of minors.", "(Jose de) San Martin", "pantaloons", "(Henry) Murger", "Aa Co O o", "Cervantes", "the palace of the Tuileries", "the Baltimore Orioles", "a plus plus fact"], "metric_results": {"EM": 0.5, "QA-F1": 0.6005485527544351}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, false, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.11764705882352941, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5, 0.8, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5555555555555556, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-3087", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-2091", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-4164", "mrqa_hotpotqa-validation-3893", "mrqa_hotpotqa-validation-1605", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-897", "mrqa_naturalquestions-validation-6272", "mrqa_naturalquestions-validation-1925", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-9670", "mrqa_triviaqa-validation-3833", "mrqa_triviaqa-validation-5110", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-7416", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-1941", "mrqa_searchqa-validation-3284", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-13439", "mrqa_searchqa-validation-8137"], "SR": 0.5, "CSR": 0.5321828358208955, "retrieved_ids": ["mrqa_squad-train-37041", "mrqa_squad-train-75260", "mrqa_squad-train-68759", "mrqa_squad-train-65924", "mrqa_squad-train-15898", "mrqa_squad-train-24842", "mrqa_squad-train-20240", "mrqa_squad-train-56781", "mrqa_squad-train-49295", "mrqa_squad-train-63223", "mrqa_squad-train-15170", "mrqa_squad-train-42667", "mrqa_squad-train-31167", "mrqa_squad-train-48097", "mrqa_squad-train-56365", "mrqa_squad-train-37794", "mrqa_hotpotqa-validation-788", "mrqa_newsqa-validation-1657", "mrqa_triviaqa-validation-7392", "mrqa_newsqa-validation-3777", "mrqa_triviaqa-validation-6368", "mrqa_naturalquestions-validation-2588", "mrqa_searchqa-validation-1684", "mrqa_squad-validation-8671", "mrqa_hotpotqa-validation-5101", "mrqa_naturalquestions-validation-564", "mrqa_hotpotqa-validation-5598", "mrqa_hotpotqa-validation-550", "mrqa_newsqa-validation-1603", "mrqa_searchqa-validation-12461", "mrqa_searchqa-validation-10066", "mrqa_naturalquestions-validation-6806"], "EFR": 1.0, "Overall": 0.7026865671641792}, {"timecode": 67, "before_eval_results": {"predictions": ["Sicily", "Diego Maradona", "mariah Carey", "Rear-Admiral of the Navy", "Bangladesh", "new Plymouth", "Union of Post Office Workers", "Minder", "argos", "yare", "Austria", "Taiwan", "a pitcher", "leeds", "Evelyn Glennie", "New Zealand", "carousel", "Algiers", "land between two rivers", "narcolepsy", "Carl Smith", "bing.com", "jump jump", "Cubism", "Joker Wild", "Brian Deane", "Zaire", "Dublin", "Battle of Agincourt", "weir", "Ever Decreasing Circles", "heptathlon", "volcanic activity", "a maritime signal, indicating that the vessel flying it is about to leave", "2003", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Mumbai Rajdhani Express", "Stephen Curry of Davidson", "the bank, rather than the purchaser, is responsible for paying the amount", "Robyn", "Delphine Software International", "Taeko Ikeda", "Prof Media", "University of Nevada, Reno", "2006", "11", "the sarod", "National Hockey League", "Jaime Andrade", "the eradication of the Zetas cartel", "Patrick McGoohan,", "more than 30 Latin American and Caribbean nations", "a Florida girl", "Friday", "images of the small girl being sexually assaulted.", "the Nazi war crimes suspect", "Johns Hopkins", "Birmingham", "Norah Jones", "The Haflinger", "the royal castle at Elsinore", "the X-Files", "Rita Mae Brown", "the Civil War"], "metric_results": {"EM": 0.5625, "QA-F1": 0.617125496031746}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 0.19999999999999998, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4641", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-3323", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-7152", "mrqa_triviaqa-validation-4029", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-4119", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3421", "mrqa_triviaqa-validation-3780", "mrqa_triviaqa-validation-34", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-237", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-1930", "mrqa_naturalquestions-validation-3303", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-5587", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-2221", "mrqa_newsqa-validation-829", "mrqa_searchqa-validation-16186", "mrqa_searchqa-validation-1152", "mrqa_searchqa-validation-9901"], "SR": 0.5625, "CSR": 0.5326286764705883, "EFR": 1.0, "Overall": 0.7027757352941177}, {"timecode": 68, "before_eval_results": {"predictions": ["Martin Aloysius Culhane", "sincerity", "Indonesian", "mammoths", "the family, which remains united and strong despite the \"tremendous hardship,\"", "Muslim country and NATO military ally", "hacker group LulzSec.", "Michael Schumacher", "Matthew Fisher", "Dog patch Labs", "San Diego,", "Technological Institute of Higher Learning of Monterrey,", "10", "dancing with the Stars", "one-shot victory in the Bob Hope Classic", "3-2", "78,000 parents of children ages 3 to 17.iReport.com:", "machine guns and two silencers", "\"The Screening Room\"", "the death of a pregnant soldier", "16", "Anil Kapoor.", "2-1", "CNN", "Joan Rivers", "on the bench", "actress", "one", "college campus.", "Kerstin", "1979", "Phillip A. Myers.", "pulmonary heart disease ( cor pulmonale ), which is usually caused by difficulties of the pulmonary circulation, such as pulmonary hypertension or pulmonic stenosis", "because of the way they used `` rule '' and `` method '' to go about their religious affairs", "205 lb ( 93 kg )", "Natya Shastra", "American Indian allies", "Bulgaria", "Dante Pastula", "Seton Hall University", "l Lancashire", "south london", "Utah", "Cheerios", "southampton", "vote", "salmon", "Hungarian Horntail", "StubHub Center", "Jaguar Land Rover Limited", "Sutton Hoo helmet", "Luis Edgardo Resto", "Confessions on a Dance Floor", "Buffalo", "FAI Junior Cup", "2013", "walrus", "Inamona", "rapier", "Newton", "salaam", "the hip", "Japan", "Alive"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6931361607142856}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true], "QA-F1": [0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.125, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1511", "mrqa_newsqa-validation-4183", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-2513", "mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-1580", "mrqa_newsqa-validation-2168", "mrqa_newsqa-validation-2906", "mrqa_naturalquestions-validation-1680", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-3492", "mrqa_naturalquestions-validation-805", "mrqa_triviaqa-validation-4973", "mrqa_triviaqa-validation-4124", "mrqa_triviaqa-validation-1628", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-4036", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-2043", "mrqa_searchqa-validation-16356", "mrqa_searchqa-validation-12454"], "SR": 0.609375, "CSR": 0.5337409420289856, "EFR": 0.96, "Overall": 0.6949981884057971}, {"timecode": 69, "before_eval_results": {"predictions": ["10 Afghan police officers in Kandahar province,", "Molotov cocktails, rocks and glass.", "Rwanda", "more than 100", "Bailey, Colorado,", "Sri Lanka", "Hanin Zoabi,", "Six", "Eric Besson", "deciding the duties of the new prime minister has been a sticking point in the negotiations.", "Iran", "education", "He was told to bend over and was beaten when he refused.", "review their emergency plans", "Holley Wimunc.", "Burhanuddin Rabbani,", "African-Americans", "a place for another non-European Union player in Frank Rijkaard's squad.", "1979", "Congress", "Venus Williams", "four months ago,", "Bowe Bergdahl", "5-0,", "hundreds of people joined a campus rally to oppose racial intolerance.", "they are angry and scared,", "a Royal Air Force helicopter", "$1.5", "Washington State's decommissioned Hanford nuclear site,", "Brian David Mitchell,", "Bill Murray, Chevy Chase", "allegedly faking a doctor's note", "November 2, 2016, in Los Angeles, California", "Nepal", "mind your manners", "Amerigo Vespucci", "the fourth quarter of the preceding year", "a substance that fully activates the receptor that it binds to ) while under other conditions, behaves as an antagonist", "for the 1994 season", "Tbilisi, Georgia", "a leaf", "New Zealand", "john seddon", "the fallopian tube", "Grayson Perry", "france", "iday Inn", "dirk Bikembergs", "University of Texas at Austin", "Robert Paul \"Robbie\" Gould III", "Patrick Swayze", "Roy Spencer", "Jumh\u016briyyat", "Indian classical", "32", "Jocelyn Moorhouse", "Hawaii", "beethoven", "aardwolf", "ceviche", "Peter Bogdanovich", "Machiavelli", "Aleksandr Vladimirovich Popov", "Bob Dylan"], "metric_results": {"EM": 0.5, "QA-F1": 0.6331456824425574}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true, true, false, false, false], "QA-F1": [0.25, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6153846153846153, 0.8, 1.0, 1.0, 0.15384615384615383, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.18181818181818182, 0.15384615384615385, 1.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 0.0, 0.6, 1.0, 0.375, 1.0, 1.0, 0.72, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3270", "mrqa_newsqa-validation-1997", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-985", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-3187", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-1300", "mrqa_newsqa-validation-2540", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-4989", "mrqa_naturalquestions-validation-4109", "mrqa_naturalquestions-validation-9749", "mrqa_triviaqa-validation-6338", "mrqa_triviaqa-validation-245", "mrqa_triviaqa-validation-4913", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-578", "mrqa_hotpotqa-validation-3761", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-2060", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-4461"], "SR": 0.5, "CSR": 0.5332589285714286, "retrieved_ids": ["mrqa_squad-train-64500", "mrqa_squad-train-59883", "mrqa_squad-train-59533", "mrqa_squad-train-35665", "mrqa_squad-train-72120", "mrqa_squad-train-21065", "mrqa_squad-train-9861", "mrqa_squad-train-55486", "mrqa_squad-train-33179", "mrqa_squad-train-8034", "mrqa_squad-train-77778", "mrqa_squad-train-29411", "mrqa_squad-train-67029", "mrqa_squad-train-36100", "mrqa_squad-train-59812", "mrqa_squad-train-84113", "mrqa_searchqa-validation-15959", "mrqa_hotpotqa-validation-2720", "mrqa_searchqa-validation-5564", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-1550", "mrqa_hotpotqa-validation-510", "mrqa_hotpotqa-validation-2341", "mrqa_newsqa-validation-3060", "mrqa_searchqa-validation-9112", "mrqa_hotpotqa-validation-232", "mrqa_naturalquestions-validation-6232", "mrqa_newsqa-validation-2416", "mrqa_triviaqa-validation-1709", "mrqa_hotpotqa-validation-861", "mrqa_searchqa-validation-8957", "mrqa_hotpotqa-validation-876"], "EFR": 1.0, "Overall": 0.7029017857142857}, {"timecode": 70, "UKR": 0.7890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-1685", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2215", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3456", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-487", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4902", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5095", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-816", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1199", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2205", "mrqa_naturalquestions-validation-2220", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3028", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4033", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-4330", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4644", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5607", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6480", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-744", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7855", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8417", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9827", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1032", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1119", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1842", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3011", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-670", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-12126", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13694", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15769", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-166", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-16759", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2701", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3413", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5692", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9742", "mrqa_searchqa-validation-9812", "mrqa_squad-validation-10024", "mrqa_squad-validation-10068", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-2591", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3372", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-3723", "mrqa_squad-validation-3745", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-486", "mrqa_squad-validation-512", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5348", "mrqa_squad-validation-5362", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6595", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-689", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7137", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-7956", "mrqa_squad-validation-816", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-9408", "mrqa_squad-validation-96", "mrqa_squad-validation-9845", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1293", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2870", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3089", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-446", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5579", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-602", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7217", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.837890625, "KG": 0.4984375, "before_eval_results": {"predictions": ["April 9, 2012", "Hank Williams", "September 2, 1945", "Lord Banquo", "King Louie", "Waylon Jennings", "Robert Irsay", "18", "Katherine Kiernan Maria `` Kate '' Mulgrew", "Castleford", "October 2012", "Sammi Smith", "2018", "Lake Powell", "on the North Shore, at locations in Beverly, Essex, Gloucester, Swampscott, Lynn, Middleton, Tewksbury, and Salem", "South Dakota ( 30.3 % )", "five", "Nickelback", "William Shakespeare's As You Like It, spoken by the melancholy Jaques in Act II Scene VII", "2", "the southeastern United States", "sperm and ova", "committed suicide", "2002", "Identification of alternative plans / policies", "The Canterbury Tales", "1956", "A. planci", "either in front or on top of the brainstem", "semi-automatic, but not fully automatic", "V\u1e5bksayurveda", "1990", "john Hurt", "Cyprus", "George W. Bush", "Durham", "snakes", "weetabix", "Hague", "Nigeria", "Harry F. Sinclair", "Russian", "1903", "motor vehicles", "2018", "pastels", "Saint Petersburg Conservatory", "middleweight", "\"don't treat the girls the way she likes,\"", "CNN affiliate WFTV.", "FBI's Baltimore field office", "ordered the release of the four men", "Apple", "228", "16-0", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "Peter Paul Rubens", "Neptune", "( Bugsy) Siegel", "(Albert) Michelson", "Wang Chung", "(Rodin) Calder", "the Normandy region", "the note"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6670099431818182}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, false, true, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.0, 1.0, 0.4, 0.4545454545454545, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.16666666666666666, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5176", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-142", "mrqa_naturalquestions-validation-7651", "mrqa_naturalquestions-validation-9703", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-2987", "mrqa_naturalquestions-validation-6050", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-2179", "mrqa_hotpotqa-validation-5809", "mrqa_hotpotqa-validation-2325", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-680", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-469", "mrqa_newsqa-validation-3805", "mrqa_newsqa-validation-2700", "mrqa_newsqa-validation-1228", "mrqa_searchqa-validation-9619", "mrqa_searchqa-validation-10389", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12794"], "SR": 0.5625, "CSR": 0.5336707746478873, "EFR": 0.9285714285714286, "Overall": 0.7175265656438632}, {"timecode": 71, "before_eval_results": {"predictions": ["Norway", "Morocco", "Casablanca", "Geneva Convention", "Classics", "New Jersey", "Baikal", "rock salt", "badger", "Kuiper Belt", "a cat", "Mimi Bobeck", "winter", "nag", "Ned", "Boston Red Sox", "New Zealand", "Ohio", "pantomime", "halfpipe", "Philistines", "Apple", "Moscow", "a bus", "shiatsu", "Lt. Columbo", "William McKinley", "Hannibal", "Ankara", "The Deep", "siamang", "\"Death, be not\"", "Anatomy", "16", "8 bytes", "China, which imported 70 percent of 2010 production, followed by Japan with 19 % and South Korea with 10 %", "more than 1,000", "Lee Mack", "October 29 - 30, 2012", "Andreas Vesalius", "Honolulu", "bird", "neurons", "working", "minder", "potatoes", "shellbark", "antelope", "281", "44", "Rockbridge County", "Freeform", "1965", "200,167", "Harvard", "Martin Ingerman", "a body", "Sonia Sotomayor", "\"I think protecting your family and giving to them is the most important achievement.\"", "A staff sergeant in the U.S. Air Force", "1913.", "Kim Jong Il", "Ricardo Valles de la Rosa,", "Tuesday"], "metric_results": {"EM": 0.625, "QA-F1": 0.7213541666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4164", "mrqa_searchqa-validation-6061", "mrqa_searchqa-validation-14509", "mrqa_searchqa-validation-7372", "mrqa_searchqa-validation-8092", "mrqa_searchqa-validation-3302", "mrqa_searchqa-validation-9576", "mrqa_searchqa-validation-14741", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-4240", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-6671", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-2132", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-7120", "mrqa_hotpotqa-validation-4321", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-1390", "mrqa_newsqa-validation-2678", "mrqa_newsqa-validation-3848", "mrqa_newsqa-validation-79"], "SR": 0.625, "CSR": 0.5349392361111112, "EFR": 0.9583333333333334, "Overall": 0.723732638888889}, {"timecode": 72, "before_eval_results": {"predictions": ["alloys", "Defending Your Life", "(Fahrid) Abraham", "(J. Q. Adams)", "Charley", "Microsoft", "Beethoven", "Latin", "high jump", "Nicole Kidman", "a grape", "Fort Sumter", "Bucharest", "the Namib Desert", "Happy Days", "a Mentor", "Vanna White", "Morris West", "trod", "Francis Albert", "Green Lantern", "infrared", "a barbecue", "the Nome", "JUNGLE JIM", "the Old North Church", "Jerry Reed", "Sartre", "Michelle Kwan", "a flautas", "Baal", "Westinghouse Electric Company", "a very powerful superficial bipennate muscle that is in the back part of the lower leg", "prophets", "Blue laws", "a state or other organizational body that controls the factors of production", "Allison Janney", "charbagh", "Shirley Mae Jones", "Barry Bonds", "Colonel Tom Parker", "george vi", "three", "Alice", "barbarella", "pentecost", "Nissan", "south wales", "nuclear weapons", "Atlas ICBM", "William Bradford", "Tim Allen", "Rochdale, North West England", "postmodern schools", "Netherlands", "Romeo Montague", "Fullerton, California,", "misdemeanor assault charges", "education", "12-1", "fill a million sandbags", "Kris Allen,", "north-south highway", "a one-shot victory"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6634943181818183}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.13333333333333336, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.3636363636363636]}}, "before_error_ids": ["mrqa_searchqa-validation-5042", "mrqa_searchqa-validation-7720", "mrqa_searchqa-validation-2904", "mrqa_searchqa-validation-9180", "mrqa_searchqa-validation-12784", "mrqa_searchqa-validation-3016", "mrqa_searchqa-validation-14727", "mrqa_searchqa-validation-16954", "mrqa_searchqa-validation-5649", "mrqa_searchqa-validation-15982", "mrqa_searchqa-validation-613", "mrqa_searchqa-validation-13204", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-25", "mrqa_triviaqa-validation-5424", "mrqa_triviaqa-validation-2478", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-3640", "mrqa_hotpotqa-validation-1350", "mrqa_hotpotqa-validation-2455", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-212", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-3031", "mrqa_newsqa-validation-2858"], "SR": 0.609375, "CSR": 0.535958904109589, "retrieved_ids": ["mrqa_squad-train-19874", "mrqa_squad-train-30113", "mrqa_squad-train-28545", "mrqa_squad-train-44882", "mrqa_squad-train-72915", "mrqa_squad-train-73880", "mrqa_squad-train-13841", "mrqa_squad-train-55861", "mrqa_squad-train-61481", "mrqa_squad-train-36619", "mrqa_squad-train-52004", "mrqa_squad-train-38470", "mrqa_squad-train-16015", "mrqa_squad-train-38716", "mrqa_squad-train-86156", "mrqa_squad-train-27289", "mrqa_newsqa-validation-1837", "mrqa_naturalquestions-validation-5396", "mrqa_newsqa-validation-2988", "mrqa_triviaqa-validation-5605", "mrqa_squad-validation-2612", "mrqa_naturalquestions-validation-7458", "mrqa_naturalquestions-validation-6650", "mrqa_hotpotqa-validation-4545", "mrqa_searchqa-validation-4907", "mrqa_hotpotqa-validation-3870", "mrqa_triviaqa-validation-2310", "mrqa_triviaqa-validation-66", "mrqa_newsqa-validation-3375", "mrqa_searchqa-validation-11604", "mrqa_newsqa-validation-560", "mrqa_triviaqa-validation-3896"], "EFR": 1.0, "Overall": 0.7322699058219178}, {"timecode": 73, "before_eval_results": {"predictions": ["Debbie Wassermanultz", "an armadillo", "Daniel Boone", "Bigfoot", "the Rupee", "Cold Mountain", "Frisbee", "nosy", "the pommel horse", "Queen Victoria", "a basement", "Einstein", "Ramayana", "Cogsworth", "Marshmallows", "The Laughing Cavalier", "a Whiskey Cocktail", "Blondes", "C-E", "an Old Manse", "the Caribbean", "a Tadpoles", "Fermium", "an accomplice", "an aerie", "Woody Guthrie", "The Netherland", "Clydesdales", "fontanels", "Winston Churchill", "defense", "Herman Melville", "upon a military service member's retirement, separation, or discharge from active duty in the Armed Forces of the United States", "1924", "1926", "Baseball Writers'Association of America", "Dumont d'Urville Station", "Gwendoline Christie", "Cambridge May Ball scene, set in 1963", "British", "China", "head of state and spiritual leader of the Tibetan people", "east coast", "Felix", "tennia mccartney", "Bodhidharma", "lincolnshire", "Marion Cotillard", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "English", "the villanelle poetic form", "late 12th Century", "1966", "Paper", "New York Shakespeare Festival", "Ribhu Dasgupta", "12 hours", "Nazi Germany", "potential revenues from oil and gas", "Stephen Tyrone Johns", "around 3.5 percent of global greenhouse emissions.", "Elin Nordegren,", "Tuesday", "Ben Roethlisberger"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6213292464114832}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9142857142857143, 1.0, 1.0, 0.39999999999999997, 1.0, 1.0, 0.2105263157894737, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.45454545454545453, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-11850", "mrqa_searchqa-validation-10017", "mrqa_searchqa-validation-9349", "mrqa_searchqa-validation-16348", "mrqa_searchqa-validation-15440", "mrqa_searchqa-validation-14992", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-136", "mrqa_searchqa-validation-3251", "mrqa_searchqa-validation-3770", "mrqa_searchqa-validation-12722", "mrqa_searchqa-validation-1486", "mrqa_naturalquestions-validation-7605", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-3524", "mrqa_naturalquestions-validation-923", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-3438", "mrqa_triviaqa-validation-3399", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2137", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3887"], "SR": 0.578125, "CSR": 0.5365287162162162, "EFR": 0.8888888888888888, "Overall": 0.710161646021021}, {"timecode": 74, "before_eval_results": {"predictions": ["at the intersection of Del Monte Blvd and Esplanade Street", "Phillip Paley", "`` Entropy ''", "architecture", "Darlene Cates", "James Long", "an active supporter of the League of Nations", "16 November 2001", "September 19 - 22, 2017", "November 1999", "on the inner wall of the pedestal of the Statue of Liberty", "mining", "April 1, 2016", "after World War II", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Spanish missionaries, ranchers and troops", "St Pancras International", "John Travolta", "1980", "Covington, Georgia", "Butter Island off North Haven, Maine in the Penobscot Bay", "December 1886", "8 January 1999", "a end - user or audience", "Ishaani Ishaan Sinha", "for operations, personnel, equipment, and activities", "Lesley Gore", "the planned Nazi pre-emptive nuclear strike on Japan, `` Operation Dandelion, '' is apparently being prevented only by Hitler's personal refusal to authorise it,", "September 8, 2017", "The Intolerable Acts", "American production duo The Chainsmokers", "British rock band Procol Harum", "tahrir Square", "Heather Stanning and Helen Glover", "fred", "Slim Whitman", "ely", "Castor", "fenus Castor", "for lighting", "Hanford Site", "Eddie Collins", "over 1.6 million", "Paul W. S. Anderson", "Roscoe Lee Browne", "dice", "Clara Petacci", "Charice", "lower house of parliament,", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear bomb.", "Somali's coast.", "Daniel Radcliffe", "Pakistan", "in a motel,", "Joe Jackson", "intention to set up headquarters in Dublin.", "Winston Churchill", "Ricky Martin", "Mark Twain", "San Salvador", "Amelia Earhart", "ethanol", "California", "the pituitary gland"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7275656114718615}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.08, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-138", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-7356", "mrqa_naturalquestions-validation-4637", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-897", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-2653", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-3002", "mrqa_triviaqa-validation-283", "mrqa_triviaqa-validation-3294", "mrqa_hotpotqa-validation-1181", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-5521", "mrqa_newsqa-validation-2066", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-192", "mrqa_searchqa-validation-14807"], "SR": 0.640625, "CSR": 0.5379166666666666, "EFR": 1.0, "Overall": 0.7326614583333334}, {"timecode": 75, "before_eval_results": {"predictions": ["North Atlantic Ocean", "Dmitri Mendeleev", "Tagore", "charbagh", "Amerigo Vespucci", "2018", "the head of Lituya Bay in Alaska", "0.30 in ( 7.6 mm )", "the thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S.", "the breast or lower chest of beef or veal", "May 2017", "Brian Steele", "2011", "19 June 2018", "Ancient Greek terms \u03c6\u03af\u03bb\u03bf\u03c2 ph\u00edlos ( beloved, dear )", "Harishchandra", "sport utility vehicles", "if a vehicle towing a trailer skids", "Cheap trick", "Julie Adams", "FIGG Bridge Engineers,", "a deboned chicken", "Incudomalleolar joint", "West Canyon County, Idaho", "Billy Idol", "2017", "2020", "August Darnell", "International System of Units", "Rufus and Chaka Khan", "a political ideology", "New York City", "sedgefield", "zelle", "green", "49", "edna Ferber", "eros", "Robinson", "argentina", "1825", "Brendan O'Brien", "33 of the 100 seats", "Manchester", "Mitsubishi", "John McClane", "Adelaide", "Exit 82", "some U.S. senators who couldn't resist taking the vehicles for a spin.", "41,280 pounds", "Spc. Megan Lynn Touma,", "operates 52 nuclear, hydroelectric and fossil-fuel facilities in the southeastern United States.", "Lindsey Vonn", "more active role in countering the spread of the drug trade", "Bill", "comments I made last night at the Annual Caddy Awards dinner in Shanghai,\"", "Byron", "a snake", "Odin", "Abstract Expressionism", "Greed", "a blubber", "Con Dolan", "Romanov"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6816659035409036}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.3333333333333333, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.14285714285714285, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.4, 1.0, 0.5, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.6666666666666666, 1.0, 0.0, 1.0, 0.7000000000000001, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-1731", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-5820", "mrqa_triviaqa-validation-1497", "mrqa_triviaqa-validation-6857", "mrqa_triviaqa-validation-6288", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2197", "mrqa_hotpotqa-validation-404", "mrqa_newsqa-validation-2927", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-2813", "mrqa_searchqa-validation-4241", "mrqa_searchqa-validation-4802", "mrqa_searchqa-validation-9741", "mrqa_searchqa-validation-11887", "mrqa_searchqa-validation-15046"], "SR": 0.5625, "CSR": 0.5382401315789473, "retrieved_ids": ["mrqa_squad-train-5130", "mrqa_squad-train-26530", "mrqa_squad-train-67976", "mrqa_squad-train-67784", "mrqa_squad-train-38349", "mrqa_squad-train-65026", "mrqa_squad-train-18059", "mrqa_squad-train-2667", "mrqa_squad-train-39793", "mrqa_squad-train-84449", "mrqa_squad-train-59516", "mrqa_squad-train-74009", "mrqa_squad-train-35295", "mrqa_squad-train-45228", "mrqa_squad-train-55517", "mrqa_squad-train-36461", "mrqa_triviaqa-validation-5958", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-4100", "mrqa_triviaqa-validation-5110", "mrqa_squad-validation-6776", "mrqa_hotpotqa-validation-4180", "mrqa_hotpotqa-validation-1870", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-12798", "mrqa_searchqa-validation-9901", "mrqa_squad-validation-9846", "mrqa_newsqa-validation-491", "mrqa_searchqa-validation-211", "mrqa_newsqa-validation-2781", "mrqa_squad-validation-6670", "mrqa_triviaqa-validation-4135"], "EFR": 0.9642857142857143, "Overall": 0.7255832941729323}, {"timecode": 76, "before_eval_results": {"predictions": ["several weeks,", "involvement during World War II in killings at a Nazi German death camp in Poland.", "the Ku Klux Klan", "Filippo Inzaghi", "The National Infrastructure Program,", "Silicon Valley.", "Booches Billiard Hall,", "Liverpool,", "Expedia.", "the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "due to a shortage of landing fields available for practice,", "Afghan National Security Forces", "Nechirvan Barzani,", "planned attacks", "$273 million", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "she wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "actress", "Brian Smith,", "Brazil", "she supports the two-state solution to the Mideast conflict,", "$3 billion,", "\"Three Little Beers,\" to the Ben Hogan biopic \"Follow the Sun,\"", "he has seen is \"terrifying.\"", "Dan Brown", "because the Indians were gathering information about the rebels to give to the Colombian military.", "mental health", "Annie Duke", "570 billion pesos ($42 billion)", "India", "response to a civil disturbance call,", "November 17, 1800", "legislation", "concerned with all legal affairs", "Giancarlo Stanton", "A status line", "David Ben - Gurion", "18", "TC", "antelopes", "Mikhail S. Gorbachev", "14", "ravi Shankar", "Baton Rouge", "Abe Reles", "Floor Exercise, Pommel Horse, Rings, Vault, Parallel Bars, and High Bar", "Mussolini", "the Hebrides", "August 21, 1995", "1966", "Algernod Lanier Washington", "Highwayman", "Michael Rispoli", "Middlesbrough", "Stu Henderson", "adhesion", "a crumpet", "the Potomac River", "dinoflagellates", "Double Indemnity", "Glengarry Glenross", "The ____ Family", "a penny"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6495416730940629}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, false, true, false, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.23529411764705882, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.23076923076923078, 0.15384615384615383, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.5, 0.9090909090909091, 0.5, 0.125, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2114", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-1759", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-3006", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-139", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-4112", "mrqa_newsqa-validation-1358", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1711", "mrqa_naturalquestions-validation-2618", "mrqa_naturalquestions-validation-5903", "mrqa_naturalquestions-validation-1452", "mrqa_triviaqa-validation-3804", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-4623", "mrqa_hotpotqa-validation-3491", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-2003", "mrqa_hotpotqa-validation-2306", "mrqa_searchqa-validation-3417", "mrqa_searchqa-validation-13091", "mrqa_searchqa-validation-12186", "mrqa_searchqa-validation-711"], "SR": 0.546875, "CSR": 0.5383522727272727, "EFR": 0.9310344827586207, "Overall": 0.7189554760971787}, {"timecode": 77, "before_eval_results": {"predictions": ["skeletal dysplasia,", "David Beckham", "Alexandre Caizergues,", "jobs", "Saturn", "Adam Yahiye Gadahn,", "debris", "1959,", "lower house of parliament,", "Six members", "East Java", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "psychotropic drugs", "3 to 17", "NASCAR.", "executive director of the Americas Division of Human Rights Watch,", "jazz", "Mark Fields", "Jeanne Tripplehorn's", "FBI Special Agent Daniel Cain,", "military trials for some Guant Bay detainees.", "home in Peshawar", "Daytime Emmy Lifetime Achievement Award.", "Jewish", "President Obama's", "Woosuk Ken Choi,", "forcibly injecting them with psychotropic drugs", "Almost all British troops", "Filippo Inzaghi", "Greeley, Colorado,", "building bombs,", "\"E! News\"", "Sophia Akuffo", "North Carolina", "Los Lonely Boys", "LED illuminated display", "ancient Rome", "1966", "1985", "sedimentary", "chiltern", "chiropractic", "Polish", "To Kill a Mockingbird", "the sacrament of Holy Communion", "samuel johnson", "makes", "just two years", "the Knight Company", "Ferdinand Magellan", "2002", "34th", "154 days", "seven members", "Snowball II", "Scottish", "Ali", "Captain Nemo", "Man Ray", "\"Saturday Night Live\"", "James Watt", "Mercury and Venus", "snakes", "Scripps National Spelling Bee"], "metric_results": {"EM": 0.59375, "QA-F1": 0.719530404491342}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, false, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6875000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.2857142857142857, 0.3636363636363636, 0.5, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-4042", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-3749", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-7203", "mrqa_naturalquestions-validation-6242", "mrqa_naturalquestions-validation-8792", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-2281", "mrqa_triviaqa-validation-1043", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-826", "mrqa_hotpotqa-validation-3504", "mrqa_searchqa-validation-10346", "mrqa_searchqa-validation-7834", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-12187"], "SR": 0.59375, "CSR": 0.5390625, "EFR": 0.8461538461538461, "Overall": 0.7021213942307692}, {"timecode": 78, "before_eval_results": {"predictions": ["a micronutrient-rich diet", "Jamie Fraser", "August 14, 1848", "American Horror Story", "Daniel Wroughton Craig", "The Zebras", "c. c. Beck and writer Paul Dini", "\"Nebo Zovyot\"", "Gainsborough Trinity", "Carlos Santana", "DreamWorks Animation", "40 Acres and a Mule Filmworks", "Fort Bragg, North Carolina", "NCAA Division I Football Bowl Subdivision", "Robert A. Iger", "The Gang", "play-by-post role-playing game", "Newcastle upon Tyne, England", "Stephen Ireland", "Rooster", "27 January 1974", "second cousin once removed", "James Dean", "Jennifer Taylor", "a Christian evangelist", "Illinois", "Twitch Interactive, a subsidiary of Amazon.com", "1853", "Coalwood", "Sarajevo", "Gareth Barry", "The Ryukyuan people", "in the west by the east coast of Queensland, thereby including the Great Barrier Reef, in the east by Vanuatu ( formerly the New Hebrides ) and by New Caledonia", "Samantha Jo `` Mandy '' Moore", "in England and Wales", "Duisburg", "the middle of the 15th century, in Yemen's Sufi monasteries", "the first two centuries", "between 1765 and 1783", "President", "Another Day in Paradise", "Ronnie Carroll", "North by Northwest", "an umbrella", "maya Sequana \"Seine goddess\" and other ex voti", "stomach", "lurence mayer", "arcelor mittal Orbit", "Afghanistan", "90", "appealed against the punishment for the player", "South Africa", "then-Sen. Obama", "The remains of Cologne's archive building", "in the Bronx and grew up in a public housing project, not too far from the stadium of her favorite team -- the New York Yankees.", "three different videos that we like and want to know which ones you think are the best.", "Cosmetology", "the Knickerbacker", "Tim Russert", "Kentucky", "Tigger", "Giovanni Bertati", "Ariel Sharon", "Iolanthe"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6067217285638339}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false], "QA-F1": [0.6666666666666666, 0.0, 0.3333333333333333, 1.0, 0.8, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 0.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16, 1.0, 0.8, 1.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.4444444444444445, 0.0909090909090909, 0.3157894736842105, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5873", "mrqa_hotpotqa-validation-2454", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-2135", "mrqa_hotpotqa-validation-4909", "mrqa_hotpotqa-validation-4476", "mrqa_hotpotqa-validation-4842", "mrqa_hotpotqa-validation-2564", "mrqa_hotpotqa-validation-3425", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-1435", "mrqa_hotpotqa-validation-5477", "mrqa_hotpotqa-validation-3914", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4791", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4711", "mrqa_naturalquestions-validation-4134", "mrqa_naturalquestions-validation-2138", "mrqa_naturalquestions-validation-1965", "mrqa_triviaqa-validation-106", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-2138", "mrqa_triviaqa-validation-5468", "mrqa_newsqa-validation-794", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-3242", "mrqa_newsqa-validation-2569", "mrqa_newsqa-validation-6", "mrqa_searchqa-validation-10111", "mrqa_searchqa-validation-9708", "mrqa_searchqa-validation-8670", "mrqa_searchqa-validation-13464"], "SR": 0.484375, "CSR": 0.5383702531645569, "retrieved_ids": ["mrqa_squad-train-72698", "mrqa_squad-train-83791", "mrqa_squad-train-12035", "mrqa_squad-train-15720", "mrqa_squad-train-35504", "mrqa_squad-train-21587", "mrqa_squad-train-41621", "mrqa_squad-train-85675", "mrqa_squad-train-32748", "mrqa_squad-train-34986", "mrqa_squad-train-4480", "mrqa_squad-train-20642", "mrqa_squad-train-70701", "mrqa_squad-train-67161", "mrqa_squad-train-83758", "mrqa_squad-train-74167", "mrqa_naturalquestions-validation-2326", "mrqa_naturalquestions-validation-2143", "mrqa_hotpotqa-validation-3870", "mrqa_naturalquestions-validation-5001", "mrqa_hotpotqa-validation-1586", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3097", "mrqa_triviaqa-validation-7584", "mrqa_hotpotqa-validation-4977", "mrqa_squad-validation-3540", "mrqa_hotpotqa-validation-3608", "mrqa_hotpotqa-validation-4362", "mrqa_searchqa-validation-12130", "mrqa_naturalquestions-validation-9726", "mrqa_searchqa-validation-10066", "mrqa_searchqa-validation-4657"], "EFR": 1.0, "Overall": 0.7327521756329114}, {"timecode": 79, "before_eval_results": {"predictions": ["St. Louis", "Warrington, Florida", "Pope John X", "Apalachee", "The Emperor of Japan", "English", "Carlos Santana", "Dana Fox", "7 January 1936", "The Onion", "Portsea", "Lapland", "1698", "Cuban descent", "2010", "California", "the first flume ride in Ireland", "Ouse and Foss", "Thrushcross Grange", "Ten Walls", "My Boss, My Teacher", "York County, Maine", "Grave Digger", "a mermaid", "Europop", "Humberside", "rotterdam Square", "Yasir Hussain", "566", "Cookstown", "Yidisher Visnshaftlekher Institut", "Kairi", "Database - Protocol driver ( Pure Java driver )", "quarterback", "Jacqueline MacInnes Wood", "Malayalam", "October 19, 1961 -- January 19, 1963", "Lyndon B. Johnson", "a maritime signal, indicating that the vessel flying it is about to leave", "18 September to 31 October", "alligators", "frottage", "polo", "Mrs Merton", "green", "12", "tower of london", "Octavian", "38", "hanged in 1979 for the murder of a political opponent", "South Carolina", "Canada.", "Michael Arrington,", "South Carolina Republican Party Chairwoman Karen Floyd", "usion teams", "The remaining 240 patients will be taken to hospitals in other provinces", "out of body", "air dominance", "Wyoming", "is not a boy to take jiart", "the Hudson River", "Peter the Great", "the Taliban", "James Naismith"], "metric_results": {"EM": 0.609375, "QA-F1": 0.715206755050505}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.8, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.25, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.0, 1.0, 0.0, 0.4444444444444445, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-714", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-5700", "mrqa_hotpotqa-validation-2922", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-1647", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-337", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-983", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-5825", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8603", "mrqa_triviaqa-validation-3825", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-4053", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-1210", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-1570"], "SR": 0.609375, "CSR": 0.5392578125, "EFR": 0.96, "Overall": 0.7249296875}, {"timecode": 80, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1392", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1533", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2757", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3046", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3439", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4873", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5077", "mrqa_hotpotqa-validation-5097", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-834", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10279", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1890", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2080", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-3978", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5118", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5298", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6416", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9842", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1022", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1434", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2766", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3082", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-794", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10087", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10780", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13829", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16416", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2173", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4167", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5440", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7060", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9073", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-332", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3577", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-3954", "mrqa_squad-validation-4127", "mrqa_squad-validation-4186", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7047", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7394", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7653", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9245", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2861", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3053", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3317", "mrqa_triviaqa-validation-3367", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4803", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5075", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5448", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5584", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-7782", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936"], "OKR": 0.818359375, "KG": 0.4578125, "before_eval_results": {"predictions": ["Parthenon", "Dennis Potter", "6", "Botticelli", "avocadoe", "Venice", "architect", "bobby valli", "Chief Inspector with HM Inspector of Prisons", "Some Like It Hot", "havre", "the Gunpowder Plot of 1605", "Prussia", "soviet arbus", "Wee Jimmy Krankie and his father", "Norway", "o'Galop", "1890", "William Shatner", "phil mcckelson", "havre", "lenny havre", "jack prism", "Camellia", "scythe", "Geochronology", "Bolivia", "gold", "polyhedron", "Denver", "warm-hearted hospitality", "bickering", "Frank Langella", "Mike Nesmith", "Gloria ( Lisa Stelly )", "a client that was first developed and popularized by the Israeli company Mirabilis in 1996", "two", "pigs", "Janie Crawford, an African - American woman in her early forties", "Because Super Saiyan 4 is brought about while in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form", "$60 million", "Alan Sokal", "Square Enix", "1955", "Alexandre Dumas", "Tom Wolfe", "\"Little Dixie\"", "8-track cartridge, an audio tape system", "$17,000", "there is not a mechanism at the federal level to ensure that drivers comply.", "think we have to rely on having a clinical breast exam once a year at a health care provider and doing your self-breast exam on a monthly basis.", "forgery and flying without a valid license,", "700", "first or second week in April.", "eight", "nine newly-purchased bicycles", "ethanol", "Florida", "the Pequots", "a flying saucer", "Voltaire", "the Jordan", "an apritif", "Prince William and Kate Middleton"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5025036161754911}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, false, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.7272727272727273, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.5, 0.5, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.125, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.15384615384615385, 0.07142857142857144, 0.2857142857142857, 1.0, 0.25, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_triviaqa-validation-3537", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-1661", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-5545", "mrqa_triviaqa-validation-259", "mrqa_triviaqa-validation-2530", "mrqa_triviaqa-validation-7411", "mrqa_triviaqa-validation-2457", "mrqa_triviaqa-validation-3793", "mrqa_triviaqa-validation-1303", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-2658", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-302", "mrqa_triviaqa-validation-6385", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3174", "mrqa_triviaqa-validation-4293", "mrqa_triviaqa-validation-2083", "mrqa_naturalquestions-validation-5014", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-10194", "mrqa_naturalquestions-validation-1801", "mrqa_hotpotqa-validation-3677", "mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-34", "mrqa_hotpotqa-validation-1142", "mrqa_newsqa-validation-2358", "mrqa_newsqa-validation-359", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-1611", "mrqa_searchqa-validation-6294", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-15708"], "SR": 0.390625, "CSR": 0.5374228395061729, "EFR": 0.9230769230769231, "Overall": 0.6945999525166192}, {"timecode": 81, "before_eval_results": {"predictions": ["catalyst", "erny mccartney", "Clara wieck", "Socrates", "carpathia", "Portugal", "Warren Commission", "perfume", "bruce", "benjamin", "the oche", "Wars of the Roses", "bruce bokes", "bertie ahern", "al jazeera", "m69", "FC Bayern M\u00fcnchen", "superficial thrombophlebitis", "Lorelei", "Mickey Spillane", "Sir Walter Scott", "Shayne Ward", "French Guiana", "Vienna", "Amsterdam", "pickled peppers", "the exosphere", "Paris", "Spain", "Arizona Diamondbacks", "little jack Horner", "mayflower", "the earlier national arms", "Have I Told You Lately", "in the New Testament", "Woody Paige", "1661", "April 21, 2015", "glycine and arginine", "the right", "University of Southern California Trojans", "January 2004", "The More", "The Adelaide Lightning", "Sun Valley", "Donald Sterling", "Richard Arthur", "Retina display", "giant mega-yacht 'Wally Island'", "an \"unnamed international terror group\"", "her decades-long portrayal of Alice Horton on", "the death of a pregnant soldier", "The iCloud service", "nearly $162 billion in war funding", "Mad Men", "$273 million", "heavy artillery", "plaque", "Brownsville", "the people", "Frank Sinatra", "Tasmania", "the Bodleian", "Ivy Dickens"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7080492424242424}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-402", "mrqa_triviaqa-validation-5346", "mrqa_triviaqa-validation-5052", "mrqa_triviaqa-validation-1920", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-2073", "mrqa_triviaqa-validation-6846", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7501", "mrqa_triviaqa-validation-2787", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-7366", "mrqa_naturalquestions-validation-686", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-370", "mrqa_newsqa-validation-1699", "mrqa_newsqa-validation-162", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-10324", "mrqa_searchqa-validation-8274", "mrqa_searchqa-validation-9043"], "SR": 0.640625, "CSR": 0.5386814024390244, "retrieved_ids": ["mrqa_squad-train-57849", "mrqa_squad-train-48808", "mrqa_squad-train-22073", "mrqa_squad-train-79369", "mrqa_squad-train-46071", "mrqa_squad-train-43422", "mrqa_squad-train-72740", "mrqa_squad-train-38449", "mrqa_squad-train-20564", "mrqa_squad-train-31326", "mrqa_squad-train-85843", "mrqa_squad-train-24415", "mrqa_squad-train-755", "mrqa_squad-train-41482", "mrqa_squad-train-12978", "mrqa_squad-train-21071", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-13464", "mrqa_naturalquestions-validation-3546", "mrqa_searchqa-validation-3016", "mrqa_naturalquestions-validation-9101", "mrqa_searchqa-validation-7204", "mrqa_hotpotqa-validation-3629", "mrqa_newsqa-validation-3006", "mrqa_hotpotqa-validation-726", "mrqa_newsqa-validation-3302", "mrqa_naturalquestions-validation-5939", "mrqa_squad-validation-6118", "mrqa_newsqa-validation-2446", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-2591", "mrqa_hotpotqa-validation-2296"], "EFR": 1.0, "Overall": 0.7102362804878048}, {"timecode": 82, "before_eval_results": {"predictions": ["Buxton", "mikeado", "Yellowstone", "tony hanks", "marc", "almond", "the Salvation Army", "bali", "0", "genesis", "john of gaunt", "Towcester", "the Benedictine Order", "Trinity College", "the Divine Creation of man", "Cuba", "Robert Carlyle", "phosphorus", "1927", "Tennessee", "architectural", "Hinduism", "Howard Keel", "australia", "USS Missouri", "oddities", "Lithium", "prince Edward, Earl of Wessex", "Colleen McCullough", "ilie nastase", "Colin Murray", "Urania", "Johannes Gutenberg", "an edible tuber", "Amanda Leighton", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "the lumbar cistern", "the delta basin", "30 months", "Lori Rom", "Charles Otto Puth Jr.", "Al Horford", "Wonder Woman", "Hellenism", "\"\u00f7\" (2015)", "Cannes Film Festival", "Dominican", "World War I", "\"What she's doing is putting a personal and human face on the issue... there's nothing more crucial,\"", "her resources", "Iggy Pop:", "Kandi Burruss,", "\"We must find ways to relieve some of this stress,\"", "Somali-based", "power-sharing talks", "Stanford University", "blimps", "\"Everybody Lies\"", "the South Beach diet", "the root", "the Southern Cross", "Indonesia", "Java", "Athol Fugard"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6263302669552668}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, true, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.25, 0.0, 0.1818181818181818, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.14285714285714285, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.3636363636363636, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3504", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-2516", "mrqa_triviaqa-validation-5933", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-3603", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-3401", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-4668", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-5034", "mrqa_naturalquestions-validation-2255", "mrqa_naturalquestions-validation-1090", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-4649", "mrqa_newsqa-validation-3934", "mrqa_newsqa-validation-1797", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2887", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-2236", "mrqa_searchqa-validation-15923", "mrqa_searchqa-validation-10249"], "SR": 0.53125, "CSR": 0.5385918674698795, "EFR": 0.8666666666666667, "Overall": 0.6835517068273094}, {"timecode": 83, "before_eval_results": {"predictions": ["Luigi Pirandello", "brahms Four", "cotton", "asphodel", "Austria Tyrol", "the tartan", "Mark Darcy", "alpha", "Spain", "nasdaq", "The Great Gatsby", "Poland", "willard blaine", "Operation", "Rarely", "mike gavaskar", "australia", "Massachusetts", "Eldorado", "geoffrey raft", "Armageddon", "Purple Rain", "Sinclair Lewis", "Herman Wouk", "Silverstone", "k Keswick", "a toad", "Runic", "blue", "John Nash", "babe i\u2019m Gonna leave You", "Pocahontas", "during the 1890s Klondike Gold Rush", "5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "St. Mary's County is part of the Baltimore -- Washington metropolitan area", "Massillon, Ohio", "Robin", "while studying All My Sons by Arthur Miller", "1997", "the frequency f, wavelength \u03bb, or photon energy E", "June 2, 2008", "Ronnie Schell", "Estadio de L\u00f3pez Cort\u00e1zar", "George Adamski", "Bangor Air National Guard Base", "actress and model", "Soci\u00e9t\u00e9 de Micro\u00e9lectronique et d' Horlogerie", "Campbellsville", "the territory", "Alejandro Peralta Alvarez,", "Darwin", "1-1", "Philippines", "three out of four questioned say that things are going well for them personally.", "the surgical anesthetic propofol", "armed robbery and kidnapping of another victim,", "the common man", "Aquitaine", "Niels Bohr", "Sicilian pizza", "the funny bone", "a large chart for family trees", "\"the most intelligent cat in the world\"", "the Beas River"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6517952755416418}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.32558139534883723, 1.0, 0.06666666666666667, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5749", "mrqa_triviaqa-validation-317", "mrqa_triviaqa-validation-4886", "mrqa_triviaqa-validation-5704", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-5786", "mrqa_triviaqa-validation-1714", "mrqa_triviaqa-validation-4943", "mrqa_triviaqa-validation-2603", "mrqa_triviaqa-validation-4160", "mrqa_triviaqa-validation-6864", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-7514", "mrqa_naturalquestions-validation-5798", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-1554", "mrqa_hotpotqa-validation-662", "mrqa_newsqa-validation-3550", "mrqa_newsqa-validation-471", "mrqa_newsqa-validation-2472", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-837", "mrqa_searchqa-validation-4860", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-15607", "mrqa_searchqa-validation-8422", "mrqa_searchqa-validation-9720"], "SR": 0.5625, "CSR": 0.5388764880952381, "EFR": 1.0, "Overall": 0.7102752976190476}, {"timecode": 84, "before_eval_results": {"predictions": ["tony blair", "brown", "samuel mendes", "Operation Dynamo", "clark", "clark", "eBU", "w", "Messenger", "Hungary", "Granada", "rugby", "olympia garland", "chamomile", "alien", "Big Dipper", "a ape", "Luigi Pirandello", "clark", "horseshoes", "Annie leibovitz", "atomic kitten", "thalia", "thomas rosy", "cuticle", "peter", "rings", "peter sampras", "Festival of Britain on London's South Bank", "Sherlock Holmes", "maxilla", "the Suez Canal", "Presley Smith", "in the blood to the liver", "The Impalas", "in southern Turkey, dividing the Mediterranean coastal region of southern Turkey from the central Anatolian Plateau", "Henry Selick", "Patricia Field", "Tulsa, Oklahoma", "`` full '' sexual intercourse", "Linda Ronstadt", "British romantic comedy", "Bonkyll Castle", "American", "Outstanding Lighting Design", "North Atlantic Conference", "12", "Terry O. Morse", "\"Larry King Live.\"", "the Taliban's", "he's been drawn to since he was a boy.", "Tim Clark, Matt Kuchar and Bubba Watson", "over the", "early detection and helping other women cope with the disease.", "Michelle Rounds", "28", "the 400th anniversary of Lima\\'s founding", "London", "the chest", "Big Brother", "Tom Cruise", "a short circuit", "Katherine Heigl", "Behind the eight ball"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6420409451659451}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.22222222222222224, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5562", "mrqa_triviaqa-validation-2968", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4060", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-6077", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-816", "mrqa_triviaqa-validation-2815", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-1046", "mrqa_triviaqa-validation-1863", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-5458", "mrqa_naturalquestions-validation-1179", "mrqa_naturalquestions-validation-8950", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4065", "mrqa_newsqa-validation-2121", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-2915", "mrqa_searchqa-validation-1257", "mrqa_searchqa-validation-15106"], "SR": 0.5625, "CSR": 0.5391544117647059, "retrieved_ids": ["mrqa_squad-train-3890", "mrqa_squad-train-80317", "mrqa_squad-train-71386", "mrqa_squad-train-34016", "mrqa_squad-train-4242", "mrqa_squad-train-21024", "mrqa_squad-train-53021", "mrqa_squad-train-16590", "mrqa_squad-train-400", "mrqa_squad-train-73689", "mrqa_squad-train-31324", "mrqa_squad-train-80397", "mrqa_squad-train-7329", "mrqa_squad-train-52198", "mrqa_squad-train-56706", "mrqa_squad-train-54472", "mrqa_squad-validation-6806", "mrqa_hotpotqa-validation-5477", "mrqa_triviaqa-validation-2132", "mrqa_newsqa-validation-2078", "mrqa_triviaqa-validation-6338", "mrqa_searchqa-validation-13484", "mrqa_naturalquestions-validation-7309", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-6368", "mrqa_squad-validation-1135", "mrqa_hotpotqa-validation-751", "mrqa_squad-validation-1965", "mrqa_hotpotqa-validation-532", "mrqa_triviaqa-validation-319", "mrqa_searchqa-validation-12187", "mrqa_searchqa-validation-2701"], "EFR": 0.9285714285714286, "Overall": 0.6960451680672269}, {"timecode": 85, "before_eval_results": {"predictions": ["Los Ticos", "\"probably from NORAD,\" or the North American Aerospace Defense Command,", "\"Something's wrong with this lady.\"", "crude oil", "Monday night", "citizenship", "The Bronx County District Attorneys Office", "1950s,", "President Obama ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons,", "flooding and debris", "unprecedented rise in American politics.", "attempted car-jacking", "five days a week.", "Africa's largest producer.", "Costa Mesa", "10 below", "Mad Men", "Iowa,", "Asashoryu's", "she was vacationing with her family on the island of Tictabon,", "at checkposts and military camps in the Mohmand agency,", "China", "President Obama", "Mashhad", "Washington State's decommissioned Hanford nuclear site,", "Bollywood superstar", "16", "prisoners at the South Dakota State Penitentiary", "Thursday and Friday", "Tuesday's iPhone 4S news,", "3-0", "you know what is important in life,", "AMX - 30", "The International System of Units ( SI )", "Nashville, Tennessee", "Chelsea ( 2009 -- 10 )", "her castle", "Beyonc\u00e9", "Brenda", "China", "Emily Davison", "Pompey", "the Treaty of Waitangi", "bacteria", "JeSuisCharlie", "business cycle", "Spanish", "stop motion effects", "international football competition", "gamecock", "Alexandre Dimitri Song Billong", "40 million", "26,000", "1911", "a Rugby Sevens competition for the twelve Aviva Premiership clubs that will play the following season", "Dirk Werner Nowitzki", "Austin Powers", "Louisiana", "the iris", "actress", "Bob Dylan", "Princess Diana", "credit", "Uncle Tom's Cabin"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5740246212121212}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true], "QA-F1": [0.0, 0.09090909090909093, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.08, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-341", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-2292", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-947", "mrqa_newsqa-validation-1072", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-2005", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-2395", "mrqa_naturalquestions-validation-9426", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-9436", "mrqa_triviaqa-validation-2511", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-4680", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-1467", "mrqa_searchqa-validation-5024", "mrqa_searchqa-validation-14366"], "SR": 0.546875, "CSR": 0.5392441860465116, "EFR": 0.9655172413793104, "Overall": 0.7034522854851645}, {"timecode": 86, "before_eval_results": {"predictions": ["australia", "the sternum", "pink", "Jessica", "kennedy simms Twins", "katy albright", "faversham", "coalbrookdale", "sp Spokane", "australia and England", "Mediterranean", "looking glass", "golf", "will carling", "Jonathan Swift", "sigmund Freud", "Nyasaland", "ken Russell", "crow family", "Enrico Caruso", "tara", "Morgan Spurlock", "spain", "Johannesburg", "Saturn", "liberty pole", "the top", "1879", "dragon", "cosmos", "time", "Diary of a Tuber", "Kimberlin Brown", "one", "it activates a relay which will handle the higher current load", "Andy Warhol", "Mel Tillis", "Organisms in the domains of Archaea and Bacteria", "hair / fur ( including wool ) and feathers", "copper ( Cu )", "Barack Obama", "1937", "bioelectromagnetics", "Len Wiseman", "Jewish", "Nathan Bedford Forrest", "castle near Muir of Ord and Tore on the Black Isle, in Ross and Cromarty, Scotland", "Jim Carrey", "15", "Mashhad", "issued his first military orders as leader of North Korea", "200.", "test scores and graduation rates", "Kim Il Sung died", "several weeks,", "digging", "The Beverly Hillbillies", "Boeing", "Kenya", "frank", "David Beckham", "The Mousetrap", "a guardian angel", "Herodotus"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6368624864718615}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.18181818181818182, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-223", "mrqa_triviaqa-validation-6882", "mrqa_triviaqa-validation-5514", "mrqa_triviaqa-validation-504", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-1822", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-1129", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-5613", "mrqa_naturalquestions-validation-10451", "mrqa_naturalquestions-validation-6148", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-5531", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-2670", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-1614", "mrqa_searchqa-validation-11348", "mrqa_searchqa-validation-16085", "mrqa_searchqa-validation-12949"], "SR": 0.609375, "CSR": 0.5400502873563218, "EFR": 1.0, "Overall": 0.7105100574712644}, {"timecode": 87, "before_eval_results": {"predictions": ["CBS", "Theatre Ventures, Inc.", "Northern Ireland", "Richard Street", "1983 Summer Universiade", "Faurot Field", "fourth-largest", "Buddha's delight", "Natalie Chandler", "Toshi Ichiyanagi", "1988", "Reinhard Heydrich", "Bass", "Paris Motor Show", "Jeffrey Chiang", "Hanford Site", "December 19, 1998", "Mickey's PhilharMagic", "Arizona Health Care Cost Containment System", "Donald Sterling", "Sada Carolyn Thompson", "Matt Groening", "All That", "glee", "Danish", "Gateways", "\"Invader (Invasor)", "July 25 to August 4", "2015", "810", "Del Mar Fairgrounds", "Pool", "electron donors", "Iowa", "sacroiliac joint or SI joint", "Stefanie Scott", "florida", "erosion", "Kerris Lilla Dorsey", "Joseph Sherrard Kearns", "spark-ignition", "jack Lemmon", "france", "Suez", "mmorpg", "jape", "loo", "Tina Turner", "Federer", "five", "Hong Kong from other parts of Asia, such as India and mainland China, and sold on the streets illegally,", "Haiti", "Michael Schumacher", "fritter", "Les Bleus", "228", "Frederick II", "burping", "Vietnam", "centigrade", "Boston", "Baltimore", "Howard Hughes", "The Curse of the Black Pearl"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6867627164502165}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0909090909090909, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1029", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-650", "mrqa_hotpotqa-validation-3319", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-4449", "mrqa_hotpotqa-validation-5266", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-8417", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-715", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-1673", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-263", "mrqa_newsqa-validation-1907", "mrqa_newsqa-validation-761", "mrqa_searchqa-validation-1818", "mrqa_searchqa-validation-256", "mrqa_searchqa-validation-280", "mrqa_searchqa-validation-12610", "mrqa_searchqa-validation-10796"], "SR": 0.609375, "CSR": 0.5408380681818181, "retrieved_ids": ["mrqa_squad-train-13509", "mrqa_squad-train-52350", "mrqa_squad-train-43378", "mrqa_squad-train-67727", "mrqa_squad-train-53610", "mrqa_squad-train-3764", "mrqa_squad-train-52598", "mrqa_squad-train-9797", "mrqa_squad-train-34053", "mrqa_squad-train-14798", "mrqa_squad-train-15362", "mrqa_squad-train-9380", "mrqa_squad-train-7828", "mrqa_squad-train-83901", "mrqa_squad-train-82922", "mrqa_squad-train-57224", "mrqa_newsqa-validation-3242", "mrqa_searchqa-validation-8357", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-6258", "mrqa_newsqa-validation-352", "mrqa_hotpotqa-validation-1533", "mrqa_naturalquestions-validation-6949", "mrqa_searchqa-validation-10078", "mrqa_naturalquestions-validation-10194", "mrqa_triviaqa-validation-607", "mrqa_newsqa-validation-3500", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-8558", "mrqa_squad-validation-8369", "mrqa_searchqa-validation-16606", "mrqa_hotpotqa-validation-5352"], "EFR": 0.96, "Overall": 0.7026676136363637}, {"timecode": 88, "before_eval_results": {"predictions": ["Sunday,", "severe flooding", "to the southern city of Naples", "cell phones", "he had tried to activate a \"kill switch\" that would cut off the well before abandoning the structure.", "be silent.", "body bags", "Chinese", "Taliban", "served in the military,", "her most important work is her charity, the Happy Hearts Fund.", "suicide car bombing", "in a volatile zone along the equator between South America and Africa.", "Gordon Brown", "\"Quiet Nights,\"", "17-month", "Asashoryu", "managing his time.", "Brazil", "Bush-era Justice Department", "London", "Ron Howard,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "the National September 11 Memorial & Museum Foundation", "Dodi Fayed.", "four", "Operation Pipeline Express.", "The motion", "piano lessons.", "the equator", "The Marines", "Nigeria,", "Abid Ali Neemuchwala", "the national and state legislatures", "early 5th century Buddhism was established in south China", "Waylon Jennings", "Bart Cummings", "You are a puzzle", "Lulu", "interstitial and intravascular", "john tennie", "end of March 1939", "centaur", "ernie", "france", "swindon town", "sisyphus", "gypsum", "Andr\u00e9 Ch\u00e9nier", "La Familia Michoacana", "nausea, vomiting, diarrhea, jaundice, fever, and abdominal pain", "Central Avenue", "Quentin Coldwater", "Corps of Discovery", "Nippon Professional Baseball", "punk rock band Pinhead Gunpowder", "William", "John Donne", "tusks", "Joan of Arc", "lilac", "the Library of Congress", "Honey Smacks", "R.E.M."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6113153606134787}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, false, true, false, false, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.888888888888889, 1.0, 0.046511627906976744, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3636363636363636, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-3036", "mrqa_newsqa-validation-2782", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-4073", "mrqa_newsqa-validation-822", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3978", "mrqa_newsqa-validation-1105", "mrqa_naturalquestions-validation-9719", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-5125", "mrqa_triviaqa-validation-3411", "mrqa_triviaqa-validation-3218", "mrqa_triviaqa-validation-1425", "mrqa_hotpotqa-validation-5355", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-3984", "mrqa_searchqa-validation-12880"], "SR": 0.53125, "CSR": 0.5407303370786517, "EFR": 0.9666666666666667, "Overall": 0.7039794007490637}, {"timecode": 89, "before_eval_results": {"predictions": ["38,", "she also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.", "Pakistan's combustible Swat Valley,", "early detection and helping other women cope with the disease.", "promotes fuel economy and safety while boosted the economy.", "\"extremely weak\" and said he weighs barely 100 pounds in a court document filed this week, but he walked on his own during the 45 minutes he was at the ceremony.", "Africa", "Bollywood superstar", "NATO.", "Wigan Athletic in northern England.", "Vivek Wadhwa,", "Africa", "Osama", "Pyongyang and Seoul", "Saturday's Hungarian Grand Prix.", "British author J.G. Ballard,", "Germany", "Hu Jintao", "cars", "estimated 400", "two women", "Tomas Olsson,", "The charter mandated the English king to cede certain basic rights to his citizens, ensuring that no man is above the law.", "58", "jazz", "last surviving British soldier from World War I", "Pixar's", "Brooklyn, New York,", "some of the best stunt ever pulled off", "near Pakistan's border with Afghanistan", "his business dealings for possible securities violations", "five", "12.9 - kilometre ( 8 mi )", "Pakistan", "1945", "Wisconsin", "wall mounted faucet and the sink rim", "`` 0 '' trunk code", "San Francisco, California ( the primary setting of the film ), and around Oahu, Hawaii", "the sidewalk between Division Street and East Broadway", "robert laver", "g Gerald Ford", "hippety hopper", "mike", "ernie dreyfuss", "potatoes", "annette Crosbie", "ernium", "Knoxville, Tennessee", "Coalwood", "Sun Records founder Sam Phillips", "Westminster system", "Charles Bronson", "Eiffel 65", "Black pudding", "northeastern", "T. S. Eliot", "Elie Wiesel", "Juliana", "brain stimulation", "shalom", "Richard Nixon", "Kublai Khan", "a packer"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5840583839918946}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, false, false, false, true, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 0.09523809523809525, 0.1702127659574468, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.42857142857142855, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-9", "mrqa_newsqa-validation-3304", "mrqa_newsqa-validation-1028", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-3281", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2048", "mrqa_newsqa-validation-4045", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-3358", "mrqa_newsqa-validation-2682", "mrqa_naturalquestions-validation-3006", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-4166", "mrqa_triviaqa-validation-2003", "mrqa_triviaqa-validation-5000", "mrqa_triviaqa-validation-5601", "mrqa_triviaqa-validation-3605", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2350", "mrqa_searchqa-validation-9368", "mrqa_searchqa-validation-8966", "mrqa_searchqa-validation-13813", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-16575"], "SR": 0.4375, "CSR": 0.5395833333333333, "EFR": 0.9722222222222222, "Overall": 0.7048611111111112}, {"timecode": 90, "UKR": 0.76953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1229", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-244", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2845", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4474", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5361", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10682", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1519", "mrqa_naturalquestions-validation-1567", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2042", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2481", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3562", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6055", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8207", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-912", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-2825", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11046", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11464", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12633", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-2024", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6265", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7073", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-711", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7554", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_squad-validation-9285", "mrqa_squad-validation-96", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2358", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-3146", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3895", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-5007", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-6588", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6658", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7012", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-7772", "mrqa_triviaqa-validation-818", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.8046875, "KG": 0.49609375, "before_eval_results": {"predictions": ["Apple Inc.", "second", "\"Wolfman,\"", "using recreational drugs", "in recent days, recent weeks, recent months,\"", "CNN", "Rwanda", "Another high tide", "scored a hat-trick", "Apple employees", "80,", "42", "Nat King Cole.", "Kenneth Cole", "in the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "drugs against their will.", "if she would try to take the children and flee to Japan.", "was being held in solitary confinement at the Prince George's County Correctional Center,", "learn in safer surroundings.", "southern city of Naples", "July", "abducting each other for ransoms or retribution.", "Kim", "Colorado prosecutor", "Citizens", "in the months ahead from a more detailed necropsy.", "Saturday,", "Kenneth Cole", "Republicans", "dismissed all charges", "NATO fighters", "Airbus A330-200", "14 \u00b0 41 \u2032 34 '' N 17.44667 \u00b0 W", "The Crescent City", "two", "slowly and haphazardly formed into a more unified propaganda effort, although never to the level of World War I", "Lenin", "eight", "Kenny Anderson", "Nefertiti", "paddington bear", "Spain", "salford", "The Hague", "tombstone", "bactrian", "Grail", "kim", "T. R. M. Howard", "Orchard County", "The Bye Bye Man", "Oregon", "35,124", "Phil Collins", "Valhalla Highlands Historic District", "Can't Be Tamed", "koma", "Howard Hughes", "the tapir", "pep", "Meyer Lansky", "Russians", "Brave New World", "Paul Revere"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7467410247798179}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.45454545454545453, 0.3333333333333333, 0.23999999999999996, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6206896551724138, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1276", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-991", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-3618", "mrqa_naturalquestions-validation-4385", "mrqa_naturalquestions-validation-7300", "mrqa_naturalquestions-validation-2078", "mrqa_naturalquestions-validation-4500", "mrqa_naturalquestions-validation-6896", "mrqa_triviaqa-validation-4041", "mrqa_triviaqa-validation-3477", "mrqa_hotpotqa-validation-3080", "mrqa_searchqa-validation-11572", "mrqa_searchqa-validation-3114", "mrqa_searchqa-validation-1049"], "SR": 0.6875, "CSR": 0.5412087912087913, "retrieved_ids": ["mrqa_squad-train-30341", "mrqa_squad-train-12927", "mrqa_squad-train-17337", "mrqa_squad-train-36968", "mrqa_squad-train-32835", "mrqa_squad-train-74592", "mrqa_squad-train-24310", "mrqa_squad-train-38062", "mrqa_squad-train-24332", "mrqa_squad-train-1533", "mrqa_squad-train-1897", "mrqa_squad-train-68556", "mrqa_squad-train-51899", "mrqa_squad-train-32463", "mrqa_squad-train-41284", "mrqa_squad-train-31233", "mrqa_newsqa-validation-3519", "mrqa_triviaqa-validation-1595", "mrqa_hotpotqa-validation-4172", "mrqa_newsqa-validation-592", "mrqa_triviaqa-validation-6043", "mrqa_newsqa-validation-4189", "mrqa_searchqa-validation-8827", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-6197", "mrqa_squad-validation-2612", "mrqa_naturalquestions-validation-5912", "mrqa_naturalquestions-validation-1632", "mrqa_triviaqa-validation-1920", "mrqa_searchqa-validation-13857", "mrqa_searchqa-validation-6337", "mrqa_hotpotqa-validation-4584"], "EFR": 0.95, "Overall": 0.7123042582417582}, {"timecode": 91, "before_eval_results": {"predictions": ["francs", "tulle", "Count Basie", "English", "Pose", "The Naked Gun", "the Clean Air Act", "Mercury & Venus", "Einstein", "The Girl Who Loved Tom Gordon", "Dunkin' Donuts", "the Vikings", "opera house", "Raven Symone", "the Clark bar", "Aladdin", "The Devil\\'s Advocate", "Cannes 1980", "Charles Dickens", "(Harry) Truman", "echinacea", "peripheral vision", "The Police", "Halley\\'s comet", "\"Just say no\"", "Namibia", "Kilimanjaro", "Ruth", "Dresden", "Magnolia", "Tennessee", "Big Ben", "Edward Furlong", "depolarization of the cardiac muscle begins at the sinus node", "the Naturalization Act of 1790", "Tim McGraw and Kenny Chesney", "seawater pearls", "May 2010", "LED illuminated display", "Jesse McCartney", "Kentucky Derby", "Campania", "la boheme", "surtsey", "vaud, Switzerland", "bottle pool", "massively multiplayer online games", "Herman G\u00f6ring", "Robert Digges Wimberly Connor", "about 3,000 inhabitants scattered in a dozen fishing villages", "1901", "member of the Executive Council of the General Anthroposophical Society at the Goetheanum in Dornach, Switzerland", "Dunlop", "Charles Hastings Judd", "1963", "Washington", "Wigan Athletic", "Democrats", "did not", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "this week,\"", "Jacob Zuma,", "Secretary of State Hillary Clinton,", "Lt. Holley Wimunc."], "metric_results": {"EM": 0.59375, "QA-F1": 0.7186325187969924}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.14285714285714288, 0.28571428571428575, 0.5714285714285715, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.761904761904762, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4210526315789474, 1.0, 1.0, 0.33333333333333337, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1061", "mrqa_searchqa-validation-13279", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-4518", "mrqa_searchqa-validation-12262", "mrqa_searchqa-validation-5935", "mrqa_naturalquestions-validation-7731", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-1745", "mrqa_naturalquestions-validation-2265", "mrqa_naturalquestions-validation-754", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-640", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-4204", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-169", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1392"], "SR": 0.59375, "CSR": 0.5417798913043479, "EFR": 0.9615384615384616, "Overall": 0.7147261705685619}, {"timecode": 92, "before_eval_results": {"predictions": ["bromley", "sash", "moon", "queen", "eglise du d\u00f4me", "whitehouse", "prince harry", "swashbucklers", "h. h. asquith", "john barbirolli", "lady Antonia Margaret Caroline Fraser", "george carlin", "testicles", "dumbo", "chromium", "staple Singers", "Baffin Island", "antilles", "casualty", "kia", "book 1: Sowing", "Israel", "aikido", "Library of Congress", "four", "Operation", "member of the third generation of the acting family", "kings cross", "pyrotechnics", "on the Braden Beat", "mexico", "q", "Afghanistan", "two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "August 15, 1971", "December 24, 1836", "November 27, 2013", "a vertebral column ( spine )", "Rachel Kelly Tucker", "21 February", "boundary river", "Piper", "Westland", "Dennis Potter", "Ben Savage", "\"the most powerful argument for the newly instigated worldwide ban on whaling.\"", "Southern State Parkway", "David Pajo", "40", "give detainees greater latitude in selecting legal representation", "$24,000-30,000", "BBC's central London offices", "beneath in a fire pit January 11 in Marine Cpl. Cesar Laurean's backyard.", "Eintracht Frankfurt", "100 to 150", "March 24,", "Ted", "piano", "bigwig", "anthrax", "ping-Pong", "jade", "Tunisia", "Don Quixote"], "metric_results": {"EM": 0.625, "QA-F1": 0.6884300595238095}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714285, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-125", "mrqa_triviaqa-validation-4802", "mrqa_triviaqa-validation-3621", "mrqa_triviaqa-validation-6415", "mrqa_triviaqa-validation-7661", "mrqa_triviaqa-validation-5934", "mrqa_triviaqa-validation-6314", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-4401", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-3385", "mrqa_triviaqa-validation-5112", "mrqa_naturalquestions-validation-2893", "mrqa_hotpotqa-validation-3927", "mrqa_hotpotqa-validation-3208", "mrqa_newsqa-validation-2881", "mrqa_newsqa-validation-2966", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2515", "mrqa_newsqa-validation-1255", "mrqa_searchqa-validation-9400", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-802"], "SR": 0.625, "CSR": 0.5426747311827957, "EFR": 0.9166666666666666, "Overall": 0.7059307795698925}, {"timecode": 93, "before_eval_results": {"predictions": ["Colombia", "wigan Warriors", "Ty Hardin", "south dakota", "river thames", "nadalia comaneci", "John Masefield", "\u00e9dith piaf", "Venus", "sesame Street", "groundhogs", "joule", "paul mkey", "m65", "britishtennis.com", "peter anka", "lighthouse keeper", "Croatia", "george quincey", "Russia", "thwaites", "cauliflower", "Ambassador Bridge", "hrogate", "seven wonders", "Sony Interactive Entertainment", "beetle", "argon", "jollity", "naboth", "plum", "sandstone Trail", "a form of fixed - mobile convergence ( FMC )", "eusebeia", "the seventh and final novel in the series", "2013", "The Blind Boys of Alabama", "Etienne de Mestre", "1938", "Florida, where new arrival Roy makes two oddball friends and a bad enemy, and joins an effort to stop construction of a pancake house which would destroy a colony of burrowing owls who live on the site", "19th-century", "Blue", "Johnny D. Bright", "saloon-keeper", "5320 km", "Hordaland", "Skipton", "Flushed Away", "women.", "12", "American Civil Liberties Union", "Michelle Obama", "Egypt", "People Against Switching Sides", "$8.8 million", "Manmohan Singh's", "the Etruscans", "Judi Dench", "Westminster Abbey", "a final contest", "a wolverine", "Richard Nixon", "Mexico", "Nebraska"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6609059343434343}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.1818181818181818, 1.0, 0.0, 0.0, 0.0, 0.06060606060606061, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4808", "mrqa_triviaqa-validation-229", "mrqa_triviaqa-validation-2855", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1402", "mrqa_triviaqa-validation-2618", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5097", "mrqa_triviaqa-validation-5647", "mrqa_triviaqa-validation-6324", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-1718", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2624", "mrqa_naturalquestions-validation-1312", "mrqa_hotpotqa-validation-1211", "mrqa_hotpotqa-validation-5718", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-1319", "mrqa_searchqa-validation-5374"], "SR": 0.609375, "CSR": 0.5433843085106382, "retrieved_ids": ["mrqa_squad-train-75226", "mrqa_squad-train-80830", "mrqa_squad-train-1987", "mrqa_squad-train-10450", "mrqa_squad-train-82683", "mrqa_squad-train-4639", "mrqa_squad-train-39217", "mrqa_squad-train-43227", "mrqa_squad-train-3644", "mrqa_squad-train-3518", "mrqa_squad-train-69533", "mrqa_squad-train-62669", "mrqa_squad-train-81832", "mrqa_squad-train-42012", "mrqa_squad-train-80109", "mrqa_squad-train-27153", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7300", "mrqa_searchqa-validation-10393", "mrqa_naturalquestions-validation-1133", "mrqa_searchqa-validation-8489", "mrqa_newsqa-validation-1025", "mrqa_triviaqa-validation-4304", "mrqa_searchqa-validation-7518", "mrqa_hotpotqa-validation-2313", "mrqa_searchqa-validation-6621", "mrqa_triviaqa-validation-5682", "mrqa_hotpotqa-validation-4036", "mrqa_naturalquestions-validation-7574", "mrqa_squad-validation-9372", "mrqa_searchqa-validation-9366", "mrqa_squad-validation-7162"], "EFR": 1.0, "Overall": 0.7227393617021276}, {"timecode": 94, "before_eval_results": {"predictions": ["Gulf of Mexico", "PC", "dandy", "tintoretto", "Vitcos", "kunigunde Mackamotski", "Blofeld", "very massive stars", "152", "Sir Hardy Amies", "paddington", "bERLIOz", "edmund", "honda", "tintoretto", "9", "Stephen Potter", "12", "mastricht Treaty", "hernia", "smithball", "Richard Krajicek", "g Gordon Wallace", "cosmos", "9", "eton college", "Richard Curtis", "South Carolina", "croquet", "John Donne", "coffee", "Boston pops", "for the red - bed country of its watershed", "February 25, 2003", "the northern bluegrass band the Greenbriar Boys", "CBS Television City", "Taiwan", "Abigail Hawk", "Lewis Carroll", "Asia", "1980", "The conversation", "five", "26,000", "London", "Democratic", "Michelle Anne Sinclair", "Matt Groening", "citizenship", "Cash for Clunkers", "A Lion Among Men.", "children of street cleaners and firefighters.", "prisoners at the South Dakota State Penitentiary", "1,500 Marines", "Pakistani officials,", "Steven Green", "Chevy Chase", "the Statue of Liberty", "House", "fog", "Prado", "(William) Tweed", "a repo man", "Graceland"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6778410674470458}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4615384615384615, 0.0, 1.0, 0.08695652173913042, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2913", "mrqa_triviaqa-validation-5611", "mrqa_triviaqa-validation-7619", "mrqa_triviaqa-validation-1562", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-164", "mrqa_triviaqa-validation-947", "mrqa_triviaqa-validation-5022", "mrqa_triviaqa-validation-2784", "mrqa_triviaqa-validation-4359", "mrqa_triviaqa-validation-1541", "mrqa_triviaqa-validation-4473", "mrqa_triviaqa-validation-3442", "mrqa_naturalquestions-validation-5006", "mrqa_naturalquestions-validation-1988", "mrqa_naturalquestions-validation-10209", "mrqa_naturalquestions-validation-4007", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-963", "mrqa_newsqa-validation-3681", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-14018"], "SR": 0.640625, "CSR": 0.544407894736842, "EFR": 0.8695652173913043, "Overall": 0.6968571224256292}, {"timecode": 95, "before_eval_results": {"predictions": ["Hollywood headquarters", "five", "last week,", "\"Rin Tin Tin: The Life and the Legend\"", "Afghan lawmakers", "The Ethiopian army's answer to the rebels has been to viciously attack civilians in the Ogaden,\"", "2,000 euros ($2,963)", "Afghanistan's restive provinces", "prostate cancer,", "Karthik Rajaram,", "kill then-Sen. Obama on October 23, 2008, shortly before the presidential election.", "Robert Barnett,", "MBA in finance", "Samoa", "Jacob Zuma,", "severe famine", "WTA Tour titles", "off the coast of Dubai with a celebrity-studded gala and a three-day party.", "an \"unnamed international terror group\"", "A family friend of a U.S. soldier", "Bhola for the Muslim festival of Eid al-Adha.", "38,", "U.S. State Department and British Foreign Office", "Jaipur", "$40 billion during the operations phase.", "$55.7 million", "Tuesday", "a student", "9 a.m.", "40", "Afghanistan and India", "Frank Ricci,", "16 June", "Lisa Stelly", "Procol Harum", "a transfer of money by a foreign worker to an individual in his or her home country", "Mahatma Gandhi", "twice", "Merrimen", "Sri Lanka Podujana Peramuna", "Jean-Paul Gaultier", "Carson City", "pol. polska", "annie leibovitz", "boron", "at Wembley", "technetium", "acetone", "Sleeping Beauty", "Lord Chancellor of England", "Mathew Sacks", "Easy", "1932", "Centennial Olympic Stadium", "February 14, 1859", "Protestant Christian", "Woodland Hills", "black", "gold", "Contender", "tuna", "a lampoon", "Tommy Franks", "Liechtenstein"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7265625}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.25, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2849", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-3259", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-880", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-7051", "mrqa_triviaqa-validation-4806", "mrqa_triviaqa-validation-216", "mrqa_triviaqa-validation-2079", "mrqa_hotpotqa-validation-4007", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-12584"], "SR": 0.65625, "CSR": 0.5455729166666667, "EFR": 1.0, "Overall": 0.7231770833333334}, {"timecode": 96, "before_eval_results": {"predictions": ["the surge,", "file papers shortly with an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "19-year-old woman", "genocide,", "Rodong Sinmun", "two satellites", "near the Somali coast", "Her husband and attorney, James Whitehouse,", "Hundreds", "Eleven people died and 36 were wounded", "near his home in Peshawar", "12-1 on aggregate.", "Aung San Suu Kyi", "July", "economic and political engagement", "bartering", "civilians,", "41,280", "Frodo, the Dr. Octopus to my Spiderman.", "suicides", "Charlotte Gainsbourg and Willem Dafoe", "to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "1940's", "\"a striking blow to due process and the rule of law.\"", "September 23,", "the first five Potter films", "five female pastors", "because a new model is simply out of their reach.", "Vice's broadband television network.", "AbdulMutallab", "has not been without disagreement and difficulty, but the notion that we must be adversaries is not predestined,\"", "American rock band R.E.M.", "Incumbent Achal Kumar Jyoti", "Kenny Anderson", "7000301604928199000 \u2660 3.016 049 281 99 ( 23 ) u", "September 2, 1945", "President", "Washington metropolitan area", "Divyanka Tripathi and Karan Patel", "jubardi", "ukraine", "elizabeth troy", "jubile", "ukraine", "ueno", "paul", "pomona", "C\u00e9cile Aubry", "two", "Karen O", "Arab", "Rage Against the Machine", "MGM Resorts International", "4,530", "The Daily Stormer", "Hawaii", "Earth", "Danube", "Fiddler", "a dice", "Redcliffe", "Grand Central Station", "shrewd"], "metric_results": {"EM": 0.5, "QA-F1": 0.6088921648204737}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, false, false, true, false, true, false, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.9411764705882353, 0.0, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-333", "mrqa_newsqa-validation-2996", "mrqa_newsqa-validation-3666", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-928", "mrqa_newsqa-validation-2738", "mrqa_newsqa-validation-3628", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2965", "mrqa_newsqa-validation-4063", "mrqa_newsqa-validation-1313", "mrqa_naturalquestions-validation-10368", "mrqa_naturalquestions-validation-7141", "mrqa_naturalquestions-validation-2182", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-2613", "mrqa_triviaqa-validation-839", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-6116", "mrqa_triviaqa-validation-1993", "mrqa_triviaqa-validation-7343", "mrqa_hotpotqa-validation-771", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1112", "mrqa_searchqa-validation-4231", "mrqa_searchqa-validation-8774", "mrqa_searchqa-validation-16255", "mrqa_searchqa-validation-13697"], "SR": 0.5, "CSR": 0.5451030927835052, "retrieved_ids": ["mrqa_squad-train-60448", "mrqa_squad-train-14644", "mrqa_squad-train-62140", "mrqa_squad-train-4498", "mrqa_squad-train-24421", "mrqa_squad-train-62569", "mrqa_squad-train-57030", "mrqa_squad-train-55215", "mrqa_squad-train-82187", "mrqa_squad-train-58999", "mrqa_squad-train-30913", "mrqa_squad-train-32137", "mrqa_squad-train-77829", "mrqa_squad-train-14697", "mrqa_squad-train-72727", "mrqa_squad-train-54524", "mrqa_newsqa-validation-1072", "mrqa_triviaqa-validation-2063", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-4664", "mrqa_naturalquestions-validation-4746", "mrqa_searchqa-validation-9313", "mrqa_triviaqa-validation-4242", "mrqa_hotpotqa-validation-2555", "mrqa_searchqa-validation-7787", "mrqa_naturalquestions-validation-2064", "mrqa_searchqa-validation-6175", "mrqa_triviaqa-validation-6696", "mrqa_triviaqa-validation-487", "mrqa_squad-validation-4771", "mrqa_newsqa-validation-1670", "mrqa_naturalquestions-validation-4318"], "EFR": 0.96875, "Overall": 0.7168331185567011}, {"timecode": 97, "before_eval_results": {"predictions": ["Kindle Fire", "President Obama and Britain's Prince Charles", "The horrific act shows how sociopathic brains develop.", "cartel from the state of Veracruz, Mexico,", "women.", "is not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.", "the iconic Hollywood headquarters of Capitol Records,", "Trevor Rees", "next year", "Lisa Brown", "Filippo Inzaghi", "there is not a mechanism at the federal level to ensure that drivers comply.", "Sheik Mohammed Ali al-Moayad and Mohammed Mohsen Zayed,", "it was \"probably from NORAD,\" or the North American Aerospace Defense Command,", "Illness", "Fayetteville apartment of 2nd Lt. Holley Wimunc,", "free laundry service.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "18th", "the American Civil Liberties Union", "15,000", "Japan", "Top Gun", "his father", "a reward car.", "U.S. Ambassador to Zimbabwe James McGee would be expelled from the country if he \"persisted in meddling in Zimbabwe's electoral process,\"", "a delegation of American Muslim and Christian leaders", "Barack Obama", "autonomy", "Jeddah, Saudi Arabia,", "1831", "a \"procedure on her heart,\"", "September 1980", "in 1905", "Amerigo Vespucci", "the foreign exchange market (FX )", "Chinese Exclusion Act", "a statistical advantage for the casino that is built into the game", "The 111th edition of the World Series", "Rory McIlroy", "Cocibolca", "Macbeth", "venus kriegler", "Santiago", "yishai!", "1961", "rounders", "racing cars", "University of Southern California", "Lucille D\u00e9sir\u00e9e Ball", "once", "Cheshire County", "Atlantic Ocean", "tomato", "odd-eyed", "Magnate", "a spoon", "Qwerty", "Austria", "garlic", "Peru", "Repent ye: for the kingdom of heaven is at hand", "C Daryl Chessman", "Paul"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6405548878205127}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.4, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-569", "mrqa_newsqa-validation-2358", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-1400", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-4147", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-2400", "mrqa_newsqa-validation-3943", "mrqa_newsqa-validation-2546", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5000", "mrqa_triviaqa-validation-1415", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-115", "mrqa_hotpotqa-validation-2827", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-1273", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-6881", "mrqa_searchqa-validation-2609", "mrqa_searchqa-validation-2778", "mrqa_searchqa-validation-15672"], "SR": 0.578125, "CSR": 0.5454400510204082, "EFR": 1.0, "Overall": 0.7231505102040816}, {"timecode": 98, "before_eval_results": {"predictions": ["chubasco", "James Blunt", "ethelred", "air", "Lancashire", "whale", "nellie melba", "alias Smith and Jones", "bubba", "lighthouse keeper", "al yganeh", "double bass", "Parliament-Funkadelic", "egypt", "marriage", "a real deejay who ran afoul of the military authorities in Saigon because of the on-air liberties he took with regard to matters of style and language and musical taste.", "(Thomas) Jefferson", "russia", "Eric coates", "Dublin", "Welcome Stranger", "brown", "violin", "nippon Sangyo", "north tennessee", "time machine", "jack Nicholson", "n\u014dmen", "new hampshire", "oliver Twist", "an arrowhead", "hand gun", "Schadenfreude", "Kelly Reno", "2018", "Ed Sheeran", "Lexie", "a form of business network, for example, a local organization of businesses whose goal is to further the interests of businesses", "Andrea Brooks", "The Impalas", "Cleveland Browns", "edith cavell", "Rick and Morty", "Steve Carell", "Jena Malone", "Red and Assiniboine Rivers", "Illinois", "Field of Dreams", "Steven Green", "South Dakota State Penitentiary", "surgical anesthetic propofol", "70,000 or so", "a quarter-mile pier crumbling into the sea along with two of his trucks.", "$250,000 for Rivers' charity: God's Love We Deliver.", "an antihistamine and an epinephrine auto-injector for emergencies,", "John and Elizabeth Calvert", "War of 1812", "Valium", "Wilbur and Orville Wright", "pie", "Waterloo", "Pan", "Qantas", "Anna and the King of Siam"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6650568181818182}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.3, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_triviaqa-validation-6214", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-7725", "mrqa_triviaqa-validation-2990", "mrqa_triviaqa-validation-6620", "mrqa_triviaqa-validation-7748", "mrqa_triviaqa-validation-3614", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-312", "mrqa_triviaqa-validation-2569", "mrqa_triviaqa-validation-1364", "mrqa_triviaqa-validation-6862", "mrqa_triviaqa-validation-1721", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-2342", "mrqa_naturalquestions-validation-6429", "mrqa_naturalquestions-validation-5509", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-1700", "mrqa_hotpotqa-validation-1434", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-4208", "mrqa_searchqa-validation-7820", "mrqa_searchqa-validation-15766", "mrqa_searchqa-validation-3710"], "SR": 0.578125, "CSR": 0.545770202020202, "EFR": 0.9629629629629629, "Overall": 0.715809132996633}, {"timecode": 99, "UKR": 0.76171875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-102", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1179", "mrqa_hotpotqa-validation-1237", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1376", "mrqa_hotpotqa-validation-1384", "mrqa_hotpotqa-validation-1400", "mrqa_hotpotqa-validation-1477", "mrqa_hotpotqa-validation-1520", "mrqa_hotpotqa-validation-1557", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-1779", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1885", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-190", "mrqa_hotpotqa-validation-1911", "mrqa_hotpotqa-validation-2019", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2453", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-2618", "mrqa_hotpotqa-validation-2720", "mrqa_hotpotqa-validation-2768", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-2793", "mrqa_hotpotqa-validation-2841", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2990", "mrqa_hotpotqa-validation-302", "mrqa_hotpotqa-validation-3134", "mrqa_hotpotqa-validation-3209", "mrqa_hotpotqa-validation-3351", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3385", "mrqa_hotpotqa-validation-3395", "mrqa_hotpotqa-validation-3448", "mrqa_hotpotqa-validation-3477", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3517", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3584", "mrqa_hotpotqa-validation-3602", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-3666", "mrqa_hotpotqa-validation-370", "mrqa_hotpotqa-validation-379", "mrqa_hotpotqa-validation-38", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4335", "mrqa_hotpotqa-validation-4395", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4555", "mrqa_hotpotqa-validation-4593", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4816", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4933", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4962", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-5053", "mrqa_hotpotqa-validation-5183", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5466", "mrqa_hotpotqa-validation-5484", "mrqa_hotpotqa-validation-5529", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5787", "mrqa_hotpotqa-validation-58", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-620", "mrqa_hotpotqa-validation-638", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-816", "mrqa_hotpotqa-validation-824", "mrqa_hotpotqa-validation-951", "mrqa_naturalquestions-validation-10257", "mrqa_naturalquestions-validation-10416", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-10653", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1312", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-1884", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1959", "mrqa_naturalquestions-validation-2064", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-2465", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2653", "mrqa_naturalquestions-validation-2732", "mrqa_naturalquestions-validation-2955", "mrqa_naturalquestions-validation-2972", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3348", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3476", "mrqa_naturalquestions-validation-3612", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-375", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-3910", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-4554", "mrqa_naturalquestions-validation-4604", "mrqa_naturalquestions-validation-4720", "mrqa_naturalquestions-validation-4785", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-5123", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-5235", "mrqa_naturalquestions-validation-5264", "mrqa_naturalquestions-validation-5467", "mrqa_naturalquestions-validation-5571", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-5710", "mrqa_naturalquestions-validation-5713", "mrqa_naturalquestions-validation-5820", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6294", "mrqa_naturalquestions-validation-6330", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-6522", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-6769", "mrqa_naturalquestions-validation-6854", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-7301", "mrqa_naturalquestions-validation-7338", "mrqa_naturalquestions-validation-7499", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7593", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7945", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8181", "mrqa_naturalquestions-validation-8603", "mrqa_naturalquestions-validation-8782", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8899", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-9119", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9692", "mrqa_naturalquestions-validation-9753", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-994", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1010", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1126", "mrqa_newsqa-validation-1196", "mrqa_newsqa-validation-1236", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1284", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-1335", "mrqa_newsqa-validation-1408", "mrqa_newsqa-validation-1471", "mrqa_newsqa-validation-1476", "mrqa_newsqa-validation-159", "mrqa_newsqa-validation-1606", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-1693", "mrqa_newsqa-validation-1709", "mrqa_newsqa-validation-1733", "mrqa_newsqa-validation-179", "mrqa_newsqa-validation-1827", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1888", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-1937", "mrqa_newsqa-validation-1991", "mrqa_newsqa-validation-1993", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2129", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-2210", "mrqa_newsqa-validation-2223", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-234", "mrqa_newsqa-validation-2437", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2514", "mrqa_newsqa-validation-2539", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-2641", "mrqa_newsqa-validation-2656", "mrqa_newsqa-validation-2714", "mrqa_newsqa-validation-272", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2855", "mrqa_newsqa-validation-287", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-2970", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3026", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-3194", "mrqa_newsqa-validation-3298", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3525", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-3602", "mrqa_newsqa-validation-3630", "mrqa_newsqa-validation-3702", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-3921", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-3963", "mrqa_newsqa-validation-3971", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3995", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-4002", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4067", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-575", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-666", "mrqa_newsqa-validation-689", "mrqa_newsqa-validation-79", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-847", "mrqa_newsqa-validation-917", "mrqa_searchqa-validation-10018", "mrqa_searchqa-validation-10035", "mrqa_searchqa-validation-10039", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-10141", "mrqa_searchqa-validation-10192", "mrqa_searchqa-validation-10280", "mrqa_searchqa-validation-10727", "mrqa_searchqa-validation-1075", "mrqa_searchqa-validation-10818", "mrqa_searchqa-validation-10890", "mrqa_searchqa-validation-11102", "mrqa_searchqa-validation-11144", "mrqa_searchqa-validation-11237", "mrqa_searchqa-validation-11477", "mrqa_searchqa-validation-11616", "mrqa_searchqa-validation-11831", "mrqa_searchqa-validation-12439", "mrqa_searchqa-validation-12612", "mrqa_searchqa-validation-12726", "mrqa_searchqa-validation-13135", "mrqa_searchqa-validation-1321", "mrqa_searchqa-validation-13632", "mrqa_searchqa-validation-13839", "mrqa_searchqa-validation-14014", "mrqa_searchqa-validation-14056", "mrqa_searchqa-validation-14366", "mrqa_searchqa-validation-14550", "mrqa_searchqa-validation-14782", "mrqa_searchqa-validation-14807", "mrqa_searchqa-validation-15311", "mrqa_searchqa-validation-15405", "mrqa_searchqa-validation-15565", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16077", "mrqa_searchqa-validation-16403", "mrqa_searchqa-validation-16439", "mrqa_searchqa-validation-16705", "mrqa_searchqa-validation-1801", "mrqa_searchqa-validation-1884", "mrqa_searchqa-validation-1951", "mrqa_searchqa-validation-1996", "mrqa_searchqa-validation-211", "mrqa_searchqa-validation-2399", "mrqa_searchqa-validation-2610", "mrqa_searchqa-validation-2800", "mrqa_searchqa-validation-2816", "mrqa_searchqa-validation-297", "mrqa_searchqa-validation-2997", "mrqa_searchqa-validation-3033", "mrqa_searchqa-validation-3147", "mrqa_searchqa-validation-317", "mrqa_searchqa-validation-3272", "mrqa_searchqa-validation-3357", "mrqa_searchqa-validation-3382", "mrqa_searchqa-validation-4055", "mrqa_searchqa-validation-4127", "mrqa_searchqa-validation-4277", "mrqa_searchqa-validation-4348", "mrqa_searchqa-validation-4831", "mrqa_searchqa-validation-5021", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-5677", "mrqa_searchqa-validation-6239", "mrqa_searchqa-validation-6680", "mrqa_searchqa-validation-6944", "mrqa_searchqa-validation-6981", "mrqa_searchqa-validation-7108", "mrqa_searchqa-validation-716", "mrqa_searchqa-validation-7472", "mrqa_searchqa-validation-7653", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8095", "mrqa_searchqa-validation-8460", "mrqa_searchqa-validation-8659", "mrqa_searchqa-validation-8792", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-932", "mrqa_searchqa-validation-950", "mrqa_searchqa-validation-9686", "mrqa_searchqa-validation-9721", "mrqa_searchqa-validation-9772", "mrqa_squad-validation-10024", "mrqa_squad-validation-10340", "mrqa_squad-validation-1052", "mrqa_squad-validation-1316", "mrqa_squad-validation-1703", "mrqa_squad-validation-199", "mrqa_squad-validation-2564", "mrqa_squad-validation-291", "mrqa_squad-validation-2934", "mrqa_squad-validation-2985", "mrqa_squad-validation-3045", "mrqa_squad-validation-3269", "mrqa_squad-validation-3302", "mrqa_squad-validation-3416", "mrqa_squad-validation-3491", "mrqa_squad-validation-3609", "mrqa_squad-validation-3611", "mrqa_squad-validation-3667", "mrqa_squad-validation-375", "mrqa_squad-validation-4127", "mrqa_squad-validation-4257", "mrqa_squad-validation-436", "mrqa_squad-validation-4630", "mrqa_squad-validation-5185", "mrqa_squad-validation-5230", "mrqa_squad-validation-5456", "mrqa_squad-validation-5504", "mrqa_squad-validation-5999", "mrqa_squad-validation-6698", "mrqa_squad-validation-6787", "mrqa_squad-validation-6958", "mrqa_squad-validation-7165", "mrqa_squad-validation-7252", "mrqa_squad-validation-7458", "mrqa_squad-validation-7701", "mrqa_squad-validation-7850", "mrqa_squad-validation-786", "mrqa_squad-validation-8278", "mrqa_squad-validation-828", "mrqa_squad-validation-8316", "mrqa_squad-validation-8332", "mrqa_squad-validation-8496", "mrqa_squad-validation-90", "mrqa_squad-validation-9087", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1144", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-1186", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-137", "mrqa_triviaqa-validation-138", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1515", "mrqa_triviaqa-validation-1551", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1744", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-1926", "mrqa_triviaqa-validation-2023", "mrqa_triviaqa-validation-2041", "mrqa_triviaqa-validation-2269", "mrqa_triviaqa-validation-2348", "mrqa_triviaqa-validation-2440", "mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-2548", "mrqa_triviaqa-validation-2630", "mrqa_triviaqa-validation-2736", "mrqa_triviaqa-validation-2754", "mrqa_triviaqa-validation-2787", "mrqa_triviaqa-validation-281", "mrqa_triviaqa-validation-2856", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-3442", "mrqa_triviaqa-validation-3736", "mrqa_triviaqa-validation-3744", "mrqa_triviaqa-validation-382", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3943", "mrqa_triviaqa-validation-3966", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4041", "mrqa_triviaqa-validation-4053", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4107", "mrqa_triviaqa-validation-4179", "mrqa_triviaqa-validation-4204", "mrqa_triviaqa-validation-4217", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-4442", "mrqa_triviaqa-validation-4664", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4695", "mrqa_triviaqa-validation-4731", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-503", "mrqa_triviaqa-validation-5072", "mrqa_triviaqa-validation-5074", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-5441", "mrqa_triviaqa-validation-5520", "mrqa_triviaqa-validation-5523", "mrqa_triviaqa-validation-5581", "mrqa_triviaqa-validation-5605", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-594", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5989", "mrqa_triviaqa-validation-6005", "mrqa_triviaqa-validation-6018", "mrqa_triviaqa-validation-6022", "mrqa_triviaqa-validation-6041", "mrqa_triviaqa-validation-6060", "mrqa_triviaqa-validation-6217", "mrqa_triviaqa-validation-6359", "mrqa_triviaqa-validation-6461", "mrqa_triviaqa-validation-66", "mrqa_triviaqa-validation-6848", "mrqa_triviaqa-validation-6863", "mrqa_triviaqa-validation-6942", "mrqa_triviaqa-validation-6949", "mrqa_triviaqa-validation-7049", "mrqa_triviaqa-validation-7058", "mrqa_triviaqa-validation-7082", "mrqa_triviaqa-validation-7120", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7547", "mrqa_triviaqa-validation-7548", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-830", "mrqa_triviaqa-validation-92", "mrqa_triviaqa-validation-936", "mrqa_triviaqa-validation-959", "mrqa_triviaqa-validation-960"], "OKR": 0.8359375, "KG": 0.50390625, "before_eval_results": {"predictions": ["caracas", "Richard Marx", "cats", "lodges", "indiana jones", "moon river", "Malcolm Turnbull", "fish", "Charlie Cairoli", "wild bill Hickok", "Addis Ababa", "jupiter", "Donald Sutherland", "alligators", "holiday inn", "equatorial Guinea", "mexico", "sally", "London Underground Piccadilly Line", "hominins", "(Paul) Gauguin", "Michael Caine", "a hard protective coating", "tennyson", "QPR", "into the Arabian Sea", "\"Raging Bull\"", "yellow", "cedars", "cycling", "24", "Helen Clark", "India", "more than a million members ( including 195,000 youth members )", "1994", "the foreign exchange market (FX )", "October 21, 2016", "drizzle, rain, sleet, snow, graupel and hail", "31 January 1934", "Phil Gallagher", "Clarence Nash", "Cheshire", "September 21, 2014", "\"Stu\" Roosa", "Kew Gardens", "Roger Jason Stone Jr.", "\"Si Da Ming Bu\"", "Bill McCutcheon", "1975", "Hamas,", "the Nazi war crimes suspect", "Adam Sandler, Bill Murray, Chevy Chase and Will Smith.", "MEND", "\"The Swiss art heist follows the recent theft in Switzerland of two paintings by Pablo Picasso,", "South African", "Oxbow,", "San Francisco", "Auguste Rodin", "(Red) Button", "the College of William", "Walt Whitman", "Time", "a pigeon", "quarks"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6812026515151515}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.5, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1946", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-5221", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-2404", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-4464", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-1026", "mrqa_naturalquestions-validation-5564", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-4911", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5146", "mrqa_newsqa-validation-4109", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-1375", "mrqa_searchqa-validation-14615", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-3358", "mrqa_searchqa-validation-14625", "mrqa_searchqa-validation-2959"], "SR": 0.609375, "CSR": 0.54640625, "retrieved_ids": ["mrqa_squad-train-24422", "mrqa_squad-train-9626", "mrqa_squad-train-32758", "mrqa_squad-train-1903", "mrqa_squad-train-33867", "mrqa_squad-train-38806", "mrqa_squad-train-58567", "mrqa_squad-train-29190", "mrqa_squad-train-55480", "mrqa_squad-train-58227", "mrqa_squad-train-4876", "mrqa_squad-train-13692", "mrqa_squad-train-33018", "mrqa_squad-train-13852", "mrqa_squad-train-19494", "mrqa_squad-train-69943", "mrqa_searchqa-validation-15476", "mrqa_newsqa-validation-2668", "mrqa_naturalquestions-validation-9987", "mrqa_hotpotqa-validation-3332", "mrqa_triviaqa-validation-5605", "mrqa_newsqa-validation-4208", "mrqa_searchqa-validation-5564", "mrqa_searchqa-validation-15986", "mrqa_naturalquestions-validation-6289", "mrqa_searchqa-validation-170", "mrqa_newsqa-validation-4109", "mrqa_naturalquestions-validation-923", "mrqa_searchqa-validation-6283", "mrqa_newsqa-validation-1068", "mrqa_searchqa-validation-10066", "mrqa_naturalquestions-validation-7138"], "EFR": 0.96, "Overall": 0.72159375}]}