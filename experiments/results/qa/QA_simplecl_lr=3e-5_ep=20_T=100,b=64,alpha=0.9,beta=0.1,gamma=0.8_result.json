{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=3e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=20.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=3e-5_ep=20_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 8240, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding", "supporting applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "it is unclear as to how or whether this connection is relevant on microscales", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "the Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Heckenberger", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Edgar Scherick", "driving them in front of the army", "business districts", "1726", "lower rates of social goods", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state", "30 July 1891", "Inherited wealth", "the journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "The relationship between some gut flora and humans is not merely commensal ( a non-harmful coexistence )", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "Principal photography began on November 2, 2016", "The song was written by Mitch Murray", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7596511243386244}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.14814814814814817, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07142857142857144, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-5758", "mrqa_squad-validation-10427", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-8662", "mrqa_squad-validation-6070", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.71875, "CSR": 0.734375, "EFR": 1.0, "Overall": 0.8671875}, {"timecode": 2, "before_eval_results": {"predictions": ["magnetic", "photosynthetic function", "Egyptians", "gold", "to fund travelers who would come back with tales of their discoveries", "lung tissue", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Metropolitan Statistical Areas", "European Union law", "monophyletic", "General Teaching Council for Scotland", "biochemical oxygen demand", "ditch digger", "hospitals and other institutions", "gold", "1998", "160 kPa", "General Board of Church and Society, and the United Methodist Women", "without the permission of the U.S. Government", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies", "St. Johns River", "The increasing use of technology", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "against governmental entities", "Anglo-Saxon", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding", "Dornbirner Ach", "combustion chamber", "to be received with thanksgiving by the whole congregation", "104 \u00b0F (40 \u00b0C)", "strict", "the property owner", "southern Europe", "1913", "patient compliance issues", "20th century", "ambiguity", "Silver Bells", "grew between two women", "Abraham Lincoln", "The Sky This Week for September 2 to September 11", "Congolese river", "Basset hound", "Dardanelles", "\"Guilt by Association\"", "City on the south side of the most congested U.S.-Mexico crossing", "the prehistoric and medieval period", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7015527950310559}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9750", "mrqa_squad-validation-2040", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-7013", "mrqa_squad-validation-6244", "mrqa_squad-validation-6753", "mrqa_squad-validation-1108", "mrqa_squad-validation-2296", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.65625, "CSR": 0.7083333333333333, "EFR": 1.0, "Overall": 0.8541666666666666}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor", "Muqali", "inversely to member state size", "if they are distinct or equal classes", "1884", "Isaac Komnenos", "printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "burning a mixture of acetylene and compressed O2", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "Duncan", "an idealized and systematized version of conservative tribal village customs", "Dongshan Dafo Dian", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "rubisco", "The Book of Roger", "", "Africa", "Pierre Bayle", "a variant of Y. pestis that may no longer exist", "32.9%", "30\u201360%", "1368\u20131644", "reciprocating", "Spanish", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "AFC", "Parlophone", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25", "largest publicly documented people to be identified as transgender", "672 km2", "Boston and Maine Railroad's Southern Division", "Dusty Dvoracek", "He is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8116156974153298}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-3456", "mrqa_squad-validation-4019", "mrqa_squad-validation-4631", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_squad-validation-7207", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-5251", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.765625, "CSR": 0.72265625, "EFR": 0.9333333333333333, "Overall": 0.8279947916666667}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant", "reformers", "Modern English", "Commission v Italy", "the West", "1893", "demand for a Scottish Parliament", "1881", "1904", "W. E. B. Du Bois", "25-minute", "captive import policy", "15th century", "two", "two", "pivotal event", "Mexico", "Black Sea", "single output", "The Central Region", "the forts Shirley had erected at the Oneida carry", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "Urarina", "domestic", "force model", "lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "Jason Bourne", "issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "the energy from the flowing hydrogen ions to phosphorylate adenosine diphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "Jean Baptiste Say", "The Perfect Storm", "Terry and June Whitfield", "architecture", "arrows", "moles", "a complex number raised to the zero", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport", "a measure of the relative length of a gun barrel", "neutrons", "James Hoban", "Amelia Earhart", "1963", "clefts", "Asuka", "The United States of America", "Tuesday's iPhone 4S news", "Charles M. Schulz"], "metric_results": {"EM": 0.671875, "QA-F1": 0.705952380952381}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-9154", "mrqa_squad-validation-6197", "mrqa_squad-validation-10251", "mrqa_squad-validation-7537", "mrqa_squad-validation-10466", "mrqa_squad-validation-603", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-873", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.671875, "CSR": 0.7125, "EFR": 1.0, "Overall": 0.85625}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62", "Maciot de Bethencourt", "Spain", "C. J. Anderson", "Cam Newton", "eastwards", "accessory pigments", "his last statement", "Pleistocene", "priority", "Nurses", "time and space", "1951", "Wales", "black earth", "Nederrijn", "opposite end from the mouth", "Hindu and Buddhist", "the mid-sixties", "Kuznets curve hypothesis", "lost chloroplast's existence", "Schr\u00f6dinger equation", "90\u00b0", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "chloroplast", "cotton spinning", "October 2016", "baeocystin", "The Clash of Triton", "Tudor music", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Fred Gwynne", "Jenn Brown", "1999 Odisha", "Fat Albert", "Frontline", "d\u00eds", "Shinola", "modern genetics", "british", "Baryeel Bulasho Guud", "british", "India on Wednesday urged Sri Lanka's Tamil rebels to \"release\" civilians"], "metric_results": {"EM": 0.53125, "QA-F1": 0.597652867965368}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-8312", "mrqa_squad-validation-3575", "mrqa_squad-validation-9176", "mrqa_squad-validation-5450", "mrqa_squad-validation-10386", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.53125, "CSR": 0.6822916666666667, "EFR": 0.9666666666666667, "Overall": 0.8244791666666667}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "an immunological memory", "Calvin cycle", "his eldest son, Zhenjin", "specialised education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "voters were supposed to line up behind their favoured candidates instead of a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "the courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "velocity", "Conservative Party", "international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "preventing it from being cut down", "baptism", "London", "one hundred pennies", "a pioneer in insurance", "Parkinson's", "Tintin", "piu forte (piu f)", "AB", "Geoff Hurst", "McKinney", "Sarek", "Solomon", "Blackstar", "Geomorphology", "Saturn", "krokos", "Richmond", "The Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "Ethiopia", "1973", "The Return of the Pink Panther", "London", "Dave Kelly", "Southaven", "East Java", "Late Registration", "Paul Biya", "trading goods and services without exchanging money"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6047439449917897}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6680", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-10287", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-426", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.5625, "CSR": 0.6651785714285714, "EFR": 1.0, "Overall": 0.8325892857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape Town", "Time", "Rhine-kilometers", "14", "150", "North American Aviation", "to manage the pharmacy department and specialised areas in pharmacy practice", "the Sovereign", "weakness in school discipline", "Fort Caroline", "the concept Distributed Adaptive Message Block Switching", "at elevated partial pressures", "His lab was torn down in 1904", "interacting", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "he signalled his reinvention as a conservative force", "March Battle of Fort Bull", "swimming-plates", "eleven", "it would undermine the law", "1332", "separately from physicians", "in the south", "Geordie", "to store and transmit both gaseous and liquid oxygen", "US$10 a week", "the harvests of their Chinese tenants", "142 pounds (64 kg)", "1806-07", "martin", "a shed", "Bill Clinton", "police car", "martin", "martin", "the Chetniks", "paris", "paris", "Rookwood", "Devil", "paris", "martin", "rowe", "deborah", "martin", "martin", "martin", "Nicholas Skeres", "iPhone", "all right angles are equal", "domenico Colombo", "two nations", "musician", "Korean War", "Federal Reserve", "eight", "Jane Eyre", "World War II", "Hussein's Revolutionary Command Council", "The meter reader", "cowardly lion", "March 22"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5538155284043442}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.2857142857142857, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.1111111111111111, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3109", "mrqa_squad-validation-6324", "mrqa_squad-validation-1404", "mrqa_squad-validation-2097", "mrqa_squad-validation-2434", "mrqa_squad-validation-6773", "mrqa_squad-validation-7961", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_newsqa-validation-858"], "SR": 0.484375, "CSR": 0.642578125, "EFR": 1.0, "Overall": 0.8212890625}, {"timecode": 8, "before_eval_results": {"predictions": ["at a flour mill", "directly every four years", "Alan Turing", "2\u20133 years", "the Working Group chairs", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "the Singing Revolution", "The Newlywed Game", "17th century", "the usual counterflow cycle", "pattern recognition receptors", "severely reduced rainfall and increased temperatures", "Glucocorticoids", "a special episode of The Late Show", "international footballers", "Newcastle Student Radio", "immunoglobulins and T cell receptors", "the City council", "Torchwood: Miracle Day", "November 1979", "in homologous recombination and replication structures similar to bacteriophage T4", "breaches of law in protest against international organizations and foreign governments", "Cobham", "Sir Edward Poynter", "Behind the Sofa", "Ras Dashen", "Florida State University", "the malleus", "Mao Zedong", "Arroz con Leche", "Hawaii", "Kiwanis International", "the log cabin", "Symphony No. 9", "a tornado", "George Sand", "the Z, and this?", "1934", "dizygotic", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "jedoublen/jeopardy", "Meg Cabot", "prosciutto", "Massachusetts", "larynx", "John Galt", "April", "garlic", "the obtuse angle", "Kentucky", "Henry Clay", "the Chinese Exclusion Act", "a fast growth rate", "1953", "Harry Nicolaides", "Mineola", "Blender", "2018\u201319 UEFA Europa League group stage"], "metric_results": {"EM": 0.5625, "QA-F1": 0.652147591991342}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, false], "QA-F1": [0.8, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3373", "mrqa_squad-validation-964", "mrqa_squad-validation-8553", "mrqa_squad-validation-3310", "mrqa_squad-validation-4357", "mrqa_squad-validation-434", "mrqa_squad-validation-8747", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-1263"], "SR": 0.5625, "CSR": 0.6336805555555556, "EFR": 1.0, "Overall": 0.8168402777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["Holyrood area of Edinburgh", "because Dutch law said only people established in the Netherlands could give legal advice", "terra nullius", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Presque Isle", "William S. Paley", "anaerobic bacteria", "more greenish", "eicosanoids and cytokines", "the \"third use of the law.\"", "50-yard line", "he followed the fishermen and captured the mermaid", "1/6", "DC traction motor", "the \"richest 1 percent in the United States now own more wealth than the bottom 90 percent\"", "the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops", "1882", "Mel Jones", "North America", "Sachin Tendulkar and Kumar Sangakkara", "Swadlincote and 6 miles south of Burton upon Trent", "inversely proportional to the wave frequency", "flytrap", "last Ice Age", "Anna Faris", "2026", "Georgia", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "1984 Summer Olympics in Los Angeles", "4 September 1936", "the forces of Andrew Moray and William Wallace", "Spencer Drever as Cyrus Rivera", "Pangaea", "Have I Told You Lately ''", "sinoatrial node", "fourth quarter of the preceding year", "2013 non-fiction book of the same name by David Finkel", "to prevent further offense by convincing the offenders that their conduct was wrong", "Bob Dylan", "September of that year", "Judicial discretion", "Lynda Carter", "virtually no limit to the number of reads from such flash memory", "A substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Dolph Lundgren -- He - Man   Frank Langella", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II", "People Against Switching Sides (PASS)", "carbon", "Billy Budd, Billy Budd", "hearing"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6668012930301319}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true], "QA-F1": [0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 0.0, 0.14814814814814814, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8372093023255813, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.1142857142857143, 1.0, 1.0, 0.8, 0.4, 0.5, 1.0, 1.0, 1.0, 0.15999999999999998, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9304", "mrqa_squad-validation-9764", "mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-2443", "mrqa_squad-validation-805", "mrqa_squad-validation-7459", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-15030"], "SR": 0.546875, "CSR": 0.625, "EFR": 0.9310344827586207, "Overall": 0.7780172413793103}, {"timecode": 10, "UKR": 0.662109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.833984375, "KG": 0.45078125, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla in his honor", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter Tesla", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "Yosemite Freeway/Eisenhower Freeway", "Switzerland", "unit-dose, or a single doses of medicine", "a corporate alternating current/direct current \"War of Currents\" as well as various patent battles.", "a \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood.", "continental European countries", "Roger Goodell", "festivals", "9 venues", "Adelaide", "once", "Around 200,000 passengers", "itty Hawk", "Nidal Hasan", "the University of Maryland", "report against fascism, antisemitism, and political corruption in 1940s Boston", "the role of Zander in Michael Damian's film, High Strung: Free Dance.", "Consigliere of the Outfit", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "a 1992 American action-thriller film directed by Andrew Davis and written by J.F. Lawton", "Nelson Rockefeller", "Fort Snelling, Minnesota", "James Gay-Rees", "at the State House in Augusta", "1970", "1978", "Barack Obama", "My Cat from Hell", "Richard B. Riddick", "Colonel", "1999", "17", "La Liga", "Buffalo Soldier", "Kal Ho Naa Ho", "Key West, Florida", "the gastrocnemius muscle", "John Roberts", "repechage", "Jean Bernadotte", "two", "Madonna", "Freddie Mercury", "the Sousa Band"], "metric_results": {"EM": 0.625, "QA-F1": 0.7351128472222223}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false], "QA-F1": [1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.6, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333334, 0.375, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.16666666666666669, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-1491", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-6526", "mrqa_squad-validation-1180", "mrqa_squad-validation-9578", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_searchqa-validation-4509"], "SR": 0.625, "CSR": 0.625, "EFR": 1.0, "Overall": 0.714375}, {"timecode": 11, "before_eval_results": {"predictions": ["the pr\u00e9tendus r\u00e9form\u00e9s", "587,000", "Bishopsgate", "Mnemiopsis", "from tomb and memorial, to portrait, allegorical, religious, mythical, statues for gardens including fountains, as well as architectural decorations", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "four", "308", "Queen Victoria", "huge", "the main opposition party, the Orange Democratic Movement", "Charlesfort", "adapt", "Battle of the Restigouche", "Boston", "force of gravity", "head writer and executive producer", "David Lynch", "a psychologist", "every year", "Sir Arthur", "blood", "Victoria Harbor", "ambidevous", "The Joker", "a horse", "Irrawaddy River", "Ed White", "River Hull", "Tet", "Samuel Johnson", "Copenhagen", "Troy", "Human Rights Organization", "John Gorman", "European Bison", "Edinburgh", "Viking", "Paul Gauguin", "Action Comics", "Enigma", "change from a liquid", "Nadal", "New Zealand", "Oasis", "The Golden Girls", "green, red, white", "Rajasthan", "the Beatles", "floating ribs", "The G8 summit is an annual meeting between leaders from eight of the most powerful countries in the world", "golf", "The current Secretary of Homeland Security", "the Bee Gees", "Adelaide", "Eddie Izzard", "Sabina Guzzanti", "a $13 million global crime ring", "a quark", "neptune", "one of the most common words"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6357638888888889}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, false, true, false, false, false, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-9255", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.5625, "CSR": 0.6197916666666667, "EFR": 0.9642857142857143, "Overall": 0.7061904761904761}, {"timecode": 12, "before_eval_results": {"predictions": ["pulmonary fibrosis", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "peace with Israel", "$5 million", "Lake George", "the Jadaran", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australia", "September 1901", "The United States of America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "Viola Larsen", "Omega SA", "September 14, 1877", "A third jersey, alternate jersey, third kit or alternate uniform is a jersey or uniform that a sports team wear in games instead of its home outfit or its away outfit", "Yasir Hussain", "Malayalam", "Kennedy Road", "2002", "31", "Grant Field", "Bill Boyd", "Jack Ryan", "Emilia-Romagna Region", "Buckingham Palace", "322,520", "executive", "Dave Lee Travis", "Bedknobs and Broomsticks", "Jane", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "The 2000 PGA Championship", "Revolver", "Jack Nicklaus", "atransformation of the Greek \u03bc\u03b5\u03c4\u03ac\u03bd\u03bf\u03b9\u03b1", "Mussolini", "joseph care less about his money", "off the coast of Dubai", "1918", "butter", "Boston", "rain"], "metric_results": {"EM": 0.671875, "QA-F1": 0.726366341991342}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.13333333333333333, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-6108", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-278", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-4320", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-1893", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790"], "SR": 0.671875, "CSR": 0.6237980769230769, "EFR": 1.0, "Overall": 0.7141346153846153}, {"timecode": 13, "before_eval_results": {"predictions": ["the riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi", "a \"creative plea,\" and will usually be interpreted as a plea of not guilty.", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "$20.4 billion, or $109 billion in 2010 dollars", "twelve residential Houses", "Anglo-Saxons", "The Christmas Invasion", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight General Education categories", "\"Of course [the price of oil] is going to rise... Certainly! And how!", "Terra nullius", "Henry Hudson", "chipmunk", "David II BRUCE", "Melbourne", "Albania", "brown trout", "Mayflower", "Tarzan", "lacrimal fluid", "George Best", "A-bomb", "The Great British Bake Off", "Red Lion", "Fenn Street School", "Smiths", "Peter Crouch", "The Nobel Prize in Literature", "Pakistan", "The Observer", "the United States", "Big Fat Gypsy Wedding", "a beard", "Andes", "Thor", "The Comitium", "Moon River", "Tina Turner", "SW19", "Lancashire", "The Pacific Ocean", "stockbridge Indians", "Rustle My Davies", "climatic types", "Charlie Brown", "vinaya", "avocado", "Black Sea", "lactic acid", "1933", "The episode typically ends as a cliffhanger showing the first few moments of Sam's next leap", "Abu Dhabi, United Arab Emirates", "Craig William Macneill", "terminal brain cancer", "800,000", "cinder cones", "giant slalom", "Serie B", "Saoirse Ronan"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6790997023809524}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.16, 0.1111111111111111, 0.4, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3863", "mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-3953", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315"], "SR": 0.59375, "CSR": 0.6216517857142857, "EFR": 1.0, "Overall": 0.7137053571428571}, {"timecode": 14, "before_eval_results": {"predictions": ["the Cathedral of Saint John the Divine", "The Ruhr", "Hulu", "the time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "the Magnetophon tape recorder", "he explored the mountains in hunter's garb", "Rotterdam", "retracted", "Charles Dickens", "force", "best teachers", "imperfect", "albatross", "a whirl wind of energy", "have passed", "The National Gallery of Art", "Portland", "water", "Solferino", "netherlands", "a irrational number", "turkeys", "Martha Graham", "lionhead", "William", "Asian parrot", "a light-year", "netherlands", "Don", "How the firebrand Shi'ite cleric became a major power broker", "Prince of Wales", "cocoa butter", "Violent Femmes", "oats", "a guardian angel", "laser", "James Fenimore Cooper", "Veep", "brilliant", "Rudyard Kipling", "superhero", "panda", "Copenhagen", "Madonna", "Jose de San", "Madrid", "a pirate", "russell", "Rocky Mountain", "a human rat-trap", "fertilization", "India occupies 2.4 % of the world's land surface area", "Nissan", "a menorah", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "Acura", "Heather Haversham"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5352678571428571}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7610", "mrqa_triviaqa-validation-7170"], "SR": 0.484375, "CSR": 0.6125, "EFR": 0.9696969696969697, "Overall": 0.7058143939393939}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "paramagnetic", "criminal", "complexity classes", "April 1, 1963", "Jamukha", "consultant", "711,988", "Agra Cantonment - H. Nizamuddin Gatimaan Express", "Speaker of the House of Representatives", "Hugo Weaving", "the Latin word autumnus", "The Dursley family", "Brobee", "the somatic nervous system and the autonomic nervous system", "Shruti Sharma", "Jethalal Gada", "Kevin Sumlin", "clay", "the first to develop lethal injection as a method of execution", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Emma Watson", "the inmates have been detained indefinitely without trial", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Incursio", "December 15, 2017", "the Sunni Muslim family", "the Magnavox Odyssey", "The Buckwheat Boyz", "Christianity", "India", "The neck", "between 1923 and 1925", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "chloroplast stroma", "San Francisco, California", "Hal Derwin", "exercise general oversight", "2007", "~ 0.058 - 0.072 mm", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "coach", "Akshay Kumar", "Harriet M. Welsch", "Bananas", "(temperature)", "a vase"], "metric_results": {"EM": 0.5, "QA-F1": 0.5840036699411699}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.1818181818181818, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 0.13333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.30769230769230765, 0.5, 0.8, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5454545454545454, 0.5, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-8385"], "SR": 0.5, "CSR": 0.60546875, "EFR": 1.0, "Overall": 0.71046875}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n", "National Broadcasting Company", "the Venetian merchant Marco Polo", "November 2006 and May 2008", "quite complex", "temperatures that are too cold in northern Europe for the survival of fleas", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the leader of the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies", "Montmorency", "\"Brings out the tiger in you, in you!\"", "Elton John", "beer", "David Davis", "a double dip recession", "Corfu", "the main vein of a leaf", "Leopold II of Belgium", "8 minutes to reach the Earth", "the National Industrial Conference Board", "four red stars with white borders to the right", "Cyclops", "oxygen", "Silent Spring", "the value of unknown electrical resistance", "white spirit", "berry-like drupes that ripen from reddish-pink to blue", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "a meteoroid", "West Point", "ostrich", "Moby Dick", "William Golding", "Fret the 5th fret on the 6th String", "The Runaways", "Kim Clijsters", "Les Dennis", "the A38", "Nicola Walker", "Richard Branson", "1948", "Port Talbot", "a geologic episode, change, process, deposit, or feature that is the result of the action or effects of rain", "Famous Epitaph", "Nicola Adams", "Sax Rohmer", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "Viscount Barnewall", "the Obama administration on June 12 announced a task force devoted to federal ocean planning", "The man ran away, police chased him and a gunfight ensued", "the stratosphere", "I Don't Want to Miss a Thing", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5258223195578072}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, false, true, false], "QA-F1": [0.6666666666666666, 0.3, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3157894736842105, 1.0, 0.5, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4, 0.0, 0.5, 0.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.2666666666666667, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-8105", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-162", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-3606", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1537", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-12952", "mrqa_hotpotqa-validation-4298"], "SR": 0.4375, "CSR": 0.5955882352941176, "EFR": 1.0, "Overall": 0.7084926470588235}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "the deportation of the French-speaking Acadian population from the area", "journalist", "Seventy percent", "modern hatred of the Jews", "Germany and Austria", "the principle of inclusions and components", "Sweynforkbeard", "the King", "eight", "the Sierra Freeway", "Mickey Mouse", "rugby school", "Spain and Portugal", "may", "Google", "dance", "born into Brothels", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "black peppers", "Virginia Woolf", "Vasco de Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial", "white", "Conan O'Brien", "Ohio State", "gwalior", "Sam Ervin", "Other Rooms", "blackhearts", "black Forest", "Roger B. Smith", "jopardyQ Bot - Reddit", "sepoys", "best", "2002", "submarines", "Joan", "pea soup", "Trinidad and Tobago", "Vladimir Nabokov", "Oreo", "Tinker Bell", "compound", "a laser beam", "Phi Beta Kappa", "Elizabeth Weber", "the angel", "Switzerland", "Prince Philip", "Cleopatra VII Philopator", "5.3 million", "pilot", "Lance Cpl. Maria Lauterbach", "may", "The Clash", "paper"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5348958333333332}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4260", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_naturalquestions-validation-3284", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-6259", "mrqa_hotpotqa-validation-5872", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-4426"], "SR": 0.4375, "CSR": 0.5868055555555556, "EFR": 1.0, "Overall": 0.706736111111111}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "the northern (German) shore of the lake", "Alfred Stevens", "domestic social reforms could cure the international disease of imperialism", "algebraic aspects", "third", "1886/1887", "clerical", "Apollo", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "a schoolteacher", "Dunlop", "Martin O'Neill", "a family member", "Tranquebar", "Attorney General and as Lord Chancellor of England", "North Dakota", "fennec", "Norwood, Massachusetts", "1993", "the 10-metre platform event", "liquidambar", "the Battle of Chester", "Flashback", "Tennessee", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "Clark Gable", "Inklings", "the paternalistic policies enacted upon Native American tribes", "The Hindu Group", "Kealakekua Bay", "1919", "Shakespeare", "2013", "Guthred", "Centers for Medicare & Medicaid Services", "South Australia", "1912", "1912", "La Scala, Milan", "How to Train Your Dragon", "pronghorn", "United States ambassador to Ghana", "I'm Not in Love", "Pete's", "Sir Ernest Rutherford", "(Darling)", "Willy", "France", "at least $20 million to $30 million", "(Ulysses) Grant", "(Virgil) Tibbs", "President Obama ordered the eventual closure of Guantanamo Bay prison", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.5, "QA-F1": 0.6115507756132756}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.28571428571428575, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.9090909090909091, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.0, 0.8, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-9888", "mrqa_squad-validation-8026", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_hotpotqa-validation-4126", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.5, "CSR": 0.5822368421052632, "EFR": 0.96875, "Overall": 0.6995723684210526}, {"timecode": 19, "before_eval_results": {"predictions": ["one in five", "\"ctenes\" or \"comb plates\"", "MHC I", "Denver's Executive Vice President of Football Operations and General Manager", "10 times", "Manhattan's lower east side", "Time magazine", "Stan Lebar", "Warszawa", "Troggs", "schizophrenia", "Cressida", "Tom Osborne", "Moses", "Pekingese dog", "Golda Meir", "Fiddler on the Roof", "Monopoly", "Al Czervik", "Leszczyski", "mask", "Alien", "Tower of London", "reptiles", "Cher", "onion", "Walter Emmons Alston", "Benazir Bhutto", "Coca-Cola", "Red Bull", "Chaillot", "Ibrahim Hannibal", "butter", "grow a beard", "Soup Nazi", "Pyrrhic victory", "Guatemala", "treasury bonds", "Edgar Allan Poe", "rice de Pimientos de Piquillo", "August Wilson", "Sacher Torte", "apartheid", "descend", "Lovebird", "Leonardo DiCaprio", "flavours", "Daisy Miller", "calculators", "Give Me Liberty or Give Me Death", "Frank Sinatra", "Sonnets", "South Africa", "USS", "the Infamy Speech of US President Franklin D. Roosevelt", "Madrid-Barajas, Barcelona and Palma de Mallorca", "River Stour", "Annales de chimie et de physique", "gull-wing doors", "In denying the show's producers the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "rape her in a Milledgeville, Georgia, bar during a night of drinking in March", "1994", "state senators", "38"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5184484649122807}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.9473684210526316, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-375", "mrqa_squad-validation-167", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_triviaqa-validation-6387", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406"], "SR": 0.46875, "CSR": 0.5765625, "EFR": 1.0, "Overall": 0.7046875}, {"timecode": 20, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.8359375, "KG": 0.45859375, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "Muslims in the semu class", "The John W. Weeks Bridge", "9th", "the clinical pharmacy movement initially began inside hospitals and clinics", "US$3 per barrel", "Trajan's Column", "in the Indian Ocean", "Victoria Rowell", "Golda Meyerson", "xerophyte", "anions", "Uranus", "George III", "Frank Morrison Spillane", "O'ahu, Hawaii", "Gandalf", "Mungo Park", "squash", "in the 1970s comedy Dad's Army", "magnetite", "Sam Mendes", "Ciudad Ju\u00e1rez", "Emeril Lagasse", "Peter", "Karl Marx", "an ornamental figure or illustration", "four and a half hours", "norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Zephyrus", "Frobisher Bay", "Dumbo", "Michael Angelo Titmarsh", "Botany Bay", "Peterborough United", "FC Porto", "albedo", "11", "Washington, D.C.", "red", "gravity", "Brainy", "Andrew Nicholson", "Prince Eddy", "Algeria", "Spain", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Peary", "Simon & Garfunkel", "Alan Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5893229166666667}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-4621", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.53125, "CSR": 0.5744047619047619, "EFR": 1.0, "Overall": 0.7105059523809524}, {"timecode": 21, "before_eval_results": {"predictions": ["1349", "the center of mass", "July 23, 1963", "60's", "James E. Webb", "eight", "foreclosure", "the ninth major version of Flash", "February 6, 2005", "the 1950s", "159", "inherit his entire fortune and the corporation", "the state of Odisha", "1975", "John Vincent Calipari", "winter solstice", "King", "Robert Hooke", "insoluble compounds", "October 30, 2017", "to avoid the inconvenienceiences of a pure barter system", "four", "mexico", "Lykan", "in the pachytene stage of prophase I of meiosis", "St. Mary's County", "Once Upon a Time in India", "Hank J. Deutschendorf II", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "Dan Stevens", "The Canterbury Tales", "May 19, 2008", "Albert Einstein", "May 26, 2017", "1992", "married to Bobby", "Master Christopher Jones", "to solve its problem of lack of food self - sufficiency", "Baron Vaughan as Nwabudike `` Bud '' Bergstein", "Spanish", "the bloodstream or surrounding tissue", "Thomas Bernard", "Fox Ranch in Malibu Creek State Park", "Gibraltar", "Dmitri Mendeleev", "declared neutrality", "31", "the disputed 1824 presidential election", "12", "local organization of businesses whose goal is to further the interests of businesses", "for control purposes", "twelve", "Paige O'Hara", "ghee", "The Crow", "Michael Schumacher", "micronutrient-rich", "parenthood", "top designers", "Heathrow", "gold", "king anderson", "Lee Harvey Oswald", "liver"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5496400564197584}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1111111111111111, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.7058823529411764, 0.0, 0.0, 0.6666666666666666, 0.7368421052631579, 0.0, 0.625, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.125, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2658", "mrqa_squad-validation-1449", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-1003", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-2143", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-4874", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_hotpotqa-validation-4181", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.453125, "CSR": 0.5688920454545454, "EFR": 0.9714285714285714, "Overall": 0.7036891233766234}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the western end of the second east-west shipping route", "Melanie Griffith", "the fowls", "lexicographer", "the Islamic Republic of Iran", "One Flew Over the Cuckoo's Nest", "a sauce", "Anne of Cleves", "Harpers Ferry", "Confeitaria Colombo", "a people who travel as pilgrims to Canterbury", "Versailles", "Target", "the meadow katydid", "the land", "the middleweight champion", "magnesium", "South Carolina", "the Union", "a German Shepherd", "peanuts", "Bhutan", "the Parker House", "Damascus", "the Jennies", "a hologram", "Dharma & Greg", "the 1906 earthquake", "Genoa and Lucca", "the Scuppernong grape", "Virginia Woolf", "apogee", "Cherry Garcia", "the book", "Diamond Jim Brady", "an axiom", "Princeton University", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T.S. Eliot", "the Andes", "the Diamonds", "asteroids", "the Nutcracker", "an earthquake", "the Conservative Party", "the 1996 World Cup of Hockey", "grated cheese", "Falstaff", "red hair", "the Republican Party", "Wojbek (bear)", "Bangor Air National Guard Base", "2009", "skin-tight black trousers", "12-hour-plus shifts of backbreaking labor, virtually zero outside recognition, and occasional accusations of being shills for the timber industry rewards", "end her trip in Crawford"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5462549603174602}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.22222222222222224, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 0.3333333333333333, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.1, 0.16666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_squad-validation-9310", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-1553", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-7685", "mrqa_triviaqa-validation-3110", "mrqa_hotpotqa-validation-1856", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-403"], "SR": 0.46875, "CSR": 0.5645380434782609, "EFR": 1.0, "Overall": 0.7085326086956522}, {"timecode": 23, "before_eval_results": {"predictions": ["the Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "49\u201315", "the 21st century", "on a lifeboat off the coast of Somalia", "republicans", "18", "Adidas", "she was going to be on the Olympic medals podium.", "the body of the aircraft", "18th century tapestries", "the United States", "Michigan", "on websites on the 24th.", "two", "Russia", "The Tinkler", "$106,482,500", "Tuesday.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "eight in 10", "tennis", "Toy Story", "Christmas parade", "90", "involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "on the 12th on the Blue Monster course at Doral", "Jeffrey Jamaleldine", "82mm mortars", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "more than 1.2 million people", "the former Massachusetts governor", "republicans", "mother's Day", "on a dangerous stretch of Highway 18 near Grand Ronde, Oregon.", "Seasons of My Heart", "raping and murdering a woman in Missouri.", "150", "Anil Kapoor", "misdemeanor assault charges", "reiterated the Vatican's policy on condom use as he flew from Rome to Yaounde, the capital of Cameroon.", "it's often no way of knowing every single ingredient that went into it, or what else touched the plate and utensils used to serve it.", "Martin Aloysius", "a model of sustainability", "Kenyan", "it will be up to each nation to \"design its own separate strategies for making progress toward achieving this long-term goal.", "motor bike accident", "the Isthmus of Corinth", "Gavin DeGraw", "Old Trafford", "Siddhartha", "Kristina Brown", "Shayne Ward", "Christina Ricci", "Floyd Nathaniel \"Nate\" Hills", "Hong Kong", "honey", "the 1860", "Melbourne"], "metric_results": {"EM": 0.375, "QA-F1": 0.480832013166572}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.2222222222222222, 0.0, 0.0, 1.0, 0.16, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.9411764705882353, 0.4, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.4615384615384615, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.07142857142857142, 0.4, 1.0, 0.0, 0.13333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-233", "mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-5398", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.375, "CSR": 0.556640625, "EFR": 1.0, "Overall": 0.706953125}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886.", "the AKS primality test", "Deficiencies existed in Command Module design, workmanship and quality control.", "Gold footballs", "1967", "Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert Allen Iger", "Regional League North", "The 2002 United States Senate election in Minnesota", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Taipei City", "Minneapolis", "Idisi", "Comte Armand de Castan", "Nicholas Andre Pinnock", "May 4, 2004.", "Everything Is wrong", "Royal Navy rank of Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "imp My Ride", "Columbia Records.", "Umar S. Israilov", "Derry City F.C.", "Fort Hood, Texas", "Port Macquarie", "London", "1999", "2006", "scorer", "Samuel Joel \" Zero\" Mostel", "October 13, 1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Morozov", "1968", "Berthold Heinrich K\u00e4mpfert", "Girl Meets World", "January 15, 1975", "Pansexuality", "leopard", "2,463,431", "Paul's", "her castle", "Pyeongchang County, South Korea", "Celts", "Rudolph", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values.", "Jay Gillespie", "glinda", "Persian Gulf"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6808808379120879}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 0.25, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2553", "mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-3210", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3909", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-855", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_triviaqa-validation-7434", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010", "mrqa_searchqa-validation-5368"], "SR": 0.578125, "CSR": 0.5575, "EFR": 1.0, "Overall": 0.707125}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "If the water level drops, such that the temperature of the firebox crown increases significantly", "Northern Rail", "South Korea", "botulism", "golf", "Romania", "Pocahontas", "Matlock", "Washington", "Argentina", "The Blue Boy", "the book Three Worlds", "Liriope", "Creation", "Pennsylvania", "Pyrenees", "(See Important Quotations Explained)", "Dutch", "Dramatizing History in Arthur Miller's \"The Crucible\"", "Gryffindor", "Sam Allardyce", "Olympics", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Sean", "Superman: The Movie", "Richard Walter Jenkins", "(b\u0259r\u02c8ki n\u0259 \u02c8f\u0251 so\u028a)", "Billy Cox", "Javier Bardem", "Independence Day", "baryons", "Jordan", "So Solid Crew", "John Regis", "Magi", "albult", "(BS)", "Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd Moss", "The Book", "Poland", "Play style", "Guerrero", "albion", "Authority", "1 mile ( 1.6 km )", "Steve Valentine", "Out of Control", "Virgin", "UFC Fight Pass", "the Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "(PIE)", "Earhart", "Final Cut Pro"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6269248188405797}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.9565217391304348, 0.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_hotpotqa-validation-5822", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-12261", "mrqa_searchqa-validation-5324"], "SR": 0.546875, "CSR": 0.5570913461538461, "EFR": 0.9655172413793104, "Overall": 0.7001467175066313}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "a nobleman who owned a village located at the modern-day site of Mariensztat neighbourhood", "the disk", "2016", "the Islamic prophet Muhammad", "Mel Tillis", "Pangaea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "eight years after an amendment increased the tenure length by two years", "Sue Perkins", "Mutt Lange", "Scottish post-punk band Orange Juice", "a photodiode", "September 9, 2010, at 8 p.m. ET", "Jesse Frederick James Conaway", "dromedary", "Dan Stevens", "Ben Fransham", "darkspawn", "1979", "October 27, 2016", "Authority", "wisdom, understanding, counsel, fortitude, knowledge, piety, and fear of the Lord", "Luther Ingram", "Jodie Foster", "Matt '' Camden", "Sanchez Navarro", "long sustained period of inflation", "1936", "British Columbia, Canada", "New York University", "2007", "2001", "Washington Redskins", "Hebrew Bible", "September 14, 2008", "Consular Report of Birth Abroad for children born to U.S. citizens", "Pasek & Paul and the book by Joseph Robinette", "the Chicago metropolitan area in 1982", "Francisco Pizarro", "1940", "the Norman given name Robert", "Mary Rose Foster", "John Smith", "The eighth and final season", "1603", "neutrality", "he cheated on Miley", "banjo", "rarities", "Rocky and Bullwinkle", "Taylor Swift", "lamas hip hop group Odd Future", "Michael Crawford", "$22 million", "14-day", "flooding", "pisco", "david", "fiscus"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5283952975129446}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.16666666666666669, 0.125, 0.0, 0.5, 0.0, 0.5, 1.0, 1.0, 0.35294117647058826, 0.33333333333333337, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.12500000000000003, 0.4444444444444445, 0.7499999999999999, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-1258", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-474", "mrqa_naturalquestions-validation-8727", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-5461"], "SR": 0.4375, "CSR": 0.552662037037037, "EFR": 0.9722222222222222, "Overall": 0.7006018518518518}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions, demonstrating Luther's concern to clarify and strengthen the text and to provide an appropriately prayerful tune", "mathematical models of computation", "Best Actor", "around 300", "an anvil", "1999 to December 31, 2007", "Larry Richard Drake", "the first to recognise that the machine had applications beyond pure calculation, and created the first algorithm intended to be carried out by such a machine", "London's West End", "she is generally considered to have liberal political views", "Hanford Site", "Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "Blackstone", "Ginger Rogers", "The Secret Garden", "churros", "Christies Beach", "Odisha", "Arsenal of England and Parma of Italy", "Don Bluth and Gary Goldman", "torpedo boats", "1969 until 1974", "the Swiss tourism boom", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defender", "Henry J. Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube channel", "Daniel Andre Sturridge", "USS Essex", "Ron Cowen and Daniel Lipman", "Fordyce", "Captain while retaining the substantive rank of Commodore", "Andrea Maffei", "Andrzej Go\u0142ota and Tomasz Adamek", "Russell T Davies", "Geraldine Page", "Manchester, England", "3,000", "Umberto II", "Minnesota to the west, and Wisconsin", "Daphnis et Chlo\u00e9", "saloon-keeper and Justice of the Peace", "John Lennon", "The International Imitation Hemingway Competition", "The Emperor of Japan", "Billy Ray ( Harry Dean Stanton )", "1986", "transposition from one scale to another for various purposes, often to accommodate the range of a vocalist", "golden wedding anniversary", "Australia", "brain injury", "denied the claim", "Amanda Knox's aunt", "100,000", "brandy", "the Sgt. Pepper album", "North Carolina"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5478692518674136}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true], "QA-F1": [0.2222222222222222, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.25, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5882352941176471, 1.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.2222222222222222, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2347", "mrqa_squad-validation-7819", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-5340", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-3860", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.4375, "CSR": 0.5485491071428572, "EFR": 1.0, "Overall": 0.7053348214285714}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "common flagellated", "Mildred", "Adidas", "Secretary of State Hillary Clinton", "billions of dollars", "one", "the Beatles", "Communist Party of Nepal", "Pope Benedict XVI", "around 8 p.m. local time Thursday", "Sri Lanka\\'s Tamil rebels", "64", "CNN", "within a few months", "Mike Griffin", "Adriano", "he eventually gave up 70 percent of his father-in-law\\'s farm,", "183", "American Civil Liberties Union", "deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan.", "40 militants and six Pakistan soldiers dead", "Aldgate East", "137", "8", "Congressional auditors", "Jacob", "South Africa", "Markland Locks and Dam", "4,000", "Seasons of My Heart", "provided Syria and Iraq 500 cubic meters of water", "Catholic League", "August 19, 2007.", "10 years", "All three pleaded not guilty", "Japan", "her children \"have no problems about the school, they are happy about everything.\"", "consumer confidence", "he was mad at the U.S. military because of what they had done to Muslims", "six", "nearly 28 years", "Friday night", "Dan Brown", "digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "agreed to cooperate with interrogators", "along the Chao Phraya River", "two", "an antihistamine and an epinephrine auto-injector", "4,000", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia.", "Jean F Kernel", "around 10 : 30am", "Johannes Gutenberg", "crossword puzzle", "Christian Wulff", "Ambroz Bajec-Lapajne", "general secretary", "the George Washington Bridge", "the Atlanta Athletic Club", "wasteland", "Aristotle\\'s lantern", "Colombia\\'s canned tuna"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6205696163303072}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.5454545454545454, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.923076923076923, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2, 0.9473684210526316, 1.0, 1.0, 0.6666666666666666, 0.888888888888889, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 0.5, 0.32, 0.4, 0.0, 1.0, 1.0, 1.0, 0.125, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-892", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-103", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-12506", "mrqa_searchqa-validation-4780"], "SR": 0.484375, "CSR": 0.5463362068965517, "EFR": 1.0, "Overall": 0.7048922413793103}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,004,721", "anchovy", "lovebirds", "Chicago", "monk seal", "Wilhelm", "british", "Take Me Out to the Ballgame", "bach", "\"What hath God wrought\"", "Stewart Island", "St. Erasmus", "Little Tommy Tucker", "H. G. Wells", "Holstein cow", "faded print", "Scrabble", "Mussolini", "Valkyries", "rain", "brisket", "Jodie Foster", "Elysian Fields", "Five Easy pieces", "Thomas Edison", "Manhattan", "Charles", "divorce", "Enchanted", "Liberty Bell", "USB", "Autobahn", "Destiny's Child", "Canto", "a spoonful", "cortisone", "Margot Fonteyn", "Coral", "Bones and Castle", "John F. Kennedy", "\"77 Sunset Strip\"", "Galileo Galilei", "Existentialism", "John Donne", "juba", "Annies", "human", "Charles Lindbergh", "a queen", "synaptic vesicles", "Vatican City", "James W. Marshall at Sutter's Mill in Coloma, California", "a single, implicitly structured data item", "Brazil", "\"Wales\"", "Monster M*S*H TV series", "Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "jenny Sanford,", "HPV"], "metric_results": {"EM": 0.453125, "QA-F1": 0.571780303030303}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 1.0, 0.0, 0.5, 1.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.8333333333333333, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9015", "mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-8878", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-15059", "mrqa_searchqa-validation-7230", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-11420", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-14999", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-1353", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1372"], "SR": 0.453125, "CSR": 0.5432291666666667, "EFR": 1.0, "Overall": 0.7042708333333333}, {"timecode": 30, "UKR": 0.671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.83984375, "KG": 0.478125, "before_eval_results": {"predictions": ["30 November 1963", "1892", "motivated students,", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Argentina", "the Baudot code", "Jacksonville", "DTM and its successor \u2014 the International Touring Car Championship", "Switzerland", "Maryland", "eastern Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres from Adelaide station", "1 December 1948", "omnisexuality", "Tea Tree Plaza", "southwest Denver, Colorado near Bear Creek", "Atlanta, Georgia", "Atlanta Braves", "Scunthorpe", "2004", "Donald McNichol Sutherland", "Towards the Sun", "south (Dolomitic)", "James Brayshaw", "An impresario", "Islamic philosophy", "January 30, 1930", "Sulla", "Female Socceroos", "Jaguar Land Rover", "tempo", "Milk Barn Animation", "Jenson Alexander Lyons Button", "Timothy Dowling", "London", "Jane", "Patricia Arquette", "Otto Robert Frisch", "AMC", "thirteen", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins Sr.", "Ben Campbell", "twenty-three", "Gararish", "P. C. Sreeram", "Akosua Busia", "September 8, 2017", "volcanic activity", "Burbank, California", "a horse's life is often difficult", "Heisenberg", "Kiel Canal", "they couldn't accept an offer from the Southeastern Pennsylvania Transportation Authority because of a shortfall in their pension fund and disagreements on some work rule issues.", "Eintracht Frankfurt", "Democrat", "p puppies", "Nickelback", "Will & Grace"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5971440018315017}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.28571428571428575, 1.0, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 1.0, 0.3076923076923077, 0.0, 0.6666666666666666, 1.0, 0.3571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7744", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-4290", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730"], "SR": 0.484375, "CSR": 0.5413306451612903, "EFR": 1.0, "Overall": 0.706234879032258}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Clara Petacci", "2007", "Marko Tapani", "Shankar", "Cody Miller", "\"Grimjack\" (from First Comics)", "the horror genre", "Carson City", "\"The Chris Rock Show\"", "Mickey's Christmas Carol", "San Antonio", "Bergen County", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Professor Frederick Lindemann", "Rawhide", "the Military Band of Hanover", "Don DeLillo", "The Seduction of Hillary Rodham", "Balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Whitesnake", "Abdul Razzak Yaqoob", "Bonny Hills", "Dan Castellaneta", "Roseann O'Donnell", "Saturday", "Taylor Swift", "Miller Brewing", "The Arizona Health Care Cost Containment System", "Indianapolis Motor Speedway", "Clark County, Nevada", "Tampa Bay Storm", "Jango Fett", "the High Court of Admiralty", "\"An All-Colored Vaudeville Show\"", "Lutheranism", "Lucy Muringo Gichuhi", "Valley Falls", "a dice game in which the players make wagers on the outcome of the roll, or a series of rolls, of a pair of dice", "Nicholas \" Nick\" Offerman", "Jewish", "JackScanlon", "Leonard Bernstein", "19", "France", "boric acid", "secretary", "eight", "a nuclear weapon", "2005", "the Beastie Boys", "Madison", "a car"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6339488636363637}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2727272727272727, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-4489", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3276", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-534", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-593", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-728", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-3737", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.53125, "CSR": 0.541015625, "EFR": 1.0, "Overall": 0.706171875}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "ill Death Us Do Part", "Laputa", "Leeds", "\"Colonel\" Thomas Andrew \"Tom\" Parker", "LSD", "Stephen", "Albania", "Tombstone", "Charlie", "Jaguar Land Rover", "Diego Maradona", "Sudan", "bubba", "football", "multiplayer online role-playing game", "fondu", "South Africa", "1934", "Steve Coogan", "Sophie Marceau", "Boston Marathon", "Carl Smith", "Humble pie", "Jorge Lorenzo", "the Devil's Eye", "checkers", "Les Dawson", "King Ferdinand", "the Grail", "Ronald Reagan", "James Hazell", "climate", "at the Coney Island Old Island Pier in New York, NY", "the Sutton Hoo", "the liver", "Guildford Dudley", "the Amoco Cadiz", "John Howard", "David", "his Holiness", "12", "Cornell", "Flybe", "The Altamont Speedway Free Festival", "fat like oil or lard", "The Lost Weekend", "Stockholm", "Switzerland", "taekwondo", "tomato", "the senior-most judge of the supreme court", "Christians", "Representatives", "2006", "Central Avenue", "middleweight", "Jacob Zuma", "work is the hardest and least rewarding work", "comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Canterbury", "Harold Macmillan", "the marsh"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6906935307017543}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.8000000000000002, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4210526315789474, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-494", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-7632", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-2330", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1023", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-6833"], "SR": 0.640625, "CSR": 0.5440340909090908, "EFR": 1.0, "Overall": 0.7067755681818182}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "early 1938", "Adidas", "Ennis", "Stratfor's website", "low prices.", "Jaime Andrade", "children ages 3 to 17", "girls", "victims of physical and sexual abuse.", "the island's dining scene", "gasoline", "vivian liberto", "The plane", "he won two Emmys", "an ice jam", "mikey", "abduction of minors.", "costa Rica", "one-of-a-kind navy dress with red lining", "jenny Sanford", "Florida", "Bhola for the Muslim festival of Eid al-Adha.", "T.I. Harris", "Pew Research Center", "nirvana", "vivian liberto", "vivian liberto", "it was based on the latter.", "he won it with unparalleled fundraising and an overwhelming ground game.", "between June 20 and July 20", "vivian liberto", "misdemeanor", "1.2 million", "100,000", "Peshawar", "crossfire by insurgent small arms fire,", "2002", "Noriko Savoie", "a \"new chapter\" of improved governance in Afghanistan", "Arsene Wenger", "when people gathered outside as the conference in the building ended.", "shelling of the compound", "in the mouth.", "Atlantic Ocean", "movahedi", "Nepal", "Jiverly Wong", "videtaping a sexual assault on a child.", "the Louvre", "September 21.", "vBS.TV", "Supplemental oxygen", "Prince Bao", "Narendra Modi", "Steve Davis", "75", "vivian liberto", "musical research", "Steve Buscemi", "Mick Jackson", "West Virginia", "Gary Oldman", "Paris"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5622796474358974}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.923076923076923, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5128205128205129, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-775", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-328", "mrqa_newsqa-validation-1247", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7079", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4643", "mrqa_searchqa-validation-44"], "SR": 0.484375, "CSR": 0.5422794117647058, "EFR": 1.0, "Overall": 0.7064246323529411}, {"timecode": 34, "before_eval_results": {"predictions": ["most common", "boudins", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "the gloved one", "Laos", "dennis taylor", "dennis taylor", "Battle of Agincourt", "alicyclic", "dennis taylor", "Kent", "dennis taylor", "Diptera", "a humdinger", "transuranic", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "dennis taylor", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Cobain and Novoselic", "Janis Joplin", "Kenya", "St. Mark's", "Moscow", "La Guiara", "skin care", "hair and fur", "art of Decoupage", "Bathsheba", "Ennio Morricone", "dita taylor", "collapsible support assembly", "republicans", "Argentina", "French", "dennis taylor", "the internal kidney structures", "a rabbit", "Rocky Marciano", "the Benedictine Order", "dennis taylor", "Hilda and Peggy Hookham,", "dennis taylor", "four", "17 -- 15", "the second half will premiere in 2018", "a lightning strike", "Danny Glover", "Trey Parker and Matt Stone", "140 to 219 passengers", "Hundreds", "Democrats", "31 meters (102 feet)", "dennis Galahad", "Sutter's Fort", "The Ryukyu Islands"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5930059523809524}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, true, true, false, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.5333333333333333, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-2680", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-2181", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5407", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-6093", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-798", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3995", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-10490", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-4416", "mrqa_searchqa-validation-3920"], "SR": 0.53125, "CSR": 0.5419642857142857, "EFR": 1.0, "Overall": 0.7063616071428571}, {"timecode": 35, "before_eval_results": {"predictions": ["1970s", "Aristotle", "daiquiri", "calvary", "armadillos", "Elizabethan Theatres", "boxleitner", "Absalom", "peter p McGeehan, Paul Smith.", "The Goonies", "vivichrome vinyl wall banners", "quito", "the Seine", "aperitif", "Alyssa Milano", "bites a dog", "the Star-Spangled Banner", "The Rolling Stones", "Lincoln's Inn Hall", "a \"royal fork\"", "Benjamin Franklin", "Bob Dylan", "a box-shaped container with a handle,", "Apollo 11", "Portugal", "Cadillac", "george Clooney", "a magnum opus", "shalom", "white", "arthur balfour", "a judicial assertion", "Easton", "scrabble", "Great Britain", "st. Francois Mountains", "a double- chamber", "wyre Sinclair", "Stephen Vincent Bent", "Brooke Ellen Bollea", "\"The time not to become a war\"", "Nancy Sinatra", "David", "Pinot noir", "Robert Lowell", "ACTIVE", "Richmond", "a bottle", "Amy Tan", "Florence", "pithos", "Carriacou", "Himalayas", "Kusha", "Only Fools and Horses", "cami de Repos", "mineral", "zero to zeroth power", "2015", "October 20, 2017,", "Columbus", "110 mph,", "piedad Cordoba,", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5492559523809524}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.4, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.28571428571428575, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4706", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-1187", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-9007", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-245", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-5487", "mrqa_searchqa-validation-8322", "mrqa_naturalquestions-validation-10026", "mrqa_naturalquestions-validation-1407", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-4710"], "SR": 0.46875, "CSR": 0.5399305555555556, "EFR": 1.0, "Overall": 0.705954861111111}, {"timecode": 36, "before_eval_results": {"predictions": ["1082", "Bowe Bergdahl", "\"It didn't matter if you were 60, 40 or 20 like I am.", "a Columbian mammoth", "kidney", "a floating National Historic Landmark,", "recall", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile", "75", "parents", "women", "CNN/Opinion Research Corporation", "the world's tallest building", "CEO of an engineering and construction company", "Ku Klux Klan", "Felipe Calderon", "137", "1-0", "\"Dancing With the Stars\"", "you", "Michael Jackson", "\"a striking blow to due process and the rule of law\"", "Venezuela", "John and Elizabeth Calvert", "the Nazi war crimes suspect", "calls,", "Mandi Hamlin", "Iraq", "Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole", "they can demonstrate they have been satisfactorily treated for at least 12 months.", "Tennessee", "\"Dance Your Ass Off\"", "Malawi", "246", "the skull", "six", "Izzat Ibrahim al-Douri,", "eight in 10", "a one-shot victory in the Bob Hope Classic", "in the Muslim north of Sudan", "41", "Clifford Harris,", "Bea Arthur", "Susan Boyle", "Florida", "Ambassador Louis CdeBaca", "United States, NATO member states, Russia and India", "27-year-old", "45", "you", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston", "1993 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "(George Glenn)"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5898313492063492}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.8, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2222222222222222, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1510", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-3577", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_hotpotqa-validation-2625", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.4375, "CSR": 0.5371621621621622, "EFR": 1.0, "Overall": 0.7054011824324324}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion", "Veneto region of Northern Italy", "Preston, Lancashire, UK", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Kathryn Bigelow", "George Orwell", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "brother-in-law", "Chrysler", "Portal", "William Shakespeare", "Terrence Alexander Jones", "S&M\"", "one", "Evey's mother", "O", "The Grandmaster", "highland regions of Scotland", "John Winston Lennon", "half", "Russian Empire", "Philipstown", "Hilary Erhard Duff", "Ogallala, Nebraska", "October 21, 2016", "My Beautiful Dark Twisted Fantasy\"", "Everything Is wrong", "Massapequa", "1988", "Dan Brandon Bilzerian", "Spitsbergen", "1967", "commercial", "Giuseppe Verdi", "band director", "1837", "$10\u201320 million", "Mandarin", "Judge Doom", "March", "Michael- Leonard Wooley", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "English musician George Harrison", "Union forces", "Alison Krauss", "Graham Henry", "earwax", "mental health and recovery.", "the Bronx", "billions of dollars", "Diamond", "Simon Legree", "Sideways", "the Olympian"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6359375}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5404", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-4687", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-8753"], "SR": 0.546875, "CSR": 0.5374177631578947, "EFR": 1.0, "Overall": 0.7054523026315789}, {"timecode": 38, "before_eval_results": {"predictions": ["a very robust and flexible simplification of a computer", "Nepal", "Wang Chung", "Panama", "a mollusk or mollusc", "Thailand", "Abraham Lincoln", "the gizzard", "Georgie Porgie", "Mork & Mindy", "Catherine de Medici", "a dressage", "Benito Mussolini", "Southern California", "Fort Leavenworth", "INXS", "a Brief History of the 21st Century", "wildebeest", "Extra-Terrestrial Intelligence", "Henry VIII", "Jackson Pollock", "Clara Barton", "Nine to Five", "snakes", "a moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Princess Margaret,", "the airship era", "marine multicellular algae", "feminism", "San Diego Comic-Con", "the gallbladder", "the cousin of his wife, Mattie Silver", "midway", "Liechtenstein", "Custer", "Mount Gilead", "salt", "Gloria Steinem", "Queen Louise of Lorraine", "Tonga", "Minos", "Gulliver", "Colada", "SeaWorld", "a final stroke", "Tyra Banks", "Dick Gephardt", "Bucharest", "Manley", "synthesizing vitamin B and vitamin K as well as metabolizing bile acids, sterols, and xenobiotics", "attack on Pearl Harbor", "positive lens", "inch", "mathematician, astronomer, physician, classical scholar, translator", "Canterbury", "Lowndes County", "Northern Rhodesia", "the actor who created one of British television's most surreal thrillers", "Goa", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after facing various challenges and turning them to", "four"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6512784090909091}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, true, true, true, true, false, true, false, false, false, false, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1814", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-5765", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-16148", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-5436", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-2041", "mrqa_searchqa-validation-13649", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059"], "SR": 0.578125, "CSR": 0.5384615384615384, "EFR": 1.0, "Overall": 0.7056610576923077}, {"timecode": 39, "before_eval_results": {"predictions": ["over the age of 18", "Nalini Negi", "Blue laws", "1980 Summer Olympics", "IB Primary Years Program", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Angus Young", "The Hightower", "After World War I", "prospective studies", "Michigan State Spartans", "Franklin and Wake counties", "60 by West All - Stars", "( / ta\u026a\u02c8t\u00e6n\u026ak / )", "Sally Field", "Elizabeth Dean Lail", "Shastri", "chili con carne", "6 March 1983", "Kevin Michael Richardson", "James Arthur", "Raymond Gosling", "Antarctica", "during the American Civil War", "Thomas Middleditch", "resistance to attempts by Northern antislavery political forces to block the expansion of slavery into the western territories", "Ernest Rutherford", "Buddhism", "1889", "diurnal and insectivorous", "ark of the covenant", "Paul Walker", "$19.8 trillion", "Sleeping with the Past", "boy or girl", "1820s", "Soviet Union", "Vienna", "Dmitri Mendeleev", "Dalveer Bhandari", "at standard temperature and pressure", "John Ernest Crawford", "2013", "Cathy Dennis and Rob Davis", "1924", "Americans", "`` central '' or `` middle ''", "Sedimentary rock", "Carmen", "an amphibian", "glass", "Rikki Farr's", "the Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "Gaslight Theater", "\"M*A*S*H\"", "the Cure", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6050193565329357}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 0.0, 0.0, 0.11764705882352941, 0.33333333333333337, 0.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 0.5, 0.0, 1.0, 0.0, 0.06451612903225806, 0.0, 1.0, 0.11764705882352941, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-8118", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-590", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-14218"], "SR": 0.515625, "CSR": 0.537890625, "EFR": 0.967741935483871, "Overall": 0.6990952620967742}, {"timecode": 40, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.85546875, "KG": 0.4796875, "before_eval_results": {"predictions": ["the architect or engineer", "Naples", "dengue", "Jefferson Davis", "rubiks cube", "a kettledrum", "the meringue", "\"No hostage will be released until all our demands are met,\"", "an axe", "Department of Justice", "Jimmy Doolittle", "John Brown", "Anamosa's", "One Hundred Years of Solitude", "Frida Kahlo", "the three naval sea forts", "Wooster", "Sicily", "aetherial", "William Pitt the Younger", "Popcorn", "Madonna", "welterweight", "yoyo", "Winston-Salem", "Streetcar Named Desire", "Edinburgh, Scotland", "long-spectrum", "central defender", "Rocky Mountain columbine", "Italy", "Kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "the Fantastical World Around You", "a ballot initiative", "Chicago", "the Great Pyramid", "Herod I", "Alaska", "\"more likely to be killed by a terrorist\"", "Asia", "anaphylaxis", "Peter Pan", "Kuwait", "1.o", "the Locust", "a diamond", "Emilio Estevez", "The Call of the Wild", "Gibraltar", "Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Sizzurp,\"", "Larry Eustachy", "Isabella II", "Stanford University", "Vicente Carrillo Leyva,", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6229166666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-13067", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-5758", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-3554"], "SR": 0.515625, "CSR": 0.5373475609756098, "EFR": 1.0, "Overall": 0.7135632621951219}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "urus", "Israel", "Prince Rainier", "Walden", "Fred Astaire", "Humphrey Bogart", "honda", "Alan Bartlett Shepard", "by burning nitrates and mercuric oxides", "Smiley", "jackstones", "Rosslyn Chapel", "Hispaniola", "Zulu", "Blood Light", "Ironside", "Aristotle", "Basil Fawlty", "South Sudan", "Tuesday", "Hungary", "Lincoln", "east coast", "Antoine Lavoisier", "NOW Magazine", "Tuscany", "Battle of the Alamo", "Beaujolais", "Edmund Cartwright", "Der Stern", "(born June 28, 1577, Siegen, Nassau, Westphalia [Germany]", "Constantine the Great", "kautta", "Barry McGuigan", "Wisconsin", "John Barbirolli", "Eton College", "Quatar Holdings", "Charles Dickens", "Ted Hankey", "General Joseph W. Stilwell", "midrib", "sternum", "Portuguese", "mexico", "Greek island located in the eastern Mediterranean Sea", "Ed Miliband", "commitment", "iron lung", "Mandate of Heaven", "in the fascia surrounding skeletal muscle", "Robin", "Distinguished Service Cross", "Indian classical", "2012\u201313 season", "11", "\"an eye for an eye,\"", "Arabic, French and English", "Schwalbe", "the runcible", "Seinfeld", "Cress"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5764136904761905}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, true, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-2826", "mrqa_triviaqa-validation-4046", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-2983", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-6548", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3062", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-1676", "mrqa_triviaqa-validation-3792", "mrqa_triviaqa-validation-4630", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-3241", "mrqa_newsqa-validation-2336", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.484375, "CSR": 0.5360863095238095, "EFR": 1.0, "Overall": 0.7133110119047619}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman's half - brother", "George Strait", "Andrew Gold", "1983", "virtual reality simulator", "Lord Banquo", "Pakistan", "October 1, 2015", "shortwave radio", "Isaiah Amir Mustafa", "Commander in Chief of the United States Armed Forces", "Paracelsus", "Dan Bern and Mike Viola", "Marshall Sahlins", "Gloria", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "( 27 January -- 16 April 1898 )", "1770 BC", "360", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "216", "Justin Bieber", "the Red Sea and the east African coast across the Indian Ocean, and across the Pacific Ocean to the west coast of Central America", "ideology", "Gatiman express", "human consumption", "Andrew Garfield", "the 90s", "Gibraltar", "electrons", "cut off close by the hip, and under the left shoulder", "Lulu", "ranking", "Tokyo for the 2020 Summer Olympics", "1972", "Howard Ellsworth Rollins Jr.", "Jaye P. Morgan", "1961", "passwords", "National Industrial Recovery Act", "adenosine diphosphate", "General George Washington", "Sam Huntington as Jake Gilmore", "Lake Wales", "1560s", "Gutenberg", "Wichita", "Tina Turner", "joseph galliano", "Henry John Kaiser", "Marilyn Martin", "SARS", "tax incentives", "patient who underwent a near-total face transplant", "over a kilometer (3,281 feet) high", "neon", "prisoners of Azkaban", "the ark of acacia", "Basilan"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6016028191579662}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.09523809523809525, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.16666666666666666, 1.0, 0.0909090909090909, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4444444444444444, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-6519", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-9474", "mrqa_naturalquestions-validation-8669", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-3395", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.453125, "CSR": 0.534156976744186, "EFR": 0.9714285714285714, "Overall": 0.7072108596345514}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a latte", "Sheffield United", "linchpin", "Wat Tyler", "Lone Ranger", "Scotland", "the Earth", "James Hogg", "Texas", "john hall", "Pears soap", "the Czech Republic", "Louis XVI", "john joseph johnhattan", "fifty-six", "Uranus", "Plato", "the chord", "Chubby Checker", "Separate Tables", "Wilson", "coal", "Stephen of Blois", "the Antitrust Documents Group", "eukharisti\u0101", "baseball", "Bear Grylls", "jaws", "Tanzania", "Val Doonican", "a tittle", "E. T. A. Hoffmann", "the Republic of Upper Volta", "Alexander Borodin", "an elephant", "the Creel Committee", "New Zealand", "Mendip Hills", "Street Art & Graffiti", "Jane Austen", "God bless America, My home sweet home.\"", "Trade Mark", "boxing", "Benjamin Disraeli", "The Jungle Book", "(The Great Leap)", "Jan van Eyck", "Rabin", "Shania Twain", "john Nash", "electron donors", "Mama Said", "a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "RCA Victor", "to use Afghan troops at night \"whenever possible\" to knock on doors of residences and compounds, and to use them if forcible action is required for entry,", "Robert Park", "Nearly eight in 10", "Cairo", "Jackson Pollock", "an elk", "tax"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6421875}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-3732", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_hotpotqa-validation-4875", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1551"], "SR": 0.578125, "CSR": 0.53515625, "EFR": 1.0, "Overall": 0.713125}, {"timecode": 44, "before_eval_results": {"predictions": ["1994\u20131999", "Aamir Khan", "Euripides", "Alfonso Cuar\u00f3n", "2013", "at the end of the 18th century", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "2013\u201314", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "from 1995 to 2012", "George Clooney", "Rothschild", "China", "lexy", "model", "alternate", "1874", "Citric acid", "North Dakota and Minnesota", "Matt Lucas", "Zambia", "The Sun", "Ron Ragin", "Hibbing", "Sullenberger", "Francis", "1909 Cuban-American Major League Clubs Series", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born", "2015", "19th", "smith", "Lev Ivanovich Yashin", "carrefour", "John Monash", "Benjam\u00edn", "Bank of China", "1848", "Battle of Hightower", "12", "Antiochia", "smith", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County, Florida", "honey bees", "squash", "Chicago", "soy", "Nineteen", "How I Met Your Mother", "ammonia", "Everest", "yin and yang", "Florence Nightingale", "Quebec"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5538714323870574}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, false, true, false], "QA-F1": [0.0, 0.5, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.28571428571428575, 1.0, 1.0, 0.5, 1.0, 0.8571428571428571, 0.3076923076923077, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3060", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-12973", "mrqa_searchqa-validation-16341"], "SR": 0.4375, "CSR": 0.5329861111111112, "EFR": 0.9722222222222222, "Overall": 0.7071354166666667}, {"timecode": 45, "before_eval_results": {"predictions": ["several critical pamphlets on Islam", "Spain", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Miene \"Minnie\" Schoenberg", "John Mortimer", "John Walsh", "Moldova", "Zeus", "London", "Sunset Boulevard", "Duke of Westminster", "The Lion King", "licensing", "Wyoming", "Benedictus", "Oasis and Blur", "Javier Bardem", "8", "Lee Harvey Oswald", "curved mirrors", "Sherlock Holmes", "Bayern Munchen", "Rotherham United", "Pesach", "Bobby Kennedy", "Skylab", "Portugal", "Rhine River", "Confucius", "Japan", "stewardi(i)", "London", "Yves Saint Laurent", "Phoenicia", "(C) Bobby Moore", "Changing Places", "Sanl\u00facar de Barrameda", "plac\u0113b\u014d", "Oliver Twist", "FC Porto", "writing", "argument", "Rochdale", "Portuguese", "Madagascar", "Helsinki", "The Landlord's Game", "myxoma virus", "Ceylon", "between 8.7 % and 9.1 %", "in a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "Mercedes -Benz G - Class", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "1957", "Tim Masters,", "South Africa", "\"B\"s", "ABBA", "Phoenicia", "Tom Coughlin"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5323756720430107}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, false, false, false, true, true, false], "QA-F1": [0.33333333333333337, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8387096774193548, 0.6666666666666666, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2291", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-648", "mrqa_triviaqa-validation-3330", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-1407", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_triviaqa-validation-3756", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_hotpotqa-validation-3195"], "SR": 0.4375, "CSR": 0.5309103260869565, "EFR": 1.0, "Overall": 0.7122758152173913}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby", "a modem", "Clinton", "", "Pennsylvania State", "Luxor", "Berlusconi", "leviathan", "Mending Wall", "wombat", "crystal", "thunder", "Josephine", "Louise de La Vallire", "iTunes", "Neptune", "Annie", "Romeo", "KLM", "Captain America", "X-Men", "the retina", "a goat", "Planet of the Apes", "a knish", "India", "Reading Railroad", "Trotsky", "cheese", "the Justice Department", "Yes I Am", "Ignace Jan Paderewski", "julius", "Charles M. Schulz", "the Chesapeake Bay", "Frida Kahlo", "Jane Austen", "Rikki-Tavi", "mutual fund", "polygons", "music", "lm", "a ferry", "New York Times", "Oresteia", "frosting", "Baltimore", "Miami Dolphins", "Thomas Mundy Peterson", "USS Chesapeake", "In 1900", "Skye terrier", "reptile", "Hindi", "London", "John Snow", "Serbia", "soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday in Los Angeles.", "1955"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5994791666666667}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-7406", "mrqa_searchqa-validation-1339", "mrqa_searchqa-validation-4703", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7238", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-15513", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-201", "mrqa_hotpotqa-validation-4185", "mrqa_newsqa-validation-1216", "mrqa_newsqa-validation-1277"], "SR": 0.5625, "CSR": 0.5315824468085106, "EFR": 0.9642857142857143, "Overall": 0.7052673822188449}, {"timecode": 47, "before_eval_results": {"predictions": ["Kokee", "The Lord Mayor of London", "Shel Silverstein", "beers", "a bus", "Liverpool", "Mount Rushmore", "Cyrus the Younger", "Greece", "Jim Bunning", "George Harrison", "The Last Starfighter", "subwoofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "drug and alcohol", "Chad", "bicentennial", "midway", "George Gershwin", "alpaca", "the Atlantic Ocean", "heredity", "Bicentennial Man", "the rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fulgencio Batista", "The Indianapolis 500", "the Twist", "(Rabbie) Burns", "the cuckoo", "London", "red-orange", "Joan of Arc", "palindromes", "quid", "Vanilla Ice", "The Roxbury", "J. Steinbeck", "Eric Knight", "Heroes", "Ganges", "Thomas Mann", "Samuel, Kings & Chronicles", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film ( or television episode, music video, commercial, or video game )", "Bedfordshire", "Charles I", "The Lion, The Witch", "Lord's Resistance Army", "South Asia and the Middle East", "Netflix", "an appeals court seeking an emergency stay to stop the judge's order in its tracks.", "Quebradillas", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6460700757575757}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-3857", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-11991", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-3452", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-6008", "mrqa_searchqa-validation-8106", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.53125, "CSR": 0.5315755208333333, "EFR": 0.9333333333333333, "Overall": 0.6990755208333332}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws", "Deseo", "Sally Burton", "(James) Patterson", "Incredibles", "a Cheetah", "Charlie Brown", "Odin", "Japan", "Sea-Monkeys", "daffodils", "\"24\"", "Neil Simon", "Voyager", "a gull", "Nez Perce", "Eva Peron", "Incense", "the Hawkeye", "Ivica Zubac", "Swiffer", "Huckleberry Hound", "Austria", "The Bourne", "Brazil", "The Trojan Horse", "the Chagos", "the Colosseum", "Cambodia's highest mountain", "Dr. Hook and the Medicine Show", "Songs of Innocence", "Uvula", "a Sacraments", "Ben-oni", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "Zambezi", "a charploo", "(2 Samuel)", "Sting", "Jamestown", "American funk rock band", "Crittenden", "St. Francis of Assisi", "the Alaska dish", "( Herman) Melville", "Tarzan & Jane", "Brett Favre", "1919", "eight years", "Taron Egerton", "The Colour Out of Space", "Stieg Larsson", "The Merchant of Venice and The Taming of the Shrew", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "India", "The EU naval force", "Ruritania"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6710858585858586}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8339", "mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-9001", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-7756", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-13972", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455"], "SR": 0.609375, "CSR": 0.5331632653061225, "EFR": 1.0, "Overall": 0.7127264030612245}, {"timecode": 49, "before_eval_results": {"predictions": ["1972", "the Stonemason's Yard", "Carmen", "shetland", "the Temple Mount", "taboo", "fourteen", "kidneys", "apple", "Thierry Roussel", "Novak Djokovic", "Apollo 11", "five", "Kirk Douglas", "john martin", "tin", "longchamp", "Japan ( Nippon or Nihon; formally 'or nihon-koku, \"State of Japan\"", "Henry Ford", "joey", "Maine", "Missouri", "Pyrenees", "basketball", "Janis Joplin", "Mr. Stringer", "basketball", "South Africa", "Pet Sounds", "Ed Miliband", "Scotland", "an aeoline", "Margaret Mitchell", "Republic of Upper Volta", "martina Navratilova", "40", "75 or older", "george VI", "John Masefield", "Rio de Janeiro", "party of God", "Bengali", "Claire", "Guatemala", "carousel", "Leicester", "Frank Saul", "radish", "bruce wayne", "Downton Abbey", "bruce du", "Garfield Sobers", "Herman Hollerith", "2014", "Golden Gate National Recreation Area", "Forbes", "The English Electric Canberra", "Ford", "pattern matching", "one of 10 gunmen who attacked several targets in Mumbai", "a rat", "tapas", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6709753787878787}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-4967", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-7720", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_hotpotqa-validation-3207"], "SR": 0.59375, "CSR": 0.534375, "EFR": 0.9615384615384616, "Overall": 0.7052764423076923}, {"timecode": 50, "UKR": 0.662109375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.837890625, "KG": 0.43359375, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "December 1886", "July 1, 1890", "March 31, 2013", "Fawcett", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "2018", "The Walking Dead", "in Koine Greek : apokalypsis", "1962", "non-ferrous", "the state sector", "the sacroiliac joint or SI joint ( SIJ )", "Sunni Muslim family", "after World War II", "the common name of a typical child's friend", "to ensure its ratification and lead to the adoption of the first ten amendments, the Bill of Rights", "L.K. Advani", "36 months for men and 24 months for women", "Jason Marsden", "Charles Lebrun", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "early 1960s", "Tempe, Gilbert, Queen Creek, San Tan Valley, Apache Junction and Fountain Hills", "the beginning", "2013", "Diego Tinoco", "If there are no repeated data values, a perfect Spearman correlation of + 1 or \u2212 1 occurs when each of the variables is a perfect monotone function of the other", "January 2004", "Glenn Close", "Cefal\u00f9, Caen, Durham, and elsewhere", "Johannes Gutenberg", "Bill Condon", "Alex Ryan", "Dr. Addison Montgomery", "Carolyn Sue Jones", "Leon Battista Alberti", "a column - like or oval ( egg - shaped ) symbol of Shiva", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "birch", "a response to the sensation of food within the esophagus itself", "dolly parton", "westminster", "river uss missouri", "Jack Murphy Stadium", "Black Abbots", "Prince Amedeo,", "mental health", "Suba Kampong township", "in the first near-total face transplant in the United States,", "Laryngitis", "Pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26"], "metric_results": {"EM": 0.5625, "QA-F1": 0.646680397903224}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.5, 0.5, 1.0, 0.0, 0.0, 0.6956521739130436, 0.14814814814814814, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.14285714285714285, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5454545454545454, 0.15384615384615383, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-6810", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-2294"], "SR": 0.5625, "CSR": 0.5349264705882353, "EFR": 0.9642857142857143, "Overall": 0.6865611869747898}, {"timecode": 51, "before_eval_results": {"predictions": ["General Armitage Hux", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander", "Franklin Roosevelt", "Orange Juice", "1837", "Louisa Johnson", "22 November 1914", "Shareef Abdur - Rahim", "2018", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "near Camarillo, California", "Piet van der Walt", "2018 and 2019", "Exodus 20 : 1 -- 17", "easily flow from the faucet into the sink", "to connect the CNS to the limbs and organs", "15 February 1998", "Los Lonely Boys", "Thomas Alva Edison", "Danish pronunciation : ( \u02c8\u0251n\u0250sn\u0329 )", "`` Mirror Image ''", "two senators, regardless of its population, serving staggered terms of six years", "most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "1623", "Eduardo", "a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes", "elephant", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "Brooklyn, New York", "2008", "Buddhism", "Rodney Crowell", "Atlanta", "peninsular", "21 June 2007", "chairman ( more usually now called the `` chair '' or `` chairperson ''", "Germany", "Ronan the Accuser", "Darlene Cates", "the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "John Daly", "Matt Monro", "Joe Willie Kirk", "Ricky nelson", "cppola", "blood", "Baugur Group", "Venice", "Hyundai Steel's", "outlet mall", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic Tamil minority"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6018420240708338}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, false, true, false, true, false, false, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, false, false, false, true, false, false, true, false, true, true, false], "QA-F1": [0.0, 0.9090909090909091, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.5, 1.0, 0.0, 0.5185185185185185, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.1111111111111111, 0.13333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4102564102564102, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11111111111111112, 1.0, 0.0, 1.0, 0.7894736842105263, 0.9302325581395349, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8333333333333333]}}, "before_error_ids": ["mrqa_naturalquestions-validation-160", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208", "mrqa_newsqa-validation-1718"], "SR": 0.453125, "CSR": 0.5333533653846154, "EFR": 0.8857142857142857, "Overall": 0.6705322802197802}, {"timecode": 52, "before_eval_results": {"predictions": ["W\u0142adys\u0142awa", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "created the American Land- Grant universities and colleges", "Pacific War", "1949", "The Dark Tower", "John Samuel Waters Jr.", "1945", "Sacramento Kings", "S6", "Magic Band", "Supergirl", "April 1, 1949", "Scottish Premiership club Hearts", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "macbeth", "Brad Silberling", "1973", "Italy", "Sunil Grover", "\"Twice in a Lifetime\"", "7 Series", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf", "Elvis' Christmas Album", "sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca", "master builder", "Godiva", "Manchester United", "David X. Cohen", "Los Alamos National Laboratory", "Russia", "Lush Ltd.", "Telugu", "1952", "a land grant college", "Restoration Hardware", "1942", "Kansas City Chiefs", "Luis Edgardo Resto", "C. H. Greenblatt", "Stephen Graham", "the three branches of the federal government", "Extroverted Intuition ( Ne )", "Belgium", "jape", "james pollock", "Alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles)", "Casalesi clan", "Linda Darnell", "Scrabble", "Wendell,", "a leap year"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6392321654040404}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, true, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, false, false, true, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 0.125, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.888888888888889, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-2084", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-4714", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-4933", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.546875, "CSR": 0.5336084905660378, "EFR": 1.0, "Overall": 0.6934404481132075}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "HPV (human papillomavirus)", "eight-day journey", "9-week-old", "American Muslim and Christian leaders", "18", "Darrel Mohler", "Spc. Megan Lynn Touma,", "Operation Pipeline Express", "both Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "a house party in Crandon, Wisconsin", "President Obama", "response the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "Grand Ronde, Oregon.", "a bag", "\" Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "14-day", "the fact that the teens were charged as adults.", "Kris Allen", "to prevent it at every turn,", "rwanda", "Arsenal manager Arsene Wenger", "went second in Serie A with a 5-1 win over Torino in the San Siro on Sunday.", "the Genocide Prevention Task Force.", "Sheik Mohammed Ali al-Moayad", "\"pleased\" with the FDA's order", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "anti-government protesters", "Saturday", "The Frisky:", "original member of The Charlie Daniels Band", "Democratic VP candidate", "Gainsbourg", "Real Madrid", "three", "between June 20 and July 20", "Nixon-Medici", "Piedad Cordoba,", "Buddhism", "Bollywood superstar", "Pakistani territory", "fight outside of an Atlanta strip club", "Britain's Got Talent", "Democratic VP candidate", "Swamp Soccer", "the man facing up, with his arms out to the side.", "stand down.", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "The ACLU", "the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "2018", "surfer", "crusty", "red", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "an eagle", "a crust of mashed potato"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6043862740325748}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 1.0, 0.8333333333333333, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.28571428571428575, 0.0, 0.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.30769230769230765, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3225806451612903, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1370", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-3864", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_triviaqa-validation-199", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-16162", "mrqa_naturalquestions-validation-10616"], "SR": 0.515625, "CSR": 0.533275462962963, "EFR": 1.0, "Overall": 0.6933738425925926}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou", "Squamish, British Columbia, Canada", "2018", "2004 American biblical drama film directed by Mel Gibson, written by Gibson and Benedict Fitzgerald, and starring Jim Caviezel as Jesus Christ, Maia Morgenstern as the Virgin Mary and Monica Bellucci as Mary Magdalene", "the fork to the right of the dinner plate", "the illegitimate son of Ned Stark", "Jason Lee", "ThonMaker", "Hans Raffert", "31", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "neutrality", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions 14 - 15, 146 - 147 and 148 - 149", "late - 2011", "Malibu, California", "desublimation", "eight", "the Anglo - Norman French waleis", "The three monkeys are Mizaru, covering his eyes,", "in cell - mediated, cytotoxic innate immunity ), and B cells ( for humoral, antibody - driven adaptive immunity )", "into the intermembrane space", "Kansas", "New England Patriots", "Chesapeake Bay", "Fred Ott", "to refer to a former sexual or romantic partner, especially a former spouse", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "The series of 16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "the topography and the dominant wind direction", "Development of Substitute Materials", "a pagan custom", "in various submucosal membrane sites of the body", "2013", "John Garfield as Al Schmid", "ummat al - Islamiyah", "and by selection of the Viceroy, Lord Irwin", "its absolute temperature", "right to property is no longer a fundamental right", "Robert Gillespie Adamson IV", "18th century", "1998", "oxygenated blood to the left atrium of the heart", "Motown Records", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the internal auditory canal of the temporal bone", "1803", "delivered appliances and other goods for department stores", "The Wrestling Classic", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller's Standard Oil Company", "misdemeanor assault charges", "$106,482,500", "introduce legislation Thursday to improve the military's suicide-prevention programs.", "Stone Temple Pilots", "real estate investment trust", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5605708609201256}, "metric_results_detailed": {"EM": [false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true], "QA-F1": [0.33333333333333337, 1.0, 1.0, 0.05882352941176471, 0.7272727272727272, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.9411764705882353, 0.0, 1.0, 1.0, 1.0, 0.0, 0.125, 0.23529411764705882, 0.0, 1.0, 1.0, 0.0, 1.0, 0.625, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 1.0, 0.4444444444444445, 0.0, 0.2, 1.0, 0.16, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-2384", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-9759", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3174", "mrqa_naturalquestions-validation-10452", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-3624", "mrqa_newsqa-validation-3250", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.4375, "CSR": 0.5315340909090909, "EFR": 0.9444444444444444, "Overall": 0.6819144570707071}, {"timecode": 55, "before_eval_results": {"predictions": ["Dr. Ruth Westheimer", "John Updike", "Smokey Bear", "incense", "Jericho", "to wash or dry surfaces: mopped along the baseboards", "asteroids", "\"plankton\"", "Sam Tilden", "Eleanor Roosevelt", "BATTLE of LAKE ERIE", "Bangladesh", "Secret", "Sudan", "Judd Apatow", "laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pH", "Aladdin Hotel", "9 to 5", "Jan & Dean", "force his", "ice cream", "Mike Huckabee", "Count Grigory Orlov", "Texas", "constellations", "a supernatural tale by William T. Vollmann", "A Good Girl, A Graduate, A Gynecologist, and A Gladiator", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "Doc", "antelope", "Anne Boleyn", "in the highlands of modern-day Guatemala", "Dizzy", "soup", "reasoning", "Enrico Fermi", "Daedalus", "suspension bridge", "Tigger", "the breath", "a marathon", "QWERTY", "Deuteronomy 5", "collect menstru flow", "13 May 1787", "the nasal septum", "Triumph", "Kansas", "the recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses", "March 17, 2015", "4.6 million", "Tibetan exile leaders,", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.5, "QA-F1": 0.6139165521978022}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-2969", "mrqa_searchqa-validation-13455", "mrqa_searchqa-validation-12353", "mrqa_searchqa-validation-12390", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-3845", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-1277", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.5, "CSR": 0.5309709821428572, "EFR": 1.0, "Overall": 0.6929129464285715}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Lorraine Woodpecker", "honey", "stoke", "stockton-on-Trent", "iron", "Little arrows", "Torino", "cats", "Reanne Evans", "Niger", "Battle of Camlannis", "David Hilbert", "1905", "Great Britain", "british", "Jack London", "Hard Times", "Muhammad Ali", "carbon", "Sierra Oscar", "M65", "Boxing Day", "cheers", "mujaheddin", "alpestrine", "a mole", "to make something better", "nor\u00f0rvegr", "skirts", "Australia", "Blucher", "Atlas", "Sachin Tendulkar", "a black Ferrari", "River Hull", "Tenerife", "South Africa", "bone", "Nutbush", "Robert Maxwell", "Shinto", "Batley", "The Greater Antilles", "malts", "Pluto", "Jim Branning", "cryonics", "Fleet Street", "Scafell Pike", "baseball", "President pro tempore", "Athens", "iOS, watchOS, and tvOS", "Leslie James \"Les\" Clark", "fourth season of \"American Idol\"", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World", "propofol", "Emmett Kelly", "Me and Julio", "William Shakespeare", "a desire to be reckoned with as an openly wounded and unabashedly portentous rock balladeer"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5770351890756302}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0588235294117647, 1.0, 1.0, 0.5, 0.4, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-6484", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-3619", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-1978", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-3696", "mrqa_naturalquestions-validation-8114", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_searchqa-validation-1086", "mrqa_searchqa-validation-378", "mrqa_naturalquestions-validation-7270"], "SR": 0.515625, "CSR": 0.5307017543859649, "EFR": 0.967741935483871, "Overall": 0.6864074879739672}, {"timecode": 57, "before_eval_results": {"predictions": ["outside influences in next month's run-off election,", "Monday,", "eight-week", "to learn which type of guy you should avoid.", "coalition forces in Afghanistan", "to fritter his cash away on fast cars, drink and celebrity parties.\"", "Stratfor,", "in a video made by his captors, members of the Taliban.\"", "Unseeded Frenchwoman Aravane Rezai", "murder in the beating death of a company boss who fired them.", "David Beckham", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Islamabad", "Dennis Davern,", "kite surfers and wind surfers", "\"Brazil will be playing a bigger role in hemispheric affairs and seeking to fill whatever vacuum the U.S. leaves behind,\"", "The opposition group, also known as the \"red shirts,\"", "Madhav Kumar Nepal of the Communist Party of Nepal (Unified Marxist-Leninist)", "Saturday", "from France, Russia, India, South Korea, China, South Africa, Brazil, Beirut and Poland.", "Lucky Dube,", "11", "hid his money,", "from concerns that the U.S. might use interceptor missiles for offensive purposes.", "Citizens", "refusal to \"turn it off\"", "combat veterans could be recruited by right-wing extremist groups.", "bicycles at the scene,", "from Pakistan and asked for a meeting with Pakistan's High Commission.", "Pakistan from Afghanistan,", "the fact that the teens were charged as adults.", "iPhone 4S", "dogs who walk on ice in Alaska.", "10 to 15 percent", "Israel", "marx marx", "The incident Sunday evening", "Landry", "President Bush of a failure of leadership at a critical moment in the nation's history.\"", "Alexandre Caizergues, of France,", "Steven Gerrard", "three", "Golden Gate Yacht Club of San Francisco", "Veracruz", "Sandy Olssen", "Camp Lejeune, North Carolina.", "2002 for British broadcaster Channel 4", "because its facilities are full.", "job bill's", "seven", "One of Osama bin Laden's sons", "in Africa", "Spain", "Manuel `` Manny '' Heffley is Greg and Rodrick's younger brother", "2004", "Pokemon", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "Chillingham Castle", "400th anniversary", "Liffey", "Scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5083967248029748}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, false, true, true, true, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.09999999999999999, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.1212121212121212, 1.0, 0.4615384615384615, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.25, 1.0, 0.0, 0.0, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.15384615384615383, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-844", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-889", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-2911", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212"], "SR": 0.390625, "CSR": 0.5282866379310345, "EFR": 1.0, "Overall": 0.6923760775862069}, {"timecode": 58, "before_eval_results": {"predictions": ["\"Ted\"", "1,467", "1989", "The Hours", "14", "the National Basketball Development League (NBDL)", "Gust Avrakotos", "involuntary euthanasia", "astronaut, naval aviator, test pilot, and businessman", "a diving duck", "the Games of the Olympiad", "Miami", "St. Louis Cardinals", "November 23, 1992", "1993", "La Salle College, the University of Pennsylvania and Temple University", "Jack Ridley", "The Pennsylvania State University", "Chicago, Illinois", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australia", "suburb", "Flex-fuel vehicle", "The Savannah River Site", "swingman", "Patriots Day", "Scotland", "Todd Emmanuel Fisher", "1944", "\" Suicide Squad\"", "1883", "23", "the Mach number (M or Ma)", "Amy Winehouse", "1999", "Bhpppavageete", "Madonna", "musicologist", "Lauren Alaina", "Prince Aimone of Savoy-Aosta", "Ben Ainslie", "\"Forbidden Quest\"", "coca wine", "paper-based card", "White Horse", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Meg Optimus", "Saudi Arabia", "Lady Gaga", "African violet", "three", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "Carrousel du Louvre,", "A Tale of Two Cities", "Angel Gabriel", "Braveheart", "( Boss) Tweed"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6289930555555555}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, false, false, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-5291", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-5780", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-544", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-1577", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_triviaqa-validation-5666", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955"], "SR": 0.515625, "CSR": 0.5280720338983051, "EFR": 1.0, "Overall": 0.6923331567796611}, {"timecode": 59, "before_eval_results": {"predictions": ["two reservoirs in the eastern Catskill Mountains", "connotations of the passing of the year", "Matt Monro", "Thespis", "in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "2010", "Coroebus", "Ewan McGregor", "1952", "iron", "Jesse Frederick James Conaway", "tolled ( quota ) highways", "supported modern programming practices and enabled business applications to be developed with Flash", "Anne Murray", "1957", "actions taken by employers or unions that violate the National Labor Relations Act of 1935", "a four - page pamphlet", "`` Have I Told You Lately ''", "the world's second most populous country", "in the straits between the mainland and Salamis, an island in the Saronic Gulf near Athens, and marked the high - point of the second Persian invasion of Greece", "Lana Del Rey", "April 1979", "`` The Crossing ''", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "The geopolitical divisions in Europe that created a concept of East and West originated in the Roman Empire", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "\u00c9mile Gagnan", "Felix Baumgartner", "1995", "2026", "the Gupta Empire", "Abigail Hawk", "Hal Derwin", "Pyeongchang", "in the 1970s", "1919", "1889", "halogenated paraffin hydrocarbons", "October 27, 2017", "three", "Richard Crispin Armitage", "Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson, Sophie Monk and Eddie Perfect", "Jack Barry", "headdresses", "Wet Wet Wet", "Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1927", "\"Kambakkht Ishq,\"", "The West", "\"wipe out\" the United States if provoked.", "Celsius", "Washington D.C.", "Swift", "LXF"], "metric_results": {"EM": 0.625, "QA-F1": 0.708487687597033}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5384615384615384, 0.0, 1.0, 0.7272727272727272, 0.5806451612903226, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-4980", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-2663", "mrqa_searchqa-validation-14090", "mrqa_searchqa-validation-2564", "mrqa_hotpotqa-validation-4642"], "SR": 0.625, "CSR": 0.5296875, "EFR": 0.9166666666666666, "Overall": 0.6759895833333334}, {"timecode": 60, "UKR": 0.705078125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.857421875, "KG": 0.4921875, "before_eval_results": {"predictions": ["Fulgencio Batista", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "1963\u201393", "Mike Holmgren", "2,627", "Swedish-Brandenburgian War", "Sparky", "Fort Frederick", "American", "Virgin", "October 21, 2016,", "Heart", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "High Knob", "Miss Universe 2010", "Accokeek, Maryland", "(2008), the re-release of her third studio album \"Good Girl Gone Bad\" (2007)", "democracy and personal freedom", "Days of Our Lives", "French Canadians", "1964 to 1974", "the fifth tier of English football", "City Mazda Stadium", "Continental Army", "Wes Archer", "1994", "Vancouver", "Lego", "Thomas Mawson", "Tony Aloupis", "various", "North Dakota", "Francis Nethersole", "The Panther", "British", "\u30d5\u30a1\u30f3\u30bf\u30b8\u30fcXII, Fainaru Fantaj\u012b Tuerubu", "The University of California", "City of Onkaparinga", "2 February 1940 \u2013 31 October 1999", "thirteen", "Princes Park", "The Bye Bye Man", "Germanic", "Blue", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950 replacing the Government of India Act ( 1935 ) as the governing document of India", "Raza Jaffrey", "Jon Stewart", "For Gallantry", "ArcelorMittal Orbit", "Government Accountability Office", "Officer Joe Harn", "$249", "high and dry", "An American Tail", "a cat", "Peru"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7331318127437079}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.15384615384615385, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6451612903225806, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-1128", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_naturalquestions-validation-8765", "mrqa_naturalquestions-validation-10257", "mrqa_newsqa-validation-4098", "mrqa_searchqa-validation-8784"], "SR": 0.65625, "CSR": 0.5317622950819672, "EFR": 1.0, "Overall": 0.7172899590163935}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "George Blake", "Orson Welles", "trout", "Aidensfield Arms", "borneo", "French", "Manchester", "sky", "Britten", "Angel Cabrera", "November", "City A.M.", "Alan Ladd", "Genghis Khan", "Ban Ki-moon", "Clifford", "left side", "Istanbul", "lamb", "Fame", "collie", "14 letters", "shark", "florida", "Mike Hammer", "Billie (Piper); she was 15 years and 287 days when she entered the chart at number 1 on 5th July 1998", "Dame Evelyn Glennie", "brain", "Zaragoza", "David Bowie", "Billy Wilder", "Mr Loophole", "palla", "4.4 million", "Today", "Westminster Abbey", "Ralph Lauren", "Whit Sunday", "Morgan Spurlock", "peaches and cream", "Debbie Reynolds", "Debbie McGee", "cation", "George Santayana", "Rudolf Nureyev", "Brian Gabbitas", "cat", "apple", "Argos", "Rodgers & Hammerstein", "part of a pre-recorded television program, Rendezvous with Destiny", "By 1770 BC", "The United States Secretary of State", "5", "Amal Clooney", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "Roanoke Island", "Louvre", "Kansas City, Missouri", "Yiddish Scientific Institute"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6539434523809524}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666665, 0.6666666666666666, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-1385", "mrqa_triviaqa-validation-6017", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-7254", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-366", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842", "mrqa_hotpotqa-validation-3793"], "SR": 0.546875, "CSR": 0.5320060483870968, "EFR": 0.9655172413793104, "Overall": 0.7104421579532814}, {"timecode": 62, "before_eval_results": {"predictions": ["Barry Sanders", "Gabriel Jesus Iglesias", "The Snowman", "Bhushan Patel", "Helsinki, Finland", "Robert Plant", "Tommy Cannon", "Scottish national team", "203", "Patricia Neal", "Illinois's 15 congressional district", "Buffalo", "7,500 and 40,000", "5,112 feet", "UTH Russia", "the lead roles of Timmy Sanders and Jack", "four months in jail", "Michael Redgrave", "Sturt", "Taylor Swift", "two Manhattan high school students who share a tentative month-long romance", "Europe", "Trilochanapala", "Live at the Electric", "small family car", "Mexican", "Algernod Lanier Washington", "14,000", "Scottish Highlands", "37", "Taoiseach of Ireland", "137th", "Mr. Nice Guy", "Japan Airlines Flight 123", "professional wrestling", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "pressure-sensitive film products", "video game", "The United States of America (USA), commonly known as the United States (U.S.) or America ( USA),", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Thiel", "orange", "Memphis, Tennessee", "Swiss federal popular initiative \"against mass immigration\" (German: \"Eidgen\u00f6ssische Volksinitiative \"Gegen Masseneinwanderung\")", "New Jersey", "Sophie Monk", "Reinhard Heydrich", "lo Stivale", "Iraq, Syria, Lebanon, Cyprus, Jordan, Israel, Palestine, Egypt", "September 2000", "Woodrow Wilson", "our mutual friend", "giraffe", "Volkswagen", "Teresa Hairston", "Pope Benedict XVI", "Joseph Maraachli,", "pearl", "sarsaparilla", "overbite", "Iran of trying to build nuclear bombs,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5727895585317461}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, true, false, false, true, true, true], "QA-F1": [0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666665, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0625, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3221", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-4494", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-1621", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-2297", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-5666", "mrqa_naturalquestions-validation-9089", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353", "mrqa_searchqa-validation-6948"], "SR": 0.4375, "CSR": 0.5305059523809523, "EFR": 0.9722222222222222, "Overall": 0.7114831349206349}, {"timecode": 63, "before_eval_results": {"predictions": ["Tinseltown", "Claude Monet", "Brazil", "Mokotedi Mpshe,", "apartment building", "in July", "2005 & 2006 Acura MDX", "Ryan Adams.", "the forehead and chin", "Olympia,", "27-year-old's", "next week.", "April 26, 1913,", "12-1", "Brazil jolted the global health community in 1996 when it began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "Swamp Soccer", "Christopher Savoie", "The Falklands, known as Las Malvinas in Argentina,", "we can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "two remaining crew members from the helicopter,", "in the 1950s", "Gary Player", "1 out of every 17 children under 3 years old", "\"Rin Tin Tin: The Life and the Legend\"", "litter reduction and recycling.", "President Bill Clinton", "an average of 25 percent", "The plane had a crew of 14 people and was carrying an additional 98 passengers,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "his son is fighting an unjust war for an America that went too far when it invaded Iraq five years ago", "Johan Persson and Martin Schibbye", "Israel", "Sunday's", "Swat Valley.", "Jeffrey Jamaleldine", "The Rev. Alberto Cutie", "all day", "The TNT series", "to try to make life a little easier for these families by organizing the distribution of wheelchairs, donated and paid for by his charity, Wheelchair for Iraqi Kids.", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "crafts poems telling of the pain and suffering of children just like her", "in the head", "American soldiers held as slaves", "neck", "island's dining scene", "Andrew Garfield", "Philadelphia Eagles", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Two Cities", "Mutiny on the Bounty", "Melbourne", "1998", "born 23 July 1989", "Tuesday", "Evian", "Tampa", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6866803448664862}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 0.2857142857142857, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 0.4, 0.21428571428571427, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.2666666666666667, 0.923076923076923, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.4347826086956522, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-271", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-3544", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-906", "mrqa_triviaqa-validation-2862", "mrqa_hotpotqa-validation-5662", "mrqa_searchqa-validation-5963"], "SR": 0.546875, "CSR": 0.53076171875, "EFR": 1.0, "Overall": 0.71708984375}, {"timecode": 64, "before_eval_results": {"predictions": ["best known for the comedies he wrote in 1722\u20131723 for the Lille Gr\u00f8nnegade Theatre in Copenhagen", "Max Martin and Shellback", "Prince Edward, Duke of Kent and Strathearn", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million", "aviation pioneer Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "Gaelic", "265 million", "2004", "The Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "thirteen", "Robert Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Dan Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander-Arnold", "The 2008\u201309 UEFA Champions League", "Kramer Guitars", "of El Nacimiento in M\u00fazquiz Municipality", "1968", "Holston River", "July 10, 2017", "London", "jazz", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "Darci Kistler", "The Terminator", "Samoa", "\"Bad Blood\"", "Timo Hildebrand", "Univision", "16th-century", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Mexico", "Julie Andrews Edwards", "Captain Mark Phillips", "Democratic VP candidate", "$75 for full-day class,", "\"L'Homme Qui Marche I, bronze\"", "cigar", "The Bridges of Madison County", "Thomas Jefferson", "currency option"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6786784670008355}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.2857142857142857]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-5781", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-755", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_hotpotqa-validation-2012", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4073", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.59375, "CSR": 0.5317307692307692, "EFR": 1.0, "Overall": 0.7172836538461539}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "U.S. Bank Stadium in Minneapolis, Minnesota", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Paul von Hindenburg", "Universal Pictures", "May 2010", "T - Bone Walker", "the entrance to the 1889 World's Fair", "Bobby Darin", "Alex Skuby", "The House at Pooh Corner ( 1928 )", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "Infiltration rate in soil science is a measure of the rate at which soil is able to absorb rainfall or irrigation", "1940", "the pulmonary arteries", "Puente Hills Mall", "1977", "an optional message body", "June 1992", "England, Northern Ireland, Scotland and Wales", "28 July 1914 to 11 November 1918", "Richard Stallman", "the year AD 1 immediately follows the year 1 BC", "October 27, 1904", "December 25", "a number of the desert's animals, including the southern marsupial mole ( Notoryctes typhlops )", "Tom Burlinson", "the final scene of the fourth season", "the 2017 Auburn Tigers football team", "during meiosis", "a contemporary drama in a rural setting", "Javier Fern\u00e1ndez", "The Italian Agostino Bassi", "Rachel Sarah Bilson", "plant food", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "Article Seven", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "In Reel Life:", "Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "The controversial quote is part of a eight-page feature article about Hogan to be published in the magazine's Friday edition.", "fifth", "Tina", "Bingo SOLO", "Amsterdam", "salve"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5439278146459252}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, true, true, false, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 0.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.5714285714285715, 0.5714285714285715, 0.0, 0.0, 0.8, 1.0, 0.1904761904761905, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-6618", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-746", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-8006", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-3394", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_naturalquestions-validation-4847", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.453125, "CSR": 0.5305397727272727, "EFR": 1.0, "Overall": 0.7170454545454545}, {"timecode": 66, "before_eval_results": {"predictions": ["touch reading and writing", "huggins", "180\u00b0", "St Conn Dan", "Strictly Come Dancing", "hugh glenn Attlee", "about a mile north of the village of Dunvegan", "hladetina", "bobby gollancz", "Iron Age", "bieber", "Estonia", "1925", "The Gunpowder Plot of 1605", "Moldova", "holly", "Edwina Currie", "sprite", "IKEA", "paul Picasso", "Some Like It Hot", "r Ralph Vaughan Williams", "Tony Blair", "Pickwick", "360", "caracas", "Ireland", "Williams F1", "Jim Peters", "horse racing", "onion", "Pat Houston", "1948", "beluga", "Sikh", "Giraffa camelopardalis", "kabuki", "email", "Zachary Taylor", "indigo", "Friday", "\u201cFor Gallantry;\u201d", "swindon", "cricket", "Jordan", "Burma", "travelcard Zone 1", "hongi", "basketball", "Snow White", "Italy", "Jos\u00e9 Gonz\u00e1lez", "Buddhism", "endocytosis", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano,", "Robert Barnett", "Jeopardy", "The Bridges of Madison County", "Paraguay", "HackThis Site.org"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6024553571428571}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.2, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5486", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-3091", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-2013", "mrqa_triviaqa-validation-1031", "mrqa_triviaqa-validation-1134", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_searchqa-validation-3081", "mrqa_hotpotqa-validation-1714"], "SR": 0.53125, "CSR": 0.5305503731343284, "EFR": 0.9333333333333333, "Overall": 0.7037142412935323}, {"timecode": 67, "before_eval_results": {"predictions": ["yann martel", "archers", "vince Lombardi", "nguni", "Cambridge", "france", "1830", "Lorraine", "jeremy", "chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james may", "red squirrel", "Richard Lester", "honda", "polish", "gooseberry", "George W. Bush", "color Purple", "Elizabeth Montgomery", "Il Divo", "Barack Obama", "1984", "williamshaven", "China", "Ecuador", "oscar roshi", "joe williamy Young", "Leon Baptiste", "360", "jeremy joachim", "1123", "Mitford", "Sparta", "Hyundai", "thirtieth", "jeremy brown Findlay", "haddock", "Yemen", "Tina Turner", "mainland China and Taiwan", "Nowhere Boy", "Vienna", "head and neck", "quant", "kell ley", "35", "back", "wyfed powys", "Robyn", "South Asia", "Uralic languages", "New York City", "1942", "a reward for ability or finding an easy way out of an unpleasant situation by dishonest means", "Larry King", "of Noida,", "Ayelet Zurer", "Oakland Raiders", "the Mediterranean", "Isabella", "Turing"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6270833333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, false, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-1662", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-1906", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-1788", "mrqa_triviaqa-validation-467", "mrqa_triviaqa-validation-2460", "mrqa_triviaqa-validation-1501", "mrqa_naturalquestions-validation-5787", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508", "mrqa_searchqa-validation-475"], "SR": 0.53125, "CSR": 0.5305606617647058, "EFR": 0.9666666666666667, "Overall": 0.7103829656862745}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence", "Brian Smith.", "Matt Kuchar", "the first sign of trouble was when drilling \"mud\" -- a mixture used to pressurize and lubricate the drills -- began falling onto the stern of his ship.\"", "Ricardo Valles de la Rosa,", "Elin Nordegren", "We Found Love", "immediate release into the United States of 17 Chinese Muslims who have been held for several years in the U.S. military facility at Guantanamo Bay, Cuba.", "millionaire's surtax,", "\"E! News\"", "about 50", "two-state solution", "Caster Semenya", "central London offices", "son of the most-wanted man in the world", "Israel and the United States", "South Africa", "the insurgency,", "all faiths", "Rosie Show", "Ricardo Valles de la Rosa,", "March 24,", "completely changed the business of music, to offering the world its first completely full-length computer-generated animated film with Pixar's \"Toy Story\" in 1995,", "chest", "100", "Frank", "The EU naval force", "five", "Joel \"Taz\" Di Gregorio,", "Ronald Cummings,", "man-made island shaped like a date palm tree", "the Somali coast", "10 municipal police officers", "hiring veterans as well as job training for all service members leaving the military.", "general astonishment", "northwestern Montana", "Iran test-launched a rocket capable of carrying a satellite,", "without bail", "February 12", "Kim Jong Il", "a place for another non-European Union player in Frank Rijkaard's squad.", "Chile", "separated in June 2004 when the boy's Brazilian mother, Bruna Bianchi Carneiro Ribeiro,", "Democratic VP candidate", "martial arts,", "poor, older than 55, rural residents or racial minorities,", "Tupolev TU-160,", "June 6, 1944,", "The escalating conflict in Mogadishu is having a devastating impact on the city's population causing enormous suffering and massive displacement,\"", "sovereignty over some or all of the current territory of the U.S. state of Texas", "advisory speed limits", "the Eurasian Plate", "horse", "julian taylor", "Brooklyn", "Tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Zanzibar", "dualism", "Wordsworth"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6353683793146554}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.23999999999999996, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.3529411764705882, 0.6666666666666666, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.15384615384615383, 1.0, 0.1111111111111111, 1.0, 1.0, 0.0, 0.12903225806451613, 1.0, 0.3, 0.787878787878788, 0.28571428571428575, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-2859", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2198", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-3025", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-5208"], "SR": 0.53125, "CSR": 0.5305706521739131, "EFR": 1.0, "Overall": 0.7170516304347826}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "high-end premium open-air shopping center, the Americana Manhasset", "Mayfair", "Minister for Social Protection", "28 January 1864,", "Arab", "Doggerland", "Larry Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "\"Eternal Flame\"", "\"Back to December\"", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Londonderry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "late 12th Century", "Christopher McCulloch", "Novel", "The Daily Stormer", "Fort Saint Anthony", "IT products and services, including storage systems, servers, workstations and data/voice communications equipment and services", "Japan", "1919", "Tak and the Power of Juju", "Washington, D.C.,", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\"", "Gerard \"Gerry\" Adams", "\"Kill Your Darlings\"", "Girls' Generation", "Robert Matthew Hurley", "September 1901", "Tuesday", "anabolic\u2013androgenic steroids", "North West England", "Division I", "\"Gliding Dance of the Maidens\"", "Kentucky", "1961", "1896", "1924", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "the Earth", "diamonds", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Asashoryu", "The Tupolev Tu-160 strategic bombers", "the Juilliard School", "lizard hips", "the Woodcraft Indians", "Inuit"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7827942251461988}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 0.4444444444444444, 0.5, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-68", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-375", "mrqa_hotpotqa-validation-4943", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-3029", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.65625, "CSR": 0.5323660714285714, "EFR": 1.0, "Overall": 0.7174107142857142}, {"timecode": 70, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.810546875, "KG": 0.48671875, "before_eval_results": {"predictions": ["Arkansas", "during the early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "port city of Aden", "Scott Eastwood", "Springfield, Massachusetts", "Patricia Veryan", "Dave Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Queensland", "\"master builder\" of mid-20th century New York City", "Waialua District", "St. Louis County", "Badfinger", "her performances of \"khyal\", \"thumri\", and \"bhajans\"", "XVideos", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "\"Currer Bell\"", "University of Nevada", "A mermaid", "1,500 ft", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech (Bohemian) and German (Franconian)", "\"Realty Bites\"", "Hanna, Alberta, Canada", "Manchester Victoria station", "drummer Seb Rochford", "\"My Love from the Star\"", "Captain Cook's Landing Place", "George I", "Seventeen", "37", "bass", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "beloved religious leaders", "Bill Russell", "Andre Agassi", "Vienna", "Phillies", "fill a million sandbags and place 700,000 around our city", "Caster Semenya", "expressed concern that taking the product off the market would result in hardship for terminally ill patients and their caregivers", "Cuyahoga River", "uranium", "Peter Sellers", "River Elbe"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7222842261904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-247", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-1449", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.609375, "CSR": 0.5334507042253521, "EFR": 1.0, "Overall": 0.7028620158450705}, {"timecode": 71, "before_eval_results": {"predictions": ["CNN/Opinion Research Corporation", "Marie-Therese Walter", "help at-risk youth", "the Russian air force", "female soldier", "three out of four", "Goa", "Iran", "100 percent", "Kenyan governments", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "hostile war zones", "National September 11 Memorial Museum", "Harlem luxury car dealer", "\"I don't think I'll be particularly extravagant.\"", "last year's", "woman", "1959.", "his", "269,000", "issued his first military orders as leader of North Korea", "iTunes", "a group of teenagers.", "Six", "Luis Carlos Ameida", "27-year-old", "outside influences", "nuclear warheads", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20", "Fiona MacKeown", "Sen. Barack Obama", "Ashley \"A.J.\" Jewell", "a motor scooter", "shelling of the compound", "Guinea, Myanmar, Sudan and Venezuela", "pine beetles", "Sudanese nor orphans", "Aspirin", "March 1", "Indo - Pacific", "mining", "quetzalcoatl", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "Oxygen", "the AH-64 Apache", "Harry Truman"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6360637626262626}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-774", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_triviaqa-validation-2418", "mrqa_triviaqa-validation-4549", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.59375, "CSR": 0.5342881944444444, "EFR": 1.0, "Overall": 0.7030295138888889}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997", "Sharyans Resources", "a marketing term for a vehicle that is both four - wheel - drive and primarily a road car", "Justice Lawrence John Wargrave, a retired judge, known as a `` hanging judge '' for liberally awarding the death penalty in murder cases", "Texas A&M University", "Stromal cells are the cells that support the parenchymal cells in any organ", "the Old Testament", "Anatomy ( Greek anatom\u0113, `` dissection '' )", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson in most appearances, by Ava Acres as a child and by Cloris Leachman as an old woman", "Eukarya -- called eukaryotes", "Mara Jade", "Gary Grimes as Hermie", "in meat technology", "Edward IV of England", "Ashrita Furman", "XXXX", "Jean Fernel", "2007 and 2008", "May 1980", "erosion", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "1960", "Ronald Reagan, who was 73 years, 274 days of age at the time of his election to a second term", "Johnny Logan", "revenge and karma", "Exodus 20 : 7", "England and Wales", "1996", "15,000 BC", "Idaho", "early Christians of Mesopotamia", "UTC \u2212 08 : 00", "Dr. Rajendra Prasad", "Carlos Alan Autry Jr. ( also known for a period of time as Carlos Brown ; born July 31, 1952 )", "Jay Baruchel", "Anthony Caruso", "its merengue and bachata music, both of which are the most popular forms of music in the country", "Butter Island off North Haven, Maine in the Penobscot Bay", "end of the 18th century", "1890s", "secure communication over a computer network", "3", "1939", "the BBC", "the fifth studio album by English rock band the Beatles", "land - living organisms, both alive and dead", "Felicity Huffman", "oregon glendower", "75", "m62", "Montana State University", "Sun Valley, Idaho", "president", "either heavy flannel or wool", "doctors", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "a river", "a 529 account offers tax benefits and will", "the Crow", "Madrid's Barajas International Airport during a stopover late Monday and informed authorities that he planned to request political asylum,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6666878213196905}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, false, true, true, true, true, true, false, true, true, false, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 0.0, 0.3636363636363636, 1.0, 0.3076923076923077, 0.0, 0.3333333333333333, 1.0, 1.0, 0.13333333333333333, 0.21052631578947367, 0.5, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 1.0, 1.0, 0.19999999999999998, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 0.38095238095238093, 1.0, 1.0, 0.11764705882352941, 1.0, 0.29629629629629634, 0.33333333333333337, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 0.09523809523809525]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-4652", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-2540", "mrqa_naturalquestions-validation-2680", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1414", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-4656", "mrqa_newsqa-validation-3504", "mrqa_searchqa-validation-3477", "mrqa_searchqa-validation-10906", "mrqa_newsqa-validation-646"], "SR": 0.515625, "CSR": 0.5340325342465753, "EFR": 0.967741935483871, "Overall": 0.6965267689460892}, {"timecode": 73, "before_eval_results": {"predictions": ["Venezuela", "The Fall Guy", "crown", "Maria Montessori", "Sue Grafton", "(Arthur George) Walker", "Sir Arthur Charles Clarke", "March of the Crosby", "Patrick Ewing", "Fletcher Christian", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liquor", "Texas", "a Condor", "John James Audubon", "Pontius Pilate", "Barry Goldwater", "neurons", "the halfpipe", "Louis Malle", "carioca", "Freakonomics", "George Washington Carver", "the Carboniferous Period", "Champagne", "Red Heat", "New Orleans", "the Dominican Republic", "a carrel", "love potions", "Prince William", "Sir Arthur Conan Doyle", "a crown", "Orion", "India", "carbon monoxide", "King John", "plug in", "a monster", "Cambodia", "manslaughter", "computer programming", "the Tennessee River", "Hipparchus", "Billy Idol", "the President", "a zodiac", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$1.528 billion", "on the left hand ring finger", "Conrad Murray", "oscarclaw", "slovia", "Sochi, Russia", "two years", "Manchester\u2013Boston Regional Airport", "President Obama", "two weeks after Black History Month", "American Civil Liberties Union", "July"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6597222222222222}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-13255", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-7195", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-12519", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-2145", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-15757", "mrqa_searchqa-validation-10919", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-5472", "mrqa_triviaqa-validation-2377", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.578125, "CSR": 0.5346283783783784, "EFR": 1.0, "Overall": 0.7030975506756757}, {"timecode": 74, "before_eval_results": {"predictions": ["sugarcane", "gail Jarvis", "germany", "liver", "private eye", "Spain", "Jack Ruby", "javelin throw", "british Airways", "Bachelor of Science", "cirencester", "Pete Best", "Bonnie and Clyde", "avatar", "c\u00f3rdoba", "st moritz", "Edmund Cartwright", "par-3", "Zeus", "Japanese silvergrass", "April", "Conan Doyle", "Wolfgang Amadeus Mozart", "honeybee", "canley", "\"The Nutcracker\"", "nauru", "adare", "muppets", "photography", "kirsty young", "Samuel Johnson", "question cards", "grizzly Adams", "ganges", "tabloid", "car door", "kolkata", "odeon", "Bangladesh", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Dame Kiri Te Kanawa", "Churchill Downs", "up stairs", "One Direction", "ulnar nerve", "Gibraltarian", "two", "Merck & Co.", "shortstop", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "President Clinton"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5568426724137931}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3972", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-7365", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-643", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621"], "SR": 0.546875, "CSR": 0.5347916666666667, "EFR": 1.0, "Overall": 0.7031302083333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Fitzroya cupressoides", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Bath, Maine", "Nippon Professional Baseball", "hiphop", "erotic thriller", "Poseidon", "Pearl Jam", "John Churchill", "Julian Dana William McMahon", "Hopi tribe", "North Kesteven", "Australian", "Annie Ida Jenny No\u00eb Haesendonck", "Steve Prohm", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Robert Sargent Shriver Jr.", "Seth Lesser", "Chinese Coffee", "Love and Theft", "Hallett Cove", "4145 ft above mean sea level", "University of Georgia", "just over 1 million", "Indian", "Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "Paul Avery", "25 October 1921", "Maersk Mc- Kinney", "J. Cole", "Idisi", "The Books", "port of Mazatl\u00e1n", "Danish", "London, England", "Rochdale", "1959", "Telugu and Tamil", "Centers for Medicare and Medicaid Services", "Laura Jeanne Reese Witherspoon", "Koch Industries", "William Howard Ashton", "Mindy Kaling", "3 October 1990", "September 21, 2016", "state - of - the - art photography of the band's performance", "earache", "Kwajalein atoll", "cuckoo", "$2 billion", "alcove.", "\"I'm really shocked to find out that the government has been using physicians and using potent medications in this way,\"", "Patrick", "the Tomb of the Unknown Soldier", "Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.546875, "QA-F1": 0.659672619047619}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.28571428571428575, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-5556", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-1908", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-3050", "mrqa_hotpotqa-validation-2057", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-2049", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-5996", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140"], "SR": 0.546875, "CSR": 0.5349506578947368, "EFR": 0.9655172413793104, "Overall": 0.6962654548548095}, {"timecode": 76, "before_eval_results": {"predictions": ["Pet Sounds", "Glenfinnan in the Scottish Highlands,", "\"Ars Gratia Artis\u201d", "Johann Strauss", "James Callaghan", "Cypress", "libor", "Dublin", "Pyrenees", "leprosy", "left", "Sid James", "avocado", "Catherine of Aragon", "The Double", "lexis", "Supertramp", "hula-Hoops", "Augustus", "\"One Night / I Got Stung\"", "Heston Blumenthal", "Arkansas", "fools and Horses", "Some Like It Hot", "\"Mr Loophole\"", "Ernest Hemingway", "Wolf Hall", "Ernests Gulbis", "Alberto Juantorena", "graffiti", "Friedrich Nietzsche", "Dee Caffari", "cheese", "Annie", "Kristiania", "piano", "Herman Melville\u2019s Moby Dick", "moss", "Sacred Wonders of Britain", "Aidensfield", "carrots", "Dr Tamseel", "Sea of Galilee", "1", "Helen of Troy", "Alzheimer's", "Inside the Fourth of July", "1966", "even break", "31536000", "Jordan", "arthropods", "Godfather Part II", "2018", "Colorado Rockies", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "head", "Rev. Alberto Cutie", "Michelle Obama", "the bass", "270", "a place", "the American Red Cross"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5982007575757576}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.1090909090909091, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-7006", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.546875, "CSR": 0.5351055194805194, "EFR": 0.9310344827586207, "Overall": 0.6893998754478281}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty", "The Kirchners", "iPods", "45 minutes, five days a week", "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\"", "Kris Allen", "Jared Polis", "ore Gold,", "Zimbabwe President Robert", "Harry Nicolaides,", "Zikuski", "April 2010.", "a mammoth's skull,", "\"[The e-mails]", "environmental", "his father's", "Iran", "head injury.", "\"Antichrist.\"", "African National Congress Deputy President Kgalema Motlanthe", "Hugo Chavez", "seven", "Frank's diary.", "\"The Da Vinci Code\"", "Matthew Fisher,", "Rawalpindi", "Colorado prosecutor", "Helmand province, Afghanistan.", "Climatecare", "dental work", "Ennis, County Clare", "United States", "physical surveillance, intelligence gathering and court-authorized electronic eavesdropping on dozens of telephones in which thousands of conversations were intercepted,", "Hamas", "The House page program", "At least 40", "four", "Courtney Love", "84-year-old", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "three", "undergoing renovation.", "Naples home.", "Washington State's decommissioned Hanford nuclear site,", "last month's Mumbai terror attacks", "sportswear", "Shanghai", "immediately accused the charity of kidnapping the children and concealing their identities.", "get better skin, burn fat and boost her energy.", "help nations trapped by hunger and extreme poverty, donating billions of dollars on health aid during the past two decades.", "a Canadian - American animated comedy television series created by Danny Antonucci for Cartoon Network, and the sixth of the network's Cartoon Cartoons", "meditation", "Raghu", "India and Pakistan", "allergic", "a lie detector", "music genres of electronic rock, electropop and R&B", "1963", "\"Black Abbots\"", "nurse", "Argentina", "Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6980773529999431}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, true, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 0.21052631578947364, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.18181818181818182, 0.0, 1.0, 1.0, 0.08695652173913043, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 0.15384615384615383, 0.8823529411764706, 0.05128205128205128, 0.0, 0.0, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877", "mrqa_searchqa-validation-356"], "SR": 0.609375, "CSR": 0.5360576923076923, "EFR": 1.0, "Overall": 0.7033834134615385}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O\"", "the Silk Road", "Denmark", "George Rogers Clark", "a mole", "a Cerberus", "Sweden", "Volleyball", "John Alden", "Ghost World", "Deuteronomy", "a map", "Japan", "Madison Avenue", "Job", "a pitch", "art deco", "Spider-Man", "Siddhartha Gautama", "Elie Wiesel", "Anna Friel", "Johnny Tremain", "a lieutenant colonel", "the National Archives", "Nostradamus", "Madrid", "Yuma", "the seventh continent", "Ian Fleming", "the Southern Christian Leadership Conference", "Moscow", "a Ford", "Beaux", "The Mormon Tabernacle Choir", "The Scarlet Letter", "Griffith", "Bangkok", "a New York war hero", "a positron", "a Crystal Ball", "Jefferson", "Jerusalem", "Pushing Daisies", "Cranberry", "a tzatziki sauce", "Shaugnhnessy", "a union", "a stroganoff", "canals", "Abraham", "a kangaroo court", "in ancient Mesopotamia", "Rachel Kelly Tucker", "a bridal shop", "London", "Kermadec Islands", "julius Caesar", "Greek mythology, the \"Mountain of the Goddess\": Mount Ida in Crete", "\"The Danny Kaye Show\"", "2012", "The Stooges comedy farce entitled \"Three Little Beers,\" to the Ben Hogan biopic \" Follow the Sun,\"", "the conversion -- which had been planned for years -- to accommodate people like Richter who had not been able to update their TVs.", "Victor Mejia Munera.", "The oceans"], "metric_results": {"EM": 0.578125, "QA-F1": 0.658099019036519}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, false, false, true, false, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.2666666666666667, 0.07692307692307693, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-3286", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-2911", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-5752", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.578125, "CSR": 0.5365901898734178, "EFR": 1.0, "Overall": 0.7034899129746836}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "De Wayne Warren", "a solitary figure who is not understood by others, but is actually wise", "Doug Pruzan", "a simple majority vote", "byte - level", "Nehemiah 1 : 5", "September 19, 2017", "A marriage officiant", "1661", "Hermann Ebbinghaus", "Agostino Bassi", "the batter would have reached first base safely but one or more of the additional base ( s ) reached was the result of the fielder's mistake", "low coercivity", "Incumbent Democratic mayor Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "9.0 -- 9.1 ( M )", "the Los Angeles Dodgers", "Dan Stevens", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze Banks", "10 June 1940", "the citizens", "a nominating committee composed of rock and roll historians selects names for the `` Performers '' category ( singers, vocal groups, bands, and instrumentalists of all kinds )", "Amanda Fuller", "`` The Forever People ''", "1997", "the mitochondrial membrane", "late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "until the 1960s", "Brian Stepanek", "2002", "Evermoist", "Pangaea or Pangea", "Selena Gomez", "Leslie and Ben", "the dress shop", "21,196 km ( 13,171 mi )", "February 27, 2007", "from 1916 to 1920", "March 2, 2016", "the Gemara", "the internal reproductive anatomy ( such as the uterus in females )", "Brundisium", "france", "Ukrainian", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "reduce the cost of auto repairs and insurance premium"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6757319647944647}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false], "QA-F1": [0.6666666666666666, 0.4, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.7407407407407407, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1714285714285714, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7000000000000001]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-5829", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-9034", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-866", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-454"], "SR": 0.546875, "CSR": 0.53671875, "EFR": 0.9655172413793104, "Overall": 0.6966190732758621}, {"timecode": 80, "UKR": 0.65234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.82421875, "KG": 0.46171875, "before_eval_results": {"predictions": ["h Sheila Sim", "Miranda v. Arizona.", "oscar Wilde", "vaughanich inlet", "violin", "Utrecht", "Vietnam", "Jane Austen", "georgia fox", "hand pulled, sail powered or powered by an engine", "brice", "korea", "CBS", "jazz", "Earthquake", "I Wanna be Like You", "joanne Woodward", "rococo", "gal", "great Dane", "self-forgetfulness, connectedness to the natural world and mysticism", "Cambodia", "jujitsu", "The Hunger Games", "head and neck", "11", "New Zealand", "the Prussian 2nd Army", "Beatrix Potter", "Whisky Galore", "Tunisia", "13", "Ted Kennedy", "cumbria", "chin", "Google", "the shoulder", "Iran", "downton Abbey", "bird", "Rudyard Kipling", "backgammon", "coventry pmore\u00ef\u00bf\u00bds", "(Hu) Rogers", "georgonzola", "barenboim", "personal chronological", "ear", "tree", "Imola Circuit", "trout", "Nia Long", "Lathrop `` Doc '' Brown, Ph. D.", "North Atlantic Ocean", "1961", "\"Boston Herald\" Rumor Clinic", "Lord Chancellor of England", "\"Britain's Got Talent\" finale", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "Lewis Carroll", "the largest research library", "Aung San Suu Kyi"], "metric_results": {"EM": 0.453125, "QA-F1": 0.532514880952381}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666665, 0.05714285714285714, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 1.0, 0.5, 0.0, 0.3333333333333333, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-6278", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-1687", "mrqa_triviaqa-validation-4889", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-1530", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_hotpotqa-validation-1657", "mrqa_newsqa-validation-4128", "mrqa_searchqa-validation-3317", "mrqa_searchqa-validation-6689", "mrqa_searchqa-validation-3618"], "SR": 0.453125, "CSR": 0.5356867283950617, "EFR": 0.9714285714285714, "Overall": 0.6890793099647267}, {"timecode": 81, "before_eval_results": {"predictions": ["Maggie Smith", "worcestershire", "Salvador domingo Felipe Jacinto Dal\u00ef\u00bf\u00bdnech", "van", "Illinois", "belgium", "paul Maskey", "Rafael Nadal", "tartar sauce", "euryoneme", "satrys", "ventdi", "kedushah", "martin van buren", "leeds", "lincoln", "funfairs", "white", "Jay-Z", "(University of) Nottingham Forest", "honda", "Runcorn", "Vietnam", "korea", "vincent van gogh", "sakhalin", "Croatia", "NBA", "steel", "davittle", "Henri Paul", "leadle", "penguin", "Samuel Johnson", "martini", "belgium", "Victor Hugo", "endosperm", "the Adriatic Sea", "heartburn", "music Stories", "HMS Conqueror", "m.W.", "braille", "Standard", "l Cynthia Nixon", "Hamlet", "vincent Tyler", "Patrick Henry", "126 mph", "Ukraine", "Eddie Murphy", "Pakistan", "Chris Coppola", "Thorgan Ganael Francis Hazard", "Lithuanian", "kent Hovind", "almost 100", "accusations of improper or criminal conduct.", "was being treated there after being admitted on Wednesday.", "Superman", "(University of) Norway", "the Towering Inferno", "member states"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5452628968253969}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false], "QA-F1": [1.0, 0.0, 0.28571428571428575, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-782", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4606", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2287", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.484375, "CSR": 0.5350609756097561, "EFR": 0.9696969696969697, "Overall": 0.6886078390613453}, {"timecode": 82, "before_eval_results": {"predictions": ["Nic\u00e9phore Ni\u00e9pce", "Netherlands", "tarn", "GM", "Sheffield", "segesta", "piano", "charleston", "pat cash", "chile", "Wild Atlantic Way", "Kyoto", "underwater diving", "repechage", "Steve Biko", "charleston", "peacock", "rita hayworth", "miss honey", "imola", "Albania", "antelope", "reptile", "boreas", "Ivan Basso", "bullfighting", "Elizabeth Berkley", "Playboy", "south africa", "Peter Ackroyd", "albert square", "Sven Goran Eriksson", "alvaro de Miranda Neto", "brian adams", "capital punishment", "Danny Alexander", "14", "Bangladesh", "adonis", "Papua New Guinea", "Lady Gaga", "Sunset Boulevard", "raging bull", "ars gratia artis", "bologna", "all things Must Pass", "air signs", "lunar new year", "Arabah", "d\u00e9j\u00e0 vu", "Lady Penelope", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "the British rock group Coldplay", "Greg Gorman and Helmut Newton", "American real estate developer, philanthropist and sports team", "Queen Isabella II", "Mexico", "Marines", "Arizona", "Frdric Chopin", "Indiana Jones", "Jakarta", "the Bellagio"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6453125000000001}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-6525", "mrqa_triviaqa-validation-4315", "mrqa_triviaqa-validation-2433", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-1664", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3551", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_hotpotqa-validation-4815", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_hotpotqa-validation-668"], "SR": 0.59375, "CSR": 0.5357680722891567, "EFR": 1.0, "Overall": 0.6948098644578314}, {"timecode": 83, "before_eval_results": {"predictions": ["Ricky Gervais", "the Green Arrow", "a parable", "alessiarux", "Spinal Tap", "Tennessee", "Detroit", "Ferris B Mueller's Day Off", "Korean", "Giza", "Ruth Bader Ginsburg", "Article VII", "touch", "The Old Fashioned", "the Osmonds", "Bonnie and Clyde", "a crayfishes", "the College of William and Mary", "a chimp", "an elk", "John Updike", "the Ganges", "Hindsight", "light", "a prostitution scandal", "a coelacanth", "Northanger Abbey", "Cheers", "Heidi", "Crosby, Stills & Nash", "Matt Leinart", "an ABO", "Charles Edward Stuart", "an albatross", "the Falklands", "a taro", "a quip", "a lighthouse", "white", "Dan Rather", "a nozzle", "Buffalo Bill", "a big Bang", "a pig", "Harvard", "a neuron", "Hawaii", "Pierian spring", "a hobo", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "witsunday", "muddy Waters", "bulgaria cop", "City and County of Honolulu", "the Australian coast", "1992", "criticized his father's parenting skills.", "Steven Chu", "Stella McCartney,", "death and destruction,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6202380952380953}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666665, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-15868", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6019", "mrqa_searchqa-validation-4565", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-14371", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7434", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-3457", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-4", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.53125, "CSR": 0.5357142857142857, "EFR": 1.0, "Overall": 0.6947991071428572}, {"timecode": 84, "before_eval_results": {"predictions": ["in the 1970s", "the end of August 2011", "sometime during the 1930s", "Lenny Jacobson", "the status line", "each team", "paved the way for integration and was a major victory of the Civil Rights Movement, and a model for many future impact litigation cases", "2010", "small packs", "687 ( Earth ) days", "the members of the actual club with the parading permit as well as the brass band", "the previous year's Palm Sunday celebrations", "Castleford", "the fourth C key from left", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "winter", "Samuel Taylor Coleridge's poem The Rime of the Ancient Mariner ( 1798 )", "the Octopus", "2001", "Lucius Verus", "marks the transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere ), when the duration of daylight becomes noticeably shorter and the temperature cools down considerably", "2004", "Xiu Li Dai and Yongge Dai", "Americans who served in the armed forces and as civilians", "Michael Crawford", "200 to 500 mg up to 7 ml", "gastrocnemius muscle", "metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "Austin, Texas", "1945", "Pebble Beach", "Andaman and Nicobar Islands", "neck", "Burj Khalifa", "Pangaea or Pangea", "the mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Johnny Cash & Willie Nelson", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing", "10 years", "2026", "eleven", "18", "As late as the 1890s, building regulations in London did not require working - class housing to have indoor toilets ; into the early 20th century", "Fats Waller", "Joanna Moskawa", "1962", "Loch Ness", "LFL", "griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "the Consumer Product Safety Commission", "\"That's ridiculous!\"", "a trailgator bars and attachment on 2 adult and 2 kids bikes", "Stones from the River", "Francis Ouimet", "\"Taz\" DiGregorio,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5790488540258761}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, false, true, true, true, false, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 1.0, 0.1142857142857143, 1.0, 0.0, 0.0, 0.1, 0.5333333333333333, 1.0, 0.625, 1.0, 1.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8181818181818181, 1.0, 0.2, 1.0, 0.14285714285714288, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.5, 0.6153846153846153, 0.6666666666666666, 0.0, 0.35294117647058826, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-10586", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-6903", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-8555", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7953", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_triviaqa-validation-2065", "mrqa_triviaqa-validation-3036", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-4132"], "SR": 0.421875, "CSR": 0.534375, "EFR": 0.918918918918919, "Overall": 0.6783150337837839}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "kyasaki", "sprint", "ganges", "gerry adams", "cancer", "Tom Mix", "Steve Jobs", "maggie Gilkeson", "nirvana", "Donna Summer", "the heel", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "chief Inspector japp", "cube", "Franklin Delano Roosevelt", "neurons", "porridge", "Yoshi", "Swordfish Mark I", "eardrum", "george best", "faggots", "11", "bluebird", "Australia and England", "pascal", "British Airways", "five", "shuttle Challenger", "The World is Not Enough", "Genoa", "Vienna", "glee", "david hockney", "iron", "Japan", "bayern munich", "Elektra King", "bologna, Italy", "Ciudad Ju\u00e1rez", "May Day", "chili peppers", "Madagascar", "beaujolais region", "david bercow", "kolkata", "dance", "davie brownie", "Charles Frederickson", "Forbes Burnham", "Game 1", "Dra\u017een Petrovi\u0107", "Costa del sol", "early Romantic period", "U.S. Open final defeat", "propofol,", "whether the reports about American Airlines are true or not doesn't really matter", "Versailles", "Zinedine Zidane", "Macduff", "a newt"], "metric_results": {"EM": 0.5625, "QA-F1": 0.5947916666666667}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-704", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-3408", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-1961", "mrqa_triviaqa-validation-5759", "mrqa_triviaqa-validation-1995", "mrqa_naturalquestions-validation-6711", "mrqa_naturalquestions-validation-8849", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261"], "SR": 0.5625, "CSR": 0.5347020348837209, "EFR": 1.0, "Overall": 0.6945966569767442}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "Tempo", "photographs, film and television", "Arthur Freed", "alt-right", "Runaways", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "8 May 1989", "Iran", "ribosomal", "murder", "London", "SBS", "quantum mechanics", "Donnchad mac Crinain, King of Scots", "February 12, 2014", "Forbes", "Anne and Georges", "David Villa S\u00e1nchez", "Big John Studd", "Super Bowl XXIX", "White Horse", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "The Social Network", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy XII", "Jim Thorpe", "De La Soul", "The Monster", "Shropshire Union Canal", "1621", "A skerry", "Oliver Parker", "FX", "Kalokuokamaile", "Colorado Buffaloes", "Roots: The Saga of an American Family", "five", "Jack Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "8 August 1907", "Maidstone, Kent", "strings of eight bits ( known as bytes )", "a action role - playing video game developed and published by Nippon Ichi Software for the PlayStation 4", "nathan leopold Jr.", "Bill Haley & His Comets", "george Carey", "Amanda Knox's aunt Janet Huff", "Number Ones", "Jeddah, Saudi Arabia,", "E.B. White", "Andrew Jackson", "Jefferson", "Willa Cather"], "metric_results": {"EM": 0.625, "QA-F1": 0.668354301948052}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.18181818181818182, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.4, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-3778", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-3231", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-2035", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4109", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-1530"], "SR": 0.625, "CSR": 0.5357399425287357, "EFR": 1.0, "Overall": 0.6948042385057471}, {"timecode": 87, "before_eval_results": {"predictions": ["one of WSU's most famous alumni, Edward R. Murrow", "Vision of the Future", "in 1754", "8 November 1978", "Hamlet", "Martin Ingerman", "New York Knicks", "McLaren-Honda", "Ferengi Quark", "The Spiderwick Chronicles", "a second spin-off", "a small-town girl who assumes the false identity of her former babysitter and current dominatrix", "Qualcomm", "water", "the 10-metre platform event", "Cincinnati Bengals", "on the shore, associated with \"the Waters of Death\" that Gilgamesh had to cross to reach Utnapishtim, the far-away", "his work on \"Nanny McPhee\", \"The Legend of Tarzan\"", "November 15, 1903", "Bury St Edmunds, Suffolk, England", "Rothschild banking dynasty", "Mr. Church", "\"Billboard\" Hot 100, \"Rich Girl\"", "Thomas Christopher Ince", "Peter 'Drago' Sell", "public house", "Los Angeles", "\"Me and You and everyone We Know\"", "Prussian-Lithuanian", "Taliban", "Darling", "Baldwin", "2 April 1977", "House of Commons", "William Finn", "Robert Sylvester Kelly", "Indian", "Type 212", "Barnoldswick", "before 7 November 1435", "Bob Gibson", "The S7 series", "729", "tenure", "Frederick Alexander", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire, in 1971", "Conservative", "The Division of Cook", "\"Peshwa\" (Prime Minister)", "Prafulla Chandra Ghosh", "innermost in the eye while the photoreceptive cells lie beyond", "Confederate forces attacked Fort Sumter in South Carolina, shortly after U.S. President Abraham Lincoln was inaugurated", "Western Samoa", "DeLorean", "katherine Cheung", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "Vivek Wadhwa,", "had to put him in \"solitary confinement.\"", "the very popular 225 H.O. (high output) model", "a scorpion", "bone", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6136767700501253}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3157894736842105, 0.4, 1.0, 0.7499999999999999, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.5, 1.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6, 0.0, 0.07142857142857144, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-5056", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3690", "mrqa_hotpotqa-validation-2094", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3226", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-4148", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.46875, "CSR": 0.5349786931818181, "EFR": 0.9705882352941176, "Overall": 0.6887696356951871}, {"timecode": 88, "before_eval_results": {"predictions": ["Sarah Palin", "\"La M\u00f4me Piaf\"", "Ulysses S. Grant", "Apollo", "richard wagner", "Atticus Finch", "The Peter Principle", "copper and zinc", "hammertone", "Dunfermline", "bison bison", "Edmund Cartwright", "Mary Poppins", "Essex", "mike mary Soros", "Samoa", "John Gorman", "The Daily Mirror", "copper", "Olympus Mons on Mars", "Poland", "Dee Caffari", "clue", "Belize", "tom Brooke Taylor", "llangollen", "prawns", "James Hogg", "mORPG", "fermanagh", "Colombia", "Kevin Painter", "llyn padarn", "katherine parr", "Muhammad Ali", "Carmen Miranda", "mishal Husain", "john McEnroe", "1982", "Estonia", "Sarajevo", "gluten", "Bangladeshi", "arthur ransome", "muthia muralitharan", "Ridley Scott", "five", "Futurama", "Adrian Edmondson", "63 to 144 inches", "1925", "September 29, 2017", "Chance's game - legged deputy", "from 13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "the surge", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "the devil's food cake", "Michelangelo Buonarroti", "Missouri", "The Jetsons"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6428503787878788}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, false, true, false, false, true, false, false, true, true, true, true, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-1202", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-3419", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-5159", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-602", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-7881", "mrqa_searchqa-validation-6490"], "SR": 0.59375, "CSR": 0.5356390449438202, "EFR": 1.0, "Overall": 0.694784058988764}, {"timecode": 89, "before_eval_results": {"predictions": ["1", "a Graphical user interface", "Scottie Pippen", "Vaseline Jelly", "savings", "silver", "Gone with the Wind", "Czanne's", "Nelly", "gladiators", "Finding Nemo", "the hyoid horns", "The Kite Runner", "a shark", "Nairobi", "Oprah Winfrey", "Dixie Chicks", "Apple pie", "Sonoma", "Best Buy", "the Ionian Sea", "Pope John Paul II", "Lobster Newburg", "Yemen", "David Geffen", "chariots", "Pablo Neruda", "the Fifth Amendment", "a mite", "Saturn", "The Nanny Diaries", "liquid crystals", "Robert Frost", "a dictum", "Nutella Cheesecake", "Crete", "Father Brown", "reuben", "The Outsiders", "the waltz", "Jacob Ming-Trent", "Jane Austen", "Wisconsin", "Charles Darnay", "Q", "When Harry Met Sally", "Mexico", "pumice", "Molson", "Jan and Dean", "Robin", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "andorra la vella", "mike faraday", "g Gerald r. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection,\"", "Fernando Gonzalez", "At least 14", "as spies for more than two years,"], "metric_results": {"EM": 0.6875, "QA-F1": 0.771780303030303}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.5, 0.5, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7272727272727273]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-1293", "mrqa_searchqa-validation-8406", "mrqa_searchqa-validation-11007", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-4328", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-14125", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-6674", "mrqa_triviaqa-validation-7721", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3145"], "SR": 0.6875, "CSR": 0.5373263888888888, "EFR": 1.0, "Overall": 0.6951215277777778}, {"timecode": 90, "UKR": 0.703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.853515625, "KG": 0.49296875, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "The Bonnie Banks o' Loch Lomond", "American reality television series", "Gweilo or gwailou", "\"The Royal Family\"", "The Ninth Gate", "James G. Kiernan", "daughter of Dejazmatch Yilma Makonnen, governor of Harar and niece of Emperor Haile Selassie of Ethiopia", "Erreway", "Protestant Christian", "\u00c6thelwald Moll", "Bellagio and The Mirage", "The Los Angeles Dance Theater", "Johnnie Ray", "Hampton University", "the special episode of the sixth season", "Jenji Kohan", "1", "'God Save the Queen'", "Scottish Premiership club Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012,", "melodic hard rock", "his eldest daughter Patricia, 2nd Countess Mountbatten", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "Bigfoot", "Cyclic Defrost", "England, Scotland, and Ireland", "\"Coal Miner's daughter\"", "Worcester", "1972", "Ang Lee", "Brad Silberling", "\"Move Your Body\"", "Ealdorman of Devon", "La Scala, Milan", "Orson Welles", "1987", "Schaffer", "Ryan Babel", "Melbourne's City Centre", "Lincoln Riley", "the world", "Enigma", "University of Nevada, Reno", "largest Mission Revival Style building in the United States", "Muhammad", "18", "Harlem River", "bulgaria", "$1", "sulfur dioxide", "1913,", "Juan Martin Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey,", "the Lord of the Rings", "Jaguar", "Wheat smut", "semi-autonomous organisational units within the National Health Service in England"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7501641281512605}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 1.0, 0.5, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2730", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-2708", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_searchqa-validation-5567"], "SR": 0.6875, "CSR": 0.5389766483516483, "EFR": 0.9, "Overall": 0.6977172046703297}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Phillip Schofield and Christine Bleakley", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "1954", "curved path", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "membranes of the body's cells", "USS Chesapeake", "the mid-1980s", "Russian citadels", "Charles Darwin and Alfred Russel Wallace", "marks locations in Google Maps", "Richard Stallman", "2004", "1940", "an armed conflict without the consent of the U.S. Congress", "perception of a decision, action, idea, business, person, group, entity", "oxidized", "Spanish", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "New England Patriots", "transmitted these messages over military telephone or radio communications nets using formal or informally developed codes built upon their native languages", "Zhu Yuanzhang", "The 1980 Summer Olympics", "American rock band Panic! at the Disco", "within a dorsal root ganglion", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017", "Julie Adams", "1881", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "550 quadrillion Imperial gallons", "Toot - Toot -- A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Theodore Roosevelt, Robert M. La Follette, Sr.", "August 5, 1937", "voters gathered as a tribe the members would be well known enough to each other that an outsider could be spotted", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings, when Cowboys quarterback Roger Staubach ( a Roman Catholic and fan of The Godfather Part II ( 1974 ),", "Payson, Lauren, and Kaylie", "2015, 2016", "Dr. Lexie Grey", "March 1 through May 10", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson", "dominoes", "Sparta", "Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "He checked male inmates out of jail and forced them to engage in sexual activity such as paddling in exchange for leniency.", "death of cardiac arrest", "\"For weeks,", "Monroe", "Babel", "James Bond", "Juan Ponce de Len"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5274477126351582}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, false, false, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.0, 0.3076923076923077, 0.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6086956521739131, 0.09302325581395347, 0.0, 0.0, 0.2222222222222222, 1.0, 0.05263157894736842, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.56, 0.5, 0.9411764705882353, 0.42424242424242425, 1.0, 0.0, 0.6666666666666666, 0.4, 0.6666666666666666, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-2084", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6874", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-283", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2777", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_hotpotqa-validation-5243", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.34375, "CSR": 0.5368546195652174, "EFR": 0.8571428571428571, "Overall": 0.688721370341615}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the Saskatchewan River", "Carlisle", "email", "Tahrir Square", "David Frost", "newbury racecourse", "Treatment of Terror Suspects", "Knutsford", "republic of Cabo Verde", "SpongeBob Squarepants", "Farthings", "China", "Maine", "Thomas Cranmer", "john ford", "Washington, D.C.,", "jack Sprat", "Ronnie", "conclave", "Dublin", "mayor of casterbridge", "foot", "Amsterdam", "John Lennon", "lusitania", "Anne of Cleves", "Australia", "antelope", "Portugal", "bechuanaland Protectorate", "Philippines", "blood", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "tears", "isambard Kingdom Brunel", "Canada", "vickers Vimy", "Jinnah International Airport", "south africa", "\u00c6thelred of Wessex", "peter paul Rubens", "john ford", "six", "Mendip Hills", "Burma", "Charles Taylor", "Pancho Villa", "changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Jerry Leiber", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County", "Linville, NC", "Lucky Dube,", "Iran", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "lobotomy"], "metric_results": {"EM": 0.65625, "QA-F1": 0.693563988095238}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5156", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-448", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-7278", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122", "mrqa_newsqa-validation-4082"], "SR": 0.65625, "CSR": 0.538138440860215, "EFR": 1.0, "Overall": 0.717549563172043}, {"timecode": 93, "before_eval_results": {"predictions": ["Peoria, Illinois", "keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "British former racing driver, commentator, and journalist from Scotland", "Coahuila, Mexico", "Atomic Kitten", "methylenedioxy meth", "Colin Vaines", "California", "racehorse breeder", "Jim Kelly", "Australian", "D\u00e2mbovi\u021ba River", "those who work with animals believe to be the line between using animals for entertainment purposes and abusing them", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "Mudvayne's third album \" Lost and Found\"", "1958", "Easter Rising of 1916", "November 23, 2012", "John Monash", "\u00c6thelstan", "Riverside Stadium", "left winger", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "considered a heavy metal band", "Goddess of Pop", "125 lb (57 kg)", "chocolate-colored", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "\" Neighbours\"", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "gulls", "chariot", "Louisiana", "a \"stressed and tired force\" made vulnerable by multiple deployments,", "Citizens", "Tuesday", "The African Queen", "cats", "Gibraltar", "a numeric scale used to specify the acidity or basicity of an aqueous solution"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6527529761904762}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false], "QA-F1": [0.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14285714285714288]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5856", "mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-657", "mrqa_hotpotqa-validation-2540", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.5625, "CSR": 0.5383976063829787, "EFR": 0.9642857142857143, "Overall": 0.7104585391337387}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "Guinea", "mayflower", "four", "Guardian News & Media", "tartan", "Toy Story", "GM Korea", "lungs", "Periodic Table", "The Left Book Club", "belgia", "st Columba", "Donald Sutherland", "New York City", "Ethiopia", "Cardiff", "sternum", "pressure", "James Murdoch", "Chicago", "fluids", "hanoncourt", "Squeeze", "The Rolling Stones", "Robert Plant", "Jerry Seinfeld", "stern tube", "kia", "lemurs", "Sir Robert Walpole", "eight", "andorra", "a braffin", "anne", "kunsky", "st paul\\'s", "27", "Formula One", "squash", "Mary Decker", "Godwin Austen", "france", "Birdman of Alcatraz", "Bernardo Bertolucci", "Amerigo Vespucci", "the buck", "Lady Godiva", "elephant house", "welding boots", "a gown", "1940s", "0.30 in", "the digestive systems of many organisms", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4", "Kenyan", "UNICEF\"", "A B", "Florence", "Saturn", "Globalization"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6860119047619048}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5663", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-2197", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2932", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-1469", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-330", "mrqa_triviaqa-validation-1058", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-431", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-7226", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2229", "mrqa_searchqa-validation-12145"], "SR": 0.640625, "CSR": 0.5394736842105263, "EFR": 1.0, "Overall": 0.7178166118421053}, {"timecode": 95, "before_eval_results": {"predictions": ["war drama", "air-cushioned sole", "local South Australian and Australian produced content", "Gal\u00e1pagos", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Tasmania", "Jim Kelly", "Stern-Plaza", "Lieutenant Martin", "Girls' Generation", "June 12, 2017", "two or three", "Dra\u017een Petrovi\u0107", "Prussia", "David Wells", "Roslin Castle", "two", "Argentine cuisine", "13th century", "Pru Goward", "Manchester United", "Matt Groening", "Hazel Keech", "Minami-Tori-shima", "1950", "Jesus", "Sulla", "\"Riot Act\"", "Larry Gatlin & the Gatlin Brothers", "right-hand batsman", "black nationalism", "Futurama", "Bayern", "Deftones", "\"Pastime Paradise\"", "Clitheroe Football Club", "Six Flags Great Adventure", "\"Cleopatra\"", "The Fault in Our Stars", "Liesl", "how the Grinch Stole Christmas", "sheepskin", "White Horse", "banjo player", "Yellow fever", "Elise Stefanik", "Francis Schaeffer", "off the northeast coast of Australia", "between 3.9 and 5.5 Mb / L ( 70 to 100 mg / dL )", "Speaker of the House of Representatives", "paul mcknard", "capture of Quebec", "cold comfort farm", "red", "lightning strikes", "Gaddafi's death.", "Guernsey", "the Southern Christian Leadership Conference", "Prussia", "meningeal"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6010044642857143}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false], "QA-F1": [0.0, 0.2666666666666667, 0.25, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428572, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-5223", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-648", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-5337", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_searchqa-validation-14797", "mrqa_naturalquestions-validation-7342"], "SR": 0.515625, "CSR": 0.5392252604166667, "EFR": 1.0, "Overall": 0.7177669270833335}, {"timecode": 96, "before_eval_results": {"predictions": ["160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "pyloric valve", "Jackie Van Beek", "Ephesus", "Michael English", "Phillip Paley", "Germany", "Robert Andrews Millikan, who had previously determined the charge of the electron, produced experimental results in perfect accord with Einstein's predictions.", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi", "Jenny Humphrey", "Panning", "31 March 1909", "$66.5 million", "pathology", "April 3, 1973", "The epidermis", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "pigs", "A standard form contract", "1657", "The musical premiered on October 16, 2012, at Ars Nova ; directed by Rachel Chavkin the show was staged as an immersive production, with action happening around and among the audience", "American country music duo Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75", "Miller Lite", "Oona Castilla Chaplin", "The speech compares the world to a stage and life to a play, and catalogues the seven stages of a man's life, sometimes referred to as the seven ages of man", "Lulu, composed by John Barry, and the lyrics to the song were written by Don Black", "the NFL", "Steve Russell", "British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag", "Profit maximization", "Melbourne", "April 1, 2016", "San Antonio", "1,281,900", "Michael Phelps", "royal oak", "The Krankies", "France", "Province of Syracuse", "June 11, 1986", "3-2", "200", "Republican Gov. Bobby Jindal", "\"reshit\"", "John Deere", "the shear", "curfew"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7213803062840956}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.48275862068965514, 1.0, 0.046511627906976744, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.06451612903225806, 0.13333333333333333, 1.0, 1.0, 0.8888888888888888, 1.0, 1.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-6612", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-16855"], "SR": 0.640625, "CSR": 0.540270618556701, "EFR": 0.9130434782608695, "Overall": 0.7005846943635141}, {"timecode": 97, "before_eval_results": {"predictions": ["Tchaikovsky", "dark places", "the Dreams", "the boll weevil", "touchpad", "Wikipedia", "the Sundance Kid", "Buddhist", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Edgar Allan Poe", "(Sergey) Brin", "Sanders", "American alternative rock band", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "The School of Athens", "an ant", "birkenstock", "The Firebird", "Hafnium", "flax", "the Muse", "the Wachowski brothers", "Rumpole of the Bailey", "John Quincy Adams", "Steve Austin", "Kurt Warner", "XL", "a small retail store", "Beauty and the Beast", "Ratatouille", "pro bono", "a bear", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "the French", "Crayola", "The Man in the Gray Flannel Suit", "Assimilation", "bright orange", "Isaiah Amir Mustafa", "September 2000", "Americans acting under orders", "Mike Hammer", "\"The Crow\"", "L.P. Hartley", "Tifinagh", "the European Champion Clubs' Cup", "second largest", "North Korea", "alcohol", "peppermint oil", "Prada"], "metric_results": {"EM": 0.59375, "QA-F1": 0.653155637254902}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.23529411764705882, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-16570", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-4924", "mrqa_searchqa-validation-6988", "mrqa_naturalquestions-validation-8759", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-96"], "SR": 0.59375, "CSR": 0.5408163265306123, "EFR": 1.0, "Overall": 0.7180851403061225}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Elliott Trudeau", "Red", "a barge within reach of the transport ship's tackle so that they can be loaded", "Billy Joel", "the cornea", "ginger ale", "Rumpole", "a telephone", "the light bulb", "Spider-Man", "Atlanta", "China", "Dick Tracy", "Queen Latifah", "James A. Van Allen", "beer", "Rinzai", "El", "Zenith", "baboon", "wine", "George Costanza", "the Q- tip", "natural selection", "Massachusetts", "the Battle of the Bulge", "an cylindrically Symmetric wheel", "W. Somerset Maugham", "the Two Sicilies", "the Battle of Trafalgar", "a republic", "the Golden Hind", "West Point", "Enrico Fermi", "dora the Explorer", "the pituitary gland", "Alfred Hitchcock", "Hank Aaron", "Special Boat Teams", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Christopher Columbus", "Joseph Haydn", "Meringue", "Babe Zaharias", "the Thought Police", "calcium", "four", "James Hutton", "961", "wollem de zwijger (William the Silent)", "the god", "mary seacole", "Orchard Central", "Fort Hood, Texas", "OutKast", "iPods", "suspend all", "Wednesday", "Nick Sager"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6729166666666667}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5468", "mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-15786", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-15890", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-6821", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-7667", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040"], "SR": 0.578125, "CSR": 0.5411931818181819, "EFR": 0.8888888888888888, "Overall": 0.6959382891414142}, {"timecode": 99, "UKR": 0.677734375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.8125, "KG": 0.48515625, "before_eval_results": {"predictions": ["Niles", "Andrea Brooks", "July 14, 2017", "2018", "classical neurology", "Hem Chandra Bose, Azizul Haque and Sir Edward Henry", "potential of hydrogen", "Peking", "Bart Howard", "2013", "Ozzie Smith", "about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "George Harrison", "the Charbagh structure", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2017", "Natural - language processing ( NLP )", "six", "a resource called / images / logo", "$2 million", "The Fellowship of the Ring ( 2001 ), The Two Towers ( 2002 ) and The Return of the King ( 2003 )", "Sohrai", "the Monsoons from the south atlantic ocean arrives in central Nigeria in July bringing with it high humidity, heavy cloud cover and heavy rainfall", "celebrity alumna Cecil Lockhart", "James Long", "the fifth-most populous city in Florida and the largest in the state that is not a county seat", "April 13, 2018", "starting quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "the ending of the second season episode `` pretty Much Dead Already ''", "Disha Vakani", "Nickelback", "1999", "King Willem - Alexander", "The horn line at the end is performed by the Phenix Horns from Earth, Wind & Fire ( particularly `` That's the Way of the World '' )", "Deuteronomy 5 : 4 -- 25", "a revolution or orbital revolution", "Ren\u00e9 Georges Hermann - Paul", "Horace Lawson Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "December 1, 1969", "1996", "Manley", "Chaplin", "Francis Matthews", "Hymenaeus", "1907", "1776", "Stapleton Cotton", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "eight-day", "101", "Spain", "Elijah", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6608112458341117}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9268292682926829, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.42857142857142855, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-9962", "mrqa_naturalquestions-validation-1722", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-4401", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-8099", "mrqa_naturalquestions-validation-791", "mrqa_naturalquestions-validation-177", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-2717", "mrqa_naturalquestions-validation-1679", "mrqa_triviaqa-validation-2741", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-111", "mrqa_searchqa-validation-3524"], "SR": 0.578125, "CSR": 0.5415625, "EFR": 0.9259259259259259, "Overall": 0.6885758101851852}]}