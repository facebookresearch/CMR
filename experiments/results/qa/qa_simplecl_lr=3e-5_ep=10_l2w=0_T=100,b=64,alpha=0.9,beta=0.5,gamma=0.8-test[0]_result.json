{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]', diff_loss_weight=0.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_simplecl_lr=3e-5_ep=10_l2w=0_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[0]_result.json', stream_id=0, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4200, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["67.9", "Mike Carey", "rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding", "their greatest common divisor is one", "the Official Report", "immunoinformatics", "lupus erythematosus", "Kublai Khan", "New Testament", "1926", "other ctenophores", "60%", "he was illiterate in Czech", "architect or engineer", "British", "Gateshead Council", "Book of Genesis", "Shing-Tung Yau", "complexity classes", "Mexico", "cabinet", "after its 1977 merger with Radcliffe College", "The Master", "chastity", "Mark Woods", "one another", "100% oxygen", "Steam engines", "the wedding banquet", "the Ohio Company of Virginia", "CBS and NBC", "aircraft manufacturing", "a school or other place of formal education", "Times Square Studios", "Normans and Norman", "it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "decreases", "Duke Richard II of Normandy, and King Ethelred II of England", "Royal Shakespeare Company", "books and articles", "between 1835 and 1842", "Edmonton, Canada", "rises in sea levels", "it is neither zero nor a unit", "University of Chicago College Bowl Team", "algorithms have been written that solve the problem in reasonable times in most cases", "13.34%", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "49\u201315", "The Mongols' extensive West Asian and European contacts", "east-west", "the kip", "Croatia", "railway locomotives", "the law is no longer to be taught to Christians but belonged only to city hall", "Josh Norman", "1964 and 1968", "expelled Jews", "Elton Rule", "gravel", "11:28", "Super Bowl XLVII", "a fee per unit of information transmitted", "seven"], "metric_results": {"EM": 0.875, "QA-F1": 0.9101325145442792}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3846153846153846, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 1.0, 1.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1187", "mrqa_squad-validation-2853", "mrqa_squad-validation-6986", "mrqa_squad-validation-2704", "mrqa_squad-validation-1891", "mrqa_squad-validation-1161", "mrqa_squad-validation-1089", "mrqa_squad-validation-2473"], "SR": 0.875, "CSR": 0.875, "EFR": 1.0, "Overall": 0.9375}, {"timecode": 1, "before_eval_results": {"predictions": ["\"Fall of the Eleventh\"", "The input string", "European Parliament", "the Meuse", "overseas colonies", "Kings Canyon Avenue and Clovis Avenue", "entertainment", "Astra 2A", "poison", "atoon", "Orange", "the BBC", "Dolby Digital", "shocked", "Executive Vice President of Football Operations", "Thomas Edison", "was particularly forceful, stating that British colonists would not be safe as long as the French were present.", "least prejudiced", "inferior", "Johann Tetzel", "miniature cydippids", "main porch", "the courts of member states", "Shah's decision to divide his army into small groups concentrated in various cities", "1st century BC", "Iberia", "Silas B. Cobb", "Aristotle", "The Swahili", "the sheepshanks Gallery", "1968", "heavy/highway, heavy civil or heavy engineering", "type III secretion system", "Commission v Italy", "Gary Kubiak", "The European Court of Justice", "oxygen compounds", "1933", "Endosymbiotic gene transfer", "proplastids", "Triassic Period of the Mesozoic Era", "Conrad of Montferrat", "sacramental union", "reciprocating", "the Arabs and much of the rest of the Third World", "mineral deposits", "Grand Canal d'Alsace", "Vince Lombardi Trophy", "\u00dcberseering BV v Nordic Construction GmbH", "first 15 years", "Neoclassical economics", "Variable lymphocyte receptors", "Exploration", "Elder", "internal migration and urbanisation", "high risk preparations and some other compounding functions", "water level", "two", "James", "A driver's license is an official document permitting a specific individual to operate one or more types of motorized vehicles", "radioisotope thermoelectric generator", "Mickey Mantle", "A lymphocytes is one of the subtypes of white blood cell in a vertebrate's immune system", "Amsterdam"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7073051948051948}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.1818181818181818, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-7689", "mrqa_squad-validation-1767", "mrqa_squad-validation-3922", "mrqa_squad-validation-6029", "mrqa_squad-validation-2672", "mrqa_squad-validation-6211", "mrqa_squad-validation-1902", "mrqa_squad-validation-375", "mrqa_squad-validation-10186", "mrqa_squad-validation-6163", "mrqa_squad-validation-3511", "mrqa_squad-validation-8927", "mrqa_squad-validation-9325", "mrqa_squad-validation-3370", "mrqa_squad-validation-7635", "mrqa_squad-validation-2315", "mrqa_naturalquestions-validation-5378", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-9342", "mrqa_newsqa-validation-1945"], "SR": 0.671875, "CSR": 0.7734375, "EFR": 1.0, "Overall": 0.88671875}, {"timecode": 2, "before_eval_results": {"predictions": ["six quadrangles", "1884", "1870 to 1939", "2010", "unit-dose, or a single doses of medicine", "2003", "Budapest Telephone Exchange", "induction motor", "markets", "to avoid being targeted by the boycott", "socialist realism", "Ten", "city's tax base dissipated", "6000", "St. Bartholomew's Day massacre", "early twentieth century homes, many of which have been restored in recent decades.", "Thesis 86", "five or more seats", "in a glass case", "1,320 kilometres (820 miles)", "force-free magnetic fields", "a Tatar chieftain, Tem\u00fcjin-\u00fcge", "The Three Doctors", "San Francisco Bay Area's Levi's Stadium", "prime", "St. Lawrence and Mississippi watersheds", "2010", "cholera", "168,637", "four", "faith", "25", "river Deabolis", "ten", "to spearhead the regeneration of the North-East", "late 1920s", "2007", "1936", "1968", "the European Parliament and the Council of the European Union", "typhus, smallpox and respiratory infections", "California", "Spanish", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "the sheepshanks Gallery", "Four thousand", "Prime ideals", "Stairs", "a lack of understanding of the legal ramifications, or due to a fear of seeming rude", "MHC class I molecules", "the Great Fire of London", "in his lab and elsewhere", "Rudy Clark", "Art Carney", "Ren\u00e9 Verdon", "honey bees may be the state's most valuable export", "July 4, 1776", "May 5, 1904", "The Lykan Hypersport is a Lebanese limited production supercar built by W Motors, a United Arab Emirates based company, founded in 2012 in Lebanon with the collaboration of Lebanese, French and Italian engineers", "E \u00d7 12", "digitization of social systems", "Trace Adkins", "he checked himself into a Los Angeles mental institution in an effort to kick the habit.", "refugees agency said on Tuesday.Islamist fighters exchange gunfire with government forces in Mogadishu on July 3."], "metric_results": {"EM": 0.75, "QA-F1": 0.828937522589681}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 0.47058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.12903225806451613, 0.0, 1.0, 1.0, 0.761904761904762, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6526", "mrqa_squad-validation-1277", "mrqa_squad-validation-7246", "mrqa_squad-validation-5751", "mrqa_squad-validation-4669", "mrqa_squad-validation-133", "mrqa_squad-validation-9012", "mrqa_squad-validation-4546", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-1173", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-3164"], "SR": 0.75, "CSR": 0.765625, "EFR": 1.0, "Overall": 0.8828125}, {"timecode": 3, "before_eval_results": {"predictions": ["the Hamiltonian path problem", "Thomas Edison", "systematic economic inequalities", "July 31, 1995", "vitamin D.", "Thomas Edison", "1987", "terra nullius", "Leukocytes", "research, exhibitions and other shows.", "private individuals, private organizations or rarely, religious groups", "along the coast", "hostile country", "17", "Book of Discipline", "67.9", "Turkey", "by over 100%", "Saffir-Simpson Scale", "New Orleans", "3,000", "Earth", "Bruno Mars", "an archipelago-like estuary", "Antigone", "Soviet Union.", "Dodge D-50", "over the age of 18", "in force-free magnetic fields.", "polynomial-time", "IPCC Members", "several years", "Danny Trevathan", "Albert of Mainz", "patient care rounds drug product selection.", "Thomas Coke", "Colony of Victoria Act 1855", "Anglo-Saxon populations", "John Debney", "Geordie.", "Ancient Greeks.", "Budget cuts", "Systemic acquired resistance", "one darkened lens; the picture would look normal to those viewers who watched without the glasses.", "Legislative Assembly", "the desire to prevent things that are indisputably bad", "any member", "Tyrion", "Manchuria", "Kenneth Cook", "to capitalize on her publicity", "1937", "Wah - Wah", "James W. Marshall", "1979", "January 12, 2017", "a loop", "Kyrie Irving", "Charles Carson", "Morgan Freeman", "Eda Reiss Merin", "the Seven Cities of Gold", "in the keyboard", "fibula l pin"], "metric_results": {"EM": 0.71875, "QA-F1": 0.7621108286815523}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false], "QA-F1": [0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4210526315789473, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1760", "mrqa_squad-validation-9764", "mrqa_squad-validation-6981", "mrqa_squad-validation-3770", "mrqa_squad-validation-3096", "mrqa_squad-validation-100", "mrqa_squad-validation-1436", "mrqa_squad-validation-1764", "mrqa_squad-validation-8523", "mrqa_squad-validation-7713", "mrqa_squad-validation-7838", "mrqa_naturalquestions-validation-5370", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-8657", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-6338"], "SR": 0.71875, "CSR": 0.75390625, "EFR": 1.0, "Overall": 0.876953125}, {"timecode": 4, "before_eval_results": {"predictions": ["immune surveillance", "Arizona Cardinals", "10,000", "3 January 1521", "Scotland Act 1998", "AC", "Chuck Howley (MVP of Super Bowl XLI and current Broncos quarterback)", "1\u20133 \u03bcm thick", "Air Force", "Meiji Restoration", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "842 pounds", "June 4, 2014", "recast as decision problems", "Gateshead Council", "more than 70", "Gamal Abdul Nasser", "the Bible", "completes replication with a rolling circle mechanism.", "inner core", "Hangzhou", "phylogen branches", "12 December 1963", "UNESCO World Heritage Site", "January 27, 1967", "cortisol and catecholamines", "constituency seats", "drummes", "Tower Theatre", "the chloroplast is cleaved more or less evenly", "a coach using the whole gamut of psychology to get each new class of rookie off the bench and into the game.", "North American Aviation", "banded iron", "Arizona Cardinals", "Super Bowl 50", "major business districts", "HO", "soluble components (molecules) found in the organism\u2019s \u201chumors\u201d", "two", "destruction of the forest", "the Commission", "the net mechanical energy", "Paul Greene", "Saul", "Stopping by Woods on a Snowy Evening", "southwestern Colorado and northwestern New Mexico", "Paul", "Carol Worthington", "1997", "accomplish the objectives of the organization", "3 October 1990", "the federal government", "Jerry Leiber and Mike Stoller", "Heroes and Villains", "both Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "positive, zero, or negative scalar quantity", "June 12, 2018", "tracy lawrence", "22 \u00b0 00 \u2032 N", "James Martin Lafferty", "La Dame aux cam\u00e9lias", "C7", "his company Polo", "the (human) mind is a \"blank slate\" without rules for"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6620862050960735}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, true, false, false, false, false, false, true, true, true, false, false, true, true, false, false, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.10526315789473684, 1.0, 1.0, 0.0, 0.6666666666666666, 0.8, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 0.7777777777777778, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-635", "mrqa_squad-validation-8750", "mrqa_squad-validation-6327", "mrqa_squad-validation-3811", "mrqa_squad-validation-1649", "mrqa_squad-validation-8683", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-8901", "mrqa_squad-validation-1920", "mrqa_squad-validation-73", "mrqa_squad-validation-186", "mrqa_squad-validation-2632", "mrqa_squad-validation-6614", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-5451", "mrqa_triviaqa-validation-4831", "mrqa_hotpotqa-validation-5838", "mrqa_newsqa-validation-1003", "mrqa_searchqa-validation-5338"], "SR": 0.546875, "CSR": 0.7125, "EFR": 0.9310344827586207, "Overall": 0.8217672413793103}, {"timecode": 5, "before_eval_results": {"predictions": ["orientalism", "Director", "east", "NL and NC", "1534", "April 1887", "\u00d6gedei Khan", "nearby open spaces", "on the West Side", "John Pell, Lord of Pelham Manor", "Dai Setsen", "821,784", "salvaging a country usually seen as one of the most stable and prosperous in Africa", "buoyancy", "ancestors", "divergence", "Islamist", "satellite television", "the deportation of the French-speaking Acadian population from the area", "justifying grace", "131", "chairman", "Maria Sk\u0142odowska-Curie Institute of Oncology", "winter of 1973\u201374", "P", "BAFTA", "Off-Off Campus", "the General Conference", "Super Bowl XLIV", "Sam Chisholm", "the mother", "LDS", "the most cost efficient bidder", "non-governmental", "non-Catholics", "southern and central parts of France", "the Aveo", "Luther's education", "1998", "administrative supervision", "XXXX", "Thunder Road", "Sylvester Stallone", "present ( 2016 -- 2018, contemporaneous with airing ) and a storyline taking place at a set time in the past", "Mohammad Reza Pahlavi", "Thomas Mundy Peterson", "biblical Book of Exodus", "two goods", "season seven", "TLC", "the town of Acolman, just north of Mexico City", "1987", "Asuka", "4 September 1936", "Amy Wong", "2002", "Pasek and Paul", "U.S. service members who have died without their remains being identified", "mostly by women", "George Bernard Shaw", "William Bradford", "Bryant Purvis", "40-40 in the fifth set", "40,400"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7436011904761906}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8386", "mrqa_squad-validation-3492", "mrqa_squad-validation-9416", "mrqa_squad-validation-1759", "mrqa_squad-validation-7819", "mrqa_squad-validation-2314", "mrqa_squad-validation-6752", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-10093", "mrqa_searchqa-validation-13515", "mrqa_hotpotqa-validation-1989"], "SR": 0.703125, "CSR": 0.7109375, "EFR": 1.0, "Overall": 0.85546875}, {"timecode": 6, "before_eval_results": {"predictions": ["monophyletic", "computational power", "late 1960s and early 1970s", "rapid combustion", "Stanford", "a continuous supply of gaseous oxygen to be pumped through a pipeline", "red algal chloroplast lineage", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "Citadel Media", "lands west of the Appalachian Mountains", "two", "ten", "clerical marriage", "average workers", "Catholic", "late 1886", "potentially dangerous", "Golovin", "changes or decreases in response to the applied force up to an upper limit determined by the characteristics of the contact between the surface and the object", "near the center of the chloroplast", "the last 7000 years", "3.6%", "God", "C. J. Anderson", "Robert R. Gilruth", "1469", "arid and semi-arid areas with near-desert landscapes", "$200,000", "slightly more than atmospheric pressure", "with observations", "6800", "after their second year", "produced no sugar", "Giuliano da Sangallo", "two", "Chuck Connors", "Eva Marie", "Lincoln Logs", "a tin star", "in the bloodstream", "treats a head with horns like an ox and a long tufted tail", "Jerusalem", "Hamlet", "Plymouth Rock", "President Abraham Lincoln", "Martina Hingis", "tuna", "TESLAR Satellite", "a kind of superman", "Gentlemen Prefer Blondes", "Decoupage", "Cesium, for example, has the electron configuration [Xe]6s1, which indicates one valence electron outside a closed shell", "Bouvier", "Paris", "Titanic", "a \"inversion\"", "New Zealand", "Alan Ladd", "January 15, 2010", "Mark Neveldine and Brian Taylor", "four", "treats", "Edward Kenway", "1770 BC"], "metric_results": {"EM": 0.625, "QA-F1": 0.6813532086120402}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.17391304347826084, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 1.0, 0.25, 0.0, 1.0, 1.0, 0.30769230769230765, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.125, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.25, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-5788", "mrqa_squad-validation-3676", "mrqa_squad-validation-2754", "mrqa_squad-validation-10316", "mrqa_squad-validation-8900", "mrqa_squad-validation-8399", "mrqa_squad-validation-3479", "mrqa_squad-validation-8529", "mrqa_squad-validation-8924", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-6514", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-3032", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-7615", "mrqa_naturalquestions-validation-8909", "mrqa_searchqa-validation-9828", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-8659"], "SR": 0.625, "CSR": 0.6986607142857143, "EFR": 1.0, "Overall": 0.8493303571428572}, {"timecode": 7, "before_eval_results": {"predictions": ["a free state", "Oireachtas funds", "May 21, 2013", "Warsaw", "communications between Yuan dynasty and its ally and subordinate in Persia, the Ilkhanate", "n > 3", "Spanish moss", "frequency and severity of micrometeorite impacts", "Charles Avison", "Warraghiggey", "Pedro Men\u00e9ndez de Avil\u00e9s", "a mainline Protestant Methodist denomination", "Capital Cities Communications", "up to 40 km wide", "meritocracy", "a very reactive allotrope of oxygen", "bitstrings", "markets", "a Gender pay gap in favor of males in the labor market", "2014", "By 9000 BP", "Neoclassical economics", "d'Hondt method", "home viewers who made tape recordings of the show", "a Kenyan coastal town of Kilifi", "first set of endosymbiotic events", "Abe Silverstein", "embroidery", "three", "1823", "British Sky Broadcasting Group plc", "Pylos and Thebes", "Supreme Court", "the north wing of the State Capitol in Saint Paul", "Syracuse", "artist and graffiti writer", "General Manager", "Bergen", "John Lennon", "David Irving", "an album", "croatan, Nantahala, and Uwharrie", "15,000 people", "2016 World Indoor Championships", "a German World War I fighter ace credited with 35 victories", "psilocybin", "Washington, D.C.", "Hero", "a state, which was both free from slavery, and ruled by non-whites and former captives", "Jena Malone", "1865", "National Hockey League", "one", "Big 12 Conference", "Walldorf", "a Douglas-long Beach built B-17G-95-DL", "nursery rhyme", "Necator americanus and Ancy lostoma duodenale", "Hamelin", "lizards", "to avoid the traffic", "eva", "C. S. Forester", "0"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6591846999427301}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, true, true, false, true, false, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.10526315789473685, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.6666666666666666, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.588235294117647, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8571428571428571, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8066", "mrqa_squad-validation-5351", "mrqa_squad-validation-9195", "mrqa_squad-validation-3497", "mrqa_squad-validation-7447", "mrqa_squad-validation-9436", "mrqa_squad-validation-9532", "mrqa_squad-validation-8453", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3387", "mrqa_hotpotqa-validation-5346", "mrqa_naturalquestions-validation-6200", "mrqa_newsqa-validation-4024", "mrqa_searchqa-validation-7219", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-221"], "SR": 0.546875, "CSR": 0.6796875, "EFR": 1.0, "Overall": 0.83984375}, {"timecode": 8, "before_eval_results": {"predictions": ["a rock concert", "50-yard line", "an increase in skilled workers", "12 December 1964", "speed-up theorem", "Schr\u00f6dinger equation", "Guinness World Records", "nominate speakers", "Louis Agassiz", "Annual Status of Education Report", "locomotion", "geophysical surveys", "The later accidental introduction of Beroe", "socially", "spanking or paddling or caning or strapping or birching the student in order to cause physical pain", "the Romantic Rhine", "British colonists", "adjustable spring-loaded valve", "complicated", "Arabic numerals", "7\u20134\u20132\u20133 system", "much higher school fees than other public schools", "WatchESPN", "Economist", "Richard Lindzen", "emigration", "Islamism", "826", "Figaro", "Mazda", "US Naval Submarine Base New London submarine school", "Secretary of Defense", "Nanyue", "Vyd\u016bnas", "1989", "American", "Anne of Green Gables", "Fainaru Fantaj\u012b Tuerubu", "Reverend Lovejoy", "he started composing Symphony No. 9 in D minor", "Walldorf", "Kings Point, New York", "Bill Miner", "Agent 99", "Outside", "Christopher Nolan", "Umina Beach", "Chicago", "Thriller", "1992", "Let's Make Sure We Kiss Goodbye", "Andrzej Go\u0142ota", "Alonso L\u00f3pez", "post\u2013World War II", "Waylon Albright", "The New Yorker", "Type 10", "need to repent in time", "gautama", "Jason Chaffetz", "sealed glass tubes", "Speaker of the House of Representatives", "1947, 1956, 1975, 2015 and 2017", "49"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6505823731007555}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, true, true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.45454545454545453, 0.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7436", "mrqa_squad-validation-8369", "mrqa_squad-validation-10386", "mrqa_squad-validation-4327", "mrqa_squad-validation-2085", "mrqa_squad-validation-7131", "mrqa_squad-validation-3124", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-671", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2481", "mrqa_naturalquestions-validation-4556", "mrqa_triviaqa-validation-2976", "mrqa_searchqa-validation-14617", "mrqa_naturalquestions-validation-5865"], "SR": 0.578125, "CSR": 0.6684027777777778, "EFR": 1.0, "Overall": 0.8342013888888888}, {"timecode": 9, "before_eval_results": {"predictions": ["Konwiktorska Street", "glass", "39", "1760", "CALIPSO", "the state", "The European Court of Justice", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "lymphokines", "solar power", "specific terminology has no more (or no less) meaning than the individual orator intends it to have", "Finsteraarhorn", "1015 kelvins", "Aaron Spelling", "1770", "833,500", "1851", "Canada", "The Northern Chinese", "between 1859 and 1865", "the judge need not allow defendants to openly seek jury nullification", "Gap", "\"missile gap\"", "Kensington", "Port of Long Beach", "an eccentric U.S. saloon-keeper", "American-Canadian children's books", "close to 50 million", "George Clooney", "Matt Groening", "Hern\u00e1n Crespo", "William Finn", "Kenny Young", "Alistair Grant", "poetry, theater, art, music, the media, and books", "The Rebirth", "sulfur mustards", "Hearts", "Christian Maelen", "a scholar during the Joseon Dynasty", "The Terminator", "Saint Petersburg Conservatory", "Michael Fred Phelps II", "Bolton, England", "Quasimodo", "Cuban", "Cleveland Browns", "the Maldives", "\u00c9cole des Beaux-Arts", "Kentucky Music Hall of Fame", "American 3D computer-animated comedy", "Agra", "Polka", "Esteban Ocon", "actress", "Kassie DePaiva", "Jaydev Shah", "Stephen Lang", "Doris Lessing", "\"breeding Lilacs out of the dead land,\" and covering \"dull roots with spring rain,\"", "voluntary homicide", "Sodra nongovernmental organization", "Tutankhamun", "n"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7367798561732385}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.2857142857142857, 1.0, 0.5, 0.28571428571428575, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3173", "mrqa_squad-validation-6814", "mrqa_squad-validation-6837", "mrqa_squad-validation-5294", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-1841", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-1980", "mrqa_triviaqa-validation-1670", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-5856"], "SR": 0.65625, "CSR": 0.6671875, "EFR": 1.0, "Overall": 0.83359375}, {"timecode": 10, "UKR": 0.763671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1482", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2208", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2274", "mrqa_hotpotqa-validation-2362", "mrqa_hotpotqa-validation-2481", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2901", "mrqa_hotpotqa-validation-2925", "mrqa_hotpotqa-validation-2931", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3672", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3875", "mrqa_hotpotqa-validation-3898", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-476", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4903", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5162", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5512", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-5591", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5701", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-60", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-812", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-965", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10678", "mrqa_naturalquestions-validation-1160", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1324", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1475", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-35", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3938", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-5338", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5882", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6200", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6604", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-7151", "mrqa_naturalquestions-validation-7278", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-835", "mrqa_naturalquestions-validation-849", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-8657", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9536", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9842", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1945", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-3164", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-534", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-8172", "mrqa_searchqa-validation-9926", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10123", "mrqa_squad-validation-10148", "mrqa_squad-validation-10174", "mrqa_squad-validation-10181", "mrqa_squad-validation-10186", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1089", "mrqa_squad-validation-1161", "mrqa_squad-validation-1177", "mrqa_squad-validation-1177", "mrqa_squad-validation-1182", "mrqa_squad-validation-1187", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1277", "mrqa_squad-validation-133", "mrqa_squad-validation-134", "mrqa_squad-validation-1356", "mrqa_squad-validation-1423", "mrqa_squad-validation-1432", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1593", "mrqa_squad-validation-1613", "mrqa_squad-validation-1614", "mrqa_squad-validation-1640", "mrqa_squad-validation-1649", "mrqa_squad-validation-1665", "mrqa_squad-validation-1678", "mrqa_squad-validation-168", "mrqa_squad-validation-1681", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1767", "mrqa_squad-validation-1779", "mrqa_squad-validation-1815", "mrqa_squad-validation-185", "mrqa_squad-validation-1859", "mrqa_squad-validation-1891", "mrqa_squad-validation-1898", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-1980", "mrqa_squad-validation-2079", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2144", "mrqa_squad-validation-215", "mrqa_squad-validation-2186", "mrqa_squad-validation-2197", "mrqa_squad-validation-2200", "mrqa_squad-validation-2214", "mrqa_squad-validation-2248", "mrqa_squad-validation-2272", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-242", "mrqa_squad-validation-2473", "mrqa_squad-validation-2490", "mrqa_squad-validation-2568", "mrqa_squad-validation-2586", "mrqa_squad-validation-2612", "mrqa_squad-validation-2632", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-271", "mrqa_squad-validation-2725", "mrqa_squad-validation-2765", "mrqa_squad-validation-2775", "mrqa_squad-validation-2807", "mrqa_squad-validation-2811", "mrqa_squad-validation-2853", "mrqa_squad-validation-2873", "mrqa_squad-validation-2893", "mrqa_squad-validation-2950", "mrqa_squad-validation-2975", "mrqa_squad-validation-2986", "mrqa_squad-validation-3007", "mrqa_squad-validation-3076", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3173", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-3327", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3464", "mrqa_squad-validation-3479", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-3511", "mrqa_squad-validation-3537", "mrqa_squad-validation-3550", "mrqa_squad-validation-3581", "mrqa_squad-validation-3676", "mrqa_squad-validation-3723", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3788", "mrqa_squad-validation-38", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3842", "mrqa_squad-validation-3852", "mrqa_squad-validation-3871", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3923", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-3945", "mrqa_squad-validation-402", "mrqa_squad-validation-4034", "mrqa_squad-validation-4179", "mrqa_squad-validation-420", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4327", "mrqa_squad-validation-4430", "mrqa_squad-validation-4437", "mrqa_squad-validation-4473", "mrqa_squad-validation-4484", "mrqa_squad-validation-4607", "mrqa_squad-validation-4612", "mrqa_squad-validation-4636", "mrqa_squad-validation-4660", "mrqa_squad-validation-4737", "mrqa_squad-validation-4750", "mrqa_squad-validation-476", "mrqa_squad-validation-4882", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-4981", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5085", "mrqa_squad-validation-5135", "mrqa_squad-validation-5147", "mrqa_squad-validation-5196", "mrqa_squad-validation-5198", "mrqa_squad-validation-5276", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5351", "mrqa_squad-validation-5389", "mrqa_squad-validation-5434", "mrqa_squad-validation-5531", "mrqa_squad-validation-5621", "mrqa_squad-validation-5634", "mrqa_squad-validation-5671", "mrqa_squad-validation-5699", "mrqa_squad-validation-5724", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-5788", "mrqa_squad-validation-5797", "mrqa_squad-validation-5869", "mrqa_squad-validation-5875", "mrqa_squad-validation-5947", "mrqa_squad-validation-5961", "mrqa_squad-validation-6029", "mrqa_squad-validation-6089", "mrqa_squad-validation-611", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-624", "mrqa_squad-validation-6318", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6351", "mrqa_squad-validation-6381", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6546", "mrqa_squad-validation-6555", "mrqa_squad-validation-6569", "mrqa_squad-validation-6589", "mrqa_squad-validation-6594", "mrqa_squad-validation-6628", "mrqa_squad-validation-6636", "mrqa_squad-validation-6648", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6716", "mrqa_squad-validation-6752", "mrqa_squad-validation-679", "mrqa_squad-validation-6814", "mrqa_squad-validation-682", "mrqa_squad-validation-6837", "mrqa_squad-validation-6838", "mrqa_squad-validation-6873", "mrqa_squad-validation-6877", "mrqa_squad-validation-6924", "mrqa_squad-validation-6960", "mrqa_squad-validation-6978", "mrqa_squad-validation-6981", "mrqa_squad-validation-6986", "mrqa_squad-validation-7126", "mrqa_squad-validation-7131", "mrqa_squad-validation-7154", "mrqa_squad-validation-7168", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-729", "mrqa_squad-validation-73", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7436", "mrqa_squad-validation-7447", "mrqa_squad-validation-7476", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7652", "mrqa_squad-validation-7656", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7713", "mrqa_squad-validation-773", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-7819", "mrqa_squad-validation-7838", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8049", "mrqa_squad-validation-8066", "mrqa_squad-validation-8118", "mrqa_squad-validation-8139", "mrqa_squad-validation-816", "mrqa_squad-validation-824", "mrqa_squad-validation-8253", "mrqa_squad-validation-8273", "mrqa_squad-validation-8283", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8453", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8505", "mrqa_squad-validation-8523", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8579", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8643", "mrqa_squad-validation-8680", "mrqa_squad-validation-8683", "mrqa_squad-validation-8750", "mrqa_squad-validation-8801", "mrqa_squad-validation-889", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8924", "mrqa_squad-validation-8927", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8969", "mrqa_squad-validation-8987", "mrqa_squad-validation-9048", "mrqa_squad-validation-9097", "mrqa_squad-validation-9135", "mrqa_squad-validation-9157", "mrqa_squad-validation-9165", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9325", "mrqa_squad-validation-9328", "mrqa_squad-validation-9367", "mrqa_squad-validation-9416", "mrqa_squad-validation-9436", "mrqa_squad-validation-9459", "mrqa_squad-validation-9470", "mrqa_squad-validation-9531", "mrqa_squad-validation-9543", "mrqa_squad-validation-9553", "mrqa_squad-validation-9559", "mrqa_squad-validation-9608", "mrqa_squad-validation-9764", "mrqa_squad-validation-9787", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9900", "mrqa_squad-validation-9901", "mrqa_squad-validation-9943", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5627", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6577", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-71", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7683"], "OKR": 0.904296875, "KG": 0.31953125, "before_eval_results": {"predictions": ["hydrogen and helium", "a supervisory church body", "alternating current", "Jerricho Cotchery", "Isaac Komnenos", "a certain number of teacher's salaries are paid by the State", "cylinders and valve gear", "ABC1", "ATP", "San Andreas fault system", "ABC on Demand", "two", "social unrest and violence", "Toyota Corona, the Toyota Corolla, the Datsun B210", "Metro Light Rail system", "receiver", "A Turing machine", "Luther", "antibodies", "1978", "George Sylvester Viereck", "1804", "seven", "april", "David Hilbert", "India", "Jennifer Saunders", "a radio star", "april", "\"Wunderbar\" and \"So in Love,\" and sizzling dance numbers", "Robert Agar", "Mumbai", "april nouveau", "Jordan", "Thailand", "A.N. Whitehead", "A-ha", "Egypt", "William Holden", "Charlie Brooker", "april", "april", "apriline dyes", "Ethiopia", "tennis", "\"Pale Rider\"", "Maria Edgeworth", "april", "steel", "Talbot Road", "Anna Johanna Henrietta Juliana Von Griesheim", "april", "The Small House of Uncle Thomas", "Lisbeth Salander", "Standard Oil Company", "Miranda v. Arizona", "Janis Joplin", "1984", "over 1.6 million", "Mark Neveldine and Brian Taylor", "Nkepile M Abuse", "Nazi Party members, shovels in hand, digging up graves of American soldiers held as slaves", "april", "Sir James Paul McCartney"], "metric_results": {"EM": 0.40625, "QA-F1": 0.4650297619047619}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.42857142857142855, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.33333333333333337, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3667", "mrqa_squad-validation-2468", "mrqa_squad-validation-780", "mrqa_squad-validation-8834", "mrqa_squad-validation-5025", "mrqa_squad-validation-5958", "mrqa_squad-validation-3708", "mrqa_squad-validation-336", "mrqa_squad-validation-1612", "mrqa_triviaqa-validation-921", "mrqa_triviaqa-validation-1922", "mrqa_triviaqa-validation-1160", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-6821", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-2372", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-3159", "mrqa_triviaqa-validation-4982", "mrqa_triviaqa-validation-4005", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-1461", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-7737", "mrqa_naturalquestions-validation-9419", "mrqa_hotpotqa-validation-1526", "mrqa_newsqa-validation-618", "mrqa_newsqa-validation-2422", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-10099"], "SR": 0.40625, "CSR": 0.6434659090909092, "EFR": 1.0, "Overall": 0.7261931818181818}, {"timecode": 11, "before_eval_results": {"predictions": ["high than normal O2 exposure for a fee", "center of the curving path", "polynomial", "1206", "Dutch East India Company", "Newton", "after the Franco-German War", "chloroplasts and other plastids", "June 4, 2014", "energy", "December 2014", "18 February 1546", "19", "meritocracy", "photooxidative damage", "tangential force", "St John the Baptist", "more or less rounded, sometimes nearly spherical and other times more cylindrical or egg-shaped", "adenosine triphosphate", "Johann Eck", "marx", "Civil War", "a woman\u2019s desire to smoke", "king David", "Charles Dickens's Bleak House", "butterfly", "John Flamsteed", "bison", "Anita Roddick", "River Cart", "Tamar", "Gorky", "The Word", "The Left Book Club", "Tchaikovsky", "Tony Blackburn", "dakil", "a spa town", "butterfly", "Tarzan", "James Hanratty", "Middlesbrough", "Stanley Kubrick's Full Metal jackets", "London Pride", "\"butcher\" Cumberland", "a jumper", "Milton Keynes", "November", "koftas", "Coventry to Leicester Motorway", "kurt craig", "\"Little arrows\"", "legion", "Saint Vitus", "Syriza", "from shore to shore", "an Ohio newspaper", "Channel 4", "Cartoon Network Too", "pattern matching", "women would be jailed if they violated the order", "Mojave", "king b Ben", "anti-trust"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5777777777777778}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 0.4, 1.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-3610", "mrqa_squad-validation-8229", "mrqa_squad-validation-361", "mrqa_squad-validation-4469", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-3014", "mrqa_triviaqa-validation-828", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5354", "mrqa_triviaqa-validation-4400", "mrqa_naturalquestions-validation-8877", "mrqa_naturalquestions-validation-6665", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-18", "mrqa_searchqa-validation-12699", "mrqa_searchqa-validation-5935", "mrqa_newsqa-validation-3918"], "SR": 0.515625, "CSR": 0.6328125, "EFR": 1.0, "Overall": 0.7240624999999999}, {"timecode": 12, "before_eval_results": {"predictions": ["Marburg Colloquy", "a negative long-term impact", "with known magnitudes of force", "greater Southern California Megaregion, one of the 11 megaregions of the United States", "Ed Whitfield", "\"Rhine knee\"", "Levi's Stadium", "Miasma theory", "a freshwater lake", "Monte Gargano", "50-yard line", "National Galleries of Scotland", "Arts & Entertainment Television (A&E)", "spring reaction force", "ethiopian economist, social theorist, political philosopher, and author Thomas Sowell", "fans", "pathogen attack", "a not-for-profit United States computer networking consortium", "1850", "CFB Trenton to the coroner's office in Toronto", "Spanish explorers", "eight", "Columbia University", "Waylon Jennings", "International Orange", "Melissa Disney", "five", "1990", "Floyd", "1980s and'90s", "Tyler, Ali, and Lydia", "May 18, 2018", "2015", "Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "$75,000", "332", "Hudson Bay", "unknown origin", "to accomplish the objectives of the organization", "elevator pitch", "1775", "insignia", "a normally inaccessible mini-game", "Charles Darwin", "insulated shipping containers", "bactrian", "2016", "to address the historic oppression, inequality and discrimination faced by those communities and to give these communities a place", "Roger Nichols and Paul Williams", "T.S. Eliot", "Michael Douglas, Kathleen Turner, and Danny DeVito", "Joe Spano", "at the 1964 Republican National Convention in San Francisco, California", "north", "Paul Lynde", "a sweet-and-sour, dark-brown vinegar", "David Seville", "Gracie Mansion", "Peter Seamus O'Toole", "Arthur E. Morgan III,", "his wife, Cabinet members, governors and other public and private officials.", "Wayne's World", "peter", "Arkansas"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6836388437950938}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, true, false, true, true, false, false, false, true, false, true], "QA-F1": [1.0, 0.8, 0.2857142857142857, 0.5333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.27272727272727276, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.33333333333333337, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.888888888888889, 0.1111111111111111, 0.0, 0.0, 0.4, 1.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5213", "mrqa_squad-validation-10321", "mrqa_squad-validation-2553", "mrqa_squad-validation-9196", "mrqa_squad-validation-4877", "mrqa_squad-validation-8037", "mrqa_naturalquestions-validation-10615", "mrqa_naturalquestions-validation-8961", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-2907", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10684", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-9142", "mrqa_naturalquestions-validation-627", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-1864", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-1575", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-3299", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-3444", "mrqa_searchqa-validation-15790"], "SR": 0.5625, "CSR": 0.6274038461538461, "EFR": 1.0, "Overall": 0.7229807692307693}, {"timecode": 13, "before_eval_results": {"predictions": ["the then-popular phlogiston theory of combustion and other metals", "1974", "Andrew Alper", "shaping ideas about the free market", "learning by providing a social networking support that allows them to reach their full cognitive potential", "The Prospect Studios", "Milton Friedman Institute", "music from the 2008\u20132010 specials (The Next Doctor to End of Time Part 2)", "1972", "the University of Northumbria at Newcastle in 1992", "The conservation of momentum can be directly derived from the homogeneity or symmetry of space", "October 1973", "Frontex, the European Union agency for external border security", "a dispute over control of the confluence of the Allegheny and Monongahela rivers", "720p high definition", "1924", "\"Sippin' on Some Sizzurp\"", "Apple Lisa", "Raden Panji Nugroho Notosusanto", "the part of a bet", "October 21, 2016", "Donald Duck", "February 12, 2014", "Eureka Hall", "An aircraft", "University of Vienna", "November 2, 2003", "Golden Calf", "2012 Summer Olympics, formally the Games of the XXX Olympiad and commonly known as London 2012", "February 13, 1946", "Ub Iwerks", "how well a watch is sealed against the ingress of water", "Nathan Rothschild", "The Grandmaster", "Montana State University.", "37", "\"Bombay Talkie\" (1970), \"Junoon\" (1978), \" Heat and Dust\" (1983), and \"Ghare Baire\" (1984)", "a 5% abv pale lager", "Hank Azaria", "Supergirl", "Nick Frost", "22 November 17615 July 1816", "Mauritian", "mixed martial arts", "Cape Cod", "Mike Farrell", "Humberside Airport", "Henry Luce", "energy trading company based in Houston, Texas", "the new king", "Connie", "Book of Judges", "12", "35", "span", "Pantalone and old age", "USS Thresher (SSN-593) sank while conducting deep-diving tests southeast of Cape Cod on the 10th of April 1963", "The Worst Command in the War", "Ralph Lauren", "the Pakistani Taliban's chief in Punjab, according to Anwar", "Detroit", "Walt", "Lennon", "terminal brain cancer"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5058134522748494}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, true], "QA-F1": [0.7142857142857143, 1.0, 1.0, 1.0, 0.125, 1.0, 1.0, 0.5, 1.0, 0.0, 0.375, 1.0, 0.2222222222222222, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 0.35294117647058826, 1.0, 1.0, 0.1818181818181818, 1.0, 0.0, 1.0, 1.0, 0.2666666666666667, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3452", "mrqa_squad-validation-1913", "mrqa_squad-validation-7763", "mrqa_squad-validation-5337", "mrqa_squad-validation-10475", "mrqa_squad-validation-891", "mrqa_squad-validation-5889", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-3736", "mrqa_hotpotqa-validation-3430", "mrqa_hotpotqa-validation-1334", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-5239", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-4323", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-405", "mrqa_naturalquestions-validation-2844", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-6981", "mrqa_newsqa-validation-1095", "mrqa_searchqa-validation-7428", "mrqa_searchqa-validation-15877", "mrqa_newsqa-validation-2128"], "SR": 0.40625, "CSR": 0.6116071428571428, "EFR": 0.9473684210526315, "Overall": 0.7092951127819548}, {"timecode": 14, "before_eval_results": {"predictions": ["Chu'Tsai", "second-largest", "17", "Ed Mangan", "suspended sentences", "send aid", "nearly two-thirds", "a dam turbine", "SAP Center in San Jose", "Niels Jerne", "Central Bridge", "Vistula River", "an enzyme called rubisco", "a fire that started as a kitchen fire", "Louis Pasteur", "\"The Tales of Hoffmann\"", "Lake Placid, New York", "four", "Elton John", "1978", "Victorian England", "Indian", "R-8 Human Rhythm Composer", "1970", "Tom Kitt", "Outside", "Forbes", "acidic", "Tampa", "Saoirse Ronan", "the richest person in the state of Georgia", "University of Kansas", "$700 million", "S\u00f8nderjyskE Ishockey", "Foxborough", "Cyclic Defrost", "Stalybridge Celtic", "John Kevin Delaney", "Sister, Sister", "1955", "2016", "2 March 1972", "Londonderry", "11 November 1869", "Sam Bettley", "Larry Drake", "Wilhelmus Simon Petrus Fortuijn", "Hockey Club Davos", "Sex Drive", "Toxics Release Inventory", "St Augustine's Abbey", "a fictional character", "Fred Claus", "La Scala, Milan", "its genome", "1998", "vitamin B3", "Israel", "Juan Martin Del Potro", "Rev. Alberto Cutie", "Gyula Halsz", "William", "Old Mother Hubbard", "milk chocolate"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6498511904761906}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9695", "mrqa_squad-validation-457", "mrqa_squad-validation-7233", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-2725", "mrqa_hotpotqa-validation-2108", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-9368", "mrqa_naturalquestions-validation-3651", "mrqa_triviaqa-validation-5714", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-13152", "mrqa_searchqa-validation-6300"], "SR": 0.546875, "CSR": 0.6072916666666667, "EFR": 1.0, "Overall": 0.7189583333333334}, {"timecode": 15, "before_eval_results": {"predictions": ["11", "1968", "relationship between teachers and children", "Westinghouse Electric", "nine months", "more than 70 pioneers", "Inherited wealth", "Super Bowl XXXIII", "Alan Turing", "North America", "Conservative", "contrasts with the newer areas", "Clair Cameron Patterson", "Esp\u00edrito Santo Financial Group", "Europe", "a multi-use indoor arena", "Ralph Stanley", "S Pictures' \"Veyyil\"", "YouTube celebrity PewDiepie", "Seoul, South Korea", "Pittsburgh", "2012", "Chinese Coffee", "Sammy Gravano", "$7.3 billion", "commentary on Isaac Newton's book \"Principia\"", "Umina Beach, New South Wales", "Port Macquarie", "\"Anomalisa\"", "Edinburgh", "Anthony Lynn", "Levi Weeks", "the Mayor of the City of New York", "soccer", "Memphis Minnie's", "The Five", "James Mitchum", "Candice Susan Swanepoel", "Bhaktivedanta Manor", "Thomas Christopher Ince", "American college football", "U.S. military", "south", "historic buildings, arts, and published works", "June 24, 1935", "James K. Polk", "March 31, 1944", "Linux Format", "Na Na", "Koch Industries", "Adelaide Botanic Garden, Hutt Street, and Victoria Park", "Eliot Cutler", "The Swan Lake", "Thomas Jefferson", "Marie Van Brittan Brown", "Billy Idol", "soldier subordinate Private Leonard \"Gomer\" Pyle", "Bombay Stock Exchange", "eight or nine young girls", "Hussein's Revolutionary Command Council", "bistro", "Anthony Fokker", "Jean Baptiste Say", "objects"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7644717261904762}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7499999999999999, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1640", "mrqa_squad-validation-4671", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-4969", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-5171", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-4564", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-4642", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-5715", "mrqa_newsqa-validation-3297", "mrqa_searchqa-validation-8148", "mrqa_searchqa-validation-1728", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-581"], "SR": 0.671875, "CSR": 0.611328125, "EFR": 1.0, "Overall": 0.719765625}, {"timecode": 16, "before_eval_results": {"predictions": ["Aston Webb", "Jin", "in his lab and elsewhere", "1253", "healthcare professionals", "RNA silencing", "50", "lectured", "Yes\u00fcgei", "a new wave of anti-imperialist propaganda", "the American Philosophical Society", "Albert Einstein", "Bristol", "Javier Bardem", "126 mph", "Ben Whishaw", "2", "The Private Public", "Fred Marriot", "Skylab", "Israelites", "Spain", "milk", "Z", "rue", "free booter", "a gold Georg Olden\u2013designed statuette", "pteropodids", "Styal", "One wolf huffing", "sergeant", "Margaret Beckett", "Brad Pitt", "a hair follicle", "Canada", "St Andrew's Saltire", "a marble campanile", "hud\u00f4r", "Sesame Street", "a group of people live together in a large house", "Avro", "Leeds", "Joe Brydon", "Joseph Priestley", "white", "a new Eurasian union", "Denbighshire", "Fernando Torres", "ADNAMS", "hartman", "a collier", "Leicestershire", "Wyoming", "Xenophon", "O'Meara", "3 total", "his sixth", "2013", "Mexico", "Newcastle retained fourth place with a 3-1 victory", "Hugh Grant", "pink", "a working-class young man", "1"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5424479166666667}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, false, true, false, true, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6316", "mrqa_squad-validation-6547", "mrqa_squad-validation-10128", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-645", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-5830", "mrqa_triviaqa-validation-5856", "mrqa_triviaqa-validation-5844", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-6996", "mrqa_triviaqa-validation-7155", "mrqa_triviaqa-validation-5436", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5226", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3685", "mrqa_triviaqa-validation-4357", "mrqa_naturalquestions-validation-8087", "mrqa_hotpotqa-validation-1953", "mrqa_newsqa-validation-2467", "mrqa_hotpotqa-validation-3468"], "SR": 0.515625, "CSR": 0.6056985294117647, "EFR": 1.0, "Overall": 0.7186397058823528}, {"timecode": 17, "before_eval_results": {"predictions": ["pattern recognition receptors", "in the northern Mokot\u00f3w", "in the courtyard adjoining the Assembly Hall", "six gold", "1527", "Zhongdu", "tea drinking", "the Chicago Bears", "Bill Clinton", "differences in value added by labor, capital and land", "geochemical evolution of rock units", "an assemblage of things laid or lying one upon the other", "an eagle", "Red", "Pink Floyd", "Constantinople", "Zeus", "10th wedding anniversary", "Mesozoic Era", "Rodney King", "ethanethiol", "Coral Reef", "forty-niners", "Clyde", "New Orleans", "bugle", "the liver", "right-to-left", "nonfiction", "Roger Williams", "ring-tailed lemurs", "money he receives becomes part of his cash on hand", "Mediolanum", "butter", "premature rupture of membranes", "Augustus", "white blood cells", "Green Lantern", "Zeus", "the New England Patriots", "Mickey Mouse", "Yves Saint Laurent", "the Trucial States", "Macy's", "in Ljubljana", "Carmen", "mammals", "Pullman", "Bali", "a fruit, vegetable, or a nut", "the Crimean War", "oresteia", "pesto", "control purposes", "1834", "Pantagruel", "the Savoy", "Leon Marcus Uris", "S6", "Nazi Party members", "leaky valve", "Ritchie Cordell", "10.5 %", "The Inn at Newport Ranch"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5458829365079365}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, false, false, false, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 0.5, 0.888888888888889, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.38095238095238093]}}, "before_error_ids": ["mrqa_squad-validation-1042", "mrqa_squad-validation-9400", "mrqa_squad-validation-5429", "mrqa_searchqa-validation-10868", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-15310", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-8485", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-4065", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-4154", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-8417", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-11410", "mrqa_naturalquestions-validation-9650", "mrqa_triviaqa-validation-1387", "mrqa_hotpotqa-validation-2978", "mrqa_newsqa-validation-2423", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-2250"], "SR": 0.484375, "CSR": 0.5989583333333333, "EFR": 0.9696969696969697, "Overall": 0.7112310606060606}, {"timecode": 18, "before_eval_results": {"predictions": ["Apollo Applications Program", "5.3%", "seizures", "Mercury", "a setup phase in each involved node", "Infrastructure", "MHC class I molecules", "European Court of Human Rights", "it is neither zero nor a unit", "neuronal dendrites", "The Lost Symbol", "President Robert Mugabe", "November 26", "Al Nisr Al Saudi", "two paintings by Pablo Picasso, Bjoern Quellenberg, a spokesman for the Kunsthaus, a major art museum in Zurich", "Marcus Schrenker", "glass shards", "Chancellor Angela Merkel", "Tibet's independence", "drug trafficking is a transnational threat, and therefore national initiatives have their limitations", "steamboat", "Osama bin Laden", "2002", "Yusuf Saad Kamel", "general secretary", "byproducts emitted during the process of burning and melting raw materials", "September", "Transit Workers Union leaders", "introducing legislation Thursday to improve the military's suicide-prevention programs", "Eintracht Frankfurt", "Jacob", "5 1/2-year-old son, Ryder Russell", "in a canyon", "eco-horror scenarios", "Bob Dole", "Kenyan and Somali governments", "the new kid on the block in the modern art scene", "U.S. senators", "April", "Chesley \"Sully\" Sullenberger", "Nigeria, Africa's largest producer", "Thursday", "funds for housing, business and infrastructure repairs", "A third beluga whale", "Martin Aloysius Culhane", "Washington", "car companies have their own versions", "Dubai", "Sri Lanka, seeking a win to level the series at 1-1, closed on 366 for eight wickets", "The worst snowstorm to hit Britain in 18 years", "At least 38", "Del Potro", "if he did cheat on you", "most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor", "two easily observed features", "The Comedy of Errors", "Xenophon", "devotional", "14th Street", "Lord Byron", "a pitch", "Jukes Jules Julia Julie Juliet Julio Julius July June Juneau Juno Jupiter Jura", "a cape", "a printing press"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6060474069435466}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.3076923076923077, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 0.0, 0.5, 0.28571428571428575, 1.0, 0.75, 0.875, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8333333333333333, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.23529411764705882, 0.2222222222222222, 0.8571428571428571, 0.6666666666666666, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1132", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-418", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-961", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3429", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-1482", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1184", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-2901", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-9803", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-8188", "mrqa_searchqa-validation-15495"], "SR": 0.453125, "CSR": 0.591282894736842, "EFR": 0.9714285714285714, "Overall": 0.7100422932330827}, {"timecode": 19, "before_eval_results": {"predictions": ["one of the most common forms of school discipline throughout much of the world.", "April 1, 1963", "Islamism", "he did not want disloyal men in his army.", "the currently known fundamental forces", "farther west, through the Waal and then, via the Merwede and Nieuwe merwede (De Biesbosch)", "154 gems", "17 February 1546", "1996", "2009,", "Friday", "Anil Kapoor.", "2005", "2008", "Ben Roethlisberger", "President Obama's surge plan to head to Afghanistan's restive provinces to support Marines and soldiers fighting a dug-in Taliban force.", "CNN's Moscow-based Senior International Correspondent Matthew Chance", "11th year in a row", "suspended", "The State newspaper in Columbia, South Carolina's capital,", "the Nazi war crimes suspect", "Lashkar-e-Tayyiba (LeT), an Islamic militant group based in Pakistan.", "of the Movement for Democratic Change,", "Steven Chu", "Wednesday at the age of 95", "\"the most important discovery\" for the museum \"of the last 90 years.\"", "on an island stronghold of the Islamic militant group Abu Sayyaf,", "opened the door for the man police say was his killer.", "2,000 euros ($2,963)", "buckling under pressure from the ruling party.", "Vertikal-T,", "The Everglades, known as the River of Grass,", "a striking blow to due process and the rule of law", "federal officers' bodies", "long deployments, lengthy separations from family and the perceived stigma associated with seeking help for someone in need.", "in a ravine a week after losing control of his car on a rural road and plunging 500 feet down an embankment into heavy brush,", "gratitude for his mother", "South African police have opened a criminal investigation into allegations that a dorm parent mistreated students at the school.", "hundreds of people joined a campus rally to oppose racial intolerance.", "girls", "blind Majid Movahedi, the man who blinded her.", "Vice presidential", "suicide car bombing", "Amy Winehouse", "2007", "\"Wolfman,\" starring Benicio del Toro, grossed an estimated $30.6 million for a per-theater average of $9,497.\"", "Vernon Forrest", "The man ran out of bullets and blew himself up.", "Jason Chaffetz", "St. Louis, Ontario", "Alicia Keys", "Oxbow,", "Zhanar Tokhtabayeba,", "In 1038", "Johannes Gutenberg", "husbands", "a method of divination", "Lithuania national team member Robertas Javtokas", "two years.", "Michael", "James Corden", "American black bear", "chicken", "Constellation family"], "metric_results": {"EM": 0.390625, "QA-F1": 0.49916204455266955}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, false, true, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.1904761904761905, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.33333333333333337, 0.13333333333333333, 0.0, 0.375, 1.0, 1.0, 0.4, 0.2857142857142857, 1.0, 1.0, 0.2727272727272727, 0.32, 0.0, 0.2, 0.18181818181818182, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.19999999999999998, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2085", "mrqa_squad-validation-9228", "mrqa_squad-validation-5613", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-4117", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-1468", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-3068", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-440", "mrqa_naturalquestions-validation-10551", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-2337", "mrqa_hotpotqa-validation-4927", "mrqa_searchqa-validation-12135", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-9391"], "SR": 0.390625, "CSR": 0.58125, "EFR": 1.0, "Overall": 0.7137499999999999}, {"timecode": 20, "UKR": 0.73828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1334", "mrqa_hotpotqa-validation-1419", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-1585", "mrqa_hotpotqa-validation-1696", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1872", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2402", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2574", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2764", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-3057", "mrqa_hotpotqa-validation-32", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3240", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3564", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3637", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4516", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4565", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4710", "mrqa_hotpotqa-validation-4717", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4780", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5319", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5399", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5838", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-982", "mrqa_hotpotqa-validation-982", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1575", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2445", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3725", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9545", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1030", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1173", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1436", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1640", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1811", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1896", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2897", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-30", "mrqa_newsqa-validation-3297", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3365", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3910", "mrqa_newsqa-validation-3931", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-4190", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-872", "mrqa_searchqa-validation-11130", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-13515", "mrqa_searchqa-validation-14767", "mrqa_searchqa-validation-14923", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16559", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5531", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-7219", "mrqa_searchqa-validation-7332", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-10059", "mrqa_squad-validation-10063", "mrqa_squad-validation-1008", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10273", "mrqa_squad-validation-10316", "mrqa_squad-validation-10346", "mrqa_squad-validation-10352", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10399", "mrqa_squad-validation-1042", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-10475", "mrqa_squad-validation-10484", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-12", "mrqa_squad-validation-1207", "mrqa_squad-validation-1219", "mrqa_squad-validation-1254", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-134", "mrqa_squad-validation-1402", "mrqa_squad-validation-1432", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1565", "mrqa_squad-validation-1612", "mrqa_squad-validation-1640", "mrqa_squad-validation-168", "mrqa_squad-validation-1764", "mrqa_squad-validation-1813", "mrqa_squad-validation-185", "mrqa_squad-validation-185", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-215", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2314", "mrqa_squad-validation-2370", "mrqa_squad-validation-246", "mrqa_squad-validation-2500", "mrqa_squad-validation-2559", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-2672", "mrqa_squad-validation-269", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2853", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2964", "mrqa_squad-validation-2975", "mrqa_squad-validation-3096", "mrqa_squad-validation-3124", "mrqa_squad-validation-3125", "mrqa_squad-validation-313", "mrqa_squad-validation-3140", "mrqa_squad-validation-3143", "mrqa_squad-validation-3275", "mrqa_squad-validation-3319", "mrqa_squad-validation-336", "mrqa_squad-validation-3370", "mrqa_squad-validation-3407", "mrqa_squad-validation-3492", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3550", "mrqa_squad-validation-36", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3802", "mrqa_squad-validation-3811", "mrqa_squad-validation-3813", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3904", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-4227", "mrqa_squad-validation-4246", "mrqa_squad-validation-4469", "mrqa_squad-validation-4473", "mrqa_squad-validation-4546", "mrqa_squad-validation-4591", "mrqa_squad-validation-4636", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4737", "mrqa_squad-validation-4754", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5007", "mrqa_squad-validation-5019", "mrqa_squad-validation-5135", "mrqa_squad-validation-5294", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5724", "mrqa_squad-validation-5797", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-5961", "mrqa_squad-validation-6025", "mrqa_squad-validation-6089", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6159", "mrqa_squad-validation-6163", "mrqa_squad-validation-6341", "mrqa_squad-validation-635", "mrqa_squad-validation-6502", "mrqa_squad-validation-6526", "mrqa_squad-validation-6579", "mrqa_squad-validation-6614", "mrqa_squad-validation-6628", "mrqa_squad-validation-6669", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-6873", "mrqa_squad-validation-6986", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-719", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7277", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7384", "mrqa_squad-validation-7428", "mrqa_squad-validation-7456", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7585", "mrqa_squad-validation-76", "mrqa_squad-validation-7652", "mrqa_squad-validation-7671", "mrqa_squad-validation-7689", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-791", "mrqa_squad-validation-8028", "mrqa_squad-validation-8043", "mrqa_squad-validation-8045", "mrqa_squad-validation-8073", "mrqa_squad-validation-8283", "mrqa_squad-validation-8386", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8529", "mrqa_squad-validation-8555", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8754", "mrqa_squad-validation-8830", "mrqa_squad-validation-8834", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-891", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8939", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-8987", "mrqa_squad-validation-9200", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9531", "mrqa_squad-validation-9532", "mrqa_squad-validation-9543", "mrqa_squad-validation-9695", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_squad-validation-9931", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1246", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1590", "mrqa_triviaqa-validation-1670", "mrqa_triviaqa-validation-1800", "mrqa_triviaqa-validation-1873", "mrqa_triviaqa-validation-1976", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2674", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3277", "mrqa_triviaqa-validation-3278", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-3955", "mrqa_triviaqa-validation-4068", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4600", "mrqa_triviaqa-validation-4740", "mrqa_triviaqa-validation-4850", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5025", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5638", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-581", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5960", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6246", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6734", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-6914", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7124", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-737", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-828"], "OKR": 0.857421875, "KG": 0.4515625, "before_eval_results": {"predictions": ["2002.", "1,548", "American Institute of Electrical Engineers", "a deficit.", "18 and 19", "Virgin Media.", "in a number of stages", "light energy", "128", "power directly or elect representatives from among themselves to form a governing body, such as a parliament.", "Terrence \"Uncle Terry\" Richardson", "the Australian Defence Force", "an American astronaut, naval aviator, test pilot, and businessman.", "1345 to 1377", "August 23, 1970 \u2013 October 31, 1993", "Vilnius", "Rounders", "music of pre-Hispanic and contemporary music of the Andes,", "Robert John Day", "Rio Gavin Ferdinand", "National Lottery", "The Battle of Prome", "M2M", "Austria", "Citizens for a Sound Economy", "right-hand", "Gatwick", "Australian", "Darkroom", "House of Commons", "La Familia Michoacana", "five months", "1983", "James Packer", "Northern Ireland.", "English", "Best Musical", "Floyd Mutrux and Colin Escott.", "poet and fellow rock musician, Patti Smith.", "the Mikoyan design bureau", "New York to Alabama.", "\"A Little Princess\"", "1943", "TD Garden", "published in the \"First Folio\" in 1623.", "Vixen", "18", "Uchinaanchu (\u6c96\u7e04\u4eba, Japanese: \"Okinawa jin\")", "Chicago", "J35-A-23.", "Axl Rose", "Prudential Center in Newark, New Jersey.", "Homer Hickam, Jr.", "CBS All Access", "Diary of a Wimpy Kid : The Long Haul", "John Dalton", "July 28, 1948.", "Department of Homeland Security Secretary Janet Napolitano", "\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "The Beatles", "Marmaduke", "Operation Cast Lead", "future relations between the Middle East and Washington.", "650"], "metric_results": {"EM": 0.5, "QA-F1": 0.6084114019660894}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, false, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.09090909090909091, 0.6666666666666666, 1.0, 0.0, 0.8571428571428571, 0.25, 0.5, 1.0, 0.2, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4010", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-2486", "mrqa_hotpotqa-validation-3304", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-5804", "mrqa_naturalquestions-validation-6298", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-3423", "mrqa_newsqa-validation-3865", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-8155", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-571"], "SR": 0.5, "CSR": 0.5773809523809523, "EFR": 0.96875, "Overall": 0.7186793154761906}, {"timecode": 21, "before_eval_results": {"predictions": ["1622", "horrible wars", "all age groups", "cabin depressurization", "largest gold rushes the world has ever seen", "1985", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "Sultan Selim II", "the European cruiserweight title from 2015 to 2016.", "A Hard Day's Night", "Lord Chancellor of England.", "Ben Elton", "West Cheshire Association Football League", "KB", "1,467 rooms", "a pioneering New Zealand food writer", "the Old Executive Office Building", "Bad Reputation", "John R. Leonetti", "The Legend of Sleepy Hollow", "Martin Amis (16 April 1922 \u2013 22 October 1995)", "Harmony Korine", "137th", "Texas Tech University", "Pisgah National Forest", "703 rooms", "King's College London", "Rochdale", "Mostly influenced by Tudor music and English folk-song", "Northern Ireland", "Pylos and Thebes", "Larry Gatlin & the Gatlin Brothers Band.", "the Beatles", "Lady Frederick Windsor", "winner of 1992 US Open", "2 November 1902", "most time in space (381.6 days)", "the NYPD's 83rd Precinct", "Patterns of Sexual Behavior", "Peel Holdings", "about 560", "wrestler, actor, and hip hop musician", "Theodor W. Adorno", "Gianna", "The City of Newcastle", "John Christopher Lujack Jr.", "gender queer", "Kiss", "David Jolly", "Minette Walters", "2004", "Tamil", "1,382 inhabitants", "most", "Barbara Windsor", "Stephenie Meyer", "Jane Austen", "Pakistan", "Michelle Rounds", "A Doll's House", "Florida", "A beast with a head and front legs of an eagle", "auction houses with watch departments and specialist watch-only auctioneers", "Mickey Mouse"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6269364316239316}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, true, false, false, true, false, true, false, false, false, true, true, true, true, false, false, true, true, false, false, true, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.75, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.7692307692307693, 1.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 0.923076923076923, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2987", "mrqa_hotpotqa-validation-1312", "mrqa_hotpotqa-validation-2016", "mrqa_hotpotqa-validation-3521", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-3232", "mrqa_hotpotqa-validation-4752", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-4513", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-984", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-5295", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5052", "mrqa_naturalquestions-validation-2159", "mrqa_triviaqa-validation-6256", "mrqa_newsqa-validation-1218", "mrqa_triviaqa-validation-2227", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-7724"], "SR": 0.53125, "CSR": 0.5752840909090908, "EFR": 1.0, "Overall": 0.7245099431818182}, {"timecode": 22, "before_eval_results": {"predictions": ["$960 billion", "European Court of Human Rights", "suffer from kidney and bladder stones, and arthritis,", "NP", "a band of visionary zealots,", "October 12, 1943", "New Orleans", "mafia clan", "inveigle", "Vaccines", "1984", "ravens", "inveigle", "insulin", "bullfight", "17 pink", "12", "The Pennine Way", "Muriel Spark", "basil", "La Mancha", "Martin Van Buren", "Bonnie and Clyde", "three other musicians", "Hillary Clinton", "Pennsylvania", "Tom Hanks", "6", "sound and light", "Panamanian politics was clouded in mid-1982 by the ouster of Colonel Florencio Fl\u00f3rez Aguilar in a shakeup of the National Guard in a surprise announcement by President Aristides Royo", "mushrooms", "in 2010", "Usain Bolt", "Mead", "To Kill a Mockingbird", "in Mos Eisley", "Sudan", "inanimate object or abstraction is endowed with human qualities or abilities", "Russia", "Doctor Doom", "Steve Jobs", "Christmas", "Rajasthan", "in a festive mood", "DIY", "David Hockney", "a compact bone", "Barnaby Rudge", "Surficial", "in 1861", "Thomas Jefferson", "Aberystwyth", "10 : 30am", "2014 Winter Olympics in Sochi, Russia", "in Seattle, Washington", "Capture of the Five Boroughs", "79 AD", "Wilmette", "Dodi Fayed", "last week", "missionary believed held in North Korea said Tuesday they are working with U.S. officials to get him returned home.", "Sounder Ri", "Sparta", "4 pecks"], "metric_results": {"EM": 0.421875, "QA-F1": 0.49483016983016986}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, false, true, false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.2857142857142857, 0.28571428571428575, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2505", "mrqa_squad-validation-2302", "mrqa_squad-validation-5697", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-4220", "mrqa_triviaqa-validation-2366", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-1286", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-3310", "mrqa_triviaqa-validation-1309", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-4040", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-6130", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-7001", "mrqa_triviaqa-validation-6192", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-3608", "mrqa_triviaqa-validation-6458", "mrqa_naturalquestions-validation-9007", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-7443", "mrqa_hotpotqa-validation-1874", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3239", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-2394"], "SR": 0.421875, "CSR": 0.5686141304347826, "EFR": 0.972972972972973, "Overall": 0.7177705456815511}, {"timecode": 23, "before_eval_results": {"predictions": ["1759-60", "tyrosinase", "Jadaran", "Israelis", "Konstantin Mereschkowski", "disease", "gounod and Reyer", "Vienna", "March 10, 1997", "marsupials", "meat", "The Benedictine Order", "police detective drama", "1985", "The Cavern Club", "John Steinbeck", "Drew Carey", "beta", "spain", "John Peel", "an Italian luthier", "Cheshire", "Rebecca Adlington", "silks", "power station", "Manchester City", "Tigris", "John Fitzgerald Kennedy", "spainning the Palazzo Rio, or Palace River", "cows", "steel", "island countries", "Steve Rogers", "whitsun", "Frank Harris", "Charles Atlas", "Alex Kramer", "harrow", "Isle of Wight", "violin", "Roberto Cammarelle", "elephant", "whitsun", "davain", "cynthia", "restless leg syndrome", "spleen", "Olympics", "Jimmy Knapp", "poland", "gargantua", "542-488 Ma", "Pebe Sebert and Hugh Moffatt", "October 29, 2015", "in the season - five premiere episode `` Second Opinion", "Leslie Knope", "First Balkan War", "Get Him to the Greek", "next year", "heavy turbulence", "weren't taking it well", "onassis", "Chuck Schumer", "spain"], "metric_results": {"EM": 0.5, "QA-F1": 0.5769230769230769}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.923076923076923, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6108", "mrqa_squad-validation-8488", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-3516", "mrqa_triviaqa-validation-3447", "mrqa_triviaqa-validation-519", "mrqa_triviaqa-validation-1250", "mrqa_triviaqa-validation-5138", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-7189", "mrqa_triviaqa-validation-4181", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-3350", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-7333", "mrqa_triviaqa-validation-3117", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-6876", "mrqa_naturalquestions-validation-3440", "mrqa_naturalquestions-validation-2818", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5271", "mrqa_newsqa-validation-1893", "mrqa_searchqa-validation-7791", "mrqa_searchqa-validation-7996"], "SR": 0.5, "CSR": 0.5657552083333333, "EFR": 1.0, "Overall": 0.7226041666666666}, {"timecode": 24, "before_eval_results": {"predictions": ["a comb jelly.", "cicadas", "gaseous oxygen", "no known case of any U.S. citizens buying Canadian drugs for personal use with a prescription,", "Medieval Latin, 9th century", "Teha'amana", "lyndon johnson", "noble", "Moulin Rouge", "Aerosmith", "Sunday, November 6,", "beautiful", "saddle oxfords", "NAFTA", "the earth", "Clarissa Dalloway", "lyndon johnson", "venial", "Toronto", "argentina", "bobcats", "greens", "lyndon johnson", "Alfred Nobel", "Smith & Wesson", "550 nm", "Emma Charlotte Duerre Watson", "Pan Am", "Cardinal Richelieu", "1863", "mansard", "lyndon johnson", "Nikita Khrushchev", "koolsla", "detective", "diamonds", "lyndon johnson", "morphine", "Antarctica", "argentina", "hand", "lyndon johnson", "lyndon johnson", "an American Tail", "pelican", "E-3", "Mt. Kenya", "lyndonbrunn", "canuck", "bathsheba", "Death Valley", "$1000", "Wembley Stadium", "Paul Gemignani", "MGM Resorts International", "Olympic Games", "bat", "halogens", "Ferdinand Magellan", "November 23, 2011", "Famous Ghost Stories", "Chester Arthur Stiles", "haitians", "leaders of more than 30 Latin American and Caribbean nations"], "metric_results": {"EM": 0.34375, "QA-F1": 0.400390625}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, true, false, false, false, true, false, false, false, true, true, false, false, false, false, true, true, false, false, true, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.33333333333333337, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.125]}}, "before_error_ids": ["mrqa_squad-validation-6383", "mrqa_squad-validation-1002", "mrqa_searchqa-validation-5038", "mrqa_searchqa-validation-5385", "mrqa_searchqa-validation-1375", "mrqa_searchqa-validation-12367", "mrqa_searchqa-validation-16821", "mrqa_searchqa-validation-12291", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-11213", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-1696", "mrqa_searchqa-validation-1004", "mrqa_searchqa-validation-14524", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-7094", "mrqa_searchqa-validation-9261", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-14808", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-8944", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-263", "mrqa_searchqa-validation-3135", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-10911", "mrqa_naturalquestions-validation-9773", "mrqa_triviaqa-validation-2047", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-4468", "mrqa_newsqa-validation-828", "mrqa_newsqa-validation-2225"], "SR": 0.34375, "CSR": 0.556875, "EFR": 1.0, "Overall": 0.7208281249999999}, {"timecode": 25, "before_eval_results": {"predictions": ["Khitan rulers", "over 100%", "vaccination", "Bermuda 419 turf.", "three", "Kgalema Motlanthe", "\"full civil equality,\"", "laid 11 healthy eggs", "a bag", "the man facing up, with his arms out to the side.", "felony drug charges", "the area where the single-engine Cessna 206 went down, half a nautical mile from the shoreline of the city of Quebradillas.", "sought Cain's help finding a job after being laid off from the trade association's education foundation in 1997.", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "the man was dead", "three different videos", "all day", "stand down", "hot and humid", "KBR", "the sun, and solar power,", "Twilight", "many of the members that have purchased our publications", "Venus Williams", "Booches Billiard Hall,", "Peshawar", "23-year-old", "three", "composing music and art, learning new languages, designing animation, collecting data, collaborating with peers across borders and accessing learning tools", "Eikenberry sent private cables to Obama last week, urging the president not to rush to send more troops to Afghanistan", "Sovereign Wealth Funds", "Besson", "Janet Napolitano", "Inmates", "a thick stack of paper", "death squad killings", "hours", "a full garden and pool, a tennis court, or several heli-pads.", "Bill", "Marcus", "Caylee Anthony", "changed the way the world consumed media", "\"GoldenEye\"", "five days a week.", "secure more funds from the region.", "upper respiratory infection", "\"He answered the questions they asked.\"", "Brown-Waite", "Shenzhen", "an off-duty New York police officer dead.", "Scudetto", "New York-based Human Rights Watch", "2026", "Mitch Murray", "the life of the Bennetts, a dysfunctional family consisting of two brothers, their rancher father, and his divorced wife and local bar owner", "Captain Mark Phillips", "Asuka Watts", "Jack", "vice president", "New York Giants", "Montgomery Clift", "The Man Trap", "Chris Matthews", "library of Congress"], "metric_results": {"EM": 0.375, "QA-F1": 0.5053902357027358}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, false, true, false, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 0.4, 0.47619047619047616, 0.0, 0.9600000000000001, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.1142857142857143, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.42857142857142855, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.7692307692307693, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.9189189189189189, 0.0, 0.5, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-448", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1429", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3636", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1701", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-4077", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2752", "mrqa_newsqa-validation-1137", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-7896", "mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-4087", "mrqa_triviaqa-validation-6614", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-1795", "mrqa_hotpotqa-validation-3375", "mrqa_searchqa-validation-16581", "mrqa_searchqa-validation-5521"], "SR": 0.375, "CSR": 0.5498798076923077, "EFR": 1.0, "Overall": 0.7194290865384615}, {"timecode": 26, "before_eval_results": {"predictions": ["investor David G. Booth", "pr\u00e9tendus r\u00e9form\u00e9s", "true Islamic system", "$20,000", "intention to set up headquarters in Dublin.", "ordered the makers of certain antibiotics to add a \"black box\" label warning", "Itawamba County School District", "baby daughter Jada,", "Elisabeth,", "Tuesday", "North Korea's announcement has triggered international consternation.", "waterboarding at least 266 times on two top al Qaeda suspects,", "U.S. District Judge Ricardo Urbina", "security from a disbanded military known for loyalty to dictators instead of the state,", "his son, Isaac, and daughter, Rebecca.", "near Garacad, Somalia,", "peanuts, nuts, shellfish, peanuts, tree nuts,", "either ignore signs of depression or lie about their use of medication for fear of losing their licenses to fly.", "How I Met Your Mother", "preserved corpses having sex", "19", "inmates", "as many as 50,000", "U.S. State Department and British Foreign Office", "the two remaining crew members from the helicopter,", "at a Little Rock military recruiting center", "up to $50,000 for her,", "Arthur E. Morgan III,", "Caster Semenya", "Ewan McGregor", "a baseball bat", "a pair of Astley Clarke earrings", "$60 million", "Iran test-launched a rocket capable of carrying a satellite,", "three", "Governor Sanford", "gun", "$1.5 million", "off the coast", "Davidson college students", "Diego Milito", "upper respiratory infection,", "officers at a Texas  airport appear to have properly followed procedures when they allegedly forced a woman to remove her", "McDonald's", "collapsed apartment building", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "leftist Workers' Party", "Hundreds of women protest child trafficking and shout anti-French slogans Wednesday in Abeche,", "full garden and pool, a tennis court,", "October 3,", "The Impeccable", "the maneuver was part of a planned training exercise designed to help the prince learn to fly in combat situations.", "the head of the Imperial Family and the traditional head of state of Japan", "1998", "November 2016", "sewing machines", "the plane's breakup and crash was due to Desmond Hume failing to enter a code into the Swan station computer, causing a large burst of electromagnetic energy.", "Duncan I", "half a million acres", "hiphop", "four", "Stanford", "a rough", "Lawrencium"], "metric_results": {"EM": 0.34375, "QA-F1": 0.5127740460219176}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true], "QA-F1": [0.8571428571428571, 1.0, 1.0, 1.0, 0.9333333333333333, 0.07142857142857142, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.28571428571428575, 0.1111111111111111, 0.0, 1.0, 0.0, 0.16666666666666669, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.8, 0.0, 0.33333333333333337, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.42857142857142855, 0.0, 0.5, 0.47058823529411764, 0.8, 0.6666666666666666, 1.0, 1.0, 0.5263157894736842, 0.0, 0.0, 0.5, 0.0, 0.5, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7935", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-2905", "mrqa_newsqa-validation-3592", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-1232", "mrqa_newsqa-validation-2671", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3289", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-4182", "mrqa_naturalquestions-validation-8429", "mrqa_naturalquestions-validation-7614", "mrqa_naturalquestions-validation-9119", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7585", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-5002", "mrqa_searchqa-validation-679"], "SR": 0.34375, "CSR": 0.5422453703703703, "EFR": 1.0, "Overall": 0.717902199074074}, {"timecode": 27, "before_eval_results": {"predictions": ["computational complexity theory", "economically", "Labor", "Children of Earth", "pelvis and sacrum -- the triangular bone within the pelvis.", "a photo album", "133", "one count of attempted murder in the second degree in the October 12 attack in Deerfield Beach, Florida.", "Sri Lanka", "Muslim", "an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "200", "was found Sunday on an island stronghold of the Islamic militant group Abu Sayyaf,", "innovative, exciting skyscrapers", "Egyptian security forces", "1831", "Glasgow, Scotland", "10 to 15 percent", "Kearny, New Jersey.", "22-10.", "four life jackets", "a number of calls,", "Mark Obama Ndesandjo", "New York City Mayor Michael Bloomberg", "The Delta Queen", "15", "opposition parties", "July 23.", "2004.", "citizenship", "\"I am sick of life -- what can I say to you?\"", "Gustav's top winds weakened to 110 mph,", "National September 11 Memorial Museum", "a curfew", "Omar bin Laden's sons", "Tim Masters", "surgical anesthetic propofol", "the Iranian consulate,", "Pixar's \"Toy Story\"", "The Da Vinci Code", "martial arts,", "Ameneh Bahrami", "producing rock music with a country influence.", "\"The American president toured a mosque, laid a wreath at the grave of the founder of the Turkish republic,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "$8.8 million", "near his home in Peshawar", "refusal or inability to \"turn it off\"", "fighting charges of Nazi war crimes for well over two decades.", "Minerals Management Service Director Elizabeth Birnbaum", "20%", "U.S. ship that was hijacked off Somalia's coast.", "a competitor or team in a sport or other tournament who is given a preliminary ranking for the purposes of the draw", "2009", "disputes between two or more states", "shorthand", "Carmen Miranda", "Lesley Garrett", "Denmark", "the tissues of the outer third of the vagina,", "Mark Neveldine and Brian Taylor.", "Ulysses S. Grant", "Kevin Bacon", "yellow"], "metric_results": {"EM": 0.5, "QA-F1": 0.6341719983447924}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 0.3636363636363636, 0.6666666666666666, 0.2222222222222222, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.4444444444444445, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.08, 0.0, 1.0, 1.0, 1.0, 0.7058823529411764, 0.5, 0.5, 0.5454545454545454, 0.16, 0.0, 0.8, 1.0, 1.0, 1.0, 0.5, 0.1818181818181818, 1.0, 0.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1496", "mrqa_newsqa-validation-1181", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-2718", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-3834", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2113", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-1028", "mrqa_naturalquestions-validation-10325", "mrqa_naturalquestions-validation-9141", "mrqa_naturalquestions-validation-8092", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-4194", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420"], "SR": 0.5, "CSR": 0.5407366071428572, "EFR": 0.96875, "Overall": 0.7113504464285715}, {"timecode": 28, "before_eval_results": {"predictions": ["Ex post facto laws,", "temperature and light", "\"The particles in the beam of force... will travel much faster than such particles... and they will travel in concentrations.\"", "\"inevitable\" Democratic nominee, Hillary Clinton, back on her heels.", "Hillary Clinton", "volatile", "older generation", "Arkansas businessman Ken Plunkett,", "800,000", "Monday and Tuesday", "a full garden and pool, a tennis court, or several heli-pads.", "\"I deal with families who lose their babies and I will cry with them, but I thought I would be stronger.", "an independent homeland for the country's ethnic Tamil minority since 1983.", "The guy next door versus the guy-liner.\"", "\"The biology professor is charged with capital murder and three counts of attempted murder in the shootings at the University of Alabama in Huntsville last week.", "prisoners", "Ashley \"A.J.\" Jewell,", "The drama of an American ship captain held hostage by Somali pirates led last Sunday's talk shows.", "July", "Iran's foreign minister said he believes a solution will be reached over a proposed deal to export uranium for enrichment abroad,", "The U.S. State Department and British Foreign Office", "former U.S. secretary of state.", "misdemeanor assault charges", "Venezuela's Tupolev Tu-160 strategic bombers", "Argentine", "Facebook photo album full of pics of you looking smiley.", "Piers Morgan Tonight", "$50,000", "Leo Frank", "Osama", "the most gigantic pumpkins in the world, going through a metamorphosis from blobs of orange to art as night falls.", "the United States and other Western nations are concerned by Iran's refusal to halt uranium enrichment activities.", "on the 11th anniversary of the September 11, 2001, terror attacks.", "St. Louis, Missouri.", "hand-painted Swedish wooden clogs", "The comprehensive response has extended the lives of tens of thousands of Brazilians and saved the government billions,", "The pontiff reiterated the Vatican's policy on condom use as he flew from Rome to Yaounde,", "The National Telecommunications and Information Administration offered a program to help people buy converter boxes that make old TVs work in the new era.", "three searches", "Basel", "more than 700", "first grand Slam,", "suppress the memories and to live as normal a life as possible; the culture of his time said that he was fortunate to have survived and that he should get on with his life,\"", "$1.4 million,", "The next day they decided they would continue trying to start a family, with both surrogacy and adoption as possible paths.", "murder", "role as a bride in the 2007 movie \"License to Wed\"", "The UNHCR recommended against granting asylum,", "2009", "Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "The six alleged victims", "Robert Barnett", "Javier Fern\u00e1ndez", "7.6 mm", "The Oxford English Dictionary's earliest example is the phrase If that's a joke I'm a monkey's uncle, from an Ohio newspaper on 8 February 1925", "Patrick Chukwuemeka Okogwu", "Petula Clark", "Lord Snooty", "Roman Kostomarov", "August 1973", "ten episodes", "The Stranger", "Los Angeles Times", "George Orwell"], "metric_results": {"EM": 0.390625, "QA-F1": 0.48144099362240295}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.07692307692307693, 0.0, 1.0, 1.0, 0.28571428571428575, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.0, 0.9444444444444444, 0.0, 0.0, 0.3333333333333333, 0.0, 0.20689655172413796, 0.0, 0.08695652173913042, 0.0, 1.0, 0.5, 0.8571428571428571, 0.06060606060606061, 0.0, 0.07692307692307693, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4096", "mrqa_squad-validation-1389", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2739", "mrqa_newsqa-validation-1478", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2291", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-2662", "mrqa_newsqa-validation-636", "mrqa_newsqa-validation-3029", "mrqa_newsqa-validation-1905", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-1765", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-1663", "mrqa_newsqa-validation-1425", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-3344", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-1384", "mrqa_newsqa-validation-950", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2030", "mrqa_naturalquestions-validation-4074", "mrqa_naturalquestions-validation-6832", "mrqa_naturalquestions-validation-6665", "mrqa_hotpotqa-validation-1824", "mrqa_searchqa-validation-1869"], "SR": 0.390625, "CSR": 0.5355603448275862, "EFR": 1.0, "Overall": 0.7165651939655173}, {"timecode": 29, "before_eval_results": {"predictions": ["fully consistent with the conceptual definition of force offered by Newtonian mechanics.", "peer tuitions", "six", "Texas", "Tennyson", "William", "Francis Drake", "woodwind", "Candice", "aluminum", "timbers, piles or stoelwork", "Soviet", "African Golden Wolf", "South America", "Idi Amin", "will", "pink", "Tom", "anemia", "Frank Sinatra", "African", "Peter", "Mozart", "Russian", "Woody Allen", "(10-9)", "Citizen Kane", "\"The Stag\"", "a pirate flag", "bears", "Shirley Jackson", "JetBlue", "a hat", "a small glass", "Peru", "the Phantom of the Opera", "an eye", "Holy Roman Empire", "Hawaii", "Bob Newhart", "shorthand", "cologne", "Parsley", "Utah", "a megrim", "a black and white tuxedo cat", "Sam Shepard", "oil", "August Wilson", "a group of characters who have safely gotten off the Island", "bay", "Blue Nile", "China ( formerly the Republic of China ), Russia (formerly the Soviet Union ), France, the United Kingdom, and the United States", "Roman Reigns", "Mexican Seismic Alert System", "violin", "Heelflip", "Simeon Williamson", "11 November 1869", "Ten Walls", "Miss Mulatto, Mani, and Nova.", "new kidney", "surgical anesthetic propofol", "the deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan."], "metric_results": {"EM": 0.46875, "QA-F1": 0.5602430555555555}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, true, false, true, false, false, true, true, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.2222222222222222]}}, "before_error_ids": ["mrqa_squad-validation-10284", "mrqa_searchqa-validation-1839", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-10614", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-14158", "mrqa_searchqa-validation-14198", "mrqa_searchqa-validation-14060", "mrqa_searchqa-validation-10199", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-12860", "mrqa_searchqa-validation-9533", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-1760", "mrqa_searchqa-validation-2536", "mrqa_searchqa-validation-10285", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-16829", "mrqa_searchqa-validation-13176", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8784", "mrqa_searchqa-validation-16060", "mrqa_searchqa-validation-5204", "mrqa_naturalquestions-validation-975", "mrqa_triviaqa-validation-2394", "mrqa_triviaqa-validation-6791", "mrqa_hotpotqa-validation-4819", "mrqa_newsqa-validation-1445"], "SR": 0.46875, "CSR": 0.5333333333333333, "EFR": 1.0, "Overall": 0.7161197916666666}, {"timecode": 30, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1076", "mrqa_hotpotqa-validation-1294", "mrqa_hotpotqa-validation-1331", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1481", "mrqa_hotpotqa-validation-1484", "mrqa_hotpotqa-validation-1574", "mrqa_hotpotqa-validation-1750", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1912", "mrqa_hotpotqa-validation-1914", "mrqa_hotpotqa-validation-1980", "mrqa_hotpotqa-validation-2073", "mrqa_hotpotqa-validation-2092", "mrqa_hotpotqa-validation-2125", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2156", "mrqa_hotpotqa-validation-2217", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2281", "mrqa_hotpotqa-validation-2310", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2440", "mrqa_hotpotqa-validation-2456", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-2605", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2741", "mrqa_hotpotqa-validation-283", "mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-2986", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3299", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4027", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4182", "mrqa_hotpotqa-validation-4189", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4422", "mrqa_hotpotqa-validation-4469", "mrqa_hotpotqa-validation-4477", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4642", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4819", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-5002", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5098", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5346", "mrqa_hotpotqa-validation-5366", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5495", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5535", "mrqa_hotpotqa-validation-5552", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5750", "mrqa_hotpotqa-validation-5807", "mrqa_hotpotqa-validation-5833", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5880", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-663", "mrqa_hotpotqa-validation-710", "mrqa_hotpotqa-validation-748", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-918", "mrqa_hotpotqa-validation-984", "mrqa_naturalquestions-validation-10070", "mrqa_naturalquestions-validation-10093", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-1376", "mrqa_naturalquestions-validation-1407", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2250", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-476", "mrqa_naturalquestions-validation-4830", "mrqa_naturalquestions-validation-485", "mrqa_naturalquestions-validation-4876", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-5390", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5781", "mrqa_naturalquestions-validation-6573", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8514", "mrqa_naturalquestions-validation-8909", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-975", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1003", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-1193", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1283", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1388", "mrqa_newsqa-validation-1519", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1659", "mrqa_newsqa-validation-1687", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1710", "mrqa_newsqa-validation-18", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1918", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1984", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2011", "mrqa_newsqa-validation-2016", "mrqa_newsqa-validation-2018", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2040", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2187", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2560", "mrqa_newsqa-validation-2608", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2724", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-2869", "mrqa_newsqa-validation-2874", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2949", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3084", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3154", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3239", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3377", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3455", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4158", "mrqa_newsqa-validation-4164", "mrqa_newsqa-validation-434", "mrqa_newsqa-validation-440", "mrqa_newsqa-validation-456", "mrqa_newsqa-validation-476", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-638", "mrqa_newsqa-validation-647", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-86", "mrqa_newsqa-validation-882", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-950", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10395", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11166", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-11414", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-12104", "mrqa_searchqa-validation-12242", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12897", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13305", "mrqa_searchqa-validation-13321", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14932", "mrqa_searchqa-validation-15156", "mrqa_searchqa-validation-15244", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15877", "mrqa_searchqa-validation-1595", "mrqa_searchqa-validation-16258", "mrqa_searchqa-validation-16948", "mrqa_searchqa-validation-1696", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1773", "mrqa_searchqa-validation-1926", "mrqa_searchqa-validation-2095", "mrqa_searchqa-validation-2329", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-5204", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5338", "mrqa_searchqa-validation-5478", "mrqa_searchqa-validation-5935", "mrqa_searchqa-validation-6010", "mrqa_searchqa-validation-6770", "mrqa_searchqa-validation-6958", "mrqa_searchqa-validation-7938", "mrqa_searchqa-validation-8443", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9277", "mrqa_searchqa-validation-9391", "mrqa_searchqa-validation-9683", "mrqa_squad-validation-100", "mrqa_squad-validation-10001", "mrqa_squad-validation-1002", "mrqa_squad-validation-10063", "mrqa_squad-validation-10174", "mrqa_squad-validation-10186", "mrqa_squad-validation-10256", "mrqa_squad-validation-10316", "mrqa_squad-validation-10370", "mrqa_squad-validation-10386", "mrqa_squad-validation-10457", "mrqa_squad-validation-10463", "mrqa_squad-validation-10470", "mrqa_squad-validation-1078", "mrqa_squad-validation-1177", "mrqa_squad-validation-1219", "mrqa_squad-validation-1263", "mrqa_squad-validation-133", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-150", "mrqa_squad-validation-151", "mrqa_squad-validation-1612", "mrqa_squad-validation-1613", "mrqa_squad-validation-168", "mrqa_squad-validation-175", "mrqa_squad-validation-1759", "mrqa_squad-validation-1779", "mrqa_squad-validation-1813", "mrqa_squad-validation-1891", "mrqa_squad-validation-1980", "mrqa_squad-validation-2018", "mrqa_squad-validation-2085", "mrqa_squad-validation-2085", "mrqa_squad-validation-2109", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2214", "mrqa_squad-validation-2302", "mrqa_squad-validation-246", "mrqa_squad-validation-2586", "mrqa_squad-validation-2666", "mrqa_squad-validation-270", "mrqa_squad-validation-2704", "mrqa_squad-validation-2754", "mrqa_squad-validation-2913", "mrqa_squad-validation-2939", "mrqa_squad-validation-2975", "mrqa_squad-validation-3037", "mrqa_squad-validation-313", "mrqa_squad-validation-3213", "mrqa_squad-validation-3370", "mrqa_squad-validation-3479", "mrqa_squad-validation-350", "mrqa_squad-validation-3535", "mrqa_squad-validation-3581", "mrqa_squad-validation-3667", "mrqa_squad-validation-3740", "mrqa_squad-validation-375", "mrqa_squad-validation-3750", "mrqa_squad-validation-3811", "mrqa_squad-validation-3842", "mrqa_squad-validation-385", "mrqa_squad-validation-3922", "mrqa_squad-validation-3937", "mrqa_squad-validation-3939", "mrqa_squad-validation-4096", "mrqa_squad-validation-4179", "mrqa_squad-validation-4246", "mrqa_squad-validation-4292", "mrqa_squad-validation-4469", "mrqa_squad-validation-4591", "mrqa_squad-validation-4669", "mrqa_squad-validation-468", "mrqa_squad-validation-4860", "mrqa_squad-validation-4877", "mrqa_squad-validation-4898", "mrqa_squad-validation-5007", "mrqa_squad-validation-5030", "mrqa_squad-validation-5042", "mrqa_squad-validation-5135", "mrqa_squad-validation-5325", "mrqa_squad-validation-5351", "mrqa_squad-validation-5613", "mrqa_squad-validation-5699", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5716", "mrqa_squad-validation-5721", "mrqa_squad-validation-5809", "mrqa_squad-validation-5875", "mrqa_squad-validation-5933", "mrqa_squad-validation-6025", "mrqa_squad-validation-6116", "mrqa_squad-validation-6157", "mrqa_squad-validation-6163", "mrqa_squad-validation-6274", "mrqa_squad-validation-6341", "mrqa_squad-validation-6383", "mrqa_squad-validation-6579", "mrqa_squad-validation-6676", "mrqa_squad-validation-6700", "mrqa_squad-validation-6705", "mrqa_squad-validation-6803", "mrqa_squad-validation-7036", "mrqa_squad-validation-7168", "mrqa_squad-validation-7233", "mrqa_squad-validation-7233", "mrqa_squad-validation-7246", "mrqa_squad-validation-7294", "mrqa_squad-validation-735", "mrqa_squad-validation-7354", "mrqa_squad-validation-7367", "mrqa_squad-validation-7428", "mrqa_squad-validation-7491", "mrqa_squad-validation-7516", "mrqa_squad-validation-7531", "mrqa_squad-validation-7531", "mrqa_squad-validation-7570", "mrqa_squad-validation-7707", "mrqa_squad-validation-7782", "mrqa_squad-validation-7788", "mrqa_squad-validation-780", "mrqa_squad-validation-7805", "mrqa_squad-validation-7838", "mrqa_squad-validation-7909", "mrqa_squad-validation-8386", "mrqa_squad-validation-850", "mrqa_squad-validation-8500", "mrqa_squad-validation-8502", "mrqa_squad-validation-8523", "mrqa_squad-validation-8555", "mrqa_squad-validation-8601", "mrqa_squad-validation-8603", "mrqa_squad-validation-8680", "mrqa_squad-validation-8861", "mrqa_squad-validation-8900", "mrqa_squad-validation-8901", "mrqa_squad-validation-8905", "mrqa_squad-validation-8927", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-8966", "mrqa_squad-validation-9254", "mrqa_squad-validation-9269", "mrqa_squad-validation-9271", "mrqa_squad-validation-929", "mrqa_squad-validation-9293", "mrqa_squad-validation-9302", "mrqa_squad-validation-9322", "mrqa_squad-validation-9328", "mrqa_squad-validation-9412", "mrqa_squad-validation-9436", "mrqa_squad-validation-95", "mrqa_squad-validation-9590", "mrqa_squad-validation-9695", "mrqa_squad-validation-9901", "mrqa_squad-validation-9989", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1156", "mrqa_triviaqa-validation-1268", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1840", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2296", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2437", "mrqa_triviaqa-validation-247", "mrqa_triviaqa-validation-2937", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3045", "mrqa_triviaqa-validation-3065", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-35", "mrqa_triviaqa-validation-3577", "mrqa_triviaqa-validation-3657", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3803", "mrqa_triviaqa-validation-3872", "mrqa_triviaqa-validation-4094", "mrqa_triviaqa-validation-4137", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4430", "mrqa_triviaqa-validation-4438", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-5111", "mrqa_triviaqa-validation-5212", "mrqa_triviaqa-validation-5226", "mrqa_triviaqa-validation-5275", "mrqa_triviaqa-validation-5316", "mrqa_triviaqa-validation-5489", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-5880", "mrqa_triviaqa-validation-5965", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6083", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-6659", "mrqa_triviaqa-validation-6698", "mrqa_triviaqa-validation-6701", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-7289", "mrqa_triviaqa-validation-7404", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7613"], "OKR": 0.861328125, "KG": 0.453125, "before_eval_results": {"predictions": ["climate change", "coronary thrombosis", "a Qutb", "(ANNA)", "D.C.", "Medusa", "The Tin Drum", "kerosene", "England", "Shirley Temple", "flagellation", "Buck Humphrey", "Bart Simpson", "(D Dwight Eisenhower)", "Frasier", "Hispanic heritage", "fish", "Dix", "a motor neuron disease", "Bill Clinton", "Emily Dickinson", "Mexico", "cosmology", "a trace of copper", "English", "decoupage", "a novel that shows the culture of the United States at a specific time", "Arethusa", "(ALA)", "Bucharest", "Down syndrome", "manager", "a biscuit mix", "Aborigines", "Dusty Old Dust", "the pancreas", "a Ninja", "World War II", "a pear tart", "Mark Twain", "Deneb IV", "Anacondas", "(Testudo nigra)", "a knesset", "Secretary's Day", "Time", "Dante's Inferio", "a bull", "Lulu", "the Great Hunger", "eulogy", "Roald Dahl", "Massachusetts", "third", "the leaves of the plant species", "Amy Dorrit", "alpestrine", "Frank Langella", "St. Patrick's Day in 1988", "Kealakekua Bay", "Santiago Herrera", "three", "Secretary of State", "Fernando Torres"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5955492424242425}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, true, true, false, true, true, false, false, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, false, false, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14778", "mrqa_searchqa-validation-3010", "mrqa_searchqa-validation-3606", "mrqa_searchqa-validation-4718", "mrqa_searchqa-validation-12276", "mrqa_searchqa-validation-1744", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-875", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-398", "mrqa_searchqa-validation-10013", "mrqa_searchqa-validation-12346", "mrqa_searchqa-validation-9336", "mrqa_searchqa-validation-2416", "mrqa_searchqa-validation-12944", "mrqa_searchqa-validation-1913", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-3032", "mrqa_searchqa-validation-6234", "mrqa_searchqa-validation-2083", "mrqa_searchqa-validation-15784", "mrqa_searchqa-validation-8117", "mrqa_searchqa-validation-16220", "mrqa_searchqa-validation-3267", "mrqa_searchqa-validation-4325", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-9172", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4435", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5627"], "SR": 0.484375, "CSR": 0.5317540322580645, "EFR": 0.9696969696969697, "Overall": 0.7112277003910068}, {"timecode": 31, "before_eval_results": {"predictions": ["corrosion", "railway locomotives", "imperfect", "Eli Lilly", "Manhattan Project", "Maryland", "the Scotch egg", "carioca", "(D Dale)hardt", "(Sacha) DUBBED", "( Ford Madox Ford)", "Cuyahoga", "(BomBED)", "Apollo 13", "a coyote", "terminal", "\"The Boss\"", "pie", "Air Force Academy", "(Harry Lime)", "a monkey", "a nucleus", "Don Juan", "Texas", "(Computing-Tabulating) Recording Corporation", "( Chuck) Yeager", "the CIA", "(Sacha) Gandhi", "lanciferous lancine", "the Wadi Hanifah valley", "Abnormal Psychology", "goat milk", "(Billy Idol) Idol", "anaphylaxis", "copper", "(Three's Company)", "Bank of America", "lampoon", "Terry Bradshaw", "Florence", "farce", "tobacco", "Columbia University", "(Evita) Clinton", "Don Quixote", "Medium", "Seattle", "insulin", "dilettante", "mules", "Ricky Martin", "(Sizzou)", "1989", "sea water", "Toronto", "(Napoleon) Bonaparte\u2019s Grande Arm\u00e9e", "a power factor", "Frederick Forsyth", "Daniil Shafran", "67,575", "various names", "(Rihanna) Graham", "At least 13", "The Everglades,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6029513888888889}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, false, false, true, true, false, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-3276", "mrqa_searchqa-validation-757", "mrqa_searchqa-validation-5869", "mrqa_searchqa-validation-2914", "mrqa_searchqa-validation-9740", "mrqa_searchqa-validation-16718", "mrqa_searchqa-validation-12607", "mrqa_searchqa-validation-16219", "mrqa_searchqa-validation-4589", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-7211", "mrqa_searchqa-validation-14766", "mrqa_searchqa-validation-9617", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-3155", "mrqa_searchqa-validation-6987", "mrqa_searchqa-validation-12978", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7978", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7582", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-11232", "mrqa_naturalquestions-validation-8628", "mrqa_triviaqa-validation-2879", "mrqa_triviaqa-validation-5038", "mrqa_newsqa-validation-3090", "mrqa_newsqa-validation-3911"], "SR": 0.515625, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.7171875}, {"timecode": 32, "before_eval_results": {"predictions": ["Eric Roberts", "the Department for Culture, Media and Sport.", "wigs", "Tennessee Williams", "Wayne Gretzky", "Boris Godunov", "Williamsburg", "Pitcairn", "air superiority", "Halloween", "Vascular", "hurricane", "The Producers", "one foot", "the Automobile Association", "Jutland", "mutual fund", "the Two Sicilies", "1773", "Prometheus", "a thick cream soup", "4 cups", "an Eagle Mountain", "Violeta Chamorro", "Horatio Nelson", "Pillsbury", "oxygen", "The Last Mimzy", "jeans", "Jenna Bush", "Jonathan Demme", "silver", "the North Atlantic Treaty Organization", "the ear", "Moulin Rouge", "Cape Cod", "the shell", "\"Western\"", "the 1989 wreck", "ice age", "the Medal of Honor", "Slavic", "Walter Payton", "Orleans", "Mars", "the gunner", "copper", "Tom Ridge", "George Babbitt", "Meg Tilly", "Jeopardy", "imperative", "18", "George Halas", "Long Island", "the Cheshire Cat", "salema", "Kent", "Gareth Jones", "an obsessed and tormented king", "Newcastle upon Tyne, England", "Illness", "Dan Parris, 25, and Rob Lehr, 26,", "2,000 euros"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6270833333333332}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, false, true, false, false, false, false, false, true, true, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_searchqa-validation-16822", "mrqa_searchqa-validation-10386", "mrqa_searchqa-validation-13952", "mrqa_searchqa-validation-5519", "mrqa_searchqa-validation-8665", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-6114", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-13050", "mrqa_searchqa-validation-6804", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-16959", "mrqa_searchqa-validation-7358", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-11313", "mrqa_searchqa-validation-2960", "mrqa_searchqa-validation-13215", "mrqa_searchqa-validation-8969", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-6372", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-12784", "mrqa_naturalquestions-validation-1223", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-1605", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2730"], "SR": 0.5625, "CSR": 0.5321969696969697, "EFR": 1.0, "Overall": 0.717376893939394}, {"timecode": 33, "before_eval_results": {"predictions": ["2100", "Apollo 5", "Benazir Bhutto,", "the Democratic VP candidate", "the missions are rewriting lunar science text books and revolutionizing what scientists know about Earth's closest neighbor.", "the i report form", "Muqtada al-Sadr", "Twitter", "\"The extensive use of advanced technologies and materials in the 2009 F-150 required us to develop new, specific procedures and repair recommendations,\"", "iPod Touch", "success as a recording artist", "Department of Homeland Security Secretary Janet Napolitano", "the chemical at the Qarmat Ali water pumping plant in southern Iraq", "citizenship", "10,000", "4,000", "CNN/Opinion Research Corporation", "California", "Michael Arrington,", "Ralph Cifaretto on the HBO series \"The Sopranos,\"", "Bobby Darin", "Two", "LulzSec.", "Transportation Security Administration", "Maude", "was it natural causes", "Nineteen", "\"Dance Your Ass Off.\"", "1918-1919.", "Liza Murphy", "Turkey", "the motherless cub defended by Elphaba", "\"I wanted to come here, and I wanted to see my kids graduate from this school district.\"", "Gary Player", "attempted murder", "Nicole", "guard in the jails", "a \"prostitute\"", "Mississippi", "40", "April 22.", "trading goods and services without exchanging money", "14", "Nairobi, Kenya", "\"Empire of the Sun,\"", "Silicon Valley.", "between 1917 and 1924", "June 6, 1944,", "giving birth to baby daughter Jada,", "Atlanta", "don't have to visit laundromats", "\"The crash occurred along the Red Line just before 5 p.m. Monday on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "Charlene Holt", "the ankle", "Miami Heat", "4", "the Finch family", "architect", "the Secret Intelligence Service", "Ready Player One", "alcoholic drinks for consumption on the premises.", "Mars", "islam", "Tartarus"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5810430869703966}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 0.5454545454545454, 0.3333333333333333, 1.0, 0.0, 1.0, 0.5, 1.0, 0.4444444444444445, 0.15384615384615383, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8333333333333333, 0.2608695652173913, 1.0, 0.4, 1.0, 0.0, 0.2857142857142857, 1.0, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.923076923076923, 1.0, 0.625, 0.07692307692307693, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-850", "mrqa_newsqa-validation-3905", "mrqa_newsqa-validation-5", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2253", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1166", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-294", "mrqa_newsqa-validation-3306", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1333", "mrqa_newsqa-validation-3020", "mrqa_newsqa-validation-3382", "mrqa_newsqa-validation-2520", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2290", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-801", "mrqa_newsqa-validation-3049", "mrqa_newsqa-validation-1288", "mrqa_naturalquestions-validation-7609", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-4032", "mrqa_triviaqa-validation-3514", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-1511", "mrqa_searchqa-validation-201"], "SR": 0.453125, "CSR": 0.5298713235294117, "EFR": 1.0, "Overall": 0.7169117647058824}, {"timecode": 34, "before_eval_results": {"predictions": ["for complicity and to Odinga declaring himself the \"people's president\"", "KGPE", "Lerotholi Polytechnic", "John Delaney", "1979", "The Catholic Church in Ireland", "Trey Parker and Matt Stone", "Province of Canterbury", "310", "Bohemia", "Czech", "people working in film and the performing arts", "Washington, D.C.", "Sophie Winkleman", "Portsmouth", "Sam the Sham", "coaxial", "Bruce McLaren", "Marika Nicolette Green", "Katherine Harris", "Frank Edward Thomas Jr.", "World War I", "John Richard Schlesinger,", "Nikolai Trubetzkoy", "3,500,000", "Rabies", "Big Bad Wolf", "Thomas Jane", "1974", "Orwell", "July 8, 2014", "Prabh Gill", "Costa del Sol", "Gabriel Iglesias", "Kolkata", "two", "Roscoe Lee Browne", "23", "video game", "a Peach or a nectarine", "U.S.", "Jimmy Ellis", "July 11, 2016", "Srinagar", "\"The 36th Chamber of Shaolin\"", "San Francisco, California", "\"The Brothers\"", "Daniel Espinosa", "Adam Levine, Blake Shelton, and Pharrell Williams", "Benjamin Andrew \" Ben\" Stokes", "Duke University during the 2014\u201315 NCAA Division I men's basketball season as members of the Atlantic Coast Conference.", "The Kennedy Center.", "Louis XV", "2,140 kilometres ( 1,330 mi )", "\u05de\u05b7\u05dc\u05b0\u05db\u05b8\u05bc\u05dd `` Malkam `` great king ''", "Munich", "Maerten Tromp", "Islamophobia", "Bago", "Authorities in Fayetteville, North Carolina,", "crude oil", "a Rolling Stone", "Robert de Niro", "General McClellan"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5334354575163398}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [0.11764705882352941, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.5, 0.0, 0.0, 0.888888888888889, 0.0, 0.8, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8422", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5483", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-653", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-3862", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-4804", "mrqa_hotpotqa-validation-207", "mrqa_hotpotqa-validation-3648", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-3510", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5039", "mrqa_hotpotqa-validation-1634", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-732", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-1447", "mrqa_hotpotqa-validation-2206", "mrqa_hotpotqa-validation-4740", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-3889", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-2085", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4748", "mrqa_newsqa-validation-735", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-3428", "mrqa_searchqa-validation-10048", "mrqa_searchqa-validation-2753", "mrqa_searchqa-validation-15973"], "SR": 0.421875, "CSR": 0.5267857142857143, "EFR": 1.0, "Overall": 0.7162946428571428}, {"timecode": 35, "before_eval_results": {"predictions": ["downward pressure on wages", "Prospect Avenue", "Cincinnati", "North Sea", "Headless Body in Topless Bar", "\"Gliding Dance of the Maidens\"", "Patti Smith", "\"Darconville\u2019s Cat\"", "\"Kitty Hawk\"", "Tiberius", "Netherlands", "Eva Ibbotson", "the 70 m and 90 m events", "Charles Otto Puth Jr.", "Tak and the Power of Juju", "Don DeLillo", "the crossroads of the Newell Highway between Melbourne and Brisbane, and the Mid-Western Highway between Sydney and Adelaide.", "comedy", "2016", "University of Southern California (USC) Trojans", "New York City", "authoritarian tendencies", "1st Marquess of Westminster", "Christopher McCulloch", "Marigold Newey", "an album", "The Wachowskis", "trolls", "WikiLeaks", "top division", "Ramsey County", "fantasy role-playing game", "New Jersey", "Leinster", "Giacomo Puccini", "Thomas Jefferson", "Winchester", "classical vocalist", "1912", "Eisenhower Executive Office Building", "\"The Original Sound Track from Five Summer Stories\"", "a motorcycle manufacturer and lifestyle brand company", "first and second segment", "in England", "Hindi", "Netherlands", "soccer", "Kings Point, New York", "a Ballon d'Or.", "Westminster system", "Bruce Grobbelaar", "The Charkhi Dadri mid-air collision occurred on 12 November 1996", "seven years earlier on Christmas Eve", "`` Mirror Image ''", "79", "Alaska", "football", "PDSA", "The elephant Sanctuary.", "Kurt Cobain", "an impromptu memorial", "Our Sea", "sedimentary", "Anton Chekhov"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6557572983124453}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, true, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false], "QA-F1": [0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.6666666666666666, 0.23529411764705882, 1.0, 0.0, 0.8, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05405405405405406, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.33333333333333337, 0.5, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-7182", "mrqa_squad-validation-5945", "mrqa_hotpotqa-validation-3216", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-1010", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-455", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-220", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5227", "mrqa_hotpotqa-validation-1602", "mrqa_hotpotqa-validation-3169", "mrqa_hotpotqa-validation-2257", "mrqa_naturalquestions-validation-4338", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1352", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-282"], "SR": 0.53125, "CSR": 0.5269097222222222, "EFR": 1.0, "Overall": 0.7163194444444445}, {"timecode": 36, "before_eval_results": {"predictions": ["a phylum", "The Walther P38", "the Commanding General", "the Veneto region of Northern Italy", "various deities, beings, and heroes", "a game setting", "Wandsworth, London", "1967", "Smithsonian", "capital crimes", "around 8000 BC", "Smoothie King Center", "Nan Britton", "1977", "9\u201310 March 1945", "Currer Bell", "Jim Davis", "Ted Bundy", "Premier League club Manchester United", "Parlophone", "McDowell County, West Virginia", "Life Is a Minestrone", "Louis Silvie \"Louie\" Zamperini", "Premier League club Stoke City", "New York Shakespeare Festival", "peat moss", "Accolade Wines", "a multi-touch mouse", "Lord Byron", "Singapore", "1853", "Big Bad Wolf", "1:00 a.m. Eastern Time Zone", "Mickey Mouse cup", "Manchester\u2013Boston Regional Airport", "Christopher Francis \"Frank\" Ocean", "Gangsta's Paradise", "is a Dutch footballer who plays for Turkish club Be\u015fikta\u015f.", "in a casino game", "331", "Kristina Ceyton and Kristian Moliere", "1835", "Helsinki", "Imelda Marcos", "Carrefour", "2014", "\"Empire Falls\"", "Wet 'n Wild", "Victoria", "The iPhone 5", "526 people per square mile", "the Caucasus region", "on the idea of laying out a tournament ladder by arranging slips of paper with the names of players on them the way seeds or seedlings are arranged in a garden : smaller plants up front, larger ones behind", "the Charlotte Hornets", "a burrow", "Billie Holiday", "Cumberland", "low-calorie", "American Civil Liberties Union", "three of the bombers", "The Grasshopper and the Ants", "The Amazing Spider-Man", "Cynthia Nixon", "France"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6087759462759462}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, false, false, false, true, false, true, false, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7142857142857143, 0.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.4, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.5, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4534", "mrqa_hotpotqa-validation-4576", "mrqa_hotpotqa-validation-5305", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-5279", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-5055", "mrqa_hotpotqa-validation-5563", "mrqa_hotpotqa-validation-3581", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-1606", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2474", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-614", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-4653", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-423", "mrqa_newsqa-validation-4076", "mrqa_newsqa-validation-891", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-3295"], "SR": 0.484375, "CSR": 0.5257601351351351, "EFR": 0.9696969696969697, "Overall": 0.710028920966421}, {"timecode": 37, "before_eval_results": {"predictions": ["castles and vineyards", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "2000", "lack of a cause of death and the absence of any soft tissue on the toddler's skeletal remains", "never pay with a credit card", "(Chadian President Idriss Deby hopes the journalists and the flight crew will be freed", "walked into the Central Methodist Church in downtown Johannesburg and joined a long queue of people waiting for shelter and food.", "President Paul Biya,", "\"Mad Men's\" Don Draper and his blatant sexual overtures to female employees is mostly a thing of the past.", "Musharraf", "offered money or other discreet aid for the effort if it could be made available,", "News of the World tabloid.", "Argentina lays claim not just to the islands, but to any resources that could be found there.", "prostate cancer", "July 23.", "Roqaya al-Sadat,", "debris", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "software magnate", "150 passengers", "Bialek", "bribing other wrestlers to lose bouts,", "did not speak to those who had gathered but shadow-boxed to spectators and cameras before meeting his distant relatives.\"", "The Delta Queen will go out of service if Congress does not grant the ship another exemption from a 1960s federal law,", "state's attorney", "photos", "\"Twilight\" book series", "\"The First Stop Resource Center assists veterans and their families through various periods of crises, including homelessness and addiction.", "cities throughout Canada", "\"made a brutal choice to step up attacks against innocent civilians.\"", "The restaurant will be serving its fast burgers in the Carrousel du Louvre", "writing and starring in 'The Prisoner'", "Friday", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "Pakistan's combustible Swat Valley", "Former Beatles", "Wackenhut Security Inc.", "Asashoryu", "the southern city of Naples", "last summer.", "Mesut Oezil scored his first goal of the season to put Bremen 2-1 ahead on 29 minutes.", "revelry", "London Heathrow's Terminal 5", "18", "two-day, two-city charm offensive", "Madrid's Barajas International Airport", "had publicly criticized his father's parenting skills.", "$250,000", "at a construction site in the heart of Los Angeles.", "$3 billion", "Croatia playmaker Luka Modric.", "7 July", "Bob Dylan", "The London terminus is St Pancras International", "a casement window", "Denver", "Brazil", "Phelan Beale", "sandstone", "1967", "Joe DiMaggio", "New York Presbyterian Hospital", "tea", "David Lodge"], "metric_results": {"EM": 0.34375, "QA-F1": 0.48306312656641603}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5263157894736842, 0.05714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 0.13333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.1904761904761905, 0.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.7499999999999999, 0.8421052631578948, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 0.9333333333333333, 1.0, 0.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-998", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-925", "mrqa_newsqa-validation-2653", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-848", "mrqa_newsqa-validation-3868", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-241", "mrqa_newsqa-validation-1776", "mrqa_newsqa-validation-2584", "mrqa_newsqa-validation-1123", "mrqa_newsqa-validation-3319", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-62", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-499", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2938", "mrqa_newsqa-validation-1128", "mrqa_newsqa-validation-3134", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1516", "mrqa_newsqa-validation-316", "mrqa_naturalquestions-validation-5457", "mrqa_naturalquestions-validation-3711", "mrqa_triviaqa-validation-2027", "mrqa_hotpotqa-validation-3250", "mrqa_searchqa-validation-11671", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-15555"], "SR": 0.34375, "CSR": 0.520970394736842, "EFR": 1.0, "Overall": 0.7151315789473685}, {"timecode": 38, "before_eval_results": {"predictions": ["vows of celibacy on Biblical grounds,", "Lars von Trier", "37", "cancerous tumor.", "her home", "is the U.N. nuclear watchdog agency's strongest warning yet that Iran could be aiming to build a nuclear bomb.", "\"To My Mother\"", "84-year-old", "free laundry service.", "Oprah Winfrey, Michael Jordan, Robert De Niro, Janet Jackson and the Duchess of York", "opium", "the situation of America wielding a big stick for the last eight years.\"", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Galveston, Texas,", "six", "Carl and Ellie", "581 points", "make life a little easier", "Yellowstone National Park", "building bombs,", "forgery and flying without a valid license,", "2050,", "Eintracht Frankfurt", "Juan Martin Del Potro.", "50", "because its facilities are full.", "engaged in \"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "Akshay Kumar", "133 people", "more than 100", "The Ski Train", "a man's lifeless, naked body", "Flint, Michigan.", "O2 Arena.", "Philippines", "$500,000", "work rule issues.", "not be allowed", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "Six members of Zoe's Ark", "second time", "noose incident occurred two weeks after Black History Month", "Congress", "Sen. Joe Lieberman,", "Marcell Jansen", "Gov. Bobby Jindal", "one bomber.", "Lilla Torg.", "\"Hairspray,\"", "Nechirvan Barzani,", "a certified question or proposition of law from one of the United States Courts of Appeals", "New Spain", "The Soviet Union, too, had been heavily affected", "jewelled Easter eggs", "an ancient supercontinent", "motorway", "ethereal", "Guinness World Records", "Citizens for a Sound Economy", "bees", "King of Ruritania", "Jean Lafitte", "a penalty kick"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4871239571027119}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false], "QA-F1": [0.2222222222222222, 0.0, 1.0, 1.0, 0.8, 0.25, 0.2222222222222222, 1.0, 0.4, 0.8181818181818181, 1.0, 0.0, 0.1212121212121212, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.2857142857142857, 0.4, 1.0, 1.0, 1.0, 0.0, 0.8524590163934427, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11764705882352941, 0.5, 0.26666666666666666, 0.4, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2757", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-3052", "mrqa_newsqa-validation-971", "mrqa_newsqa-validation-3833", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-4107", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-1639", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-2415", "mrqa_newsqa-validation-3651", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2638", "mrqa_newsqa-validation-394", "mrqa_newsqa-validation-2328", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-3006", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8623", "mrqa_naturalquestions-validation-4860", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-1849", "mrqa_hotpotqa-validation-4020", "mrqa_searchqa-validation-3701", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-14613"], "SR": 0.359375, "CSR": 0.5168269230769231, "EFR": 1.0, "Overall": 0.7143028846153847}, {"timecode": 39, "before_eval_results": {"predictions": ["brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait.", "Western New York and Central New York", "on the bladder", "Andy Serkis", "Martin Lawrence", "Malloy as Pierre, Phillipa Soo as Natasha, Lucas Steele as Anatole", "Sedimentary rocks", "in all land - living organisms, both alive and dead, as well as carbon stored in soils", "John Young", "semi-autonomous organisational units", "W. Edwards Deming", "Tom Tucker", "Cyndi Grecco", "any unfavourable and unintended sign", "orbit", "Christine McVie", "the governor of West Virginia", "December 12, 2017", "in South Africa", "1832", "Curtis Armstrong", "fibrous tissue", "St Pancras International", "Jerry Leiber and Mike Stoller", "De Wayne Warren", "The Raiders", "Jesse Wesley Williams", "Peter Cetera", "276", "about 6 : 00 p.m.", "Book of Exodus", "April 25 -- 30", "Kristy Swanson", "2001", "1994", "Washington", "Triple Alliance of Germany, Austria - Hungary, and Italy", "Australia", "Wisconsin", "one of the nine Primal cuts of beef", "Lyndon B. Johnson", "David Tennant", "Ben Findon, Mike Myers and Bob Puzey", "Charles Path\u00e9", "Barbara Windsor", "1857", "1997", "eleven", "Mel Gibson", "boy", "works in a bridal shop", "London", "The Wrestling Classic", "Airbus and Boeing", "Mike Fiers", "Rockland", "Delilah Rene", "three empty vodka bottles", "inferior,", "Frank Ricci", "Zanzibar", "Mao Zedong", "The first Telephone", "Drew Kesse,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5466658889356257}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, false, true, false, true, false, true, false, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false], "QA-F1": [0.07142857142857142, 1.0, 0.0, 1.0, 1.0, 0.3076923076923077, 0.5, 0.21052631578947367, 1.0, 0.5, 1.0, 0.0, 1.0, 0.30303030303030304, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.22222222222222224, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7272727272727273, 0.0, 0.0, 0.0, 0.4, 1.0, 0.5, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9513", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8474", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-10523", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-3373", "mrqa_naturalquestions-validation-2588", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-6074", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-3624", "mrqa_triviaqa-validation-2365", "mrqa_hotpotqa-validation-2210", "mrqa_hotpotqa-validation-1952", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3661", "mrqa_searchqa-validation-12993", "mrqa_newsqa-validation-3331"], "SR": 0.453125, "CSR": 0.515234375, "EFR": 0.9142857142857143, "Overall": 0.6968415178571428}, {"timecode": 40, "UKR": 0.740234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1840", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3310", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-3942", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4266", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4443", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4526", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4729", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4944", "mrqa_hotpotqa-validation-5034", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5180", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5424", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5590", "mrqa_hotpotqa-validation-5604", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5671", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-854", "mrqa_hotpotqa-validation-866", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2085", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3145", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-3525", "mrqa_naturalquestions-validation-3651", "mrqa_naturalquestions-validation-370", "mrqa_naturalquestions-validation-3711", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4658", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-4951", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6597", "mrqa_naturalquestions-validation-6665", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6864", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7574", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-7896", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1146", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2108", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2280", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3007", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3182", "mrqa_newsqa-validation-3200", "mrqa_newsqa-validation-3255", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3960", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-484", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-607", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-872", "mrqa_newsqa-validation-916", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-1177", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13156", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14978", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15004", "mrqa_searchqa-validation-15504", "mrqa_searchqa-validation-15505", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15872", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16246", "mrqa_searchqa-validation-16353", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1788", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-3398", "mrqa_searchqa-validation-41", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-4764", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6338", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8236", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8366", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8791", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1498", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2167", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2473", "mrqa_squad-validation-2640", "mrqa_squad-validation-2672", "mrqa_squad-validation-270", "mrqa_squad-validation-2757", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3407", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3774", "mrqa_squad-validation-3786", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-3939", "mrqa_squad-validation-4010", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4484", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4799", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5019", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5634", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6318", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6594", "mrqa_squad-validation-6630", "mrqa_squad-validation-6981", "mrqa_squad-validation-7023", "mrqa_squad-validation-7168", "mrqa_squad-validation-7194", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7294", "mrqa_squad-validation-7466", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7907", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8066", "mrqa_squad-validation-8127", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8399", "mrqa_squad-validation-8488", "mrqa_squad-validation-8501", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8901", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9855", "mrqa_squad-validation-9868", "mrqa_squad-validation-9901", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1052", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1310", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1427", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-1921", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2311", "mrqa_triviaqa-validation-2365", "mrqa_triviaqa-validation-2889", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4112", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4357", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4382", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-60", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6249", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6418", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6681", "mrqa_triviaqa-validation-6704", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-7724", "mrqa_triviaqa-validation-7757", "mrqa_triviaqa-validation-895"], "OKR": 0.83203125, "KG": 0.47890625, "before_eval_results": {"predictions": ["Turkana", "Action Jackson", "Ben Willis", "Phillip Schofield and Christine Bleakley", "The highway connects most of the major cities of the East Coast", "The Third Five - year Plan", "Hermann Ebbinghaus", "Ronald Reagan", "Baltimore, Maryland", "A firm, flexible cup - shaped device worn inside the vagina to collect menstrual flow", "April 1st", "around the time when ARPANET was interlinked with NSFNET in the late 1980s", "Lesley Gore", "Andrew Lincoln", "Joe Spano", "the image becomes virtual", "political ideology", "Matt Monro", "Wednesday, 5 September 1666", "the five - year time jump for her brother's wedding to Serena van der Woodsen", "response to a perceived harmful event, attack, or threat to survival", "President pro tempore of the Senate", "multinational retail corporation", "July 2, 1776", "formal legal instruments called promissory notes", "after the title page, copyright notices, and, in technical journals, the abstract", "2018 and 2019", "Fred Ott", "Tevin Campbell", "September 24, 2017", "The oldest tradition is to use dyed and painted chicken eggs, but a modern custom is to substitute chocolate eggs wrapped in colourful foil, hand - carved wooden eggs, or plastic eggs filled with confectionery such as chocolate", "Anna Murphy", "a mountainous, peninsular mainland", "`` new version ''", "Santo Domingo", "1980", "Brad Johnson", "between the North Atlantic and northern Indian Oceans", "Montreal", "2003", "May 30, 2017", "A turlough", "Hathi Jr", "The flag of the United States of America", "The euro", "local authorities", "`` Two Fathers ''", "2017 / 18 Divisional Round game against the New Orleans Saints", "Nick Birch", "Phillip Paley", "1,350", "Zach Johnson", "The Jetsons", "Venezuela", "Point coloration", "Taipei City", "China", "Sunday,", "Elizabeth Birnbaum", "Karen Floyd", "Michelob", "burritos", "8 Simple Rules", "Australian"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5892230731225296}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6060606060606061, 0.5, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.1739130434782609, 1.0, 1.0, 0.0, 0.3636363636363636, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-5876", "mrqa_naturalquestions-validation-2387", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-3432", "mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-1611", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-46", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-7235", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-1178", "mrqa_naturalquestions-validation-9963", "mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-1378", "mrqa_naturalquestions-validation-6794", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-5976", "mrqa_triviaqa-validation-336", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-306", "mrqa_searchqa-validation-15726", "mrqa_hotpotqa-validation-1076"], "SR": 0.53125, "CSR": 0.515625, "EFR": 1.0, "Overall": 0.713359375}, {"timecode": 41, "before_eval_results": {"predictions": ["many middle eastern scientists", "1889", "sovereignty over some or all of the current territory of the U.S. state of Texas", "the Islamic Community", "2018", "Anna Muttathupadathu", "Peggy Lipton", "in the basic curriculum -- the enkuklios paideia or `` education in a circle ''", "air moisture", "the right of the dinner plate", "Yondu Udonta", "Terry Kath", "1830s", "November 5, 2017", "the electric potential generated by muscle cells when these cells are electrically or neurologically activated", "\u20b9 39.50 lakh", "one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century.", "supervillains", "a group of seemingly unconnected people in Atlanta come to terms with the relationships they have with their mothers", "16 May 2007", "Australia", "Bed and breakfast", "Masha Skorobogatov", "an empty line", "Jack Gleeson", "clockwise rotation", "October 6, 2017", "Joudeh Al - Goudia family", "160km / hour", "Gram - negative, facultatively anaerobic, rod - shaped, coliform", "Roman Reigns", "Erica Carroll", "Audrey II", "acid rain", "Kaley Christine Cuoco", "mathematical modeling and statistical estimation or statistical inference", "Jules Shear", "Scott Schwartz", "1952", "Randy VanWarmer", "April 3, 1973", "the actress'audition for the role", "computer science and artificial intelligence", "Detroit Tigers", "Gustav Bauer", "9.1", "Bruno Mars", "the Mediterranean Sea to the north", "Lord's", "the history of the island", "Honor\u00e9 Mirabeau", "Daniel Defoe", "1961", "birmingham", "First Street", "Steve Biko", "Princes Park", "cancerous tumor.", "Kyra and Violet,", "\"Empire of the Sun,\"", "Bulldog", "Petsmart", "Carrie Underwood", "1970"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5249508079747051}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true, true, false, true, false, true, true, false, true, true, true, true, true], "QA-F1": [1.0, 0.5, 0.787878787878788, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666665, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.6666666666666666, 0.56, 0.25, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444445, 0.5714285714285715, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1953", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-1552", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-8000", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-886", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-2621", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2229", "mrqa_naturalquestions-validation-1455", "mrqa_triviaqa-validation-7151", "mrqa_hotpotqa-validation-5038", "mrqa_newsqa-validation-3380"], "SR": 0.421875, "CSR": 0.5133928571428572, "EFR": 0.9459459459459459, "Overall": 0.7021021356177607}, {"timecode": 42, "before_eval_results": {"predictions": ["they were at least partly the product of a declining state of mind", "1861\u20131865", "private Ivy League research university", "Minnesota Timberwolves", "703", "December 1974", "Sufism", "Capellini", "1614", "international association football competitions such as the FIFA World Cup, AFC Asian Cup and East Asian Football Championship", "German and American composer and music teacher", "age thirteen", "McComb, Mississippi", "Santa Fe", "Clarence Nash", "Kew Gardens", "the northeastern part", "American Horror Story", "Chicago, Illinois", "Free Range Films", "The Deep Blue Sea", "Vernon Smith", "Australian actor", "Frank Fertitta, Jr. Station Casinos", "from September 30, 2011 to October 16, 2015", "the Battelle Energy Alliance", "athlete Colin Colin", "I Write What I Like", "October 2016", "Sid Vicious", "51,271", "Sun Valley, Idaho", "\"Guardians of the Galaxy Vol.  2\"", "American burlesque", "Pulitzer Prize for drama in 2002", "Scott Paul Carson", "IFFHS World's Best Goalkeeper", "Suspiria", "an English singer, songwriter, actress, and radio and television presenter", "Michelle Anne Sinclair", "Russell T Davies", "\"Creed\"", "his involvement in spot-fixing", "Erich Schmidt-Leichner", "\"as-Sindib\u0101du al- Ba\u1e25riyy\"", "Alfred in \"Die Fledermaus\"", "Movie Masters", "6,241", "Hennepin County", "Australian coast", "Rymill Park", "the 1967 film Cool Hand Luke", "Michael Buffer", "Presley Smith", "AFC Wimbledon", "Erewhon", "Sushi and or Sashimi", "a share in the royalties for the tune.", "In January 2003,", "\"The Real Housewives of Atlanta\"", "Antnio Guterres", "ratify the Constitution of the United States", "n", "Dairy Queen"], "metric_results": {"EM": 0.46875, "QA-F1": 0.6017282196969697}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, true], "QA-F1": [0.5333333333333333, 0.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.7499999999999999, 0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.2, 0.1818181818181818, 1.0, 1.0, 1.0, 0.0, 0.4, 0.7499999999999999, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2523", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-3352", "mrqa_hotpotqa-validation-4614", "mrqa_hotpotqa-validation-4296", "mrqa_hotpotqa-validation-1461", "mrqa_hotpotqa-validation-973", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-682", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-742", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5385", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-2599", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-4201", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-2239", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-5125", "mrqa_naturalquestions-validation-5293", "mrqa_triviaqa-validation-3569", "mrqa_triviaqa-validation-748", "mrqa_newsqa-validation-2151", "mrqa_newsqa-validation-1125", "mrqa_searchqa-validation-3012", "mrqa_searchqa-validation-732", "mrqa_searchqa-validation-11496"], "SR": 0.46875, "CSR": 0.5123546511627908, "EFR": 1.0, "Overall": 0.7127053052325582}, {"timecode": 43, "before_eval_results": {"predictions": ["25", "Evey", "from 1852 to 1870", "Adelaide", "the George Washington Bridge", "1966", "1996", "Jacking", "February 5, 2015", "The Worm", "MGM Resorts International", "Sam Raimi", "StubHub Center", "2001", "balloon Street, Manchester", "Liverpool Bay", "St. Louis, Missouri", "Stephen King", "\"Seducing Mr. Perfect\"", "British Labour Party", "The Five", "Ang Lee", "Taylor Swift", "Objectivism", "\"Traumnovelle\"", "Mandarin", "the lead female role of London Tipton", "KlingStubbins", "the Goddess of Pop", "Chevron Corporation", "Black Panther Party", "Baldwin", "John John Florence", "YouTube", "Kohlberg K Travis Roberts", "Salzkammergut", "\"Big Top Pee-wee\"", "feats of exploration", "video game", "Father Dougal McGuire", "1 million", "\"Smile, Mom\" (2010)", "London", "Subway restaurants", "cancer", "Campbellsville", "Mark Helfrich", "three", "Rickie Lee Skaggs", "Field Marshal Lord Gort", "Owsley Stanley", "Donna Mills", "16 seasons", "gas exchange", "the underground organization of the Irish Republican Brotherhood", "West Ham boss Allardyce", "gin", "almost 9 million", "In fashionable neighborhoods of Tokyo customers are lining up for vitamin injections", "to lose bouts,", "(call)", "The Oxford University Dramatic Society", "(KEP) John", "Haiti"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6955729166666667}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.25, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.8, 0.0, 0.3333333333333333, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5487", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-5136", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-2770", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4257", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-4239", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3856", "mrqa_hotpotqa-validation-4501", "mrqa_hotpotqa-validation-662", "mrqa_hotpotqa-validation-992", "mrqa_naturalquestions-validation-8767", "mrqa_triviaqa-validation-2810", "mrqa_triviaqa-validation-5974", "mrqa_newsqa-validation-3325", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-14771"], "SR": 0.578125, "CSR": 0.5138494318181819, "EFR": 1.0, "Overall": 0.7130042613636364}, {"timecode": 44, "before_eval_results": {"predictions": ["a broken arm", "Mexico", "Missouri", "Franklin D. Roosevelt", "zero", "Copenhagen", "the Pulitzer Prize", "gymnastics", "New Zealand", "\"Follies\"", "The Honeymooners", "samuel butler", "enamel", "J.L. Hudson's", "method acting", "sam Kinison", "to ascend", "Roman Empire", "Alaska", "Matt Leinart", "the great khan", "Jeremy Bentham", "the United States", "the Mekong", "King Neptune", "the Olympic Games", "Dan Morrison", "sam samuel karnick", "a gas", "the Danforth Foundation", "Ivory Coast", "Lord of the Rings: The Return of the King", "Birch", "Alanis Morissette", "donkeys", "samuel butler", "King Minos", "Sulman Seti", "Schindler\\'s List", "Jonathan Rhys", "Stephen Crane", "Mississippi", "ankle", "the Kansas Collection", "the return of the", "Steely Dan", "Linda Tripp", "the Sierra Nevada", "samluk sultans Baibars", "adios", "the Gadsden Treaty", "New Guinea", "Roman Reigns", "the empire begins to enter decline and instability", "the Marshall Plan", "Dick Turpin", "black", "15", "Cymbeline", "Mary-Kay Wilmers", "the shipping industry -- responsible for 5% of global greenhouse gas emissions,", "Hundreds", "2008", "three"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5361742424242424}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, true, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, false, true, false, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-7525", "mrqa_searchqa-validation-3120", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-14186", "mrqa_searchqa-validation-6399", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-508", "mrqa_searchqa-validation-12098", "mrqa_searchqa-validation-15476", "mrqa_searchqa-validation-8837", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-3792", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-2669", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-16207", "mrqa_searchqa-validation-540", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-14848", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14539", "mrqa_searchqa-validation-11005", "mrqa_naturalquestions-validation-5995", "mrqa_naturalquestions-validation-6157", "mrqa_hotpotqa-validation-5093", "mrqa_newsqa-validation-3979"], "SR": 0.484375, "CSR": 0.5131944444444445, "EFR": 0.9393939393939394, "Overall": 0.7007520517676767}, {"timecode": 45, "before_eval_results": {"predictions": ["the Lunar Roving Vehicle (LRV)", "an army major assigned to a guard unit protecting Mexican President Felipe Calderon.", "Robert Barnett,", "Too many glass shards", "Sunday.", "\"including taking any and all appropriate personnel actions including termination, discipline and referral of any wrongdoing for criminal prosecution.\"", "farmer Alan Graham", "Islamabad", "he was battling a potentially fatal disease that required a life-saving lung transplant,", "prosecutors of buckling under pressure from the ruling party.", "Oregon Fire Lines", "Siemionow", "12 hours", "the United States", "the Obama and McCain camps", "$273 million", "Wigan Athletic", "it", "at least 25", "former U.S. secretary of state", "helped finance the insurgency against U.S. troops in Iraq with Iraqi funds he transferred to Syria before Hussein's government collapsed in April 2002.", "a prisoners at the South Dakota State Penitentiary", "U.S.", "more than 1.2 million", "North Korea", "an ancient Masonic artifact whose clues lead him on a treasure hunt to various D.C. tourist spots as he searches for a secret long hidden by the brotherhood.", "(It is not the world that has to prove that Iran is building a bomb,\"", "at least 12 months.", "Passers-by", "Almost all British troops in Iraq", "a pregnant soldier", "Picasso's muse and mistress, Marie-Therese Walter.", "80", "Polo", "bartering", "the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.\"", "fluoroquinolone", "Tim Cahill", "human rights organizations", "his comments", "Colombia's most sought-after criminals", "a student who admitted to hanging a noose in a campus library,", "to make space for two ocean wind farms -- taking up 2 percent of the state's waters", "South Korea", "a brilliant maiden Test century", "six", "flooding and debris", "then-presidential candidate Barack Obama", "Unseeded Frenchwoman Aravane Rezai", "the radical Islamist militia that controls the city", "201", "IBM", "Gorakhpur", "Chief Inspector of Prisons", "Charlie Cairoli", "Sweden", "2017", "King \u00c6thelred the Unready", "Comme des Gar\u00e7ons", "Theodore Roosevelt", "A high body", "the Shang dynasty", "Germany"], "metric_results": {"EM": 0.375, "QA-F1": 0.491303228021978}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, false, true, false, false, false, true, true, false, true, true, true, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.06666666666666667, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.3076923076923077, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.4, 0.5, 0.1111111111111111, 0.5, 0.0, 0.0, 1.0, 0.3333333333333333, 0.4, 0.0, 0.25, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4027", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-1557", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1848", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-1708", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-1257", "mrqa_newsqa-validation-2518", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-3824", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-2043", "mrqa_newsqa-validation-205", "mrqa_newsqa-validation-876", "mrqa_newsqa-validation-393", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-3232", "mrqa_newsqa-validation-4091", "mrqa_newsqa-validation-1696", "mrqa_newsqa-validation-3258", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-19", "mrqa_naturalquestions-validation-4280", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-1863", "mrqa_hotpotqa-validation-3844", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-11067", "mrqa_searchqa-validation-4478"], "SR": 0.375, "CSR": 0.5101902173913043, "EFR": 1.0, "Overall": 0.7122724184782608}, {"timecode": 46, "before_eval_results": {"predictions": ["Thames River", "an independent homeland for the country's ethnic", "Turkish President Abdullah Gul,", "series.\"", "no reports of ground strikes or interference with aircraft in flight,", "in Fayetteville, North Carolina,", "\"surge\" strategy", "Noriko Savoie", "work together to stabilize Somalia and cooperate in security and military operations.", "in self defense in punching businessman Marcus McGhee.", "series of circumstances.\"", "\"You can go from rags to riches there.", "United States", "Manny Pacquiao", "Garth Brooks", "Ali Bongo,", "super-yacht designers", "a body", "Prince George's County Correctional Center,", "\"The first car [ of the second train] overrode the rear car [of the front train], and much of the survivable space on that first car of the striking train,\"", "Karen Floyd", "opposed the Iraq war and considered Afghanistan the \"good war.\"", "since 1983.", "$20 million to $30 million,", "Daryeel Bulasho Guud", "Zulfikar Ali Bhutto,", "the insurgency,", "Stoke City.", "Former detainees", "environmental videos", "a national telephone survey of more than 78,000 parents", "not doing everything within its power to prevent more people from needlessly suffering disabling tendon ruptures.\"", "his past and his future", "his father,", "travel in cars with tinted windows", "mild to moderate depression", "crafts poems telling of the pain and suffering of children just like her", "Iraqi economy.\"", "Mitt Romney", "\"The Sopranos,\"", "The Louvre", "Jason Chaffetz", "a delegation of American Muslim and Christian leaders", "legitimacy of that race.", "Australian officials", "misdemeanor", "$10 billion", "\"procedure on her heart,\"", "safety issues in the company's cars", "Newcastle", "54", "1 mile ( 1.6 km )", "Abanindranath Tagore CIE", "1996", "Kiri Te Kanawa", "HMS thetis", "1960's", "the onset and progression of Alzheimer's disease", "The Future", "1770", "Jacob and Wilhelm Grimm", "Seasons in the Sun", "Geraldine Farrar", "Michael Harney"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7491875312187812}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, true, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.9166666666666666, 1.0, 0.0, 0.5, 0.15384615384615385, 0.0, 0.0, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.14285714285714285, 1.0, 0.09090909090909091, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5454545454545454, 0.9696969696969697, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-3999", "mrqa_newsqa-validation-158", "mrqa_newsqa-validation-2804", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-1968", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1697", "mrqa_newsqa-validation-1288", "mrqa_newsqa-validation-4171", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-123", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-851", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-2547", "mrqa_newsqa-validation-247", "mrqa_hotpotqa-validation-5485", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-13873", "mrqa_naturalquestions-validation-3802"], "SR": 0.59375, "CSR": 0.511968085106383, "EFR": 0.9615384615384616, "Overall": 0.7049356843289689}, {"timecode": 47, "before_eval_results": {"predictions": ["waffles", "President Obama", "Harry Potter", "a stagecoach", "Cyrillic", "chocolate Pecan pie", "the Charleston", "shiatsu", "\"The Ring of the Nibelungen\"", "the Branches of Biology", "Anne Rice", "Anne Murray", "the lithosphere", "second marriage", "the Nine-Day Queen", "Santeria", "the Duggar family", "the Lincoln cent", "Sydney", "cheerful or optimistic", "Belarus", "Airplane", "French toast", "gingerbread", "Swiss Cheese", "the Monkees", "Samuel Johnson", "Agatha Christie", "Jack Dempsey", "conglomerare", "brood", "Thomas Edison", "Mount Everest", "black- Eyed Pea dip", "La Bohme", "Battlestar Galactica", "Yugoslavia", "the Surgeon General", "a Place Bet", "War and Peace", "Frank Lloyd Wright", "Falcon Crest", "William the Conqueror", "Grant and Sherman", "Adam Smith", "Yeast", "Pearl S. Buck", "the guillitine", "Atlanta", "Ayn Rand", "Sisyphus", "2017 season", "Nathan Hale", "the United Kingdom", "John F. Kennedy", "non-pathogens", "the Gettysburg Civil War", "Sam Raimi", "1940s and 1950s", "Sleepy Brown", "to harm indigenous populations.", "a judge to order the pop star's estate to pay him a monthly allowance,", "ALS6", "Stevie Wonder"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6735294117647058}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, false, true, true, true, false, false, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.7058823529411764, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-15255", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-6492", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-3768", "mrqa_searchqa-validation-482", "mrqa_searchqa-validation-11413", "mrqa_searchqa-validation-8160", "mrqa_searchqa-validation-14195", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10067", "mrqa_searchqa-validation-16434", "mrqa_searchqa-validation-11677", "mrqa_searchqa-validation-6486", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-15591", "mrqa_searchqa-validation-2263", "mrqa_searchqa-validation-15327", "mrqa_naturalquestions-validation-9246", "mrqa_triviaqa-validation-3549", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-6789", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1952"], "SR": 0.5625, "CSR": 0.5130208333333333, "EFR": 1.0, "Overall": 0.7128385416666666}, {"timecode": 48, "before_eval_results": {"predictions": ["Argentine composer Lalo Schifrin", "the mid-1980s", "the 1971 motion picture Willy Wonka & the Chocolate Factory", "annually in late January or early February", "The Turbo Charged Prelude", "John Adams of Massachusetts, Benjamin Franklin of Pennsylvania, Thomas Jefferson of Virginia, Robert R. Livingston of New York, and Roger Sherman of Connecticut", "Ute", "the sixth series", "the summer of 2003", "season two", "in the 1970s", "from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "Phosphorus pentoxide", "the Intercollegiate Socialist Society", "Donald", "Paul and Timothy", "The Geography of Oklahoma", "a burden to be carried as penance", "Michael Phelps", "the British", "22 \u00b0 00 \u2032 N 80 \u00b0 00\u2032 W", "Tarrence Howard", "1986", "1932", "Sophocles", "Sunday evenings", "the New York and New Jersey campaigns", "The Actor's Home", "for the 2009 model year", "The Cylinder", "Saint Alphonsa", "Billie Jean King", "the Battle of Antietam", "C\u03bc and C\u03b4", "Daya", "HTM", "the Gentiles", "Andy", "a wide surrounding area, in the Georgia counties of Newton ( where Covington is located ), Rockdale, Walton, Morgan, and Jasper", "pools campaign contributions from members and donates those funds to campaign for or against candidates, ballot initiatives, or legislation", "the majority of members present at that time approved the bill either by voting or voice vote", "eight", "Julie Deborah Kavner", "September 19, 2017", "May 2002", "Chelsea", "San Antonio, Texas", "Carl Hampton and Raymond Jackson", "Omar Khayyam", "1984", "Senator Joseph McCarthy's Senate Permanent subcommittee on Investigations, an investigation known as the Army -- McCarthy hearings", "Tarzan", "Puck", "Harrods", "Adolfo", "Tool", "seven", "a satellite.", "The son of Gabon's former president", "the neck", "The Sisters Rosensweig", "The Chinese Orphan", "Vatican", "Timothy Dalton"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5296590539126336}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 0.0, 0.4444444444444444, 0.0, 0.16, 0.19999999999999998, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.8571428571428571, 0.6666666666666666, 0.0, 0.8421052631578948, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.8666666666666666, 1.0, 0.6363636363636364, 1.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 0.0, 1.0, 1.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-156", "mrqa_naturalquestions-validation-9774", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-8441", "mrqa_naturalquestions-validation-9564", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-74", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-5396", "mrqa_naturalquestions-validation-3122", "mrqa_naturalquestions-validation-8478", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7211", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-923", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-3633", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-3436", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-124", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-3591", "mrqa_naturalquestions-validation-9999", "mrqa_naturalquestions-validation-1449", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-9160", "mrqa_triviaqa-validation-6730", "mrqa_hotpotqa-validation-4233", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-3923", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4956"], "SR": 0.359375, "CSR": 0.5098852040816326, "EFR": 0.975609756097561, "Overall": 0.7073333670358387}, {"timecode": 49, "before_eval_results": {"predictions": ["Louisa May Alcott", "Davy Crockett", "Sugar Ray", "Suspicious Minds", "Jupiter", "fmina", "the Wise Men", "Ladies Professional Golf Association", "Henry VIII", "corpulent", "fictional Women Painters", "Copacabana", "Krakow", "Daredevil", "Algiers", "emerald", "La Casara Roncolato", "Mauna Loa", "John Lennon", "Macy\\'s", "Sonny Corleone", "Fred Claus", "the Philosopher's Stone", "the Kremlin", "\"The Raven\"", "the New York Cosmos", "Richard Feynman", "John Grisham", "Positron emission tomography", "Catherine the Great", "Eisenhower", "Ellen Wilson", "rheumatoid arthritis", "The National Gallery of Art", "Crispix", "Gabriela Sabatini", "St. Louis", "Imperfect", "the Caspian tern", "the Beagle", "the Autry Museum of the American West", "the Wing-T", "Luzon", "Henry Hudson", "a diamond", "Roger Brooke Taney", "the United Nations", "the Teapot Dome scandal", "The Unbearable Lightness of Being", "the Appian Way", "Joseph", "`` Nearer, My God, to Thee ''", "marriage officiant, solemniser, or `` vow master ''", "1 October 2006", "Nupedia", "Kenny Everett", "Matthew Bellamy", "Jack White", "Isabella II", "villanelle", "Revolutionary Armed Forces of Colombia", "Oaxacan countryside of southern Mexico", "$5.5 billion", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\""], "metric_results": {"EM": 0.546875, "QA-F1": 0.6513821248196248}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.9090909090909091, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6521", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-9168", "mrqa_searchqa-validation-14716", "mrqa_searchqa-validation-9072", "mrqa_searchqa-validation-8091", "mrqa_searchqa-validation-1620", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-4798", "mrqa_searchqa-validation-2632", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-12896", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-15908", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-1953", "mrqa_searchqa-validation-9558", "mrqa_searchqa-validation-4582", "mrqa_searchqa-validation-683", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-2231", "mrqa_naturalquestions-validation-8217", "mrqa_triviaqa-validation-2643", "mrqa_triviaqa-validation-1277", "mrqa_hotpotqa-validation-3245", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3761"], "SR": 0.546875, "CSR": 0.510625, "EFR": 1.0, "Overall": 0.712359375}, {"timecode": 50, "UKR": 0.671875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2458", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2668", "mrqa_hotpotqa-validation-273", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2766", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-2941", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3198", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3468", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3835", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4659", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4744", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-805", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10551", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2127", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3027", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3757", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-6830", "mrqa_naturalquestions-validation-6875", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8427", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9098", "mrqa_naturalquestions-validation-9160", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9494", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1656", "mrqa_newsqa-validation-1660", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-2083", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2467", "mrqa_newsqa-validation-2470", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3676", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-413", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-10981", "mrqa_searchqa-validation-11232", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12474", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13740", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14449", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-15203", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4567", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6765", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7287", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8176", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8630", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-8995", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_searchqa-validation-9803", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1330", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1586", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-1920", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-2640", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3770", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4671", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-5751", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8253", "mrqa_squad-validation-8386", "mrqa_squad-validation-8488", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-8948", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9819", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-1291", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-30", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-410", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-4949", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-694", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-7601", "mrqa_triviaqa-validation-7683", "mrqa_triviaqa-validation-895"], "OKR": 0.791015625, "KG": 0.44921875, "before_eval_results": {"predictions": ["Nixon offered money or other discreet aid for the effort if it could be made available, the document shows.\"We must try and prevent new Allendes and Castros, and try where possible to reverse these trends,\"", "recall notices", "American", "helicopters and unmanned aerial vehicles from the White House to patrol the border region with Mexico.", "2-1", "President Obama", "Adam Lambert and Kris Allen", "Arabic, French and English", "Saturday", "reptiles.", "a skilled hacker could disrupt the system and cause a blackout.", "a Starbucks this summer.", "a violent government crackdown seeped out.\"", "the former Massachusetts governor", "my parents", "telling CNN his comments had been taken out of context. He said the murder of the boss could never be justified.", "her fianc\u00e9,", "\"We tortured (Mohammed al ) Qahtani,\"", "a woman who accuse him of sexual harassment", "Haiti.", "228", "to Nazi Germany", "say these planning processes are urgently needed and have been a long time in coming.One reason it's taken so long is that people can't see that the oceans are filling up,", "black is beautiful,\"", "Sri Lanka", "2009", "President Obama", "At least 38", "abuses, including the massacre of innocent civilians, and sentenced him to 25 years in prison", "Rod Blagojevich,", "10 years", "Dr. Maria Siemionow,", "Tom Hanks", "her boyfriend, Dodi Fayed, and their driver, Henri Paul.", "flying", "14", "Britain.", "Dogpatch Labs Europe", "two", "is pretty much resolved,\"", "16", "the idea that the Richmond students did nothing because of the \"bystander effect\":", "Tom Hanks", "\"a potential hazard may occur due to re-entry of satellite debris into the earth's atmosphere.\"", "boyhood experience in a World War II internment camp", "the United States", "2002", "Khaled El Islambouly, the lead gunmen,", "At least 15", "three", "London", "Taiwan", "the inner core", "Turducken", "Bonnie and Clyde", "'reactive to far ambush'", "coffee", "wine and cellar door region", "Belgian", "Guthred", "soothsayer", "Campbell\\'s", "Lotte New York Palace (New York)", "Fringillidae"], "metric_results": {"EM": 0.40625, "QA-F1": 0.54135646289018}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, true, false, false, false, false, false, false, false, false, true, false, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false], "QA-F1": [0.625, 0.0, 1.0, 0.5263157894736842, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.5, 0.0, 0.0, 0.5, 0.1904761904761905, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.09090909090909091, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.5, 0.8, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.9600000000000001, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3870", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1445", "mrqa_newsqa-validation-3909", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-552", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-3210", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-4161", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1553", "mrqa_newsqa-validation-1725", "mrqa_newsqa-validation-2561", "mrqa_newsqa-validation-2763", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-3629", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-172", "mrqa_newsqa-validation-2958", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-565", "mrqa_newsqa-validation-3994", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-914", "mrqa_naturalquestions-validation-10209", "mrqa_triviaqa-validation-1242", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-471", "mrqa_searchqa-validation-7167", "mrqa_searchqa-validation-15063", "mrqa_triviaqa-validation-4377"], "SR": 0.40625, "CSR": 0.508578431372549, "EFR": 1.0, "Overall": 0.6841375612745099}, {"timecode": 51, "before_eval_results": {"predictions": ["Thursday night,", "a judge to order the pop star's estate", "\"Red Lines,\"", "tax credits to companies hiring jobless veterans", "Herman Thomas", "growing in coming decades.", "Raymond Soeoth of Indonesia and Amadou Diouf of Senegal in West Africa,", "2,000 euros ($2,963)", "Christopher Savoie", "learn in safer surroundings.", "eight-week", "\"Oprah is an angel, she is God-sent,\"", "D.J. Knight of Pearlman, Texas,", "Grayback Forestry in Medford, Oregon,", "January", "Anjuna beach in Goa", "A member of the group dubbed the \"Jena 6\"", "the IV cafe.", "Venus Williams", "two", "Monday", "Seoul", "\"Itsy Bitsy Teeny Weeny Yellow Polka Dot Bikini.\"", "collaborating with the Colombian government,", "trying to cover up the truth behind her daughter's murder,", "countering the spread of the drug trade,", "to a hospital in Amstetten,", "Pakistan's", "Charles Lock", "\"This is not something that anybody can reasonably anticipate,\"", "Somali-based", "gun charges,", "\"17 Again,\"", "more than 20 times during the 1992 campaign.", "bipartisan", "helping on the sandbags", "to sniff out cell phones.", "2,000", "41,", "a \"happy ending\"", "New York Philharmonic Orchestra in North Korea to Dharamsala, India.", "Liza Murphy,", "Arsene Wenger", "four months ago,", "India", "that Birnbaum had resigned \"on her own terms and own volition.\"", "hundreds", "photos", "the ranch that's home to members of a polygamist sect,", "raping and killing a 14-year-old Iraqi girl.", "from Spain to the Caribbean", "`` E-mail surveillance ''", "Warren Hastings", "Bob Dylan, George Harrison, Jeff Lynne, Roy Orbison, and Tom Petty", "Bactrian", "faggot", "green", "a creek", "2007", "the zona glomerulosa of the adrenals cortex in the adrenal gland", "Think Big", "vice presidential running mate", "Animal Crackers", "the Ogaden"], "metric_results": {"EM": 0.53125, "QA-F1": 0.647153063949939}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.3333333333333333, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.7692307692307693, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.6250000000000001, 0.37499999999999994, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.6666666666666666, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3076923076923077, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.4, 1.0, 0.3333333333333333, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3224", "mrqa_newsqa-validation-1545", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-1188", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3326", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-2985", "mrqa_newsqa-validation-2194", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3529", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-464", "mrqa_newsqa-validation-784", "mrqa_newsqa-validation-475", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-6874", "mrqa_hotpotqa-validation-2609", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-397", "mrqa_searchqa-validation-7781"], "SR": 0.53125, "CSR": 0.5090144230769231, "EFR": 0.9666666666666667, "Overall": 0.6775580929487179}, {"timecode": 52, "before_eval_results": {"predictions": ["Pope", "mild to moderate depression", "Summer", "a goal of starting to withdraw forces from the country in July 2011.\"", "VBS.TV", "children of street cleaners and firefighters.", "Switzerland", "2009", "Tim Clark, Matt Kuchar and Bubba Watson", "Araceli Valencia,", "400 farmers", "Friday,", "Lisa Brown and her family", "President Robert Mugabe", "two Israeli soldiers,", "attack two Shiite mosques, police stations, and a Norwegian telecommunications company in Punjab,", "Teen Patti", "Seasons of My Heart", "an angry mob.", "improve the environment by taking on greenhouse gas emissions.", "Jared Polis", "dancing against a stripper's pole.", "bragging about his sex life on television", "\"Three Little Beers,\"", "12.3 million", "almost 30 tunnels,", "English", "Arkansas", "Barbara Dainton-West,", "a pure meritocracy,", "American Bill Haas", "the war of words in the Republican Party centered around Rush", "Tuesday in Los Angeles.", "after giving birth to baby daughter Jada,", "co-writing credits", "speed sailing", "Jeffrey Jamaleldine", "2.5 million", "the state's first lady,", "Somali President Sheikh Sharif Sheikh Ahmed", "NATO fighters", "U.S. Vice President Dick Cheney", "5:20 p.m.", "Larry King", "\"The Rosie Show,\"", "a remote highway in Michoacan state,", "helicopters and unmanned aerial vehicles", "a \"prostitute\"", "Hanin Zoabi,", "two", "April 2010.", "1985", "1599", "`` Seeing Red ''", "a shoji door", "PPTH", "helps managers understand employees' needs", "the Dutch Empire", "24 December 1692", "Cushman", "the bobtail squid", "imperative", "the Whitewater Development Corporation", "Drums"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6901792241019215}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, true, true, true, true, true, true, false, true, false, false, false, true, false, true, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.1904761904761905, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8421052631578948, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2413", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-1136", "mrqa_newsqa-validation-1407", "mrqa_newsqa-validation-1096", "mrqa_newsqa-validation-3015", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2743", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-2152", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3375", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2821", "mrqa_naturalquestions-validation-6545", "mrqa_triviaqa-validation-934", "mrqa_triviaqa-validation-2801", "mrqa_triviaqa-validation-2536", "mrqa_hotpotqa-validation-5395", "mrqa_searchqa-validation-14214", "mrqa_searchqa-validation-9278", "mrqa_searchqa-validation-4725"], "SR": 0.578125, "CSR": 0.5103183962264151, "EFR": 1.0, "Overall": 0.684485554245283}, {"timecode": 53, "before_eval_results": {"predictions": ["fire an anvil into the air with gunpowder", "Abigail", "Encore", "Rod Smith", "9 November 1955", "Japan", "Adrian Peter McLaren", "Anthony John Herrera", "2008\u201309", "Ion Nunweiller", "Wayne Rooney", "Mot\u00f6rhead", "Solace", "1943", "Interscope Records", "Wings of Desire", "May 27, 2016", "Super Bowl XXIX", "Kris Kristofferson", "Hudson Bay Mining and Smelting Company", "Bourbon County", "100 metres", "\"Northeast Regional\"", "5249", "Bob Hope", "Harry Booth", "John McClane", "James Packer", "black nationalism", "USC Marshall School of Business", "Saint Motel", "\"You're Next\"", "its eclectic mix of musical styles incorporating elements of disco, pop, reggae, and early rap music", "Christian", "Fife", "Peter 'Drago' Sell", "Marktown", "Hidden America with Jonah Ray", "Bangkok", "Gregg Popovich", "Roy Spencer", "Chicago", "a union", "2012", "torpedoes", "Saint Paul", "1501", "the Man Booker Prize", "Wayne Rooney", "Nicholas John \"Nic\" Cester", "Sharon Sheeley", "two tectonic plates move towards each other at a convergent plate boundary", "the 2013 -- 14 television season", "1975", "Prime Minister of the United Kingdom", "Vanaheim", "Manchester", "Sri Lanka's", "Felipe Massa.", "\"It seemed to be kind of laid-back -- it didn't seem to be that dangerous,\"", "Admiral Hyman Rickover", "snowy Evening", "Vestal Virgins", "Mike Mushok"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6128517316017317}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, false], "QA-F1": [0.25, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 1.0, 1.0, 0.42857142857142855, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-3707", "mrqa_hotpotqa-validation-2634", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-1854", "mrqa_hotpotqa-validation-5670", "mrqa_hotpotqa-validation-3658", "mrqa_hotpotqa-validation-2235", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3485", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-2696", "mrqa_hotpotqa-validation-506", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-3014", "mrqa_hotpotqa-validation-3024", "mrqa_hotpotqa-validation-766", "mrqa_naturalquestions-validation-8699", "mrqa_naturalquestions-validation-9330", "mrqa_triviaqa-validation-4355", "mrqa_triviaqa-validation-2833", "mrqa_newsqa-validation-371", "mrqa_newsqa-validation-2941", "mrqa_searchqa-validation-127", "mrqa_searchqa-validation-7762", "mrqa_naturalquestions-validation-1089"], "SR": 0.546875, "CSR": 0.5109953703703703, "EFR": 0.9655172413793104, "Overall": 0.6777243973499362}, {"timecode": 54, "before_eval_results": {"predictions": ["Betty Crocker", "Frank Sinatra", "khaki", "Frank Sinatra", "Antarctica", "robota", "Easter Island", "the troposphere", "Huntsville, Alabama", "Khrushchev", "Venus", "the pupil", "Jordan", "German", "toga", "George Washington", "the Peace River", "Hairspray", "The Prince and the Pauper", "Ayuthaya", "oxygen", "IRA", "Jason Voorhees", "Islamic", "Imagine", "Spanish Republic", "Three Coins in the Fountain", "George Wallace", "sushi", "Heaven", "the Byzantine Empire", "Fall Out Boy", "freezing", "silver", "beak", "William Jennings Bryan", "The Blues Brothers", "Ford Madox Ford", "the flyer", "Copenhagen", "Blossom", "blues", "Flight of the Bumblebee", "I", "Panama Canal", "Dan Marino", "Crustaceans", "Hestia", "Band of Brothers", "Prussia", "self-evident", "Profit maximization happens when marginal cost is equal to marginal revenue", "Havana Harbor", "( 2017 - 12 - 10 )", "Thai", "Heather Stanning and Helen Glover", "redheaded", "April 1, 1949", "Lantern Waste", "Glam metal", "July in the Philippines", "Abhisit Vejjajiva", "Brad Blauser,", "1989"], "metric_results": {"EM": 0.609375, "QA-F1": 0.691786858974359}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3076923076923077, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6583", "mrqa_searchqa-validation-4968", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-3177", "mrqa_searchqa-validation-7142", "mrqa_searchqa-validation-7317", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-1059", "mrqa_searchqa-validation-15081", "mrqa_searchqa-validation-1553", "mrqa_searchqa-validation-16507", "mrqa_searchqa-validation-7878", "mrqa_searchqa-validation-8786", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-197", "mrqa_searchqa-validation-12780", "mrqa_searchqa-validation-7435", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-10730", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-53", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-7321", "mrqa_newsqa-validation-3407", "mrqa_newsqa-validation-1149"], "SR": 0.609375, "CSR": 0.5127840909090908, "EFR": 0.96, "Overall": 0.6769786931818181}, {"timecode": 55, "before_eval_results": {"predictions": ["\"The Transportation Security Administration said Friday its officers at a Texas  airport appear to have properly followed procedures", "an open window", "helping to plan the September 11, 2001, terror attacks,", "Iran", "1960s", "Janet Napolitano", "legislation Wednesday requiring federal oil industry regulators to wait at least two years after leaving government service before going to work for companies they helped regulate.", "Stanford University,", "\"How I Met Your Mother,\"", "the outlet mall", "three", "February 5,", "Arabic, French and English", "Fernando Gonzalez", "The minister later apologized,", "10 below", "KBR managers", "L'Aquila earthquake,", "crocodile eggs", "200", "Sam Raimi,", "the Palm", "82", "1975", "Leo Frank,", "50", "the inspector-general of the House", "Friday,", "two", "France's famous Louvre museum", "Ashley \"A.J.\" Jewell,", "be silent.", "SSM Cardinal Glennon Children's Medical Center in St. Louis.", "two weeks after Black History Month", "Brian Smith,", "one of Africa's most stable nations.", "whether to recognize Porfirio Lobo as the legitimate president of Honduras.", "federal officers' bodies", "\"These are not stupid people,\"", "an engineering and construction company", "$50,000", "a national telephone survey", "These planning processes are urgently needed", "Michelle Rounds", "Daryeel Bulasho Guud (DBG)", "a strict interpretation of the law,", "Monday and Tuesday", "\"A good vegan cupcake has the power to transform everything for the better,\"", "sailing", "246", "Patrick McGoohan,", "`` Return of Organization Exempt From Income Tax ''", "President Gerald Ford", "thicker consistency and a deeper flavour than sauce", "windmills", "France", "Kenya", "R&B vocal group", "Donald Duck", "American", "the Edo era", "the Grand Canal", "ten-dollar", "bachata music"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6930714610073163}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, false, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, true, false, true, true, false, false], "QA-F1": [0.3157894736842105, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4444444444444445, 0.9600000000000001, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 0.4, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13333333333333333, 0.8, 0.25, 0.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-387", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2148", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-2666", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3561", "mrqa_newsqa-validation-1073", "mrqa_newsqa-validation-1165", "mrqa_newsqa-validation-629", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-3522", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-695", "mrqa_newsqa-validation-4165", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-3183", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-2943", "mrqa_triviaqa-validation-4633", "mrqa_triviaqa-validation-5309", "mrqa_hotpotqa-validation-2866", "mrqa_hotpotqa-validation-5565", "mrqa_searchqa-validation-2063", "mrqa_naturalquestions-validation-4925"], "SR": 0.53125, "CSR": 0.5131138392857143, "EFR": 1.0, "Overall": 0.6850446428571428}, {"timecode": 56, "before_eval_results": {"predictions": ["aid agencies", "$150 billion over 10 years in clean energy.", "Kellogg Brown", "Sunday", "\"Empire of the Sun,\"", "Buenos Aires.", "planning for life after baseball.", "Sonia Sotomayor", "Herman Cain,", "There's no chance", "15-year-old", "137", "22-year-old", "the immorality of these deviant young men does not provide solutions that prevent gang rape from happening.", "Bryant Purvis,", "570 billion pesos ($42 billion)", "Ralph Lauren,", "will not support the Stop Online Piracy Act,", "outstanding performance by a female actor in a drama series", "Krishna Rajaram,", "helping to plan the September 11, 2001,", "The pilot,", "$10 billion", "can", "Washington State's decommissioned Hanford nuclear site,", "\"It hurts my heart to see him in pain, but it enlightens at the same time to know my son is strong enough to make it through on a daily basis,\"", "Pakistan's", "in a canyon in the path of the blaze Thursday.", "left the medical engineering company where she worked.", "an antihistamine and an epinephrine auto-injector for emergencies,", "U.S. Holocaust Memorial Museum,", "racial intolerance.", "Jobs", "Al-Shabaab,", "in an appearance last week in Broward County Circuit Court.", "3-0", "Tuesday", "Hamas ministry spokesman Taher Nunu called al-Maqdessi's group \"outlaws\" and said they have been \"terrorizing the country and attacking civilians.\"", "off the coast of Dubai", "would be able to return with these two women otherwise it would be a terrible loss of face for him,\"", "Dr. Jennifer Arnold and husband Bill Klein,", "80 percent of the woman's face", "five", "British Prime Minister Gordon Brown's", "Pacific Ocean territory of Guam within striking distance,", "Angola,", "The drama of the action in-and-around the golf course", "May 4", "a \"happy ending\" to the case.", "Harry Nicolaides,", "amyotrophic Lateral Sclerosis", "dispense summary justice or merely deal with local administrative applications in common law jurisdictions", "autompne ( automne in modern French ) or autumpne in Middle English", "a cascade of events through phosphorylation of intracellular proteins that ultimately transmit ( `` transduce '' ) the extracellular signal to the nucleus", "hound", "fish", "Cambodia", "Cold Spring", "February 13, 1946", "Arrowhead Stadium", "Thomas Jefferson", "refrigerator", "X-Files", "1961"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6891627846790891}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 0.4, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1904761904761905, 1.0, 0.7450980392156863, 1.0, 0.0, 1.0, 1.0, 0.5882352941176471, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.782608695652174, 0.0, 0.13333333333333333, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-933", "mrqa_newsqa-validation-675", "mrqa_newsqa-validation-1164", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-2872", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-566", "mrqa_newsqa-validation-2660", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1641", "mrqa_newsqa-validation-3724", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-2733", "mrqa_newsqa-validation-2596", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-3347", "mrqa_newsqa-validation-228", "mrqa_naturalquestions-validation-2476", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-9271", "mrqa_triviaqa-validation-617", "mrqa_hotpotqa-validation-298"], "SR": 0.609375, "CSR": 0.5148026315789473, "EFR": 1.0, "Overall": 0.6853824013157894}, {"timecode": 57, "before_eval_results": {"predictions": ["$55.7 million", "led the weekend box office,", "Kurdish militant group in Turkey", "eight surgeons at the Cleveland Clinic.", "Alwin Landry's supply vessel Damon Bankston", "\"Golden City,\"", "\"nationwide shopping sprees, staying at five-star hotels, renting luxury automobiles and private jets, and purchasing tens of thousands of dollars worth of high-end electronics and expensive handbags and jewelry with forged credit cards,\"", "free services.", "and consumers should not try to wash off the bacteria,", "Amitabh Bachchan", "\"Rent,\" \"Cabaret\" and \" Proof,\"", "Dancy-Power Automotive in Harlem,", "closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "anyone wanting to harm them.", "Screen Actors Guild", "11 healthy eggs", "Salt Lake facility", "Guinea, Myanmar, Sudan and Venezuela.", "four", "the federal chamber of deputies,", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing rampage.", "$60 billion on America's infrastructure.", "\" Seeing bullet wounds in the back of a friend's head, seeing friends grabbing their arms, and blood just everywhere.", "Tim Clark, Matt Kuchar and Bubba Watson", "Mark Fields of Ford,", "three", "al Qaeda", "Isabella", "Larry Abrahamson", "near his home in Peshawar", "Dogpatch Labs", "David J. Lavau's son, Sean, found his father after hearing \"faint yelling for help on the roadway from the canyon below,\"", "cross-country skiers who used the football matches in knee-deep mud to strengthen their leg muscles.", "no one is sure", "\"The Sopranos,\"", "a one-shot victory in the Bob Hope Classic on the final hole", "bullet-riddled body was rushed to the Maadi Military Hospital and the president was proclaimed dead at 2.40 p.m. due to \"intense nervous shock and internal bleeding in the chest cavity.\"", "Stratfor,", "Adam Yahiye Gadahn,", "an antihistamine and an epinephrine auto-injector", "The EU naval force", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "Deputy Treasury Secretary", "heavy turbulence", "rock music with a country influence.", "SSM Cardinal Glennon Children's Medical Center in St. Louis, Missouri.", "two Emmys", "Henrik Stenson", "Three", "ore Gold, former Israeli ambassador to the United Nations said, \"The IAEA has inspected the known nuclear sites of Iran.", "for pulling on the top-knot of an opponent,", "Roger Federer", "Iran", "December 2, 2013", "Hayes", "ocellaris", "Morgan Spurlock", "football", "\"War & Peace\"", "Eugene Levy", "Floyd Patterson", "Jan & Dean", "Sweeney Todd: The Demon Barber of Fleet Street", "bass"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5247131411193912}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.5, 0.0, 0.0, 0.0, 0.0, 0.1818181818181818, 0.8918918918918919, 1.0, 0.0, 1.0, 0.0, 0.0, 0.24242424242424243, 0.0, 1.0, 0.5, 0.5714285714285715, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1, 0.25, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.4615384615384615, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.4444444444444445, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3158", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-2205", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3635", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-903", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-2479", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-1975", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-2450", "mrqa_newsqa-validation-2414", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-3626", "mrqa_newsqa-validation-1141", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-1121", "mrqa_naturalquestions-validation-2170", "mrqa_triviaqa-validation-6757", "mrqa_hotpotqa-validation-3321", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-10144"], "SR": 0.421875, "CSR": 0.5132004310344828, "EFR": 0.972972972972973, "Overall": 0.6796565558014912}, {"timecode": 58, "before_eval_results": {"predictions": ["at a depth of about 1,300 meters in the Mediterranean Sea.", "machine guns and silencers,", "the 3rd District of Utah.", "Kenneth Cole", "\"If Russian long-range bombers should need to land in Venezuela, we would not object to that either.", "President Bush", "Stephen Tyrone Johns", "Transport Workers Union leaders", "Two soldiers, a policeman and four militants", "23-year-old", "prisoners at the South Dakota State Penitentiary", "responsibility for the abductions", "The food, music, culture and language of Latin America", "Workers'", "is fighting an unjust war for an America that went too far when it invaded Iraq", "Pope", "boyhood experience in a World War II internment camp", "Secretary of State", "Mom", "San Diego,", "opium poppies", "1979", "Alexandre Caizergues,", "sportswear,", "military trial system", "Paul and Ringo", "voice-assistant software", "the invention of iTunes,", "Indonesian", "eight-week", "order", "Natalie Cole,", "Somali-based", "folding table", "the Obama administration", "Ventures", "British", "Long Island convenience store", "former Procol Harum bandmate Gary Brooker", "Former Mobile County Circuit Judge", "Sen. Barack Obama", "July 23.", "suicides", "the finding of the body \"has really cut the legs out of the defense,\"", "Stuttgart", "a motion for a preliminary injunction against a Mississippi school district and high school", "105-year", "helping consumers move beyond these hard times", "a man's lifeless,", "iTunes,", "23", "summer of 1990", "Pakhangba", "Ricky Nelson", "Cream", "Portugal,", "sphinx", "Andrew Johnson", "Charles Nungesser", "United States of America (USA),", "almond", "tides", "bananas", "Leonard Bernstein"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6487342212526036}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, true, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.5714285714285715, 0.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7000000000000001, 0.6666666666666666, 0.5, 0.0, 0.6, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.888888888888889, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1241", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3088", "mrqa_newsqa-validation-2184", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-2135", "mrqa_newsqa-validation-887", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1979", "mrqa_newsqa-validation-1976", "mrqa_newsqa-validation-995", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-2453", "mrqa_newsqa-validation-4043", "mrqa_newsqa-validation-3952", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-977", "mrqa_naturalquestions-validation-661", "mrqa_triviaqa-validation-6533", "mrqa_triviaqa-validation-3070", "mrqa_hotpotqa-validation-20", "mrqa_hotpotqa-validation-722", "mrqa_searchqa-validation-14467", "mrqa_searchqa-validation-7269"], "SR": 0.546875, "CSR": 0.513771186440678, "EFR": 0.9655172413793104, "Overall": 0.6782795605639976}, {"timecode": 59, "before_eval_results": {"predictions": ["Supplemental oxygen", "John Smith", "McFerrin", "( 7th century )", "March 6, 2018", "multiple", "Egypt", "Adam Carolla", "the coast of Guant\u00e1namo Bay in Cuba", "1840s", "1940s", "1970", "helps scientists better understand the spread of pollution around the globe", "U.S. Electoral College", "The resulting molecule, now mature insulin, is stored as a hexamer in secretory vesicles", "the Holy See", "Carpenter", "John F. Kelly", "$343.6 million worldwide", "the 9th century", "crossbar", "the 2017 -- 18 network television season", "White Sox", "`` Reveille ''", "country", "Tara / Ghost of Christmas Past", "Stephen Foster", "1975", "6 -- 14 July", "majority of members present at that time", "down the Australian northeast coast", "`` Blood is the New Black ''", "continent of Antarctica", "the courts", "the primal rib", "positions Arg15 - Ile16", "Steve Valentine", "political ideology", "Anakin Skywalker", "Lesley Gore", "David Sabara", "Khoisan language of the \u01c0Xam people", "peninsular", "Terry Kath", "16 best - selling religious novels", "`` skin - changer ''", "Johnny Cash", "pre-Columbian times", "human induced greenhouse warming", "American rock band The Romantics", "Gorakhpur Junction", "Maerten Tromp", "sternum", "horseracing", "\"good character\"", "travel", "\"Nebo Zovyot\"", "Caster Semenya", "Berga an der Elster", "North Korean Foreign Ministry spokesman described U.S. Vice President", "Nellie Bly", "sergeant. Forrest Irvin", "godliness", "sour"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5976573773448773}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, true, true, false, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1555", "mrqa_naturalquestions-validation-10232", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-8439", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-4519", "mrqa_naturalquestions-validation-2462", "mrqa_naturalquestions-validation-9367", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-3611", "mrqa_naturalquestions-validation-6076", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-8972", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8093", "mrqa_triviaqa-validation-4112", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-2280", "mrqa_newsqa-validation-2405", "mrqa_searchqa-validation-7286", "mrqa_searchqa-validation-8408"], "SR": 0.546875, "CSR": 0.5143229166666667, "EFR": 0.9655172413793104, "Overall": 0.6783899066091954}, {"timecode": 60, "UKR": 0.71875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1003", "mrqa_hotpotqa-validation-1016", "mrqa_hotpotqa-validation-109", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1328", "mrqa_hotpotqa-validation-137", "mrqa_hotpotqa-validation-1417", "mrqa_hotpotqa-validation-1604", "mrqa_hotpotqa-validation-1684", "mrqa_hotpotqa-validation-1824", "mrqa_hotpotqa-validation-1826", "mrqa_hotpotqa-validation-1895", "mrqa_hotpotqa-validation-1952", "mrqa_hotpotqa-validation-2059", "mrqa_hotpotqa-validation-2130", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-2207", "mrqa_hotpotqa-validation-2225", "mrqa_hotpotqa-validation-2302", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2484", "mrqa_hotpotqa-validation-2494", "mrqa_hotpotqa-validation-2573", "mrqa_hotpotqa-validation-2606", "mrqa_hotpotqa-validation-2739", "mrqa_hotpotqa-validation-2850", "mrqa_hotpotqa-validation-2856", "mrqa_hotpotqa-validation-2914", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-310", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3429", "mrqa_hotpotqa-validation-3462", "mrqa_hotpotqa-validation-3474", "mrqa_hotpotqa-validation-3485", "mrqa_hotpotqa-validation-3523", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3591", "mrqa_hotpotqa-validation-3767", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-389", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-405", "mrqa_hotpotqa-validation-409", "mrqa_hotpotqa-validation-4108", "mrqa_hotpotqa-validation-418", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-420", "mrqa_hotpotqa-validation-4233", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-4598", "mrqa_hotpotqa-validation-4671", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4753", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-4999", "mrqa_hotpotqa-validation-5038", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-520", "mrqa_hotpotqa-validation-5261", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5301", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5309", "mrqa_hotpotqa-validation-5317", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-558", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5734", "mrqa_hotpotqa-validation-5834", "mrqa_hotpotqa-validation-587", "mrqa_hotpotqa-validation-597", "mrqa_hotpotqa-validation-6", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-870", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-10546", "mrqa_naturalquestions-validation-10712", "mrqa_naturalquestions-validation-114", "mrqa_naturalquestions-validation-1173", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-1851", "mrqa_naturalquestions-validation-1888", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2130", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-2347", "mrqa_naturalquestions-validation-239", "mrqa_naturalquestions-validation-2503", "mrqa_naturalquestions-validation-2646", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-2937", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3013", "mrqa_naturalquestions-validation-3052", "mrqa_naturalquestions-validation-3062", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3795", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-4190", "mrqa_naturalquestions-validation-4212", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4247", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-467", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4804", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6041", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-6278", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6592", "mrqa_naturalquestions-validation-714", "mrqa_naturalquestions-validation-7142", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7609", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7728", "mrqa_naturalquestions-validation-7848", "mrqa_naturalquestions-validation-7960", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-8087", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-872", "mrqa_naturalquestions-validation-9071", "mrqa_naturalquestions-validation-9270", "mrqa_naturalquestions-validation-9281", "mrqa_naturalquestions-validation-9404", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9557", "mrqa_naturalquestions-validation-9778", "mrqa_naturalquestions-validation-9944", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-1044", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1226", "mrqa_newsqa-validation-125", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1277", "mrqa_newsqa-validation-1279", "mrqa_newsqa-validation-1291", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1379", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1590", "mrqa_newsqa-validation-1610", "mrqa_newsqa-validation-1638", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1678", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1700", "mrqa_newsqa-validation-1708", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-1737", "mrqa_newsqa-validation-176", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1818", "mrqa_newsqa-validation-1833", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1850", "mrqa_newsqa-validation-1853", "mrqa_newsqa-validation-186", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-1904", "mrqa_newsqa-validation-191", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2001", "mrqa_newsqa-validation-2027", "mrqa_newsqa-validation-2031", "mrqa_newsqa-validation-2058", "mrqa_newsqa-validation-2060", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2099", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-2241", "mrqa_newsqa-validation-2244", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2495", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2544", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2665", "mrqa_newsqa-validation-2669", "mrqa_newsqa-validation-2683", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2826", "mrqa_newsqa-validation-284", "mrqa_newsqa-validation-2842", "mrqa_newsqa-validation-2857", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-2891", "mrqa_newsqa-validation-2941", "mrqa_newsqa-validation-295", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-3120", "mrqa_newsqa-validation-3138", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-3169", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3231", "mrqa_newsqa-validation-3249", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3286", "mrqa_newsqa-validation-3331", "mrqa_newsqa-validation-3362", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3501", "mrqa_newsqa-validation-3521", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-3565", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-37", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-3825", "mrqa_newsqa-validation-3867", "mrqa_newsqa-validation-3955", "mrqa_newsqa-validation-396", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-398", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4062", "mrqa_newsqa-validation-410", "mrqa_newsqa-validation-4168", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-431", "mrqa_newsqa-validation-50", "mrqa_newsqa-validation-51", "mrqa_newsqa-validation-525", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-533", "mrqa_newsqa-validation-534", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-556", "mrqa_newsqa-validation-65", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-779", "mrqa_newsqa-validation-898", "mrqa_newsqa-validation-904", "mrqa_newsqa-validation-922", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-959", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-983", "mrqa_searchqa-validation-1000", "mrqa_searchqa-validation-10210", "mrqa_searchqa-validation-10394", "mrqa_searchqa-validation-1073", "mrqa_searchqa-validation-10752", "mrqa_searchqa-validation-10787", "mrqa_searchqa-validation-10955", "mrqa_searchqa-validation-11415", "mrqa_searchqa-validation-11496", "mrqa_searchqa-validation-11558", "mrqa_searchqa-validation-11688", "mrqa_searchqa-validation-11955", "mrqa_searchqa-validation-11973", "mrqa_searchqa-validation-12267", "mrqa_searchqa-validation-12333", "mrqa_searchqa-validation-12678", "mrqa_searchqa-validation-12698", "mrqa_searchqa-validation-1271", "mrqa_searchqa-validation-12763", "mrqa_searchqa-validation-12908", "mrqa_searchqa-validation-12959", "mrqa_searchqa-validation-13328", "mrqa_searchqa-validation-13503", "mrqa_searchqa-validation-1399", "mrqa_searchqa-validation-14128", "mrqa_searchqa-validation-1420", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14865", "mrqa_searchqa-validation-14917", "mrqa_searchqa-validation-14969", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15194", "mrqa_searchqa-validation-15541", "mrqa_searchqa-validation-15730", "mrqa_searchqa-validation-15738", "mrqa_searchqa-validation-15839", "mrqa_searchqa-validation-15973", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16132", "mrqa_searchqa-validation-16461", "mrqa_searchqa-validation-16505", "mrqa_searchqa-validation-16642", "mrqa_searchqa-validation-16746", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16894", "mrqa_searchqa-validation-2119", "mrqa_searchqa-validation-2183", "mrqa_searchqa-validation-2242", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2791", "mrqa_searchqa-validation-3098", "mrqa_searchqa-validation-3295", "mrqa_searchqa-validation-3349", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-4219", "mrqa_searchqa-validation-4432", "mrqa_searchqa-validation-4922", "mrqa_searchqa-validation-5320", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5484", "mrqa_searchqa-validation-5521", "mrqa_searchqa-validation-5697", "mrqa_searchqa-validation-5698", "mrqa_searchqa-validation-5856", "mrqa_searchqa-validation-6659", "mrqa_searchqa-validation-6880", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-7317", "mrqa_searchqa-validation-7855", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7989", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8214", "mrqa_searchqa-validation-8361", "mrqa_searchqa-validation-8566", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8917", "mrqa_searchqa-validation-8953", "mrqa_searchqa-validation-9176", "mrqa_searchqa-validation-9347", "mrqa_searchqa-validation-947", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9683", "mrqa_searchqa-validation-9698", "mrqa_squad-validation-100", "mrqa_squad-validation-10370", "mrqa_squad-validation-10457", "mrqa_squad-validation-12", "mrqa_squad-validation-1254", "mrqa_squad-validation-1402", "mrqa_squad-validation-1436", "mrqa_squad-validation-1540", "mrqa_squad-validation-1613", "mrqa_squad-validation-175", "mrqa_squad-validation-185", "mrqa_squad-validation-1902", "mrqa_squad-validation-194", "mrqa_squad-validation-2102", "mrqa_squad-validation-2129", "mrqa_squad-validation-2276", "mrqa_squad-validation-2313", "mrqa_squad-validation-2314", "mrqa_squad-validation-2523", "mrqa_squad-validation-270", "mrqa_squad-validation-2762", "mrqa_squad-validation-2916", "mrqa_squad-validation-2986", "mrqa_squad-validation-3037", "mrqa_squad-validation-3125", "mrqa_squad-validation-3319", "mrqa_squad-validation-3511", "mrqa_squad-validation-3545", "mrqa_squad-validation-3708", "mrqa_squad-validation-3811", "mrqa_squad-validation-3830", "mrqa_squad-validation-3842", "mrqa_squad-validation-3934", "mrqa_squad-validation-402", "mrqa_squad-validation-4179", "mrqa_squad-validation-448", "mrqa_squad-validation-4546", "mrqa_squad-validation-4750", "mrqa_squad-validation-4860", "mrqa_squad-validation-4949", "mrqa_squad-validation-4961", "mrqa_squad-validation-5050", "mrqa_squad-validation-5135", "mrqa_squad-validation-5276", "mrqa_squad-validation-5311", "mrqa_squad-validation-533", "mrqa_squad-validation-5349", "mrqa_squad-validation-5474", "mrqa_squad-validation-5669", "mrqa_squad-validation-570", "mrqa_squad-validation-5704", "mrqa_squad-validation-5736", "mrqa_squad-validation-6029", "mrqa_squad-validation-6351", "mrqa_squad-validation-6481", "mrqa_squad-validation-6522", "mrqa_squad-validation-6526", "mrqa_squad-validation-6630", "mrqa_squad-validation-7023", "mrqa_squad-validation-7217", "mrqa_squad-validation-7233", "mrqa_squad-validation-7531", "mrqa_squad-validation-7635", "mrqa_squad-validation-7647", "mrqa_squad-validation-7689", "mrqa_squad-validation-7763", "mrqa_squad-validation-8066", "mrqa_squad-validation-8209", "mrqa_squad-validation-8386", "mrqa_squad-validation-8754", "mrqa_squad-validation-8772", "mrqa_squad-validation-8931", "mrqa_squad-validation-9012", "mrqa_squad-validation-9322", "mrqa_squad-validation-9495", "mrqa_squad-validation-9506", "mrqa_squad-validation-955", "mrqa_squad-validation-9590", "mrqa_squad-validation-9868", "mrqa_squad-validation-9931", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-2088", "mrqa_triviaqa-validation-2112", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3692", "mrqa_triviaqa-validation-3727", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3754", "mrqa_triviaqa-validation-3880", "mrqa_triviaqa-validation-4354", "mrqa_triviaqa-validation-4371", "mrqa_triviaqa-validation-4376", "mrqa_triviaqa-validation-4400", "mrqa_triviaqa-validation-4445", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4778", "mrqa_triviaqa-validation-5631", "mrqa_triviaqa-validation-5794", "mrqa_triviaqa-validation-6102", "mrqa_triviaqa-validation-617", "mrqa_triviaqa-validation-6175", "mrqa_triviaqa-validation-6263", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6519", "mrqa_triviaqa-validation-6579", "mrqa_triviaqa-validation-6614", "mrqa_triviaqa-validation-6724", "mrqa_triviaqa-validation-6730", "mrqa_triviaqa-validation-7045", "mrqa_triviaqa-validation-721", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-7405", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7540", "mrqa_triviaqa-validation-7580", "mrqa_triviaqa-validation-7683"], "OKR": 0.81640625, "KG": 0.49375, "before_eval_results": {"predictions": ["hydrogen isotopes", "Sax Rohmer", "Nicole Brown Simpson", "the Dominican Republic", "Crystal Gayle", "carry On Cleo", "Crispin", "a healthy naturally occurring rock, crystal or sea salt", "Dijon", "Wisconsin", "plum", "marries John Burb", "Wonga", "Secretary of State William H. Seward", "cryogenically frozen", "The World is Not Enough", "Barnaby Rudge", "James Chadwick", "Gary Barlow", "meteoroid", "Lesley Hornby", "California", "cesium", "Picasso", "Alzheimer's disease", "trumpet", "King George I", "whipped cream", "arm", "earache", "90%", "muezzin", "the moon", "watts", "Ryan Ladd", "black bean", "Virginia", "Charles Darwin", "PJ Harvey", "Jim Jones", "the grueling decathlon,", "Runcorn", "\"the Gentile Times,\"", "Amnesty International", "a crater on the far side of the Moon", "naomi watts", "Arthur Miller", "Carousel", "Frankenstein", "potash", "Paris", "headdresses", "Janie Crawford", "Julia Roberts", "their unusual behavior", "The Seduction of Hillary Rodham", "Apsley George Benet Cherry-Garrard", "\"Toy Story,\"", "86", "allegations that a dorm parent mistreated students at the school.", "daiquiri", "apples", "Morocco", "the nerves and ganglia outside the brain and spinal cord"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7026041666666667}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3379", "mrqa_triviaqa-validation-1388", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-2291", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-705", "mrqa_triviaqa-validation-240", "mrqa_triviaqa-validation-3200", "mrqa_triviaqa-validation-1643", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-3854", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-6726", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-2261", "mrqa_triviaqa-validation-7394", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-6285"], "SR": 0.671875, "CSR": 0.516905737704918, "EFR": 0.9523809523809523, "Overall": 0.699638588017174}, {"timecode": 61, "before_eval_results": {"predictions": ["a paragraph about the king and crown prince that authorities deemed a violation of a law that makes it illegal", "President Bush", "not", "Samuel Herr, 26, and Juri Kibuishi,", "checkposts and military camps in the Mohmand agency,", "Cannes", "customers are lining up for vitamin injections that promise", "l Yusuf Saad Kamel", "Andrew Morris,", "Alberto Espinoza Barron,", "22 million", "41,280", "Caylee Anthony,", "18", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "Jaime Andrade", "kryptonite", "\"Wicked.\"", "between 1917 and 1924", "Mississippi", "$3 billion,", "the Lakes Golf Club", "school in South Africa", "Dr. Jennifer Arnold and husband Bill Klein,", "\"Up,\"", "\"The Real Housewives of Atlanta\"", "bollywood", "Premier League club side Wigan Athletic in northern England.", "40-year-old", "1,500 Marines", "Current TV", "civilians,", "Sarah Brown's wife, Sarah, wore an outfit from designer Britt Lintner", "Adriano", "returning combat veterans", "hanmanka Garden.", "said the rig's captain, Curt Kutcha, told him he had tried to activate a \"kill switch\"", "phay Siphan,", "staff sergeant", "nine", "Dereks", "george Washington", "chairman of the House Budget Committee", "Kenyan", "Hamburg.", "says his grandfather was a \"Zionist\" and \"a zealous supporter of the usurper entity, and a prominent member of a number of Zionist hate organizations.\"", "HSH Nordbank Arena", "German Chancellor", "rare thriller writer", "Monday", "Carrousel du Louvre,", "Prem Lata Agarwal", "Ms. Stout", "Terry Kath", "aries", "William Shakespeare", "Nicolas Sarkozy", "Melanie Owen", "Thor", "Queenston Delta", "george III", "the Taj Mahal", "colon", "Carol Worthington"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5622778537252222}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, true, true, false, true, false, false, true, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, true, true, true, true, false, true, true, true, true], "QA-F1": [0.5454545454545454, 0.0, 0.6666666666666666, 0.5, 0.0, 0.5, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.047619047619047616, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.10526315789473685, 0.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-702", "mrqa_newsqa-validation-663", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-940", "mrqa_newsqa-validation-2885", "mrqa_newsqa-validation-626", "mrqa_newsqa-validation-1929", "mrqa_newsqa-validation-1668", "mrqa_newsqa-validation-1000", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-108", "mrqa_newsqa-validation-859", "mrqa_newsqa-validation-3010", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-3777", "mrqa_newsqa-validation-1923", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-4089", "mrqa_newsqa-validation-3209", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-3139", "mrqa_newsqa-validation-293", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-80", "mrqa_newsqa-validation-2955", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-400", "mrqa_triviaqa-validation-4714", "mrqa_hotpotqa-validation-4692"], "SR": 0.484375, "CSR": 0.5163810483870968, "EFR": 0.9090909090909091, "Overall": 0.6908756414956011}, {"timecode": 62, "before_eval_results": {"predictions": ["500 feet down an embankment", "creation of an Islamic emirate in Gaza,", "1960s song \"A Whiter Shade of Pale\"", "Ciudad Juarez,", "Kenneth Cole", "little blue booties.", "David McKenzie", "$8.8 million", "\"I'm just getting started.\"", "Saturday,", "are", "14", "planning processes are urgently needed", "Mandi Hamlin", "The towering figure of a Muslim revolutionary named Malcolm X", "financial gain,", "a face-to-face interview with the president", "six", "Steve Jobs", "a head injury.", "Daniel Radcliffe", "Tillakaratne Dilshan", "\"The train ride up there is spectacular. You see wonderful vistas as you leave Denver through the northern plains and into the mountains,\"", "a construction site in the heart of Los Angeles.", "Jaipur", "Al-Aqsa mosque", "Majid Movahedi,", "criminals", "whether to close some entrances, bring in additional officers, and make security more visible,\"", "\"theoretically\"", "Transportation Security Administration", "E. coli", "Three French journalists, a seven-member Spanish flight crew and one Belgian", "The United States", "100 percent", "Brian Mabry", "150", "\"Watchmen\"", "autonomy.", "\"She was focused so much on learning that she didn't notice,\"", "will be proof that the sun, and renewable energy at home everyday,\"", "two", "Kenyan forces who have entered Somalia,", "Brian David Mitchell,", "Omar bin Laden", "as part of its 18-month journey around the world.", "Italian Serie A title", "1983.", "Tutsi and Hutu", "are co-chairs of the Genocide Prevention Task Force.", "have been looking into it for the past two years,\"", "William Shakespeare's As You Like It", "DeWayne Warren", "Flag Day in 1954", "jackstones", "fool", "The Merchant of Venice", "designated hitter", "\"The Leader In Me \u2014 How Schools and Parents Around the World Are Inspiring Greatness, One Child at a Time\"", "11 June 1959", "Plutarch", "Buddhism", "cab", "seven nights a week"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6424305626971069}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, false, false, true, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.7692307692307693, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.12121212121212123, 1.0, 1.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7058823529411764, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.19999999999999998, 0.7692307692307692, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2736", "mrqa_newsqa-validation-2170", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1021", "mrqa_newsqa-validation-3064", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-722", "mrqa_newsqa-validation-2336", "mrqa_newsqa-validation-1488", "mrqa_newsqa-validation-2731", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-2169", "mrqa_newsqa-validation-978", "mrqa_newsqa-validation-1269", "mrqa_newsqa-validation-388", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-3617", "mrqa_naturalquestions-validation-2844", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-6532", "mrqa_hotpotqa-validation-2543", "mrqa_hotpotqa-validation-5789", "mrqa_hotpotqa-validation-3266"], "SR": 0.5625, "CSR": 0.5171130952380952, "EFR": 0.9642857142857143, "Overall": 0.7020610119047619}, {"timecode": 63, "before_eval_results": {"predictions": ["Jaime Andrade", "\"It was never our intention to offend anyone,\"", "President Obama and Britain's Prince Charles", "the insurgency,", "Reporters Without Borders", "Genocide Prevention Task Force,", "Britain's", "five female pastors", "Bob Bogle,", "Sri Lanka", "82", "Fullerton, California,", "$50 less,", "Zulfikar Ali Bhutto,", "two", "and Jonathan Breeze,", "Paul Blart: Mall Cop\" (No. 5)", "President Obama and Britain's Prince Charles", "public opinion in Turkey.\"", "10 municipal police officers", "left his indelible fingerprints on the entertainment industry.", "murder in the beating death of a company boss who fired them.", "\"And even though she's not here anymore, I'm not afraid to say it, sometimes she was a pain in the ass,\"", "\"political and religious\"", "Siri", "Derek Mears", "Dean Martin, Katharine Hepburn and Spencer Tracy", "Heshmatollah Attarzadeh", "\"Zed,\"", "because the Indians were gathering information about the rebels to give to the Colombian military.", "--the Louvre.", "Mubarak,", "Princess Diana", "revelry", "Human Rights Watch", "45 minutes,", "on February 12", "1959,", "London's", "\"illeg illegitimate.\"", "BBC building in Glasgow, Scotland", "Russia", "and and so the portion of the world that needs aid is far less today than it was 50 years ago.", "Zuma", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "to sniff out cell phones.", "shock, quickly followed by speculation about what was going to happen next,\"", "U.S. Food and Drug Administration", "$1.5 million.", "former Pakistani Prime Minister Nawaz Sharif", "raping and killing a 14-year-old Iraqi girl.", "1837", "( Austin Winkler ) and his former lover ( Emmanuelle Chriqui )", "the following year", "\"Raging Bull.\"", "bluebell", "FIFA World Cup", "General Allenby", "turns out to be a terrible date", "The Treaty of Gandamak", "England", "James Carville", "beak", "Candide"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7123367154889684}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.26666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.17142857142857143, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.8, 0.0, 1.0, 0.6666666666666666, 0.8333333333333333, 1.0, 0.0, 1.0, 0.5581395348837209, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 0.8571428571428571, 0.25, 0.923076923076923, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2497", "mrqa_newsqa-validation-2277", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-3160", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3223", "mrqa_newsqa-validation-3293", "mrqa_newsqa-validation-1041", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-658", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3401", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-1098", "mrqa_naturalquestions-validation-3482", "mrqa_naturalquestions-validation-1912", "mrqa_triviaqa-validation-3824", "mrqa_hotpotqa-validation-5720", "mrqa_hotpotqa-validation-4086", "mrqa_searchqa-validation-1971"], "SR": 0.59375, "CSR": 0.518310546875, "EFR": 1.0, "Overall": 0.709443359375}, {"timecode": 64, "before_eval_results": {"predictions": ["Wolfgang Amadeus Mozart", "Denmark\u2013Norway, Brandenburg and Sweden", "John Schlesinger", "business magnate, investor, and philanthropist", "Phineas and Ferb", "Two Is Better Than One", "Karl-Anthony Towns", "Omega SA", "9 November 1967", "the designated hitter rule", "Jay Park", "Wayne County, Michigan", "Hong Kong, New York City, London, Taipei, China, Bangkok and Singapore.", "Cleopatra VII Philopator", "8,211", "The Allies of World War I, or Entente Powers", "August Heckscher", "Orange County, Florida, United States", "Gareth Barry", "capital crimes or capital offences", "Ned Flanders", "Westley Sissel Unseld", "Ken Howard", "Fat Albert", "Thomas Joseph \"T. J. Lavin", "15", "Germany", "I-League club Salgaocar", "London Tipton", "1887", "Saturday", "January 2001", "PlayStation 2", "England", "Ry\u016bkyuan people", "The Lost Battalion", "fennec", "from 1993 to 1996", "Yunnan-Fu", "Port Moresby, Papua New Guinea", "Macau Peninsula, Macau", "1993,", "The Bangor Daily News is an American newspaper covering a large portion of rural Maine,", "The Land of Enchantment", "Hawaii", "William Finn", "ZZ Top", "the controversial and explicit nature", "The Tales of Hoffmann", "Shakespeare criticism", "October 3, 2017", "1984 Summer Olympics in Los Angeles", "Afghanistan", "Michael Crawford", "parallelogram", "Operation Frequent Wind", "Magi", "toxic smoke from burn pits", "15,000", "breast cancer.", "mast", "Casablanca", "Vilna", "Asaph Hall"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5884391216422467}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, false, true, false, true, false, false, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.39999999999999997, 0.0, 0.4, 0.7692307692307693, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 0.125, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-83", "mrqa_hotpotqa-validation-4702", "mrqa_hotpotqa-validation-833", "mrqa_hotpotqa-validation-1969", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-2383", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-230", "mrqa_hotpotqa-validation-2729", "mrqa_hotpotqa-validation-589", "mrqa_hotpotqa-validation-5115", "mrqa_hotpotqa-validation-802", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-1776", "mrqa_hotpotqa-validation-1867", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-270", "mrqa_hotpotqa-validation-1394", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-4052", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-5655", "mrqa_naturalquestions-validation-75", "mrqa_triviaqa-validation-6706", "mrqa_triviaqa-validation-305", "mrqa_newsqa-validation-1107", "mrqa_newsqa-validation-4147", "mrqa_searchqa-validation-7441", "mrqa_searchqa-validation-4036"], "SR": 0.484375, "CSR": 0.5177884615384616, "EFR": 1.0, "Overall": 0.7093389423076923}, {"timecode": 65, "before_eval_results": {"predictions": ["10 October 2010", "45th Vice President of the United States", "Claudio Javier L\u00f3pez", "the Las Vegas Strip in Paradise", "the Swiss Confederation", "I Should Have Known Better", "John McClane", "Steve Coulter", "Philadelphia Naval Shipyard", "The Pentagon", "1958", "our greatest comedienne - Australia's Lucille Ball", "Canadian", "Seventeen", "spot-fixing", "Love the Way You Lie", "Easter Rising of 1916", "Spain, Mexico and France", "Juilliard School", "Mark \"Chopper\" Read", "small forward", "California State University", "Floridians", "Erreway", "Eric Liddell", "Sam Kinison", "Spring city", "Bobby Hurley", "fortnightly", "to negotiate peace with the United Kingdom during World War II", "9,984", "Sunset Publishing Corporation", "Pennacook", "1966", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "The Soloist", "1st Earl Grosvenor", "Labour Party", "Walt Disney", "1983", "stolperstein", "American rapper", "BMW X6", "Cleveland Cavaliers", "Larnelle Steward Harris", "May 5, 2015", "Bank of China Tower", "general secretary of the Norwegian Anthroposophical Society", "Barnoldswick", "New Orleans, Louisiana", "eight", "IB", "Mark Jackson", "1991", "Antigua and Barbuda", "trout", "Montpelier", "allergic reaction to peanuts, her mother, Tara, has worried about her daughter's food whenever they eat out.", "African National Congress Deputy President Kgalema Motlanthe,", "seven", "Portugal", "splice", "law firm of Crane, Poole and Schmidt", "the 2009 model year"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6695714008214009}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.11111111111111112, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.30769230769230765, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 0.5714285714285715, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4485", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-1054", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-3383", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1135", "mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-486", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-4408", "mrqa_hotpotqa-validation-3536", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-2112", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-1123", "mrqa_naturalquestions-validation-9130", "mrqa_triviaqa-validation-1138", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-4057", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-16601"], "SR": 0.5625, "CSR": 0.5184659090909092, "EFR": 1.0, "Overall": 0.7094744318181818}, {"timecode": 66, "before_eval_results": {"predictions": ["January 27, 1838", "James Fell", "1970", "Despicable Me 3", "Bill Clinton", "1,800", "Duke Frederick", "Bring Me Sunshine (1994) was originally a three-part retrospective in tribute to Eric Morecambe", "Juventus F.C.", "June 26, 2018", "four sections", "the fifth level", "five", "Macau, China", "American black bear", "1345 to 1377", "sandstone", "May 4, 2004", "VIMN Russia", "rickyard", "Harlem neighborhood", "January", "Parapsychologist", "2014 New Year Honours", "Greg Gorman and Helmut Newton", "Prince Ioann Konstantinovich", "2015", "Winecoff Hotel fire", "the B-17 Flying Fortress bomber", "Bit Instant", "Hawaii", "The Emperor of Japan", "the Ruul", "Dana Fox", "remix", "Issaquah", "Ghana's Asamoah Gyan", "Scott Paul Carson", "John Snow", "Manchester, England", "\"Twister\"", "Tamaulipas", "\"boundary river\"", "\"Naked\"", "a software programmer", "Victoria, Duchess of Kent", "Las Vegas", "Cleveland, Ohio", "Brenton Thwaites", "AVN Adult Entertainment Expo", "Boyd Gaming", "Hirschman", "The Romantics", "the brain, muscles, and liver", "Chamberlain", "the French Open", "iceland", "are", "dual nationality", "Cpl. Richard Findley,", "rain", "iceland", "Packard", "customers are lining up for vitamin injections that promise"], "metric_results": {"EM": 0.484375, "QA-F1": 0.567906746031746}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, false, true, false, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, false, false, false, true, false, true, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.4, 0.3333333333333333, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.4, 1.0, 0.2222222222222222, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2884", "mrqa_hotpotqa-validation-2153", "mrqa_hotpotqa-validation-2421", "mrqa_hotpotqa-validation-3578", "mrqa_hotpotqa-validation-2541", "mrqa_hotpotqa-validation-4515", "mrqa_hotpotqa-validation-2021", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-5532", "mrqa_hotpotqa-validation-490", "mrqa_hotpotqa-validation-1676", "mrqa_hotpotqa-validation-3287", "mrqa_hotpotqa-validation-5544", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-5359", "mrqa_hotpotqa-validation-3126", "mrqa_hotpotqa-validation-886", "mrqa_hotpotqa-validation-5281", "mrqa_hotpotqa-validation-1284", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4117", "mrqa_hotpotqa-validation-618", "mrqa_hotpotqa-validation-2786", "mrqa_hotpotqa-validation-627", "mrqa_hotpotqa-validation-3419", "mrqa_naturalquestions-validation-7704", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-95", "mrqa_triviaqa-validation-6684", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-2698", "mrqa_searchqa-validation-14617"], "SR": 0.484375, "CSR": 0.5179570895522387, "EFR": 0.9696969696969697, "Overall": 0.7033120618498416}, {"timecode": 67, "before_eval_results": {"predictions": ["her brother, Brian", "2017", "Portugal", "North Atlantic Ocean", "eleven", "Anthony Hopkins", "Andrew Gold", "$19.8 trillion", "a hyper - active kinase, that confers an aberrant, ligand - independent, non-regulated growth stimulus to the cancer cells", "January 15, 2007", "1982", "XXXIX", "Brittany Paige Bouck", "31", "Louis XV", "White Sox", "won", "Gospel of Luke", "Executive chef Danny Veltri", "Narendra Modi", "Ossie Schectman", "Max Martin", "Marty Robbins", "Lightning thief", "state legislators of Assam", "by their grandfather Samuel who was also resurrected", "to feel close to his son", "Guy Berryman", "`` The person who has existence in two paradise", "Spanish / Basque", "1439", "Jason Momoa", "Terrence Howard", "1868 war veterans", "Elena Anaya", "body", "2014", "Pangaea or Pangea", "Key West, Florida", "Robin", "the foot of biblical Mount Sinai", "Vincent Price", "Nepal", "National Labor Relations Act of 1935", "December 12, 2017", "April 8, 2018", "Manhattan", "1969", "on location", "Neil Young", "provinces along the Yangtze River", "Aaron Burr", "Land of the Rising Sun", "Something In The Air", "Adam Dawes", "Robbie Gould", "Musicology", "two years", "Lashkar-e-Tayyiba", "HPV (human papillomavirus)", "ballet dancer", "necropolis", "Spider-Man", "Fidel Castro"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6924614448051948}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, true, true, false, false, false, false, false, false, false, true, false, false, true, false, true, false, false, true, false, true, true, false, true, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, true, true, true], "QA-F1": [0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.25, 0.0, 0.7999999999999999, 0.8, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.125, 1.0, 0.5, 0.375, 1.0, 0.9090909090909091, 1.0, 1.0, 0.2727272727272727, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8933", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-4329", "mrqa_naturalquestions-validation-1946", "mrqa_naturalquestions-validation-3770", "mrqa_naturalquestions-validation-5775", "mrqa_naturalquestions-validation-70", "mrqa_naturalquestions-validation-9546", "mrqa_naturalquestions-validation-7144", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-2068", "mrqa_naturalquestions-validation-9767", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-1531", "mrqa_naturalquestions-validation-2381", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-8796", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-6193", "mrqa_triviaqa-validation-4221", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-1629", "mrqa_newsqa-validation-1170", "mrqa_searchqa-validation-13420"], "SR": 0.546875, "CSR": 0.5183823529411764, "EFR": 0.9310344827586207, "Overall": 0.6956646171399594}, {"timecode": 68, "before_eval_results": {"predictions": ["Jeff Barry and Andy Kim", "Golde", "2018", "James Madison", "one season", "September 29, 2017", "c. 1000 AD", "Samantha Jo `` Mandy '' Moore", "March 1995", "1957", "Andrew McCarthy", "Anthony Mayfield", "Erica Rivera", "the epidermis", "the cavities and surfaces of blood vessels and organs throughout the body", "England, Wales, Ireland and the Isle of Man", "toasted wheat bun, a breaded chicken patty, shredded lettuce, and mayonnaise", "11 : 15 p.m.", "Sylvester Stallone", "Frankel, who had worked with Patricia Field on his feature - film debut Miami Rhapsody as well as Sex and the City", "Director of National Intelligence", "November 25, 2002", "S - shaped", "Marley & Me", "State Bar of Arizona", "19th - century India", "wintertime", "The Romantics", "W. Edwards Deming", "Danny and Pam", "Comancheria", "1992", "presidential representative democratic republic", "Eddie Murphy", "Vincent Price", "Andy Serkis", "Queen Taramis ( Sarah Douglas ) of Shadizar", "O'Meara", "Kimberlin Brown", "writ of certiorari", "international capital flows to developing countries", "The Blind Boys of Alabama", "Reveille", "Part XI of the Indian constitution", "StubHub Center", "Johannes Gutenberg", "plate tectonics", "Himadri Station", "Rajendra Prasad", "1997", "March 11, 2016", "b-man", "the Irish Red Setter", "Hattie McDaniel", "Tian Tan Buddha", "Bonkyll Castle", "Alf Clausen", "the area was sealed off, so they did not know casualty figures.", "Australia is asking an international court to weigh in on Japan's whale-hunting practices,", "Turkey from inside northern Iraq.", "piracy", "lifejackets", "french", "Josh"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6243516899766899}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, false, false, false, true, true, false, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4615384615384615, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9677", "mrqa_naturalquestions-validation-3523", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-2026", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-8950", "mrqa_naturalquestions-validation-9877", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6995", "mrqa_naturalquestions-validation-8027", "mrqa_naturalquestions-validation-4605", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-7051", "mrqa_naturalquestions-validation-10557", "mrqa_naturalquestions-validation-1829", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-1340", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-516", "mrqa_hotpotqa-validation-3965", "mrqa_newsqa-validation-1785", "mrqa_newsqa-validation-101", "mrqa_newsqa-validation-1504", "mrqa_searchqa-validation-1397"], "SR": 0.578125, "CSR": 0.5192481884057971, "EFR": 0.9629629629629629, "Overall": 0.702223480273752}, {"timecode": 69, "before_eval_results": {"predictions": ["Syracuse University", "The Fault in Our Stars", "Stadio Olimpico in Rome, Italy", "Bolivian folk troupe", "water", "May 4, 2004", "Northumbrian", "1987", "American Horror Story", "Harare", "University of Nevada, Las Vegas (UNLV)", "Scars to Your Beautiful", "November of that year", "Alfond Stadium", "small family car", "Jos\u00e9 Bispo Clementino dos Santos", "David Arquette", "the Austrian Empire", "Russell T Davies", "Emile Ardolino", "Oliver Parker", "he turned 51, he died of cancer", "Snowball II", "Oakland", "FCI Danbury", "The Division of Barton", "Bob Day", "The parkway", "1875", "London", "Shut Up", "James Lofton", "Big Bad Wolf", "Dennis Hull,", "books, films and other media", "Iranian-American", "The Guest", "Bay of Fundy", "2009", "Umberto II of Italy", "2016", "War Is the Answer", "1891", "Supergirl", "DI Humphrey Goodman", "Cinderella", "1,382", "right-hand", "Syracuse", "Buckingham Palace", "Rhode Island", "The Church of England", "the Western Bloc ( the United States, its NATO allies and others )", "1972", "a person whose occupation is mainly to cut, dress, groom, style", "Rodgers & Hammerstein", "Ramadan", "the Dalai Lama's current \"middle way approach,\"", "70,000", "1994", "lord Peter Wimsey", "carbonic acid", "neon", "irish goose"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6800426136363635}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, false, true, true, false, false, false, false, true, false, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 0.4, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.0, 0.8, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5692", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-5821", "mrqa_hotpotqa-validation-5619", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4669", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3504", "mrqa_hotpotqa-validation-4315", "mrqa_hotpotqa-validation-3703", "mrqa_hotpotqa-validation-4352", "mrqa_hotpotqa-validation-3949", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-1371", "mrqa_hotpotqa-validation-278", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-5942", "mrqa_triviaqa-validation-2405", "mrqa_triviaqa-validation-4021", "mrqa_newsqa-validation-477", "mrqa_searchqa-validation-4425", "mrqa_searchqa-validation-10452", "mrqa_triviaqa-validation-7105"], "SR": 0.59375, "CSR": 0.5203125, "EFR": 1.0, "Overall": 0.70984375}, {"timecode": 70, "UKR": 0.724609375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-139", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1511", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2232", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2357", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3452", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3534", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4081", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4124", "mrqa_hotpotqa-validation-4130", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5099", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5383", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5611", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-875", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2437", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6849", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-9056", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-925", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1230", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1655", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2346", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2719", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2942", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2956", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3067", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3191", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3540", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3643", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-4178", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-45", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-720", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-845", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11005", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-12941", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13441", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13909", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14752", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5466", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7326", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-7903", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8363", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-10284", "mrqa_squad-validation-10352", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1498", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2123", "mrqa_squad-validation-215", "mrqa_squad-validation-2197", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3464", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-3904", "mrqa_squad-validation-4096", "mrqa_squad-validation-4469", "mrqa_squad-validation-457", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-4949", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-6636", "mrqa_squad-validation-682", "mrqa_squad-validation-6838", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-76", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8028", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9165", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-2976", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4953", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6784", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6879", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.802734375, "KG": 0.45625, "before_eval_results": {"predictions": ["November 17, 2017", "Melissa Disney", "the Archies", "writ of certiorari", "Edgar Lungu", "December 25", "Marcus Atilius Regulus", "Ahmad Givens", "You are a puzzle", "San Francisco", "mashed potato", "Gregor Mendel", "October 1, 2015", "Stone Street Studios", "Pittsburgh", "pit road", "the dome", "101.325 kPa", "1923", "Narin Niruttinanon", "Michael Schumacher", "dispense summary justice", "John Quincy Adams", "Ray Charles", "Days of Our Lives", "( 1 ) 2013", "epithelia", "Upstate New York", "Henry Purcell", "4,840", "Lord Irwin", "three", "Roman Reigns", "Efren Manalang Reyes", "the second Persian invasion of Greece", "money to specific federal government departments, agencies, and programs", "2004", "Fix You", "Ed Roland", "Jos\u00e9 Mart\u00ed", "16 August 1975", "when the forward reaction proceeds at the same rate as the reverse reaction", "Sumitra", "The management team", "Ravi River", "Sir Rowland Hill", "Pangaea", "Sally Field", "a cylinder of glass or plastic that runs along the fiber's length", "Harendra Coomar Mookerjee", "Barbara Windsor", "limestone", "The Hague", "The World is Not Enough", "from 1930 to 1974", "Leona Lewis", "Hong Kong Disneyland", "'We want to reset our relationship and so we will do it together.'\"", "25 percent", "Stuart Gaffney, media director for Marriage Equality USA,", "Roget", "Galileo Galilei", "Aristophanes", "Amy Bishop Anderson,"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6880952380952381}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.7142857142857143, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428572, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-5675", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-8470", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-1636", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-1119", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-3246", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-9634", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-10533", "mrqa_naturalquestions-validation-7058", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-8326", "mrqa_hotpotqa-validation-2588", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-260", "mrqa_searchqa-validation-10531", "mrqa_newsqa-validation-2288"], "SR": 0.609375, "CSR": 0.5215669014084507, "EFR": 0.92, "Overall": 0.6850321302816902}, {"timecode": 71, "before_eval_results": {"predictions": ["Jack", "Chester", "pinball machine", "76,416", "Rawhide", "Chris Weidman", "University of Southern California Trojans", "Orpheus", "Martin Scorsese", "Citgo", "Kagoshima Airport", "Southern Progress Corporation", "Laura Dern", "British Conservative Party", "Mickey's PhilharMagic", "45%", "1241", "Hamburger SV", "Kolkata", "Liga MX", "Queen City", "Lithuanian Basketball League", "Texas", "sixteen", "Rob Reiner", "The R-8 Human Rhythm Composer", "WAMC", "126,202", "Claude", "Ribosomes", "Sacramento Kings", "1945", "Fairfax County", "Syracuse University", "Sir Thomas Daniel Courtenay", "Martin \"Marty\" McCann", "Green Mountain", "Republic of Maldives", "Uzbekistan", "Ella", "1986", "0.500", "William Bradley DuVall (born September 6, 1967)", "Coronation Street", "Duke", "Richard Masur", "anabolic\u2013androgenic steroids", "diving duck", "House of Hohenstaufen", "local South Australian and Australian produced content", "EBSCO Industries Inc.", "Georgia Groome", "as a pH indicator, a color marker, and a dye", "Thomas Edison", "wool", "Runic", "cycling", "Brett", "58 people.", "Security officer Stephen Johns reportedly opened the door for the man police say was", "osprey", "Fyodor Dostoevsky", "Turtle Wax", "The company says it recycles 100% of its byproducts which supplies 80% of the operation energy at the plant."], "metric_results": {"EM": 0.515625, "QA-F1": 0.6075024801587302}, "metric_results_detailed": {"EM": [false, false, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.25, 0.3333333333333333, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1395", "mrqa_hotpotqa-validation-2612", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-1220", "mrqa_hotpotqa-validation-1175", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-2938", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-1000", "mrqa_hotpotqa-validation-5116", "mrqa_hotpotqa-validation-5277", "mrqa_hotpotqa-validation-3621", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4550", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-1545", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-2592", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-5763", "mrqa_naturalquestions-validation-7849", "mrqa_triviaqa-validation-4244", "mrqa_newsqa-validation-3956", "mrqa_newsqa-validation-2439", "mrqa_searchqa-validation-14348", "mrqa_newsqa-validation-1048"], "SR": 0.515625, "CSR": 0.521484375, "EFR": 0.967741935483871, "Overall": 0.6945640120967742}, {"timecode": 72, "before_eval_results": {"predictions": ["the theory of direct scattering and inverse scattering", "doctoral", "Australian Supercars Championship", "The Jacksonville Jaguars", "\"Crossed: Psychopath\"", "U.S. Navy", "\"gemeinn\u00fctzige\"", "B507", "Laurie Metcalf", "Three-card brag", "\"All Good Gifts\"", "freshman", "Free Range Films", "Mondays", "\"Beauty and the Beast\"", "Jehovah", "Prime Minister of Pakistan", "Kate Garraway", "Newport", "Tomorrowland", "1985", "David Michael Bautista Jr.", "1002", "1", "John Meston", "late 19th and early 20th centuries", "Terrence Alexander Jones", "1967", "Casey Bond", "actress", "Chancellor of Austria", "Naomi Elaine Campbell", "The dyers of Lincoln", "2013\u201314", "Ghostbusters Spooktacular", "an American financier", "Maldives", "Them", "its air-cushioned sole", "4,972", "Brittany Snow", "Perth, Western Australia", "Harry F. Sinclair", "Interstate 22", "William McKinley", "Denmark", "French", "lieutenant general", "Indian", "Sir Philip Anthony Hopkins", "Raimond Gaita", "In 2015, 13.5 % ( 43.1 million ) of Americans lived in poverty", "1980", "Isaiah Amir Mustafa", "Michigan", "David Letterman", "Australia", "seven", "Congress", "July", "Michael Angelo", "airplanes", "the Confederate Memorial", "charge"], "metric_results": {"EM": 0.515625, "QA-F1": 0.593888013028638}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, true, true, true, true, true, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.1818181818181818, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4830", "mrqa_hotpotqa-validation-3467", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1391", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-294", "mrqa_hotpotqa-validation-1612", "mrqa_hotpotqa-validation-4940", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3314", "mrqa_hotpotqa-validation-3320", "mrqa_hotpotqa-validation-3380", "mrqa_hotpotqa-validation-1341", "mrqa_hotpotqa-validation-723", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-1007", "mrqa_naturalquestions-validation-9824", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-982", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-270", "mrqa_searchqa-validation-3973", "mrqa_searchqa-validation-16142", "mrqa_searchqa-validation-5340"], "SR": 0.515625, "CSR": 0.5214041095890412, "EFR": 1.0, "Overall": 0.7009995719178083}, {"timecode": 73, "before_eval_results": {"predictions": ["SAS", "1972", "Indianapolis, Indiana", "a controversial public figure", "1970", "the 1745 rebellion of Charles Edward Stuart", "at least 96", "the life of USA Olympian and army officer Louis \"Louie\" Zamperini", "West African descendants", "Rigoletto", "James Stenbeck", "December 19, 1998", "Philip Mark Quast", "November 20, 1942", "Argentine cuisine", "22 September 2015", "Matt Lucas", "Waylon Smithers", "2006", "Scotland", "Lakshmibai", "Umberto II", "England", "beer and soft drinks", "once", "C. H. Greenblatt", "mermaid", "1535", "There Is Only the Fight", "England", "Field Marshal John Standish Surtees Prendergast Vereker, 6th Viscount Gort & Two Bars", "Laurel, Mississippi", "antelope", "Wayne Conley", "choux", "1802", "Key West", "Pablo Escobar", "David Michael Bautista", "Iftikhar Ali Khan", "October 25, 1881", "Bob Dylan", "Chief Creative Officer", "Harrison Ford", "PBS", "In his prime, the brake over (also known as the brakeada, the den\u00edlson, or the scissors) is a dribbling move, or feint, in football", "1943", "wineries", "Animorphs", "Jeff Meldrum", "quantum mechanics", "early - to - mid fourth century", "in Pebble Beach", "$1.528 billion", "england", "Jeffrey Archer", "Oliver Wendell \"Spearchucker\" Jones", "in downtown Houston", "celebrities", "This is meant to be the leading edge,\" Hethmon said. \"If you are going to work on developing a state-based response to this enormous problem -- the lack of a national immigration policy", "Ohio", "beaver", "William Somerset Maugham", "to wound and weaken bulls in the fight, attached to their backs, some spattered with fake blood"], "metric_results": {"EM": 0.5, "QA-F1": 0.6142755681818182}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.6, 1.0, 0.3636363636363636, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.14285714285714285, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-4154", "mrqa_hotpotqa-validation-1423", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4394", "mrqa_hotpotqa-validation-5363", "mrqa_hotpotqa-validation-1664", "mrqa_hotpotqa-validation-957", "mrqa_hotpotqa-validation-5747", "mrqa_hotpotqa-validation-4177", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-3363", "mrqa_hotpotqa-validation-1146", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5332", "mrqa_hotpotqa-validation-838", "mrqa_hotpotqa-validation-2364", "mrqa_hotpotqa-validation-1116", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-4547", "mrqa_triviaqa-validation-535", "mrqa_triviaqa-validation-7264", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-3584", "mrqa_searchqa-validation-3745", "mrqa_triviaqa-validation-4080"], "SR": 0.5, "CSR": 0.5211148648648649, "EFR": 0.96875, "Overall": 0.694691722972973}, {"timecode": 74, "before_eval_results": {"predictions": ["1947", "Bury Football Club", "Nye County", "1930s and 1940s", "two", "a governor", "40 Acres", "The Panther", "al-Qaeda", "Soviet Union", "India", "Skyscraper", "17 October 2006", "April 1, 1949", "Big Machine Records", "Brady Haran, a former BBC video journalist", "Jesus", "Allen Stewart Konigsberg", "liquidambar styraciflua", "Elliot Fletcher", "Joe McCoy and Memphis Minnie", "Martha Wainwright", "Christopher McCulloch", "Strange Interlude", "1989 until 1994", "Tallahassee City Commission", "Royal", "Mika H\u00e4kkinen", "World Music Awards", "1998", "Peter 'Drago' Sell,", "\"Barney Miller\"", "\"Coyote Ugly\"", "\"Odorama\", whereby viewers could smell what they saw on screen through scratch and sniff cards", "Cersei Lannister", "Andrew Stephen Roddick", "Bigfoot (also known as Sasquatch) is a simian-like creature", "Kim Jong-hyun", "College Football Scoreboard", "A Boltzmann machine", "January 1930", "12", "the \"Pour le M\u00e9rite\" 1", "Franklin, Indiana", "The virus is zoonotic,", "October 6, 1931", "New Boston Air Force Station", "Herb Brooks", "Elizabeth River", "Metrolink", "\"Complex\" magazine", "the oneness of the body, the church, through what Christians have in common, what they have communion in", "Covington, Kentucky", "Elizabeth Dean Lail", "the women's doubles", "Newcastle Falcons", "Rihanna", "Sarah, wore an outfit from designer Britt Lintner to greet President Obama and his wife, Michelle,", "order after demonstrators rose up across Greece", "a nuclear weapon", "Pirates of the Burning Sea", "In 1997, the City of Bridgeport, Connecticut (pop. 140,000) received a Best Practices award from the US Conference of Mayors", "Gag", "Stem cells"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6835813492063492}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, false, true, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 0.28571428571428575, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.125, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.8571428571428571, 1.0, 0.5, 0.3333333333333333, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.125, 0.25, 1.0, 0.4, 0.1111111111111111, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-1765", "mrqa_hotpotqa-validation-2520", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4972", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2554", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-2523", "mrqa_hotpotqa-validation-1591", "mrqa_hotpotqa-validation-3634", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-5475", "mrqa_hotpotqa-validation-5042", "mrqa_hotpotqa-validation-4462", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-2579", "mrqa_triviaqa-validation-2322", "mrqa_newsqa-validation-3778", "mrqa_newsqa-validation-122", "mrqa_searchqa-validation-5810", "mrqa_searchqa-validation-14546"], "SR": 0.546875, "CSR": 0.5214583333333334, "EFR": 1.0, "Overall": 0.7010104166666666}, {"timecode": 75, "before_eval_results": {"predictions": ["France", "Shanghai", "Paul Bunyan", "Tom Baker", "Pandora", "Cheetham Close", "Alaska", "Cowboy Builders", "The DMC-12", "VXX", "Lundy Island", "Enterprise", "David Hockney", "cylinder", "Janis Joplin", "A.K.A. pasta spirals", "Humphrey Bogart", "Christopher Robin", "Antoine Lavoisier", "1960", "in the Turrialba Valley", "King County Executive", "Steely Dan", "Secretary of State", "Jane Austen", "the Bolshevik Revolution", "lime", "The Live Read of Space Jam", "littoral", "Venus", "Declaration of Independence", "decorat", "Hot Chocolate", "Jim Peters", "Armageddon", "Kansas", "carry On Cleo", "Mitte", "Neuna", "salmon", "the Old West", "the Island", "Project Gutenberg", "Bloodaxe", "California", "Nissan", "Isar", "A View to a Kill", "President Salvador Allende", "The Green Mile", "Poland", "around 1600 BC", "Sean O' Neal", "May 18, 2018", "Dissection", "Baden-W\u00fcrttemberg", "March", "$60 billion", "Leo Frank,", "$500,000", "A", "Azkaban", "conga drums", "Vibe"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5559237637362637}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.30769230769230765, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2230", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-6826", "mrqa_triviaqa-validation-451", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-447", "mrqa_triviaqa-validation-7705", "mrqa_triviaqa-validation-7384", "mrqa_triviaqa-validation-444", "mrqa_triviaqa-validation-3553", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-2721", "mrqa_triviaqa-validation-191", "mrqa_triviaqa-validation-3759", "mrqa_triviaqa-validation-225", "mrqa_triviaqa-validation-4612", "mrqa_triviaqa-validation-85", "mrqa_triviaqa-validation-2963", "mrqa_triviaqa-validation-3735", "mrqa_triviaqa-validation-1493", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-6727", "mrqa_naturalquestions-validation-5682", "mrqa_naturalquestions-validation-7549", "mrqa_hotpotqa-validation-65", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3850", "mrqa_searchqa-validation-15506"], "SR": 0.515625, "CSR": 0.5213815789473684, "EFR": 0.967741935483871, "Overall": 0.6945434528862477}, {"timecode": 76, "before_eval_results": {"predictions": ["brutal brutality", "a \"happy ending\" to the case.", "a head injury.", "Bryant Purvis", "1983", "Chaffetz", "Bryant Purvis", "jazz", "$30 million,", "1994,", "dental work", "peanuts, nuts, shellfish and fish", "fusion teams", "April 2010.", "ran out", "October 19,", "Addis Ababa,", "a president who understands the world today, the future we seek and the change we need. We need Barack Obama as the next president of the United States.", "five", "St. Louis.", "whether findings revealed if Gadhafi suffered the wound in crossfire or at close-range -- a key question that has prompted the United Nations and international human rights groups to call for an investigation", "off Somalia's coast.", "the Indian army and separatist militants in Indian-administered Kashmir", "The Charlie Daniels Band,", "France", "planning processes are urgently needed", "Wednesday", "Obama", "\"a whole new treasure trove of fossils\"", "and renewable energy at home everyday,\"", "insect stings,", "Kenneth Cole", "one-shot victory", "Natalie Cole's", "\"The people kill him with the blocks, because the people are angry. They are not hungry, they are angry,\"", "Hundreds", "California-based Current TV", "Colorado prosecutor", "100%", "shark River Park in Monmouth County", "Galveston, Texas, to Veracruz, Mexico,", "nine-wicket", "70,000 or so", "the Nazi war crimes suspect", "his former caddy,", "three", "returned to his home and was showing several folks his many guns when he snapped his double-barreled 12-gauge", "named his company Polo", "overhaul domestic policies", "the administration's progress, while we also encourage him to 'evolve faster' on supporting full marriage equality,\"", "Patrick McGoohan,", "Hathi Jr", "minimum thermodynamic work ( i.e. energy ) needed to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "Bacon", "raw hides", "Gustave III, ou Le bal masque", "fort boyard", "1,521", "British", "Edward R. Murrow", "dizzy germany", "Duncan", "dwight d. Eisenhower", "neo-Nazi"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6408980598508995}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, true, true, true, false, true, false, true, false, true, false, true, false, false, false, true, false, true, false, true, false, false, false, false, false, true, true, true, false, false, false, true, true, true, false, false, false, true], "QA-F1": [0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 0.0689655172413793, 1.0, 0.9333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.23529411764705882, 1.0, 0.5, 1.0, 0.47619047619047616, 1.0, 0.8, 1.0, 0.4, 0.8, 0.5714285714285715, 1.0, 0.3, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.23529411764705882, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2382", "mrqa_newsqa-validation-225", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-426", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-2385", "mrqa_newsqa-validation-3894", "mrqa_newsqa-validation-2610", "mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-3723", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-530", "mrqa_newsqa-validation-2595", "mrqa_newsqa-validation-1047", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-4012", "mrqa_newsqa-validation-1717", "mrqa_newsqa-validation-2811", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-1011", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-259", "mrqa_newsqa-validation-2061", "mrqa_triviaqa-validation-7570", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-6091", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-3999", "mrqa_searchqa-validation-7657"], "SR": 0.53125, "CSR": 0.5215097402597403, "EFR": 1.0, "Overall": 0.701020698051948}, {"timecode": 77, "before_eval_results": {"predictions": ["Terry the Tomboy", "Gatwick Airport (also known as London Gatwick)", "Abdul Razzak Yaqoob", "Robert Matthew Hurley", "William Shakespeare", "Syracuse University", "American", "\"Peshwa\" (Prime Minister)", "(also the Free Republic of Franklin or the State of Frankland)", "Lowe's Companies, Inc.", "\"Traumnovelle\" (\"Dream Story\")", "Hindi", "\"Big Mamie\"", "first", "Katherine Murray Millett", "The 2000 Summer Olympic Games, officially known as the Games of the XXVII Olympiad", "Leucippus", "\"good\" by Ofsted", "Columbine", "381.6 days", "Richard Strauss", "South West Peninsula League", "1994", "Yorgos Lanthimos,", "Roscoe Lee Browne", "the \"Black Abbots\"", "chocolate-colored", "more than 265 million", "\"The Brothers Karamazov\"", "Netrobalane canopus", "Telugu and Tamil film industries", "Australian-American", "Albert Park", "\"In Another's Eyes\"", "Stu Henderson", "Charles and Thomas Guard, the British born sons of Howard Guard", "1919", "Elise Stefanik", "Lower Manhattan, New York City", "King of the Polish-Lithuanian Commonwealth", "pop music and popular culture", "Almeda Mall", "Ronald Joseph Ryan", "robot Overlords", "Doctor of Philosophy", "Anne of Green Gables", "Madeleine L' Engle", "February 5, 2015", "Nathan Bedford Forrest", "Operation Overlord", "capital crimes or capital offences", "the Tigris and Euphrates rivers", "between 1765 and 1783", "2001", "(Rita) Hayworth", "Heisenberg", "conservative", "\"You saw the joy that the British had, that the Americans had, and saw them here through their representatives celebrating and acting as if we Zimbabwe are either an extension of Britain or... America.", "change course", "\"It's hard for everyone... I thought it was better for me here,\"", "Lotus", "pig in a poke", "October", "The Transportation Security Administration"], "metric_results": {"EM": 0.546875, "QA-F1": 0.669171626984127}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, true, true], "QA-F1": [1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4, 0.5333333333333333, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.25, 1.0, 0.5, 0.8, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-287", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-2583", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-266", "mrqa_hotpotqa-validation-2550", "mrqa_hotpotqa-validation-3013", "mrqa_hotpotqa-validation-2205", "mrqa_hotpotqa-validation-2974", "mrqa_hotpotqa-validation-2306", "mrqa_hotpotqa-validation-1615", "mrqa_hotpotqa-validation-5420", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-5199", "mrqa_hotpotqa-validation-2468", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-3130", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-4061", "mrqa_naturalquestions-validation-3515", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-2009", "mrqa_newsqa-validation-3946", "mrqa_newsqa-validation-2651"], "SR": 0.546875, "CSR": 0.5218349358974359, "EFR": 1.0, "Overall": 0.7010857371794872}, {"timecode": 78, "before_eval_results": {"predictions": ["albatross", "Gian Lorenzo Bernini", "the Land of the Hummingbird", "Denzel Washington", "a prologue", "Ben- Hur", "al Capone", "a prism", "Bucureti", "Tennessee", "Dick Wolf", "a Mace", "Helena Bonham Carter", "Cincinnati", "Robinson Crusoe", "Miss Havisham", "Thames", "at the top", "combust", "New Jersey", "Tarsus", "gold rush", "grain", "Eli Whitney", "Breckenridge", "the \"Dongfanghong-I\"", "bishops", "Esperanto", "the Hundred Years War", "Mending Wall", "Twelfth Night", "Kurt Kelly", "the Cyclopes", "special warfare missions", "tamales", "Today Show", "Sally Field", "earmark", "Turin", "a letter", "septum", "the Golden Girls", "polo", "wikipedian", "the Maritimes", "1773", "Richard Daley", "nanocrystals", "Bee", "the best man", "Missouri Waltz", "The Romantics", "Danny Veltri", "a popular and influential campaign song of the Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "breath", "the Tower of London", "Wildcats", "Knowlton Hall", "Ronald Wilson Reagan", "\"Elysium\"", "U.S. relief effort in Haiti", "183", "Jacob Zuma", "1994"], "metric_results": {"EM": 0.625, "QA-F1": 0.676223830049261}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, true, true, false, true, true, false, true, true, true, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, true, true, false, false, false, true, true, false], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.20689655172413793, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14500", "mrqa_searchqa-validation-14670", "mrqa_searchqa-validation-7064", "mrqa_searchqa-validation-9428", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-10493", "mrqa_searchqa-validation-5575", "mrqa_searchqa-validation-7541", "mrqa_searchqa-validation-14621", "mrqa_searchqa-validation-8526", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-7331", "mrqa_searchqa-validation-6796", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-3743", "mrqa_searchqa-validation-5831", "mrqa_naturalquestions-validation-4552", "mrqa_triviaqa-validation-3185", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-4713", "mrqa_newsqa-validation-93", "mrqa_triviaqa-validation-3363"], "SR": 0.625, "CSR": 0.5231408227848101, "EFR": 0.9583333333333334, "Overall": 0.6930135812236287}, {"timecode": 79, "before_eval_results": {"predictions": ["Marrix", "pale yellow to golden in color", "Montgomery", "fish", "six", "Unicorn", "Ross Bagdasarian", "Whiskas", "giuseppe encore, second right, jumps Becher's Brook", "a keat", "kitchen", "a glassworks", "Mujib", "a scarlet tanager", "Jack Lemmon", "a long jump", "Kipps: The Story of A Simple Soul", "a pound", "Venezuela", "port Talbot", "President Ford", "giuseppe gooch", "golf", "16 ounces", "Hugh Hefner", "Florentius", "Kofi Annan", "I Faraglioni", "right", "George Eliot", "Richard II", "national Space Centre", "mountain", "a form of contactless communication", "the Rock of Gibraltar", "philistines", "South African", "Topeka", "Brazil", "George Osborne", "The Good Life", "Florence", "Sicily", "Peter Gabriel", "Space Oddity", "the Smiths", "Seth", "Jeffery Deaver", "\u00c9dith Piaf", "Cuban cigars", "Henry Dunant", "Asuka", "in Canada south of the Arctic", "Teri Hatcher", "Ballarat Bitter", "Mani", "\"the backside", "110 mph,", "the union has sent his clients threatening letters for using his company, staged noisy protests, confronted employees, blocked building entrances and even released balloons in a client's building", "a common enemy to both countries.", "Mike Rowe", "Enigma", "Edinburgh", "obscenity"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6091327308362369}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, false, true, false, true, false, true, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true], "QA-F1": [0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.22222222222222224, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 0.04878048780487805, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-5320", "mrqa_triviaqa-validation-2346", "mrqa_triviaqa-validation-11", "mrqa_triviaqa-validation-7743", "mrqa_triviaqa-validation-1090", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-7311", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-1601", "mrqa_triviaqa-validation-4156", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-2684", "mrqa_triviaqa-validation-630", "mrqa_triviaqa-validation-3787", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-7729", "mrqa_triviaqa-validation-5747", "mrqa_triviaqa-validation-2277", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-438", "mrqa_triviaqa-validation-3545", "mrqa_triviaqa-validation-6818", "mrqa_naturalquestions-validation-1872", "mrqa_hotpotqa-validation-507", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2234"], "SR": 0.5625, "CSR": 0.5236328125, "EFR": 0.9642857142857143, "Overall": 0.6943024553571429}, {"timecode": 80, "UKR": 0.712890625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1619", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2229", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3967", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4646", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5343", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5468", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5683", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10559", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3330", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6346", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8208", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1527", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-1998", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2149", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2252", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2959", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3083", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3621", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4025", "mrqa_newsqa-validation-4095", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-1011", "mrqa_searchqa-validation-10176", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10493", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-14094", "mrqa_searchqa-validation-14204", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-16473", "mrqa_searchqa-validation-16534", "mrqa_searchqa-validation-16733", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-16891", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-5457", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7045", "mrqa_searchqa-validation-7179", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8086", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8807", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-944", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2248", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3750", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4669", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-5869", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-7932", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8500", "mrqa_squad-validation-8529", "mrqa_squad-validation-8719", "mrqa_squad-validation-8754", "mrqa_squad-validation-8966", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-2095", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-221", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3446", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3750", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4646", "mrqa_triviaqa-validation-4663", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5928", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6791", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7511", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-920", "mrqa_triviaqa-validation-934"], "OKR": 0.8125, "KG": 0.478125, "before_eval_results": {"predictions": ["eight", "habsburg monarchy", "jochebed", "Lucy", "hedgehog", "liqueur", "Rod Stewart", "Hungary", "Greek", "groucho Marx", "USS Constitution", "collage", "To Kill a Mockingbird", "fish", "cincy", "1966", "Benedictine", "Billy Fury", "Tuesday", "Py", "whist", "George Clooney", "in the front offices", "A4", "Mussolini", "George Osborne", "Margaret Thatcher", "peter davison", "sheep", "Abram", "strictly come dancing", "Chicago", "Hague", "kyiawar peninsula", "indigo montoya", "nixon", "Temple of the Dog", "Aleister Crowley", "Elton John", "armada", "setts", "batsman", "menelaus", "word", "Saddam Hussein", "Tombstone", "meeinasis", "Indonesia", "lenford Christie", "Swiss", "Daedalus", "in the books of Exodus and Deuteronomy", "starch", "David Ben - Gurion", "IndyCar Series", "Tetrahydrogestrinone", "7pm", "Ma Khin Khin Leh,", "prisoners at the South Dakota State Penitentiary", "Osan Air Base.", "chest", "julie julius", "think big", "on the inner edge of the Nebula Arm"], "metric_results": {"EM": 0.5, "QA-F1": 0.5119047619047619}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.47619047619047616]}}, "before_error_ids": ["mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-5231", "mrqa_triviaqa-validation-7274", "mrqa_triviaqa-validation-5921", "mrqa_triviaqa-validation-7182", "mrqa_triviaqa-validation-5116", "mrqa_triviaqa-validation-96", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-6871", "mrqa_triviaqa-validation-5580", "mrqa_triviaqa-validation-91", "mrqa_triviaqa-validation-4960", "mrqa_triviaqa-validation-7439", "mrqa_triviaqa-validation-7475", "mrqa_triviaqa-validation-6292", "mrqa_triviaqa-validation-4563", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-2903", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-2928", "mrqa_triviaqa-validation-5168", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-7616", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-6791", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9726", "mrqa_hotpotqa-validation-411", "mrqa_newsqa-validation-2488", "mrqa_searchqa-validation-4839", "mrqa_searchqa-validation-4136", "mrqa_naturalquestions-validation-808"], "SR": 0.5, "CSR": 0.523341049382716, "EFR": 1.0, "Overall": 0.7053713348765431}, {"timecode": 81, "before_eval_results": {"predictions": ["Cleckheaton", "the Haitian Revolution", "pommel horse", "15", "jon pertne", "jon pertenhoff", "bear", "cannons", "Columba", "jon pertah", "power outage", "snapdragons", "weather", "For Gallantry", "fillmore", "1951", "drake", "Venice", "Cambridge", "meerkat", "Saturn", "Liverpool", "baltimore", "dwain chambers", "bexhill", "Laos", "Florida", "Minute Maid", "vanilla", "wirt", "Apocalypse Now", "drake", "let It Snow", "turkish", "turkish", "edwina Currie", "mezzikovsky", "turkish", "india", "Charlotte Corday", "algiers", "the QM2", "golf", "yellow", "drake", "Trenton", "hose", "sea otter", "cardamom", "Ghana", "jon pertin", "243 days", "the Sunni Muslim family", "Terry Reid", "a leg injury", "8 May 1989", "David Yates", "Mohamed Mohamud Qeyre", "15-year-old", "The paper said the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "vote", "zechariah", "bison", "baltimore"], "metric_results": {"EM": 0.484375, "QA-F1": 0.490625}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, true, false, true, false, false, true, false, false, false, true, true, false, true, false, false, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, false, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6689", "mrqa_triviaqa-validation-2392", "mrqa_triviaqa-validation-2315", "mrqa_triviaqa-validation-6399", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-5684", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-2942", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-4943", "mrqa_triviaqa-validation-1944", "mrqa_triviaqa-validation-3108", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6414", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-7053", "mrqa_triviaqa-validation-6692", "mrqa_triviaqa-validation-2837", "mrqa_triviaqa-validation-262", "mrqa_triviaqa-validation-5145", "mrqa_triviaqa-validation-7204", "mrqa_triviaqa-validation-704", "mrqa_naturalquestions-validation-9728", "mrqa_naturalquestions-validation-522", "mrqa_hotpotqa-validation-3778", "mrqa_newsqa-validation-541", "mrqa_newsqa-validation-2984", "mrqa_searchqa-validation-8180", "mrqa_searchqa-validation-4624", "mrqa_searchqa-validation-16755", "mrqa_searchqa-validation-14428"], "SR": 0.484375, "CSR": 0.5228658536585367, "EFR": 0.9696969696969697, "Overall": 0.6992156896711013}, {"timecode": 82, "before_eval_results": {"predictions": ["2-digit", "the chord", "thailand", "fibroblasts", "Yardbirds", "Brazil", "1123", "prime minister", "bruce alexander", "niece", "new manhunt 2", "Carson City", "spirit-lifting jingle", "jewellers", "prawns", "martina hingis", "the wren", "Northwestern University", "Love Never Dies", "Tet", "island", "Frank Saul", "steam locomotive", "David Davis", "Eric Coates", "\u00e9dith piaf", "henry taylor", "anteros", "football", "wyoming", "antelope", "The Breakfast Club", "horse", "baldrick", "jonne", "lomond", "Salt Lake City", "red", "a horse", "the Battle of Austerlitz", "Venus", "the Compact Pussycat", "a goat", "Mercury", "guitar", "Celsius", "Phil Spector", "germany", "Hans Lippershey", "lazy Susan", "Yann Martel", "foreign investors", "the Gilbert building", "a German World War II super-heavy tank", "Starship Planet", "The Sleaford and North Hykeham by-election", "Four Weddings and a Funeral", "his mother, Katherine Jackson, his three children", "last survivor of the Titanic, 97-year-old Millvina Dean,", "1979", "Israel", "phil Mickelson", "the Crimean War", "Larry Ellison,"], "metric_results": {"EM": 0.59375, "QA-F1": 0.61640625}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.25, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-1361", "mrqa_triviaqa-validation-73", "mrqa_triviaqa-validation-2577", "mrqa_triviaqa-validation-7696", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-2705", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-2425", "mrqa_triviaqa-validation-4490", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-4817", "mrqa_triviaqa-validation-4127", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-4859", "mrqa_triviaqa-validation-5389", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-7246", "mrqa_hotpotqa-validation-3037", "mrqa_newsqa-validation-1953", "mrqa_newsqa-validation-1594", "mrqa_searchqa-validation-3536"], "SR": 0.59375, "CSR": 0.5237198795180723, "EFR": 1.0, "Overall": 0.7054471009036145}, {"timecode": 83, "before_eval_results": {"predictions": ["the arm", "Swedish", "Toyota", "March 19", "Julie Andrews", "rage", "scalp", "verona", "phil Spector", "fidelio", "the one the following week", "arthur daley", "Pisces", "pickled peppers", "Young Men's Christian Association", "topographic", "macbeth", "crossword puzzle", "Diana Ross", "President Reagan", "nettle leaves", "cartoons", "nautes", "riyal", "Jennifer Aniston", "baron edelman", "one Thousand and one", "john of gaunt", "sewing machine", "Bristol Aeroplane Company", "tenor sax", "king Tutankhamun", "colony", "fedora", "Indus valley", "Helen Gurley Brown", "david johnson", "krakatoa", "pinocchio", "Oregon", "bertrand bar\u00e8re de Vieuzac", "ed Emilio de Cavalieri", "the dashboard area of the car", "Israel", "rue", "Ken Platt", "HP.52 Hampden", "julia hargreaves", "Sarah Vaughan", "Mr Loophole", "bingo", "Humpty Dumpty and Kitty Softpaws", "Watson and Crick", "the appendicular skeleton ( 32 \u00d7 2 in the upper extremities including both legs )", "25 November 2015", "Jean Erdman", "Netherlands", "9 a.m.", "U.S. Holocaust Memorial Museum", "heightened interest in its breed -- Portuguese water dog", "Robert Langdon", "Parris Island", "watts", "sunflower"], "metric_results": {"EM": 0.546875, "QA-F1": 0.604882097069597}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, true, true, false, false, true, true, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.5714285714285715, 0.5, 0.3076923076923077, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4062", "mrqa_triviaqa-validation-1866", "mrqa_triviaqa-validation-2688", "mrqa_triviaqa-validation-6348", "mrqa_triviaqa-validation-2956", "mrqa_triviaqa-validation-6317", "mrqa_triviaqa-validation-2247", "mrqa_triviaqa-validation-987", "mrqa_triviaqa-validation-4471", "mrqa_triviaqa-validation-4917", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-273", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-2587", "mrqa_triviaqa-validation-3919", "mrqa_triviaqa-validation-4240", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-7495", "mrqa_triviaqa-validation-6094", "mrqa_triviaqa-validation-5028", "mrqa_triviaqa-validation-5470", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-667", "mrqa_naturalquestions-validation-7346", "mrqa_naturalquestions-validation-3468", "mrqa_naturalquestions-validation-893", "mrqa_hotpotqa-validation-413", "mrqa_newsqa-validation-144", "mrqa_searchqa-validation-9567"], "SR": 0.546875, "CSR": 0.5239955357142857, "EFR": 0.9310344827586207, "Overall": 0.6917091286945813}, {"timecode": 84, "before_eval_results": {"predictions": ["Eliyahu Dobkin", "is said to be unattainable", "Karina Smirnoff", "the public", "the eighth episode of Arrow's second season", "Major General Clarence L. Tinker", "2002", "1988", "the words spoken to Adam and Eve", "John Roberts", "Walter Farley", "butane", "due to Parker's pregnancy at the time of filming", "17 in October 2004", "a state or other organizational body that controls the factors of production", "Ben", "The Enchantress", "medieval", "Lou Rawls", "the tongue", "nano", "March of Dimes", "copper ( Cu ), silver ( Ag ), and gold ( Au )", "The management team", "Milan, Italy", "The Parlement de Bretagne", "Matt Flinders", "during prenatal development", "25 -- 30 \u00b0 C / km ( 28 -- 34 \u00b0 F / mi )", "anticonvulsant", "Ferm\u00edn Francisco de Lasu\u00e9n", "three", "Coriolis effect", "94 by 50 feet", "The enthalpy of fusion of a substance, also known as ( latent ) heat of fusion", "Washington metropolitan area", "Zoe McLellan as Meredith Brody", "Leslie", "ice giants", "2010", "1920s", "in different parts of the globe", "Empire of Japan", "cylinder of glass or plastic that runs along the fiber's length", "The Intolerable Acts", "Holly Unwin", "Internal epithelia", "Bob Dylan", "Latitude", "a medium of introduction for the beginning of Buddhism in China", "statistical", "Sikhism", "ronald reagan", "John Mortimer", "Florida", "niece", "the second", "Kitty Kelley,", "Strategic Arms Reduction Treaty", "the \"race for the future... and it won't be won with a president who is stuck in the past.\"", "cecilius calvert", "governor", "Ontario", "Medellin"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5769435017596782}, "metric_results_detailed": {"EM": [false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, false, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, false, true, false, true, true, false, false, false, true, true, true, true, true, true, false, false, false, false, false, true, true], "QA-F1": [0.0, 0.33333333333333337, 1.0, 1.0, 0.7692307692307692, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.823529411764706, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4444444444444445, 1.0, 1.0, 0.4, 1.0, 0.8, 0.9523809523809523, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.15384615384615385, 1.0, 0.8571428571428572, 1.0, 0.0, 1.0, 1.0, 0.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-2890", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-3836", "mrqa_naturalquestions-validation-7507", "mrqa_naturalquestions-validation-5921", "mrqa_naturalquestions-validation-25", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-4145", "mrqa_naturalquestions-validation-243", "mrqa_naturalquestions-validation-7021", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-1767", "mrqa_naturalquestions-validation-4442", "mrqa_naturalquestions-validation-1085", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-9104", "mrqa_naturalquestions-validation-8937", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-4382", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-5809", "mrqa_naturalquestions-validation-5555", "mrqa_newsqa-validation-4006", "mrqa_newsqa-validation-2355", "mrqa_newsqa-validation-430", "mrqa_searchqa-validation-13409", "mrqa_searchqa-validation-1600"], "SR": 0.421875, "CSR": 0.5227941176470587, "EFR": 0.918918918918919, "Overall": 0.6890457323131955}, {"timecode": 85, "before_eval_results": {"predictions": ["July 14, 2017,", "7 August 2021", "telecommunications, pharmaceuticals, aircraft, heavy machinery", "North Dakota", "Harrison Ford", "Nicolas Anelka", "18 - season", "flour and water", "Mahatma Gandhi", "J. Presper Eckert and John William Mauchly's ENIAC", "Missouri River", "Fa Ze Banks", "Orangeville, Ontario, Canada", "May 2016", "up to 100,000", "Kiss", "one season", "30 months", "October 2012", "winter festivals", "gesture in which the head is tilted in alternating up and down arcs along the sagittal plane", "July 2, 1928, fifteen months before the United Kingdom", "push the food down the esophagus", "Jackie Robinson", "The Lightning thief", "TLC - All That Theme Song 1 : 04", "24 hours, 39 minutes, and 35.244 seconds", "September 8, 2017", "William the Conqueror", "Abid Ali Neemuchwala", "Sri Lanka Podujana Peramuna, led by former president Mahinda Rajapaksa", "the city of Chicago", "The User State Migration Tool ( USMT )", "1807", "the season - five premiere episode `` Second Opinion ''", "Ming dynasty", "Saint Peter ( the keeper of the `` keys to the kingdom '' )", "1773", "September 25, 1987", "20 years from the filing date subject to the payment of maintenance fees", "a homodimer of 37 - kDa subunits", "Billie Jean King", "Internal epithelia", "March 2, 2016", "detritus from the settlement of the sedimentation", "Ravi River", "Michael Biehn", "12 to 36 months old", "Alice Cooper", "in Connecticut", "on a bronze plaque and mounted inside the pedestal's lower level", "earwigs", "acetone", "William Claude Dukenfield", "21 August 1986", "Taylor Swift", "2006", "8 to 10 inches of snow", "Mugabe", "South Africa", "Labour", "Charles Boyer", "Last of the Mohicans", "the michelada"], "metric_results": {"EM": 0.46875, "QA-F1": 0.583538742201426}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, true, false], "QA-F1": [1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.35294117647058826, 1.0, 0.18181818181818182, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1818181818181818, 1.0, 0.5, 0.125, 0.2222222222222222, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.5882352941176471, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.5714285714285715, 0.0, 0.6666666666666666, 0.42857142857142855, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-9530", "mrqa_naturalquestions-validation-7659", "mrqa_naturalquestions-validation-622", "mrqa_naturalquestions-validation-4032", "mrqa_naturalquestions-validation-824", "mrqa_naturalquestions-validation-5312", "mrqa_naturalquestions-validation-3750", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-1359", "mrqa_naturalquestions-validation-5413", "mrqa_naturalquestions-validation-5590", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-1368", "mrqa_naturalquestions-validation-1789", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-91", "mrqa_naturalquestions-validation-633", "mrqa_naturalquestions-validation-9409", "mrqa_naturalquestions-validation-1798", "mrqa_naturalquestions-validation-1008", "mrqa_naturalquestions-validation-8352", "mrqa_naturalquestions-validation-4225", "mrqa_naturalquestions-validation-3095", "mrqa_naturalquestions-validation-4633", "mrqa_triviaqa-validation-1705", "mrqa_hotpotqa-validation-3641", "mrqa_newsqa-validation-1019", "mrqa_newsqa-validation-656", "mrqa_searchqa-validation-5288", "mrqa_searchqa-validation-13973"], "SR": 0.46875, "CSR": 0.5221656976744187, "EFR": 0.9705882352941176, "Overall": 0.6992539115937072}, {"timecode": 86, "before_eval_results": {"predictions": ["Georgia Groome", "Glen W. Dickson", "Louis Hynes", "Pure Java", "David H. Splane", "the Soviet Union", "the Turco - Mongol Timurid dynasty of Central Asia", "July 4, 1776", "Germany", "blighted ovum or anembryonic gestation", "left hand ring finger", "the Arafura Sea", "The Michael Scott Paper Co", "a mathematical representation", "from statute or the Constitution itself", "2012", "Connecticut", "Lewis Hamilton", "Ace", "1998", "Brazil", "to form a higher alkane", "1947", "the Ramones", "Phoenix neighborhood of Ahwatukee", "Jurchen Aisin Gioro clan", "China", "\u00c9mile Gagnan", "New Mexico", "Nepal", "538", "the Twelvers", "the President", "R / T", "2018", "Gary Player", "1997", "Holden Nowell", "pathology", "332", "if the occurrence of one does not affect the probability of occurrence of the other", "the legs", "2018", "mitosis", "The Maidstone Studios in Maidstone, Kent", "the Outfield", "the International Border", "Samara Cook", "photoelectric", "1858", "1991", "George III", "Barbara Good", "Donington Park", "Kolkata", "The Handmaid's Tale", "fourth", "Takashi Saito,", "Herman Cain,", "billboards", "a hormone", "Beldar Conehead", "the House of Lords", "Tom Ellis"], "metric_results": {"EM": 0.390625, "QA-F1": 0.49955357142857143}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, false, true, false, false, false, false, false, true, false, true, false, false, true, false, true, false, false, false, false, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false], "QA-F1": [0.5714285714285715, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8333333333333333, 1.0, 0.0, 0.0, 0.0, 0.33333333333333337, 0.5, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.19999999999999998, 1.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4784", "mrqa_naturalquestions-validation-5485", "mrqa_naturalquestions-validation-3710", "mrqa_naturalquestions-validation-5010", "mrqa_naturalquestions-validation-6482", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-9903", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-1063", "mrqa_naturalquestions-validation-1878", "mrqa_naturalquestions-validation-5676", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-9181", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-9639", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-1338", "mrqa_naturalquestions-validation-4039", "mrqa_naturalquestions-validation-2420", "mrqa_naturalquestions-validation-2562", "mrqa_naturalquestions-validation-579", "mrqa_naturalquestions-validation-188", "mrqa_naturalquestions-validation-5822", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-8493", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6727", "mrqa_naturalquestions-validation-10554", "mrqa_triviaqa-validation-5546", "mrqa_hotpotqa-validation-4560", "mrqa_newsqa-validation-1122", "mrqa_newsqa-validation-3150", "mrqa_searchqa-validation-10490", "mrqa_searchqa-validation-15707", "mrqa_hotpotqa-validation-4281"], "SR": 0.390625, "CSR": 0.5206537356321839, "EFR": 0.9743589743589743, "Overall": 0.6997056669982317}, {"timecode": 87, "before_eval_results": {"predictions": ["Norway", "le belle femme skunk", "Romeo and Juliet", "The Golden Girls", "Niger", "Artemis", "using a vertical stroke with surrounding arcs", "Spanish", "driving Miss Daisy", "Malta", "Robert Galbraith", "Dubai", "Bristol", "Double Trouble", "France", "Barack Obama", "the Observer", "Yakutat, Alaska", "Captain Mark Phillips", "Brian Clough", "Peter Paul Rubens", "Pembrokeshire Coast National Park", "blood", "willy", "the 1500 meter event", "Port Moresby", "winter girl", "Robert Maxwell", "non-Orthodox synagogues", "Cambridge", "winnie Mae", "Richard Curtis", "Keswick", "Louis Le Vau", "horseshoes", "David Copperfield", "the Monkees", "Gatcombe Park", "india", "Charlotte", "Michael Phelps", "Vietnam", "Mumbai", "mumps", "stop motion creatures", "Sebastian Flyte", "The Eagle", "seaweed", "Hindi", "Madrid", "Paul Bunyan", "the Reverse - Flash", "the primary palate", "summer of 1990 and continued until 1992", "1986", "Chad", "1967", "the 3rd Platoon, A Company, 2nd Light armored Reconnaissance Battalion, based at Lejeune.", "1,500", "came forward Monday \"for the other women who couldn't or wouldn't.\"", "cholesterol", "volcano", "eight", "Baldwin"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true], "QA-F1": [1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3998", "mrqa_triviaqa-validation-994", "mrqa_triviaqa-validation-7459", "mrqa_triviaqa-validation-1508", "mrqa_triviaqa-validation-7209", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-4991", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-2780", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-1925", "mrqa_triviaqa-validation-4680", "mrqa_triviaqa-validation-7042", "mrqa_triviaqa-validation-225", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-8386", "mrqa_newsqa-validation-1861", "mrqa_searchqa-validation-1045", "mrqa_searchqa-validation-1657"], "SR": 0.6875, "CSR": 0.5225497159090908, "EFR": 0.9, "Overall": 0.6852130681818182}, {"timecode": 88, "before_eval_results": {"predictions": ["Cool Runnings", "five times", "an explosion and a fire", "video games", "elderships", "Andrew Davis", "Drowning Pool", "Lombardy", "7", "\"Realty Bites\"", "the 2007 Formula One season", "Florida Panthers", "capital crimes or capital offences", "10-metre platform event", "Saturday Night Live", "3,000 people", "beer and soft drinks", "Shohola Falls", "The Kennedy Center", "Saoirse Ronan", "H. R. Haldeman", "the Netherlands", "Miss Universe 2010 Ximena Navarrete", "John R. Leonetti", "Fat Albert", "Oklahoma State", "stoneware", "Prudence Jane Goward", "Ra.One", "Walcha", "egypt", "Manhattan Project", "Apatosaurus", "Currer Bell", "Anne Perry", "New Jersey", "Argentina", "Province of Syracuse", "David Irving", "Northside", "219", "Charice", "Harlem", "Francis Schaeffer", "Eric Whitacre", "George Balanchine", "Laura Dern", "pont de Grenelle", "Axl Rose", "the second", "one of the youngest publicly documented people to be identified as transgender", "Jason Lee", "1928", "October 30, 2017", "arthur", "bird", "Oregon", "a book.", "South Africa", "Russia", "14th", "teeth", "Lady Jane Grey", "Yemen"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6710937499999999}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, true, true, true, true, false, false, true, true, true, true, false, true, false, true, true, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.6, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.8, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333326, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3636", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-1405", "mrqa_hotpotqa-validation-4463", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-568", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3679", "mrqa_hotpotqa-validation-2211", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-5413", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-1870", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-1135", "mrqa_triviaqa-validation-3584", "mrqa_triviaqa-validation-5824", "mrqa_newsqa-validation-1389", "mrqa_searchqa-validation-15444", "mrqa_searchqa-validation-9394"], "SR": 0.609375, "CSR": 0.5235252808988764, "EFR": 0.96, "Overall": 0.6974081811797752}, {"timecode": 89, "before_eval_results": {"predictions": ["2012", "848", "Skipton", "Consigliere", "a midtempo hip hop ballad", "Tangerine Dream", "shortstop", "Francis Egerton", "Tamworth", "\"Teenage Dream\"", "Eric Edward Whitacre", "online role-playing video game", "Meryl Streep", "Karl Kraus", "water", "\"Orchard County\"", "he turned 51, he died of cancer", "1947", "2004", "1903", "170", "2013", "Boulder, Colorado", "Tian Tan Buddha", "Rocky Mountain Institute", "Macau, China", "La Nouba", "Stephanie Plum", "Memphis", "National Collegiate Athletic Association", "Humphrey the Bear", "Eve Hewson", "Leonard Cohen", "U2 360\u00b0 Tour", "Eielson Air Force Base in Alaska", "1,462", "Bachelor of Commerce", "the God of Israel", "three times", "Budget Rent a Car System, Inc.", "Dealey Plaza", "Terrence Jones", "Republican", "2015", "\u00c6thelstan", "black", "half a million acres", "Vikram, Jyothika and Reemma Sen", "Morning Edition", "the Gentleman Bandit", "50JJB Sports Fitness Clubs", "the Tigris and Euphrates rivers", "$2.187 billion", "the egg", "Goldie Hawn", "the Home Guard", "Tintin", "U.N.", "$40", "hooked up with Mildred, a younger woman of about 80, in March.", "Andy Rooney", "a volcano", "skinner", "Capitol Hill."], "metric_results": {"EM": 0.5625, "QA-F1": 0.6778645833333332}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.8, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.6666666666666666, 0.7499999999999999, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.6666666666666665, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1221", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-1501", "mrqa_hotpotqa-validation-3058", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-1074", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-3350", "mrqa_hotpotqa-validation-436", "mrqa_hotpotqa-validation-5712", "mrqa_hotpotqa-validation-526", "mrqa_hotpotqa-validation-1172", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-3763", "mrqa_hotpotqa-validation-673", "mrqa_hotpotqa-validation-4221", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-2833", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-5603", "mrqa_naturalquestions-validation-6931", "mrqa_newsqa-validation-1658", "mrqa_newsqa-validation-1981", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-339", "mrqa_searchqa-validation-4758"], "SR": 0.5625, "CSR": 0.5239583333333333, "EFR": 1.0, "Overall": 0.7054947916666666}, {"timecode": 90, "UKR": 0.673828125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1130", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2151", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2338", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2761", "mrqa_hotpotqa-validation-2762", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3005", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4175", "mrqa_hotpotqa-validation-4297", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4914", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5547", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1912", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-2745", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3295", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6771", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7443", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7600", "mrqa_naturalquestions-validation-7626", "mrqa_naturalquestions-validation-7901", "mrqa_naturalquestions-validation-7950", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8546", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2041", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2235", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2538", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4087", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-450", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11407", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14771", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-4977", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7657", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8561", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3813", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5325", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9531", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3703", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-570", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6193", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7717", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.828125, "KG": 0.5125, "before_eval_results": {"predictions": ["Delaware", "The Shirehorses", "Detroit", "Mortal Kombat X", "Bob Hill", "Pennsylvania State University", "Princess Aisha bint Hussein", "1951", "Muskogean", "Clyde", "aging issues", "Indooroopilly Shopping Centre", "Chow Tai Fook Enterprises", "Rhode Island School of Design", "Bill Walton", "Vanessa Hudgens", "1983", "Andrew Preston", "NBA Rookie of the Year", "Rhodesia", "Ed O'Neill", "My Backyard", "Lommel", "Bharat Ratna", "Aberdeenshire", "three", "1969", "Stalybridge Celtic Football Club", "Property management", "Vincent Anthony Guaraldi", "Pulitzer Prize", "ten", "2002", "Neon City", "New York", "1932", "Hopi", "American", "pop music", "Australian", "SKUM", "Rain Man", "Wake Atoll", "Carlos Santana", "1942", "2027", "historic buildings, arts, and published works", "great man", "The Washington Post", "143,007", "King James I", "died", "1970", "nucleus", "lung", "discus", "spach Zarathustra", "three", "police dogs", "\"increasingly aggressive\"", "Sierra Leone", "spain", "Airborne", "11th year in a row."], "metric_results": {"EM": 0.5625, "QA-F1": 0.700595238095238}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, false, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-699", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-4514", "mrqa_hotpotqa-validation-946", "mrqa_hotpotqa-validation-3610", "mrqa_hotpotqa-validation-3311", "mrqa_hotpotqa-validation-1048", "mrqa_hotpotqa-validation-5498", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-3202", "mrqa_hotpotqa-validation-1827", "mrqa_hotpotqa-validation-4647", "mrqa_hotpotqa-validation-2503", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-777", "mrqa_hotpotqa-validation-5537", "mrqa_hotpotqa-validation-1522", "mrqa_hotpotqa-validation-744", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-5403", "mrqa_naturalquestions-validation-6854", "mrqa_triviaqa-validation-5991", "mrqa_newsqa-validation-2935", "mrqa_newsqa-validation-414", "mrqa_newsqa-validation-3310", "mrqa_searchqa-validation-6970", "mrqa_searchqa-validation-10274"], "SR": 0.5625, "CSR": 0.5243818681318682, "EFR": 1.0, "Overall": 0.7077669986263736}, {"timecode": 91, "before_eval_results": {"predictions": ["Atlanta", "The Producers", "E.M. Forster", "James Joyce", "Jamestown", "Helen Hayes", "dollar", "Philip", "Dobermann", "Vermont", "Moses", "the Moors", "Margaret Mitchell", "Henrik Ibsen", "cruller", "the day of the Venus landing", "Hungary", "Sugar Smacks", "Groundhog Day", "New Balance Athletic Shoe", "the Siberian Husky", "Animal Crackers", "oxygen", "a platypus", "the Supreme Court justice", "a bicycle", "the Magic Mountain", "Fiji", "Manassas", "Jacqueline Kennedy Onassis", "Forbes", "Death Row", "Jimmy Hoffa", "a double jeopardy violation", "a rabbit", "a car buying scam", "Austria", "apartheid", "Browning", "Compound Interest", "Alison Ringwald", "The Home Depot", "the Marine Corps", "Ho Chi Minh", "Ghost", "Arkansas", "the Stone of Destiny", "remoulade", "Florida", "The Bee Gee", "Latin", "Jane Addams", "Thomas Chisholm", "a lone athlete ( Norman Pritchard )", "Ron Howard", "spider", "Istanbul", "Battle of the Rosebud", "40 Acres and a Mule Filmworks", "1851", "26", "$22 million", "Sixteen", "Ricky Nelson"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7702651515151515}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.3636363636363636, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14853", "mrqa_searchqa-validation-2638", "mrqa_searchqa-validation-4712", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-12826", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-10797", "mrqa_searchqa-validation-8663", "mrqa_searchqa-validation-524", "mrqa_searchqa-validation-13732", "mrqa_searchqa-validation-392", "mrqa_searchqa-validation-8137", "mrqa_searchqa-validation-10072", "mrqa_searchqa-validation-9931", "mrqa_searchqa-validation-15184", "mrqa_searchqa-validation-10600", "mrqa_searchqa-validation-13793", "mrqa_searchqa-validation-3990", "mrqa_searchqa-validation-13167", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-974"], "SR": 0.640625, "CSR": 0.5256453804347826, "EFR": 0.9565217391304348, "Overall": 0.6993240489130435}, {"timecode": 92, "before_eval_results": {"predictions": ["a bust", "A Chorus Line", "Pamplona", "French toast", "Pop-Tarts", "Aesop", "Shawnee", "postscript", "Babe Ruth", "brothels", "the Orinoco river", "John Bunyan", "Sicilian pizza", "Punjabi", "insulin", "James Riddle \" Jimmy\" Hoffa", "Utah", "Newton", "Cincinnati", "Alexander Hamilton", "the Timberland boot", "bronchoconstriction", "the Perseid", "Love Story", "the wall", "a person who carries luggage at a", "Davy Crockett", "the Basilica of St. Peter", "Penny Lane", "grease", "Kathleen Kennedy Townsend", "Henry Cavendish", "Israel", "euphoria", "Don Quixote", "Charlie and the Chocolate Factory", "Baboon", "Last Summer", "Yerushalayim", "heredity", "the Wild Thornberrys", "Michelle Ryan", "Selma Blair", "Wynona Judd", "a palace of salt", "the knightly club", "the HIV/AIDS", "the Smothers Brothers", "Henry Hudson", "Sidecar", "Ford", "The management team", "United States, the United Kingdom, and their respective allies", "2017", "table salt", "ThunderCats", "Annie", "AMC Entertainment Holdings, Inc.", "The Danny Kaye Show", "Montreal, Quebec", "Gary Brooker", "$150 billion", "Tennessee.", "Johannes Gutenberg"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7286458333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, true, true, true, true, true, true, true, true, false, false, false, false, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4213", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-2402", "mrqa_searchqa-validation-6725", "mrqa_searchqa-validation-16940", "mrqa_searchqa-validation-1094", "mrqa_searchqa-validation-13243", "mrqa_searchqa-validation-8294", "mrqa_searchqa-validation-9520", "mrqa_searchqa-validation-14776", "mrqa_searchqa-validation-9414", "mrqa_searchqa-validation-9090", "mrqa_searchqa-validation-1940", "mrqa_searchqa-validation-5443", "mrqa_searchqa-validation-8996", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-3896", "mrqa_searchqa-validation-9483", "mrqa_searchqa-validation-14310", "mrqa_searchqa-validation-8750", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-582", "mrqa_newsqa-validation-2150"], "SR": 0.640625, "CSR": 0.5268817204301075, "EFR": 0.9130434782608695, "Overall": 0.6908756647381955}, {"timecode": 93, "before_eval_results": {"predictions": ["the hat", "7-Eleven", "Oakland", "Australia", "the Grail", "the Hippopotamus", "Grant & Sherman", "carbon", "Elgar", "the fingers", "a carriage", "the catacombs", "Iraq", "the Cohans", "Jean Harlow", "the yardarm", "a fish", "February 29, 2016", "mistletoe", "the council", "Shia", "Tijuana", "Cleopatra", "Ryan Merriman", "the Capitol", "Bauhaus", "Homeland Security", "the Jumper", "the catfish", "paradise", "the Baltic Sea", "red hot Saddles", "Jean-Honor Fragonard", "While You Were Out", "2.25 Liters", "c", "Love Story", "Ramen", "President Harry Truman", "St. Louis", "the Confederate Flag", "chili rellenos", "Elizabeth Edward", "Stephen I", "Sgt. Pepper", "cheddar", "the Amistad", "Prince", "the Milky Way", "a noun", "Tycho Brahe", "displacement", "Office of Inspector General", "February 6, 2005", "congregation", "Harvard", "Jane Austen", "the Peninsular War", "The Sun", "24 NCAA sports", "the Red River", "jobs", "removal of his diamond-studded braces.", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6376302083333334}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, true, false, true, false, false, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 0.5, 0.5, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.5, 0.0, 0.125, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-7905", "mrqa_searchqa-validation-11754", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-2718", "mrqa_searchqa-validation-5239", "mrqa_searchqa-validation-9631", "mrqa_searchqa-validation-8845", "mrqa_searchqa-validation-13462", "mrqa_searchqa-validation-11296", "mrqa_searchqa-validation-5163", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-13212", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-9980", "mrqa_searchqa-validation-11618", "mrqa_searchqa-validation-10682", "mrqa_searchqa-validation-12543", "mrqa_searchqa-validation-7097", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-13274", "mrqa_searchqa-validation-5137", "mrqa_naturalquestions-validation-6993", "mrqa_triviaqa-validation-4180", "mrqa_triviaqa-validation-6153", "mrqa_hotpotqa-validation-5555", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-1260", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-4042"], "SR": 0.53125, "CSR": 0.5269281914893618, "EFR": 1.0, "Overall": 0.7082762632978724}, {"timecode": 94, "before_eval_results": {"predictions": ["Southend Pier", "9 February 2018", "the 7th century", "France's Legislative Assembly", "Peter Gardner Ostrum", "Jesse McCartney", "1902", "Jonathon Dutton", "February 27, 2007", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Chuck Noland", "his teenage role as the title character on the Disney Channel television series The Famous Jett Jackson ( 1998 -- 2001 )", "1911", "60 by West All - Stars", "around 1872", "the plane crash", "Majo to Hyakkihei 2", "aorta", "Pittsburgh", "the beginning of the American colonies", "James Ray", "Los Angeles", "`` the bush ''", "regulate the employment and working conditions of civil servants", "Abigail Hawk", "by January 2018", "The Archers", "Cody Fern", "the homicidal thoughts of a troubled youth", "August 5, 1937", "San Francisco", "the Great Crash", "2 June 2011 ( UK )", "The Royalettes", ". java", "the second season episode ``Pretty Much Dead Already ''", "Juliet", "1963", "Roman Reigns", "1901", "the 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII", "Michael Rooker", "1977", "Richard Stallman", "Keeley Clare Julia Hawes", "Abid Ali Neemuchwala", "Jay Baruchel", "March 26, 1973", "July 25, 2017", "1922", "Eddie Van Halen", "the Cyclades", "Some Like It Hot", "a crow", "Vikram Bhatt", "Dar es Salaam", "96,867", "Bob Johnson", "Iran's parliament speaker", "Sri Lanka", "love", "Mathew Brady", "the Rings", "Labau"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6630571654831523}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, true, true, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, false, false, true, true, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 0.5714285714285715, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.19999999999999998, 0.0, 1.0, 0.25, 0.4, 1.0, 0.0, 0.0, 0.5925925925925926, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-10311", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-4048", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-5936", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-3170", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-6468", "mrqa_naturalquestions-validation-1084", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8837", "mrqa_naturalquestions-validation-7166", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-1911", "mrqa_triviaqa-validation-4353", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-3557", "mrqa_hotpotqa-validation-1134", "mrqa_searchqa-validation-9281", "mrqa_triviaqa-validation-7098"], "SR": 0.578125, "CSR": 0.5274671052631579, "EFR": 0.9259259259259259, "Overall": 0.6935692312378168}, {"timecode": 95, "before_eval_results": {"predictions": ["Jet Republic", "Frank Ricci,", "fining the computer chip giant a record  $1.45 billion for abusing its dominant position in the computer processing unit (CPU) market.", "drug cartels", "the Little Rock Nine.", "five", "Ben Roethlisberger", "Alan Graham", "Muslim", "Asashoryu", "\"Slumdog Millionaire\"", "Ma Khin Khin Leh,", "$17,000", "\"oil may be present in thin intervals but that reservoir quality is poor.\"", "the pirates", "10 municipal police officers", "Bangladesh", "Rima Fakih", "Mikkel Kessler", "Friday,", "would slow economic growth with higher taxes.", "third-time road race world champion,", "suppress the memories and to live as normal a life as possible; the culture of his time said that he should get on with his", "Hayden", "Malcolm X", "Dangjin", "\"mentally deranged person", "In the east and the north of the city, water was at waist-level in some neighborhoods.", "The patient, who prefers to be anonymous,", "stole four Impressionist paintings worth about $163 million (180 million Swiss francs)", "only one", "Black History Month", "opposition parties", "YouTube", "Cambodian territory", "2.5 million", "using injectable vitamin supplements", "\"I sort of had a fascination with John Dillinger when I was about 10, 11 years old, for some reason,\"", "we seek a new way forward, based on mutual interest and mutual respect.", "Abhisit Vejjajiva", "Tuesday's iPhone 4S news,", "in the Yemeni port city of Aden", "228", "More than 15,000", "Theoneste Bagosora, 67, a colonel in the Rwandan army,", "Operation Pipeline Express.", "roughly $5.5 billion to build.", "$106,482,500", "Long troop deployments in Iraq,", "prostate cancer,", "bartering", "William Wyler", "1608", "Philippians", "kachhi", "a line representing points of equal water with equal humidity", "fractal geometry", "Paper", "SBS", "The Number Twelve Looking Like You", "Harry Potter and the Chamber of Secrets", "diphthong", "Lotus", "nuclear weapons"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6743285054406378}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.42857142857142855, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.07692307692307693, 0.0, 0.08333333333333334, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 0.0, 0.15384615384615385, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.15384615384615385, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.3333333333333333, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3913", "mrqa_newsqa-validation-3711", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-1417", "mrqa_newsqa-validation-1025", "mrqa_newsqa-validation-302", "mrqa_newsqa-validation-1973", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3534", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1092", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-1745", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-4136", "mrqa_newsqa-validation-2565", "mrqa_newsqa-validation-2249", "mrqa_newsqa-validation-4144", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-1044", "mrqa_triviaqa-validation-1304", "mrqa_triviaqa-validation-2184", "mrqa_hotpotqa-validation-3247", "mrqa_searchqa-validation-6473"], "SR": 0.59375, "CSR": 0.5281575520833333, "EFR": 0.9230769230769231, "Overall": 0.6931375200320513}, {"timecode": 96, "before_eval_results": {"predictions": ["one of the most magnificent", "off Somalia's coast.", "2009", "club managers,", "5,600", "a construction site in the heart of Los Angeles.", "central Cairo,", "free services.", "a big speech", "Basilan", "the highest ranking former member of Saddam Hussein's regime still at large,", "Fareed Zakaria", "Gov. Mark Sanford", "56,", "$17,000", "ClimateCare,", "Australian officials", "fake his own death by crashing his private plane into a Florida swamp.", "longest domestic relay in Olympic", "Police", "one", "President Alvaro Uribe", "that the soldiers were exposed to sodium dichromate, an inorganic compound containing a highly toxic form of chromium known as hexavalent chromium", "in a campus library,", "July", "the two remaining crew members from the helicopter,", "Darrel Mohler", "a hazardous jump from horseback to a truck as Indiana Jones in \" Raiders of the Lost Ark.\"", "$1.5 million.", "\"People have lost their homes, their jobs, their hope,\"", "30", "Larry King", "HPV (human papillomavirus)", "\"The strike means all buses, subways and trolleys in Philadelphia and on the Frontier line in Bucks and Montgomery counties stopped running at 3 a.m.", "Body Tap,", "the Southern Baptist Convention,", "Stephen Tyrone Johns", "Math teacher Mawise Gumba", "\"The first line of law and order", "Facebook", "It's helping consumers move beyond these hard times and has reignited a whole industry.", "A mother whose daughter and granddaughter attend Oprah Winfrey's school in South Africa", "Susan Atkins,", "off the coast of Dubai", "\"She wasn't the best \"coach,\" and she was kind of picky, but she had such a good eye,", "\"falling space debris,\"", "part", "Silicon Valley.", "137", "\"Slumdog Millionaire\"", "Lillo Brancato Jr.", "bridal shop", "Adam Sandler", "a decorative ornament", "butterflies", "vanilla", "France", "1241 until his death in 1250", "February 1940", "Modbury", "Jacob Marley", "Peter Jonas", "Pin the Tail on the Donkey", "Mount Hood"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7494258056758056}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, true, true, true, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, true, true, false, false, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.42857142857142855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727272, 1.0, 1.0, 0.0, 0.2962962962962963, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.07142857142857142, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.09523809523809525, 0.13333333333333333, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.4444444444444445, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2658", "mrqa_newsqa-validation-1839", "mrqa_newsqa-validation-3408", "mrqa_newsqa-validation-1840", "mrqa_newsqa-validation-1161", "mrqa_newsqa-validation-752", "mrqa_newsqa-validation-1163", "mrqa_newsqa-validation-3585", "mrqa_newsqa-validation-3861", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2648", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-3800", "mrqa_newsqa-validation-2261", "mrqa_newsqa-validation-3570", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-5305", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-5864", "mrqa_searchqa-validation-14622", "mrqa_searchqa-validation-6959"], "SR": 0.640625, "CSR": 0.5293170103092784, "EFR": 0.9565217391304348, "Overall": 0.7000583748879426}, {"timecode": 97, "before_eval_results": {"predictions": ["Fred Astaire", "a parachute", "Manchester United", "Pocahontas", "Chaucer", "Stubbs", "Jordan", "John Donne", "West Virginia", "dogs", "watchmaking", "West Africa", "ballet", "elephant house", "Cornwall", "the Blackstone River", "green", "Fred Trueman", "The Mayor of Casterbridge", "Athens", "Yemen", "loch Morar", "leprosy", "Manhunt 2", "a Mobile Phone Phobia", "piano", "a hand gun", "collies", "new york", "the best", "Uriah the Hittite", "George Fox", "bathe", "secretary", "France", "Melissa Duck/ Tina Russo Duck", "haddock", "a single groove", "Ross MacManus", "music (to be performed) in a fiery manner", "dry rot", "cuckoo", "Northumberland", "six", "The Graduate", "2", "1911", "a skein", "Northern Ireland", "Lorenzo", "Pat Houston", "McFerrin, Robin Williams, and Bill Irwin", "1273.6 cm", "John von Neumann", "The Florida Panthers", "Fortunino Francesco Verdi", "67,575", "July", "sailing", "Mary Procidano,", "Eva Maria Kiesler", "Quinn the Eskimo", "the Komodo Dragon", "goalkeeper"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6512152777777778}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, false, false, true, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.5, 0.1111111111111111, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6617", "mrqa_triviaqa-validation-1624", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2917", "mrqa_triviaqa-validation-5438", "mrqa_triviaqa-validation-2342", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-3406", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-7539", "mrqa_triviaqa-validation-7413", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-5665", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-1142", "mrqa_triviaqa-validation-2595", "mrqa_triviaqa-validation-3485", "mrqa_triviaqa-validation-1585", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-7458", "mrqa_hotpotqa-validation-3953", "mrqa_searchqa-validation-15102", "mrqa_searchqa-validation-1862"], "SR": 0.578125, "CSR": 0.5298150510204082, "EFR": 1.0, "Overall": 0.7088536352040815}, {"timecode": 98, "before_eval_results": {"predictions": ["The Kite Runner", "Yves Saint Laurent", "jacob", "shepherd", "Chopin", "curly", "water", "Bolshoi Ballet", "Mending Wall", "Nathan Lane", "Cheaper by the Dozen", "king rechiar", "Marlon Brando", "Long Island", "niger", "Copenhagen", "Arctic", "Hudson", "Spectre", "jorge sand", "Richard Cory", "( Francois) Truffaut", "barbie", "one", "Chlorine", "(Bill) Clinton", "Hanoi", "the Byzantine Empire", "ndongo", "flavor Flav", "McDonald\\'s", "roma", "Hawaii", "(Jimmy) Hoffa", "India", "Cincinnati", "Sapper", "bianco vermouth", "jones", "walk the plank", "Three Amigos", "Haunted Mansion", "George II", "the people buried at Stonehenge", "Grease", "Nevada", "a tick", "alkrangea macrophylla", "Halsey", "Bangkok", "lungs", "DeWayne Warren", "Neela Montgomery", "New England Patriots", "Philippines", "micky jones", "brashy", "Peitho", "400 MW", "high court", "won", "\"The U.S. subcontraction out an assassination program against al Qaeda... in early 2006.\"", "the end of TV's rabbit-ears era.", "\"godfather\""], "metric_results": {"EM": 0.515625, "QA-F1": 0.5947916666666666}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15220", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-2008", "mrqa_searchqa-validation-12340", "mrqa_searchqa-validation-15451", "mrqa_searchqa-validation-9719", "mrqa_searchqa-validation-8935", "mrqa_searchqa-validation-8320", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-5479", "mrqa_searchqa-validation-10272", "mrqa_searchqa-validation-13423", "mrqa_searchqa-validation-14990", "mrqa_searchqa-validation-5008", "mrqa_searchqa-validation-11126", "mrqa_searchqa-validation-7874", "mrqa_searchqa-validation-7997", "mrqa_searchqa-validation-9594", "mrqa_searchqa-validation-14239", "mrqa_searchqa-validation-3332", "mrqa_searchqa-validation-10985", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-12148", "mrqa_searchqa-validation-12782", "mrqa_triviaqa-validation-5403", "mrqa_triviaqa-validation-6849", "mrqa_hotpotqa-validation-4103", "mrqa_hotpotqa-validation-3975", "mrqa_newsqa-validation-2473", "mrqa_newsqa-validation-509", "mrqa_hotpotqa-validation-4241"], "SR": 0.515625, "CSR": 0.5296717171717171, "EFR": 1.0, "Overall": 0.7088249684343435}, {"timecode": 99, "UKR": 0.693359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1014", "mrqa_hotpotqa-validation-1028", "mrqa_hotpotqa-validation-1182", "mrqa_hotpotqa-validation-1192", "mrqa_hotpotqa-validation-1239", "mrqa_hotpotqa-validation-1256", "mrqa_hotpotqa-validation-1336", "mrqa_hotpotqa-validation-1349", "mrqa_hotpotqa-validation-1390", "mrqa_hotpotqa-validation-140", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1547", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-1660", "mrqa_hotpotqa-validation-1681", "mrqa_hotpotqa-validation-1762", "mrqa_hotpotqa-validation-1853", "mrqa_hotpotqa-validation-1857", "mrqa_hotpotqa-validation-19", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-1954", "mrqa_hotpotqa-validation-2095", "mrqa_hotpotqa-validation-2231", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-2266", "mrqa_hotpotqa-validation-2267", "mrqa_hotpotqa-validation-2319", "mrqa_hotpotqa-validation-2395", "mrqa_hotpotqa-validation-2430", "mrqa_hotpotqa-validation-245", "mrqa_hotpotqa-validation-2461", "mrqa_hotpotqa-validation-251", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2733", "mrqa_hotpotqa-validation-2737", "mrqa_hotpotqa-validation-2838", "mrqa_hotpotqa-validation-288", "mrqa_hotpotqa-validation-2882", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-3007", "mrqa_hotpotqa-validation-3060", "mrqa_hotpotqa-validation-3070", "mrqa_hotpotqa-validation-3142", "mrqa_hotpotqa-validation-3250", "mrqa_hotpotqa-validation-3266", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3424", "mrqa_hotpotqa-validation-3522", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-3589", "mrqa_hotpotqa-validation-3626", "mrqa_hotpotqa-validation-3694", "mrqa_hotpotqa-validation-3721", "mrqa_hotpotqa-validation-3779", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-3812", "mrqa_hotpotqa-validation-3844", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3895", "mrqa_hotpotqa-validation-3932", "mrqa_hotpotqa-validation-3971", "mrqa_hotpotqa-validation-4054", "mrqa_hotpotqa-validation-4080", "mrqa_hotpotqa-validation-4112", "mrqa_hotpotqa-validation-4127", "mrqa_hotpotqa-validation-4342", "mrqa_hotpotqa-validation-4372", "mrqa_hotpotqa-validation-4416", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4683", "mrqa_hotpotqa-validation-4694", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4807", "mrqa_hotpotqa-validation-4839", "mrqa_hotpotqa-validation-4844", "mrqa_hotpotqa-validation-4860", "mrqa_hotpotqa-validation-4895", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-5124", "mrqa_hotpotqa-validation-5203", "mrqa_hotpotqa-validation-5205", "mrqa_hotpotqa-validation-5226", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5568", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-5605", "mrqa_hotpotqa-validation-5606", "mrqa_hotpotqa-validation-5635", "mrqa_hotpotqa-validation-5638", "mrqa_hotpotqa-validation-5764", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5857", "mrqa_hotpotqa-validation-612", "mrqa_hotpotqa-validation-681", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-1260", "mrqa_naturalquestions-validation-1273", "mrqa_naturalquestions-validation-1279", "mrqa_naturalquestions-validation-129", "mrqa_naturalquestions-validation-1429", "mrqa_naturalquestions-validation-1494", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-1659", "mrqa_naturalquestions-validation-1762", "mrqa_naturalquestions-validation-1785", "mrqa_naturalquestions-validation-1856", "mrqa_naturalquestions-validation-1901", "mrqa_naturalquestions-validation-1909", "mrqa_naturalquestions-validation-191", "mrqa_naturalquestions-validation-1975", "mrqa_naturalquestions-validation-2005", "mrqa_naturalquestions-validation-2196", "mrqa_naturalquestions-validation-2440", "mrqa_naturalquestions-validation-2586", "mrqa_naturalquestions-validation-2798", "mrqa_naturalquestions-validation-2799", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2904", "mrqa_naturalquestions-validation-3521", "mrqa_naturalquestions-validation-3648", "mrqa_naturalquestions-validation-390", "mrqa_naturalquestions-validation-4104", "mrqa_naturalquestions-validation-425", "mrqa_naturalquestions-validation-4271", "mrqa_naturalquestions-validation-4326", "mrqa_naturalquestions-validation-4345", "mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4744", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-4879", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5058", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5234", "mrqa_naturalquestions-validation-5241", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-538", "mrqa_naturalquestions-validation-5384", "mrqa_naturalquestions-validation-5417", "mrqa_naturalquestions-validation-5451", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5537", "mrqa_naturalquestions-validation-5647", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-5845", "mrqa_naturalquestions-validation-5865", "mrqa_naturalquestions-validation-5983", "mrqa_naturalquestions-validation-6130", "mrqa_naturalquestions-validation-6347", "mrqa_naturalquestions-validation-6465", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-6754", "mrqa_naturalquestions-validation-6777", "mrqa_naturalquestions-validation-6778", "mrqa_naturalquestions-validation-6993", "mrqa_naturalquestions-validation-7078", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7190", "mrqa_naturalquestions-validation-7206", "mrqa_naturalquestions-validation-7452", "mrqa_naturalquestions-validation-7516", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7991", "mrqa_naturalquestions-validation-8062", "mrqa_naturalquestions-validation-8093", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8119", "mrqa_naturalquestions-validation-8206", "mrqa_naturalquestions-validation-8309", "mrqa_naturalquestions-validation-8408", "mrqa_naturalquestions-validation-8664", "mrqa_naturalquestions-validation-8761", "mrqa_naturalquestions-validation-8767", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8973", "mrqa_naturalquestions-validation-9060", "mrqa_naturalquestions-validation-9157", "mrqa_naturalquestions-validation-9260", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9284", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9508", "mrqa_naturalquestions-validation-9545", "mrqa_naturalquestions-validation-9836", "mrqa_naturalquestions-validation-9866", "mrqa_naturalquestions-validation-9880", "mrqa_naturalquestions-validation-99", "mrqa_naturalquestions-validation-997", "mrqa_newsqa-validation-0", "mrqa_newsqa-validation-1006", "mrqa_newsqa-validation-1008", "mrqa_newsqa-validation-1191", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1244", "mrqa_newsqa-validation-1251", "mrqa_newsqa-validation-1259", "mrqa_newsqa-validation-1328", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-149", "mrqa_newsqa-validation-1504", "mrqa_newsqa-validation-1525", "mrqa_newsqa-validation-1589", "mrqa_newsqa-validation-1594", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-17", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1749", "mrqa_newsqa-validation-1809", "mrqa_newsqa-validation-1851", "mrqa_newsqa-validation-1876", "mrqa_newsqa-validation-1887", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1908", "mrqa_newsqa-validation-1974", "mrqa_newsqa-validation-1990", "mrqa_newsqa-validation-2025", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-206", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-207", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2074", "mrqa_newsqa-validation-2088", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2234", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2242", "mrqa_newsqa-validation-2274", "mrqa_newsqa-validation-2296", "mrqa_newsqa-validation-2343", "mrqa_newsqa-validation-2367", "mrqa_newsqa-validation-2476", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2619", "mrqa_newsqa-validation-2620", "mrqa_newsqa-validation-2686", "mrqa_newsqa-validation-2695", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2757", "mrqa_newsqa-validation-2797", "mrqa_newsqa-validation-2819", "mrqa_newsqa-validation-2839", "mrqa_newsqa-validation-2932", "mrqa_newsqa-validation-2954", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3071", "mrqa_newsqa-validation-3106", "mrqa_newsqa-validation-312", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3150", "mrqa_newsqa-validation-3152", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3172", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-3251", "mrqa_newsqa-validation-3253", "mrqa_newsqa-validation-3277", "mrqa_newsqa-validation-3361", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3384", "mrqa_newsqa-validation-3404", "mrqa_newsqa-validation-3422", "mrqa_newsqa-validation-3428", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-3441", "mrqa_newsqa-validation-352", "mrqa_newsqa-validation-3573", "mrqa_newsqa-validation-3595", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3617", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3749", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-3965", "mrqa_newsqa-validation-4024", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-555", "mrqa_newsqa-validation-677", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-949", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10079", "mrqa_searchqa-validation-10099", "mrqa_searchqa-validation-10202", "mrqa_searchqa-validation-1021", "mrqa_searchqa-validation-10633", "mrqa_searchqa-validation-10642", "mrqa_searchqa-validation-10911", "mrqa_searchqa-validation-11165", "mrqa_searchqa-validation-11412", "mrqa_searchqa-validation-11943", "mrqa_searchqa-validation-12010", "mrqa_searchqa-validation-12055", "mrqa_searchqa-validation-12184", "mrqa_searchqa-validation-12251", "mrqa_searchqa-validation-12551", "mrqa_searchqa-validation-1256", "mrqa_searchqa-validation-13040", "mrqa_searchqa-validation-13104", "mrqa_searchqa-validation-13147", "mrqa_searchqa-validation-13167", "mrqa_searchqa-validation-13297", "mrqa_searchqa-validation-13374", "mrqa_searchqa-validation-13673", "mrqa_searchqa-validation-13830", "mrqa_searchqa-validation-14348", "mrqa_searchqa-validation-14389", "mrqa_searchqa-validation-14596", "mrqa_searchqa-validation-14617", "mrqa_searchqa-validation-14826", "mrqa_searchqa-validation-14979", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15020", "mrqa_searchqa-validation-15063", "mrqa_searchqa-validation-1513", "mrqa_searchqa-validation-15275", "mrqa_searchqa-validation-1548", "mrqa_searchqa-validation-15694", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-15823", "mrqa_searchqa-validation-1600", "mrqa_searchqa-validation-1654", "mrqa_searchqa-validation-16796", "mrqa_searchqa-validation-16882", "mrqa_searchqa-validation-1702", "mrqa_searchqa-validation-1826", "mrqa_searchqa-validation-2231", "mrqa_searchqa-validation-2286", "mrqa_searchqa-validation-2302", "mrqa_searchqa-validation-2620", "mrqa_searchqa-validation-328", "mrqa_searchqa-validation-3386", "mrqa_searchqa-validation-3489", "mrqa_searchqa-validation-3571", "mrqa_searchqa-validation-380", "mrqa_searchqa-validation-3863", "mrqa_searchqa-validation-4103", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4206", "mrqa_searchqa-validation-4316", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-483", "mrqa_searchqa-validation-5423", "mrqa_searchqa-validation-5635", "mrqa_searchqa-validation-5831", "mrqa_searchqa-validation-6363", "mrqa_searchqa-validation-6595", "mrqa_searchqa-validation-6635", "mrqa_searchqa-validation-6708", "mrqa_searchqa-validation-6753", "mrqa_searchqa-validation-7010", "mrqa_searchqa-validation-7267", "mrqa_searchqa-validation-7353", "mrqa_searchqa-validation-7860", "mrqa_searchqa-validation-8237", "mrqa_searchqa-validation-8285", "mrqa_searchqa-validation-8297", "mrqa_searchqa-validation-8350", "mrqa_searchqa-validation-8418", "mrqa_searchqa-validation-8740", "mrqa_searchqa-validation-8983", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9154", "mrqa_searchqa-validation-9568", "mrqa_searchqa-validation-9660", "mrqa_searchqa-validation-9807", "mrqa_squad-validation-1263", "mrqa_squad-validation-1402", "mrqa_squad-validation-1593", "mrqa_squad-validation-1681", "mrqa_squad-validation-2314", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-270", "mrqa_squad-validation-271", "mrqa_squad-validation-2762", "mrqa_squad-validation-3076", "mrqa_squad-validation-3140", "mrqa_squad-validation-3276", "mrqa_squad-validation-3497", "mrqa_squad-validation-3634", "mrqa_squad-validation-3852", "mrqa_squad-validation-3869", "mrqa_squad-validation-4469", "mrqa_squad-validation-4671", "mrqa_squad-validation-4898", "mrqa_squad-validation-5042", "mrqa_squad-validation-5531", "mrqa_squad-validation-5634", "mrqa_squad-validation-611", "mrqa_squad-validation-6318", "mrqa_squad-validation-6579", "mrqa_squad-validation-682", "mrqa_squad-validation-6981", "mrqa_squad-validation-7246", "mrqa_squad-validation-7466", "mrqa_squad-validation-7491", "mrqa_squad-validation-7531", "mrqa_squad-validation-773", "mrqa_squad-validation-7763", "mrqa_squad-validation-8045", "mrqa_squad-validation-836", "mrqa_squad-validation-8369", "mrqa_squad-validation-8529", "mrqa_squad-validation-8754", "mrqa_squad-validation-9306", "mrqa_squad-validation-9768", "mrqa_squad-validation-9855", "mrqa_squad-validation-9901", "mrqa_squad-validation-9916", "mrqa_triviaqa-validation-1003", "mrqa_triviaqa-validation-1084", "mrqa_triviaqa-validation-1227", "mrqa_triviaqa-validation-129", "mrqa_triviaqa-validation-1316", "mrqa_triviaqa-validation-1351", "mrqa_triviaqa-validation-1381", "mrqa_triviaqa-validation-1387", "mrqa_triviaqa-validation-1458", "mrqa_triviaqa-validation-153", "mrqa_triviaqa-validation-1636", "mrqa_triviaqa-validation-2171", "mrqa_triviaqa-validation-218", "mrqa_triviaqa-validation-2195", "mrqa_triviaqa-validation-2225", "mrqa_triviaqa-validation-2391", "mrqa_triviaqa-validation-2575", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-258", "mrqa_triviaqa-validation-2677", "mrqa_triviaqa-validation-2765", "mrqa_triviaqa-validation-3173", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-3399", "mrqa_triviaqa-validation-3423", "mrqa_triviaqa-validation-3514", "mrqa_triviaqa-validation-3556", "mrqa_triviaqa-validation-3660", "mrqa_triviaqa-validation-3815", "mrqa_triviaqa-validation-3878", "mrqa_triviaqa-validation-4216", "mrqa_triviaqa-validation-4234", "mrqa_triviaqa-validation-4246", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4422", "mrqa_triviaqa-validation-4585", "mrqa_triviaqa-validation-4770", "mrqa_triviaqa-validation-4831", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4983", "mrqa_triviaqa-validation-4984", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5167", "mrqa_triviaqa-validation-5280", "mrqa_triviaqa-validation-534", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-5566", "mrqa_triviaqa-validation-5628", "mrqa_triviaqa-validation-5837", "mrqa_triviaqa-validation-5852", "mrqa_triviaqa-validation-5932", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-629", "mrqa_triviaqa-validation-6310", "mrqa_triviaqa-validation-6327", "mrqa_triviaqa-validation-6381", "mrqa_triviaqa-validation-6439", "mrqa_triviaqa-validation-6446", "mrqa_triviaqa-validation-6462", "mrqa_triviaqa-validation-6489", "mrqa_triviaqa-validation-6491", "mrqa_triviaqa-validation-6707", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6789", "mrqa_triviaqa-validation-6922", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-6988", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-7564", "mrqa_triviaqa-validation-7573", "mrqa_triviaqa-validation-761", "mrqa_triviaqa-validation-7615", "mrqa_triviaqa-validation-7640", "mrqa_triviaqa-validation-7751", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-920"], "OKR": 0.82421875, "KG": 0.5015625, "before_eval_results": {"predictions": ["Gunpei Yokoi", "2022", "Thomas Mundy Peterson", "$2 million", "the Israeli company Mirabilis", "first Sunday after Easter", "Acid rain", "he cheated on Miley", "1943", "Coriolis effect", "who the better fighters are relative to their weight ( i.e., adjusted to compensate for weight class )", "Marley & Me", "1966", "in a curved path", "arm", "England and Wales", "Nicole Gale Anderson", "Massachusetts", "an explosion", "John Goodman", "In the episode `` Kobol's Last ''", "31 December 1600", "Ethiopia", "third", "the Mongol Yuan Dynasty", "Christian rock band MercyMe", "Victory gardens", "The management team", "Nick Kroll", "Longline", "Waylon Jennings", "on the chest, back, shoulders, torso and / or legs", "a minority report", "Massachusetts", "the conclusion of a syllogism", "king and parliament", "2017 / 18", "2001", "cadmium", "Jason Momoa", "Phillip Paley", "Munchkin", "July 21, 1861", "Bonnie Aarons", "Aaron Harrison", "in the cell nucleus", "drizzle, rain, sleet, snow, graupel and hail", "Internal epithelia", "Cleveland Indians", "controlled synthesis of materials", "in the eastern Alps region of Switzerland", "Coco Chanel", "George W. Bush", "Lily Allen", "Tatton Park", "Selina D'Arcy", "1964", "Ashley \"A.J.\" Jewell,", "42 years old", "step up.\"", "Uruguay", "(Charlie) McCarthy", "William Shakespeare", "HBO World Championship Boxing"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7089052287581699}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, true, true, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.2, 1.0, 1.0, 0.4444444444444444, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.25, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.2222222222222222, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.5, 0.11111111111111112, 0.7058823529411764, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-6844", "mrqa_naturalquestions-validation-1989", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-6022", "mrqa_naturalquestions-validation-5283", "mrqa_naturalquestions-validation-303", "mrqa_naturalquestions-validation-8907", "mrqa_naturalquestions-validation-7963", "mrqa_naturalquestions-validation-4837", "mrqa_naturalquestions-validation-3564", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2698", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-6197", "mrqa_naturalquestions-validation-8096", "mrqa_naturalquestions-validation-6564", "mrqa_triviaqa-validation-1348", "mrqa_searchqa-validation-14211", "mrqa_searchqa-validation-872"], "SR": 0.609375, "CSR": 0.53046875, "EFR": 0.96, "Overall": 0.7019218749999999}]}