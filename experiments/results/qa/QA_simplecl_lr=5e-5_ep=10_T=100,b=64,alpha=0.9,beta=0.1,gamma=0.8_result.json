{"method_class": "simple_cl", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/er/QA_simplecl_lr=5e-5_ep=10_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8', gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=5e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/QA_simplecl_lr=5e-5_ep=10_T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8_result.json', submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.1,gamma=0.8.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4370, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["high cost injectable, oral, infused, or inhaled medications", "a plastid that lacks chlorophyll", "Observations on the Geology of the United States", "1887", "2000", "gain support from China", "the south", "push", "New England Patriots", "A cylindrical Service Module", "gold", "Fermat primality test", "highly diversified", "WWSB and WOTV", "the end itself", "Chen's theorem", "La Rochelle", "Fort Caroline", "around half", "the move from the manufacturing sector to the service sector", "1.7 billion years ago", "reserved to, and dealt with at, Westminster (and where Ministerial functions usually lie with UK Government ministers)", "July 18, 2006", "electromagnetic force", "Robert Bork", "East Smithfield burial site in England", "non-violent", "John Houghton", "Enthusiastic teachers", "high voltage", "Johann Walter", "Shoushi Li", "evidence in 2009 that both global inequality and inequality within countries prevent growth by limiting aggregate demand", "priest", "business districts", "BankAmericard", "Bruno Mars", "Jamukha", "German New Guinea", "Onon", "good, clear laws", "the International Stanis\u0142aw Moniuszko Vocal Competition", "forces", "Factory Project", "2010", "fundraising drives", "1000 CE", "Van Nuys Airport", "overinflated", "basic design typical of Eastern bloc countries", "the tax rate", "sequential proteolytic activation of complement molecules", "customs of his tribe", "Robert Guiscard", "wide sidewalks", "CBS Sports.com", "the March Battle of Fort Bull", "a rendezvous", "6 feet 2 inches", "formalism", "the sale of indulgences", "the English Court of Appeal, the German Bundesgerichtshof, the Belgian Cour du travail", "British failures in North America", "Besan\u00e7on Hugues"], "metric_results": {"EM": 0.75, "QA-F1": 0.7846657363104732}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, true, true, false, true, true], "QA-F1": [0.2, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.1111111111111111, 1.0, 0.3636363636363636, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.21052631578947367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6393", "mrqa_squad-validation-8452", "mrqa_squad-validation-5", "mrqa_squad-validation-6091", "mrqa_squad-validation-7382", "mrqa_squad-validation-9489", "mrqa_squad-validation-10483", "mrqa_squad-validation-4902", "mrqa_squad-validation-2145", "mrqa_squad-validation-7430", "mrqa_squad-validation-680", "mrqa_squad-validation-9896", "mrqa_squad-validation-6645", "mrqa_squad-validation-6072", "mrqa_squad-validation-525", "mrqa_squad-validation-4361"], "SR": 0.75, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 1, "before_eval_results": {"predictions": ["fast forwarding of accessed content", "applications such as on-line betting, financial applications", "San Jose State", "DeMarcus Ware", "two poles", "Presiding Officer", "1206", "high fuel prices and new competition from low-cost air services", "lens-shaped, 5\u20138 \u03bcm in diameter and 1\u20133 \u03bcm thick", "Regis Philbin", "defensins", "Sweden", "linebacker", "the Calvin cycle", "ships", "Archbishop of Westminster", "a coherent theory of quantum gravity", "\"Roentgen rays\" or \"X-Rays\"", "Fridays", "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, and a cataract in one eye", "island of Oahu", "1784", "William of Volpiano and John of Ravenna", "yellow fever outbreaks", "Philippines", "$125 per month", "in any other group of chloroplasts", "Abercynon", "Michael Weinenberger and colleagues of the University of Florida", "only \"essentials\"", "a pointless pursuit", "United Nations", "a plug-n-play system", "Roone Arledge", "driving them in front of the army", "business districts", "1726", "lower rates of social goods (life expectancy by country, educational performance, trust among strangers, women's status, social mobility, even numbers of patents issued", "main hymn", "France", "extinction of the dinosaurs", "ABC Entertainment Group", "the 17th century", "U.S. flags left on the Moon during the Apollo missions were found to still be standing", "T cells", "1080i HD", "the state (including the judges)", "30 July 1891", "wealth than half of all Americans combined", "journal Science", "administration", "elected by citizens", "Trypanosoma brucei", "Falls", "1975", "over half", "1835", "France", "a mutualistic relationship. Some human gut microorganisms benefit the host by fermenting dietary fiber into short - chain fatty acids", "its initial home range spanning from Iran, Pakistan, India, Nepal, Bhutan, Bangladesh and Sri Lanka", "The Wrinkle in Time premired at the El Capitan Theatre on February 26, 2018, and with a theatrical release on March 9, 2018", "The song was written by Mitch Murray, who offered it to Adam Faith and Brian Poole but was turned down", "Rigveda, Atharvaveda and Taittiriya Samhita", "1947"], "metric_results": {"EM": 0.703125, "QA-F1": 0.7713574196284794}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.19999999999999998, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.14814814814814817, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.2608695652173913, 0.1904761904761905, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-809", "mrqa_squad-validation-8749", "mrqa_squad-validation-5758", "mrqa_squad-validation-1504", "mrqa_squad-validation-2506", "mrqa_squad-validation-2780", "mrqa_squad-validation-8662", "mrqa_squad-validation-4240", "mrqa_squad-validation-7571", "mrqa_squad-validation-4206", "mrqa_squad-validation-3998", "mrqa_squad-validation-7457", "mrqa_squad-validation-8576", "mrqa_squad-validation-3922", "mrqa_naturalquestions-validation-7393", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-6050"], "SR": 0.703125, "CSR": 0.7265625, "EFR": 1.0, "Overall": 0.86328125}, {"timecode": 2, "before_eval_results": {"predictions": ["a magnetic field", "photosynthetic function", "Egyptians", "gold", "fund travelers who would come back with tales of their discoveries", "reactive allotrope of oxygen", "aligning his personal goals with his academic goals", "ABC Circle Films", "Jews", "Kaifeng", "passion", "Combined Statistical Area", "European Union law", "monophyletic", "\"Provisional Registration\" status after a year if there is sufficient evidence to show that the \"Standard for Full Registration\" has been met", "biochemical oxygen demand, or the amount of O2 needed to restore it to a normal concentration", "to work at various electrical repair jobs and even as a ditch digger for $2 per day", "hospitals and other institutions", "gold", "1998", "160 kPa (about 1.6 atm)", "The General Board of Church and Society, and the United Methodist Women", "successfully preventing it from being cut down", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "St. Johns River", "The increasing use of technology, specifically the rise of the internet over the past decade, has begun to shape the way teachers approach their roles in the classroom.", "10 years", "Genghis Khan", "HIV", "1857", "Rijn", "Caris & Co.", "Stage 2", "\u00d6gedei", "against governmental entities. Brownlee argues that disobedience in opposition to the decisions of non-governmental agencies such as trade unions, banks, and private universities can be justified if it reflects \"a larger challenge to the legal system", "Anglo-Saxon", "two populations of rodents", "The Deadly Assassin and Mawdryn undead", "Dave Logan", "the top row of windows", "fast forwarding of accessed content", "The Dornbirner Ach", "combustion chamber", "a gift", "104 \u00b0F (40 \u00b0C)", "in order to increase the number of problems that can be solved", "the property owner", "northern and western Europe", "1913", "patient compliance issues", "20th century", "ambiguity", "\"Bells\" was introduced by Bob Hope in the 1951 movie \"The Lemon Drop Kid\"", "grew the... white one upon becoming one of these | a geisha.", "Parceit: The Secret Life of J. Edgar Hoover from a Sibert Medalist.... For all of his respect for his subject's complexity, Aronson's contempt", "around the North Star... Distant Uranus has finally become a nice target for evening observers.... Evenings this week are great times for exploring the constellation Sagittarius the Archer", "Carefully paddling down this Congolese river that lends its name to a deadly virus", "The Basset hound is second only to this dog breed in having the keenest sense of smell", "1810", "March, and two others pleaded guilty in 2013 on similar charges", "half the northbound cars wait 90 minutes | Tijuana", "the prehistoric and medieval period", "James Edward Kelly", "2 March 1972"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6883497941097284}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.18181818181818182, 0.35294117647058826, 0.56, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5957446808510638, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.19999999999999998, 0.25, 0.07999999999999999, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3497", "mrqa_squad-validation-2717", "mrqa_squad-validation-2040", "mrqa_squad-validation-3633", "mrqa_squad-validation-1308", "mrqa_squad-validation-3692", "mrqa_squad-validation-1880", "mrqa_squad-validation-6244", "mrqa_squad-validation-6753", "mrqa_squad-validation-1108", "mrqa_squad-validation-7162", "mrqa_squad-validation-1808", "mrqa_squad-validation-9895", "mrqa_squad-validation-6361", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16877", "mrqa_searchqa-validation-3385", "mrqa_hotpotqa-validation-1393"], "SR": 0.609375, "CSR": 0.6875, "EFR": 1.0, "Overall": 0.84375}, {"timecode": 3, "before_eval_results": {"predictions": ["a strange odor in their spacesuits", "Muqali", "inversely to member state size", "it is not known if they are distinct or equal classes", "1884", "Richard the Lion-hearted", "the printing press", "1997", "June 6, 1951", "Marshall Cohen", "1.7 billion years ago", "a not-for-profit United States computer networking consortium", "contemporary accounts were exaggerations", "residency registration", "Tower District", "individual state laws", "October 2007", "Moscone Center", "Voice in the Wilderness", "September 1944", "\u015ar\u00f3dmie\u015bcie", "by burning a mixture of acetylene and compressed O2", "9.6%", "Commander", "macrophages and lymphocytes", "kill Luther", "Edgar Atheling", "spreading \"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia to an entire country", "the Lord's Enclosure", "Jean Cauvin", "220 miles", "\"Blue Harvest\" and \"420\"", "Thomas Commerford Martin", "a light-driven method of synthesizing ATP to power the Calvin cycle without generating oxygen", "\"The Book of Roger\"", "proportional to the object's mass", "Africa", "Pierre Bayle", "The results of the Haensch study have since been confirmed and amended", "32.9%", "30\u201360% of Europe's total population", "1368\u20131644", "reciprocating Diesel engines", "Pedro Men\u00e9ndez de Avil\u00e9s", "a liquid oxygen tank exploded", "$105 billion", "1688\u20131692", "He currently plays for AFC", "Parlophone", "Super Bowl XXIX", "The Number Twelve", "end of the 18th century", "Tulsa", "26,788", "Richa Sharma", "Stage Stores", "25 laps", "a national transgender figure", "672 km2", "the Boston and Lowell Railroad", "Dusty Dvoracek", "He is telling me to regain the trust of those customers who are driving our vehicles", "Himalayan", "murder"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7545577380136204}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, false, true, true, false, true, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.9090909090909091, 0.7777777777777778, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6923076923076924, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.4, 1.0, 0.11764705882352941, 0.0, 0.33333333333333337]}}, "before_error_ids": ["mrqa_squad-validation-4210", "mrqa_squad-validation-1780", "mrqa_squad-validation-1142", "mrqa_squad-validation-3456", "mrqa_squad-validation-4019", "mrqa_squad-validation-1116", "mrqa_squad-validation-9740", "mrqa_squad-validation-6286", "mrqa_squad-validation-4631", "mrqa_squad-validation-8872", "mrqa_squad-validation-10413", "mrqa_squad-validation-4901", "mrqa_squad-validation-3370", "mrqa_hotpotqa-validation-1964", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-1124", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-3564"], "SR": 0.671875, "CSR": 0.68359375, "EFR": 0.9523809523809523, "Overall": 0.8179873511904762}, {"timecode": 4, "before_eval_results": {"predictions": ["consultant at the Westinghouse Electric & Manufacturing Company's Pittsburgh labs", "the radical reformers who threatened the new order by fomenting social unrest and violence", "Modern English", "Commission v Italy", "the West", "1893", "demand for a Scottish Parliament", "1881", "1421", "W. E. B. Du Bois", "between 25-minute episodes (the most common format)", "their captive import policy", "15th century", "two", "two", "a pivotal event", "Mexico", "Black Sea", "a single output (of a total function)", "The Central Region", "March Battle of Fort Bull", "Murray Gold and Ben Foster", "ambiguity", "Super Bowl XLIV", "the Urarina", "a global to a domestic scale", "friction", "was lost in the 5th Avenue laboratory fire of March 1895", "Westwood One", "free", "The Secret Life of Pets", "on issues related to the substance of the statement", "1763\u20131775", "classical position variables", "512-bit", "Deabolis", "necessity", "adenosine triphosphate", "cartels", "Hughes Hotel", "88", "8 November 2010", "John Maynard Keynes", "The Perfect Storm by Sebastian Junger", "Terry and June Whitfield", "architectural equivalent of the Nobel Prize", "arrows", "the common mole", "a complex number raised to the zero power", "Mikhail Gorbachev", "Good Will Hunting", "Quentin Blake", "The History Boys", "a valid passport, Air NEXUS card, or an Alien Registration Card", "\"caliper\"", "anucleons", "James Hoban", "elia Earhart", "1963", "The 28 inch length trunks are then cut into what are called clefts", "WrestleMania 34", "The United States of America", "Tuesday's iPhone 4S news", "the Charles M. Schulz Museum"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6728804181929182}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [0.2222222222222222, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.2857142857142857, 0.8571428571428571, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.923076923076923, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-1312", "mrqa_squad-validation-2437", "mrqa_squad-validation-9334", "mrqa_squad-validation-7708", "mrqa_squad-validation-3752", "mrqa_squad-validation-6197", "mrqa_squad-validation-1601", "mrqa_squad-validation-7537", "mrqa_squad-validation-603", "mrqa_squad-validation-9484", "mrqa_squad-validation-8905", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-873", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-2248", "mrqa_searchqa-validation-4355"], "SR": 0.578125, "CSR": 0.6625, "EFR": 1.0, "Overall": 0.83125}, {"timecode": 5, "before_eval_results": {"predictions": ["7:00 to 9:00 a.m.", "ammed", "vaccination", "62 acres", "Enrique P\u00e9rez de Guzm\u00e1n", "Spain", "Carolina's defense", "Cam Newton", "eastwards", "accessory pigments", "a piece of paper", "Pleistocene", "priority", "Nurses", "time and space", "1951", "Marches", "black earth", "Nederrijn", "the opposite end from the mouth", "Refined Hindu and Buddhist sculptures", "the mid-sixties", "the Kuznets curve hypothesis", "the lost chloroplast's existence", "Schr\u00f6dinger", "90\u00b0 out of phase", "anticlines and synclines", "Tanaghrisson", "Siegfried", "Sydney", "220 miles (350 km)", "Northern San Diego", "Video On Demand content", "Genghis Khan", "Arizona Cardinals", "Pleurobrachia", "the chloroplast", "operations requiring constant speed", "2010", "psilocybin", "\"Krabby Road\"", "English folk-song", "England", "2009", "Ella Fitzgerald", "sarod", "1981", "Kris Kristofferson", "Nia Sanchez", "German", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies", "Fran", "Al Bundy", "Kristine Leahy", "1999 Odisha", "Fat Albert", "Frontline", "d\u00eds", "Shinola", "modern genetics", "a person trained to pilot, navigate, or otherwise participate as a crew member of a spacecraft", "U.N.", "british", "an independent homeland for the country's ethnic Tamil minority"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5611201298701298}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, true, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.45454545454545453, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8333333333333333]}}, "before_error_ids": ["mrqa_squad-validation-9029", "mrqa_squad-validation-1095", "mrqa_squad-validation-791", "mrqa_squad-validation-2476", "mrqa_squad-validation-8312", "mrqa_squad-validation-3575", "mrqa_squad-validation-1064", "mrqa_squad-validation-9176", "mrqa_squad-validation-10386", "mrqa_squad-validation-3257", "mrqa_squad-validation-6044", "mrqa_squad-validation-4458", "mrqa_squad-validation-8900", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_naturalquestions-validation-3663", "mrqa_triviaqa-validation-2357", "mrqa_newsqa-validation-539", "mrqa_searchqa-validation-1523", "mrqa_newsqa-validation-1718"], "SR": 0.484375, "CSR": 0.6328125, "EFR": 0.9696969696969697, "Overall": 0.8012547348484849}, {"timecode": 6, "before_eval_results": {"predictions": ["2010", "the adaptive immune system", "Calvin cycle", "his eldest son, Zhenjin", "education and training", "June 11, 1962", "Jean-Claude Juncker", "68,511", "voters were supposed to line up behind their favoured candidates instead of a secret ballot", "1880", "8 mm cine film", "Pittsburgh", "the seal of the Federal Communications Commission", "\u00a3250,000", "Michael Jayston", "radiography", "Norway", "courts of member states", "Texas", "shortening the cutoff", "12.5 acres", "a few hundred feet", "an innate force of impetus", "the Conservative Party", "an international data communications network", "the environment in which they lived", "Darian Stewart", "the Great Fire of London", "acular", "the Moscone Center in San Francisco", "The View and The Chew", "Parliament of the United Kingdom at Westminster", "successfully preventing it from being cut down", "baptism", "England", "one hundred pennies", "a coffee house", "Parkinson's disease", "Tintin", "piu forte", "inch", "Geoff Hurst", "McKinney", "Spock", "Solomon", "Blackstar", "geomorphology", "Earth", "krokos", "Richmond in North Yorkshire", "Passenger Pigeon", "Richard Wagner", "false teeth", "Debbie Rowe", "China", "1973", "The Rough Guide To Comedy", "London", "Dave Kelly", "Southaven", "Papua province", "\"Gold Digger\"", "Paul Biya", "for people to be able to buy things or get things"], "metric_results": {"EM": 0.578125, "QA-F1": 0.647824302134647}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, false, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6284", "mrqa_squad-validation-4297", "mrqa_squad-validation-8295", "mrqa_squad-validation-89", "mrqa_squad-validation-512", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1561", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-3080", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-4197", "mrqa_naturalquestions-validation-8889", "mrqa_hotpotqa-validation-426", "mrqa_newsqa-validation-3541", "mrqa_searchqa-validation-13016", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-714"], "SR": 0.578125, "CSR": 0.625, "EFR": 0.9629629629629629, "Overall": 0.7939814814814814}, {"timecode": 7, "before_eval_results": {"predictions": ["ten times their own weight", "Cape of Good Hope", "Time", "length of the Rhine", "14", "150", "North American Aviation", "to register as a professional on the General Pharmaceutical Council (GPhC) register", "the Sovereign", "weakness in school discipline", "Fort Caroline", "the concept Distributed Adaptive Message Block Switching", "at elevated partial pressures", "His lab was torn down in 1904", "interacting and working directly with students", "Omnicare, Kindred Healthcare and PharMerica", "Tiffany & Co.", "conservative", "forts Shirley had erected at the Oneida carry", "swimming-plates", "eleven", "in order to create a test case as to the constitutionality of a law", "1332", "separately from physicians", "in the south", "geordie", "a fuel", "US$10", "from the harvests of their Chinese tenants eaten up by costs of equipping and dispatching men for their tours of duty", "142 pounds", "1806-07", "richmond", "he built a shed", "richmond", "a police car", "no place to play", "martin peters", "the Chetniks", "neptune", "richmond", "richmond", "jackson", "martin peters", "richmond", "richmond", "richmond", "martin peters", "richmond", "richmond", "Christopher Marlowe", "Tom Krazit", "all right angles are congruent", "richmond", "daughters of Rebekah", "jackson", "end the Korean war", "martin peters", "eight", "martin peters", "World War II", "Hussein's Revolutionary Command Council", "cursory search", "cowardly lion", "March 22"], "metric_results": {"EM": 0.4375, "QA-F1": 0.4763797514619883}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, false, false, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2105263157894737, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.4444444444444444, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9109", "mrqa_squad-validation-6324", "mrqa_squad-validation-1404", "mrqa_squad-validation-10251", "mrqa_squad-validation-6773", "mrqa_squad-validation-7961", "mrqa_squad-validation-3483", "mrqa_squad-validation-1272", "mrqa_squad-validation-8238", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-6726", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-8040", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-4533", "mrqa_searchqa-validation-14514", "mrqa_triviaqa-validation-2045", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-858"], "SR": 0.4375, "CSR": 0.6015625, "EFR": 0.9722222222222222, "Overall": 0.7868923611111112}, {"timecode": 8, "before_eval_results": {"predictions": ["flour mill", "every four years", "Alan Turing", "2\u20133 years", "coordinating lead authors", "effectiveness of treatment regimens", "43 million tons", "720p high definition", "Denver", "Singing Revolution", "The Newlywed Game", "17th century", "counterflow", "pattern recognition receptors", "climate change in addition to deforestation", "Glucocorticoids", "The Late Show", "entertainers", "Newcastle Student Radio", "classical molecules", "City council", "Torchwood: Miracle Day", "November 1979", "linear", "breaches of law in protest against international organizations and foreign governments", "Cobham", "Edward Burne-Jones", "Behind the Sofa", "the Simien Mountain National Park", "Florida State University", "the eardrum", "Mao Zedong", "arroz con Leche", "Hawaii", "Kiwanis International", "the log cabin", "Symphony No.9", "jedoublen/jeopardy", "the Chateau", "the American Sign Language", "the Clinica Regina Margherita", "dizygotic", "the DASH Diet", "Hawaii", "lox", "neurotransmitters", "jedoublen/jeopardy", "Meg Cabot", "prosciutto", "Massachusetts", "larynx", "John Galt", "Arbor Day", "cinnamomum", "the right angle", "Kentucky", "Henry Clay", "Chinese Exclusion Act", "jane", "1995", "Harry Nicolaides", "Mineola", "Blender", "2018\u201319 UEFA Europa League"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6533854166666666}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, false, false, true, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-434", "mrqa_squad-validation-5374", "mrqa_squad-validation-6674", "mrqa_squad-validation-8747", "mrqa_squad-validation-5422", "mrqa_searchqa-validation-16960", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-9679", "mrqa_searchqa-validation-8139", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-5070", "mrqa_naturalquestions-validation-10012", "mrqa_triviaqa-validation-4730", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-5174"], "SR": 0.609375, "CSR": 0.6024305555555556, "EFR": 1.0, "Overall": 0.8012152777777778}, {"timecode": 9, "before_eval_results": {"predictions": ["Edinburgh", "because Dutch law said only people established in the Netherlands could give legal advice", "Aboriginal", "assisting in fabricating evidence or committing perjury", "kicker", "relativity", "Red Turban rebels", "Jurassic Period", "Fort Le Boeuf", "William S. Paley", "anaerobic bacteria", "can appear more greenish", "eicosanoids and cytokines", "in his or her vocations on a daily basis", "50-yard line", "heard her songs", "1/6", "DC traction motor", "therichest 1 percent", "rejected the divinity of Jesus", "EastEnders", "J. S. Bach", "highest", "a few drops of the liquid", "1882", "Mel Jones", "North America", "Alastair Cook", "Swadlincote", "\u03bb", "flytrap", "the last Ice Age", "Bonnie Plunkett", "2026", "Georgia", "it showed such a disregard for the life and safety of others as to amount to a crime and deserve punishment", "1984 Summer Olympics in Los Angeles", "4 September 1936", "Andrew Moray and William Wallace", "the heart", "Pangaea", "Have I Told You Lately", "sinoatrial node", "the fourth quarter of the preceding year", "the 2013 non-fiction book of the same name by David Finkel. Finkel", "prevent further offense by convincing the offender that their conduct was wrong", "Bob Dylan", "September of that year", "a judge", "Lynda Carter", "100,000 writes", "A substitute good", "September 27, 2017", "President Gerald Ford", "Monk's Caf\u00e9", "Courteney Cox", "Tintin", "Alaska", "140 million", "he flew solo to Scotland in an attempt to negotiate peace with the United Kingdom during World War II.", "American Samoa", "carbon taxed", "Billy Budd", "En banc"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6508473446772864}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, false, true, false, true, true, false, false, false, true, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.2, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8372093023255813, 0.4444444444444444, 1.0, 0.8333333333333333, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615385, 1.0, 1.0, 0.8, 0.4, 0.0, 1.0, 1.0, 1.0, 0.15999999999999998, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10204", "mrqa_squad-validation-8596", "mrqa_squad-validation-2443", "mrqa_squad-validation-805", "mrqa_squad-validation-7459", "mrqa_squad-validation-2571", "mrqa_squad-validation-3475", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-5502", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-801", "mrqa_hotpotqa-validation-3481", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-2507", "mrqa_searchqa-validation-12968"], "SR": 0.53125, "CSR": 0.5953125, "EFR": 0.9666666666666667, "Overall": 0.7809895833333333}, {"timecode": 10, "UKR": 0.6796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1124", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-1205", "mrqa_hotpotqa-validation-1258", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-171", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-2327", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2829", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3871", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-4217", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-5075", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-524", "mrqa_hotpotqa-validation-5251", "mrqa_hotpotqa-validation-5268", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-961", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1091", "mrqa_naturalquestions-validation-1372", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1941", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3663", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-677", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7407", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-956", "mrqa_naturalquestions-validation-9871", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1664", "mrqa_newsqa-validation-1718", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10305", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-1151", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12243", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-12371", "mrqa_searchqa-validation-12649", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12963", "mrqa_searchqa-validation-12968", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14435", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14572", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14879", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2115", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2499", "mrqa_searchqa-validation-2561", "mrqa_searchqa-validation-3075", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5075", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5713", "mrqa_searchqa-validation-5814", "mrqa_searchqa-validation-5916", "mrqa_searchqa-validation-5936", "mrqa_searchqa-validation-6095", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-6666", "mrqa_searchqa-validation-679", "mrqa_searchqa-validation-6900", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8411", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-10067", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10127", "mrqa_squad-validation-10204", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-1023", "mrqa_squad-validation-10251", "mrqa_squad-validation-10251", "mrqa_squad-validation-10260", "mrqa_squad-validation-10287", "mrqa_squad-validation-10351", "mrqa_squad-validation-10386", "mrqa_squad-validation-10387", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-10504", "mrqa_squad-validation-1051", "mrqa_squad-validation-1064", "mrqa_squad-validation-1071", "mrqa_squad-validation-1078", "mrqa_squad-validation-1104", "mrqa_squad-validation-1108", "mrqa_squad-validation-1108", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1142", "mrqa_squad-validation-1181", "mrqa_squad-validation-1236", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1282", "mrqa_squad-validation-1301", "mrqa_squad-validation-1308", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1378", "mrqa_squad-validation-1401", "mrqa_squad-validation-1461", "mrqa_squad-validation-1504", "mrqa_squad-validation-1506", "mrqa_squad-validation-1552", "mrqa_squad-validation-1553", "mrqa_squad-validation-1554", "mrqa_squad-validation-159", "mrqa_squad-validation-1601", "mrqa_squad-validation-1636", "mrqa_squad-validation-1706", "mrqa_squad-validation-1780", "mrqa_squad-validation-1808", "mrqa_squad-validation-1813", "mrqa_squad-validation-1831", "mrqa_squad-validation-1856", "mrqa_squad-validation-1875", "mrqa_squad-validation-1880", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-2040", "mrqa_squad-validation-2069", "mrqa_squad-validation-2097", "mrqa_squad-validation-2135", "mrqa_squad-validation-2145", "mrqa_squad-validation-2210", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2449", "mrqa_squad-validation-2451", "mrqa_squad-validation-2453", "mrqa_squad-validation-2476", "mrqa_squad-validation-2506", "mrqa_squad-validation-2571", "mrqa_squad-validation-2603", "mrqa_squad-validation-2643", "mrqa_squad-validation-2643", "mrqa_squad-validation-2717", "mrqa_squad-validation-2753", "mrqa_squad-validation-2780", "mrqa_squad-validation-2807", "mrqa_squad-validation-2832", "mrqa_squad-validation-2865", "mrqa_squad-validation-2888", "mrqa_squad-validation-2955", "mrqa_squad-validation-3086", "mrqa_squad-validation-3092", "mrqa_squad-validation-31", "mrqa_squad-validation-3109", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3196", "mrqa_squad-validation-3223", "mrqa_squad-validation-3257", "mrqa_squad-validation-3310", "mrqa_squad-validation-3320", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3370", "mrqa_squad-validation-3374", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3456", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-350", "mrqa_squad-validation-351", "mrqa_squad-validation-3551", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3607", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3692", "mrqa_squad-validation-3724", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3823", "mrqa_squad-validation-3865", "mrqa_squad-validation-3890", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3939", "mrqa_squad-validation-3998", "mrqa_squad-validation-401", "mrqa_squad-validation-4018", "mrqa_squad-validation-4019", "mrqa_squad-validation-4100", "mrqa_squad-validation-4110", "mrqa_squad-validation-4162", "mrqa_squad-validation-4206", "mrqa_squad-validation-4210", "mrqa_squad-validation-4232", "mrqa_squad-validation-4240", "mrqa_squad-validation-4297", "mrqa_squad-validation-4316", "mrqa_squad-validation-4343", "mrqa_squad-validation-441", "mrqa_squad-validation-4430", "mrqa_squad-validation-4458", "mrqa_squad-validation-4460", "mrqa_squad-validation-4473", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4615", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4665", "mrqa_squad-validation-4729", "mrqa_squad-validation-4783", "mrqa_squad-validation-4791", "mrqa_squad-validation-4795", "mrqa_squad-validation-4824", "mrqa_squad-validation-4841", "mrqa_squad-validation-4857", "mrqa_squad-validation-4860", "mrqa_squad-validation-4870", "mrqa_squad-validation-4901", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-5", "mrqa_squad-validation-50", "mrqa_squad-validation-510", "mrqa_squad-validation-5115", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5187", "mrqa_squad-validation-525", "mrqa_squad-validation-5275", "mrqa_squad-validation-5310", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5422", "mrqa_squad-validation-5450", "mrqa_squad-validation-5471", "mrqa_squad-validation-5492", "mrqa_squad-validation-5591", "mrqa_squad-validation-5602", "mrqa_squad-validation-5624", "mrqa_squad-validation-5638", "mrqa_squad-validation-5714", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5844", "mrqa_squad-validation-5883", "mrqa_squad-validation-5889", "mrqa_squad-validation-5943", "mrqa_squad-validation-5971", "mrqa_squad-validation-5978", "mrqa_squad-validation-60", "mrqa_squad-validation-6015", "mrqa_squad-validation-603", "mrqa_squad-validation-6044", "mrqa_squad-validation-6070", "mrqa_squad-validation-6072", "mrqa_squad-validation-6091", "mrqa_squad-validation-6120", "mrqa_squad-validation-6143", "mrqa_squad-validation-6181", "mrqa_squad-validation-6197", "mrqa_squad-validation-62", "mrqa_squad-validation-6255", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-6361", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6408", "mrqa_squad-validation-6428", "mrqa_squad-validation-6454", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6625", "mrqa_squad-validation-6626", "mrqa_squad-validation-6645", "mrqa_squad-validation-6657", "mrqa_squad-validation-6658", "mrqa_squad-validation-6658", "mrqa_squad-validation-6680", "mrqa_squad-validation-6725", "mrqa_squad-validation-6753", "mrqa_squad-validation-6753", "mrqa_squad-validation-6773", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6958", "mrqa_squad-validation-6997", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7013", "mrqa_squad-validation-7021", "mrqa_squad-validation-7040", "mrqa_squad-validation-7082", "mrqa_squad-validation-7101", "mrqa_squad-validation-7162", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7317", "mrqa_squad-validation-7382", "mrqa_squad-validation-7395", "mrqa_squad-validation-7430", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7463", "mrqa_squad-validation-7537", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7670", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7708", "mrqa_squad-validation-7765", "mrqa_squad-validation-7831", "mrqa_squad-validation-7837", "mrqa_squad-validation-7867", "mrqa_squad-validation-787", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-7959", "mrqa_squad-validation-7961", "mrqa_squad-validation-7961", "mrqa_squad-validation-805", "mrqa_squad-validation-806", "mrqa_squad-validation-8135", "mrqa_squad-validation-8227", "mrqa_squad-validation-8233", "mrqa_squad-validation-8238", "mrqa_squad-validation-8242", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-8553", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8596", "mrqa_squad-validation-8602", "mrqa_squad-validation-8627", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8755", "mrqa_squad-validation-8781", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-89", "mrqa_squad-validation-8900", "mrqa_squad-validation-8971", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9109", "mrqa_squad-validation-9154", "mrqa_squad-validation-9176", "mrqa_squad-validation-9226", "mrqa_squad-validation-9240", "mrqa_squad-validation-9304", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9371", "mrqa_squad-validation-9405", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9546", "mrqa_squad-validation-9562", "mrqa_squad-validation-9611", "mrqa_squad-validation-9619", "mrqa_squad-validation-968", "mrqa_squad-validation-9750", "mrqa_squad-validation-9764", "mrqa_squad-validation-9856", "mrqa_squad-validation-9890", "mrqa_squad-validation-9895", "mrqa_squad-validation-9896", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-3249", "mrqa_triviaqa-validation-3591", "mrqa_triviaqa-validation-3681", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-478", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5671", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5803", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6413", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7430", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-873"], "OKR": 0.876953125, "KG": 0.43984375, "before_eval_results": {"predictions": ["Warszawa", "the SI unit of magnetic flux density the tesla", "2007", "Duval County", "2003", "the father of the house when in his home", "Electrical Experimenter", "Richard Wilkinson and Kate Pickett", "some teachers and parents", "Governor Vaudreuil", "pastors and teachers", "Justin Tucker", "1543", "None", "State Route 168", "Duisburg", "unit-dose", "War of Currents", "\"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood. Olivier Roy", "continental European countries", "RogerNFL", "events", "9", "Adelaide", "once", "Around 200,000 passengers", "itty Hawk", "Nidal Hasan", "the University of Maryland", "Boston Herald", "the role of Zander in Michael Damian's film, High Strung: Free Dance.", "Consigliere", "Pierce County", "Harry F. Sinclair", "Homebrewing", "December 1974", "2012", "1999", "2004", "an Academy Award in the category Best Sound", "Nelson Aldrich Rockefeller", "Fort Snelling, Minnesota", "Asif Kapadia", "at the State House in Augusta", "1970", "1978", "2005", "My Cat from Hell", "Mark Sinclair", "Colonel", "1999", "17", "La Liga", "the 10th Cavalry Regiment", "Kal Ho Naa Ho", "Key West, Florida", "stomach of leg", "John Roberts", "repechage", "Carl Johan", "two", "Madonna", "Freddie Mercury", "The Marine Band"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7293536324786325}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, false, true, true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.8, 0.3333333333333333, 0.0, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1251", "mrqa_squad-validation-2318", "mrqa_squad-validation-10259", "mrqa_squad-validation-2337", "mrqa_squad-validation-4562", "mrqa_squad-validation-9578", "mrqa_squad-validation-85", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2751", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-2896", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-33", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-5810", "mrqa_hotpotqa-validation-3807", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-674", "mrqa_naturalquestions-validation-7608"], "SR": 0.609375, "CSR": 0.5965909090909092, "EFR": 1.0, "Overall": 0.7186150568181818}, {"timecode": 11, "before_eval_results": {"predictions": ["the pr\u00e9tendus r\u00e9form\u00e9s", "587,000 square kilometres", "Bishopsgate", "Mnemiopsis", "from tomb and memorial", "Beirut", "smaller trade relations with their neighbours", "Tommy Lee Jones", "150", "about four", "308", "Victoria", "large compensation pools", "the main opposition party", "Charlesfort", "a rudimentary immune system", "Battle of the Restigouche", "Boston", "simple devices", "head writer and executive producer", "Joseph Merrick", "a psychologist", "every ten years", "Conan Doyle", "A Blood Light", "Hong Kong", "ambilevous", "The Wizard", "a horse", "the Irrawaddy River", "Ed White", "the Humber estuary", "the lunar new year holiday", "James", "Copenhagen", "Troy", "Freebase(0.00 / 0 votes)", "Andrew Anthony", "European Bison", "Edinburgh", "Viking feet", "Paul Gauguin", "Action Comics", "the Bombe", "energy Changes", "Novak Djokovic", "New Zealand", "Oasis", "\"101 Best Written TV Series Of All Time\"", "red", "Rajasthan", "the Beatles", "floating ribs", "two days", "golf", "The current Secretary of Homeland Security", "Barry and Robin Gibb", "Adelaide", "Edward John \"Eddie\" Izzard", "Sabina Guzzanti", "the largest and perhaps most sophisticated ring of its kind in U.S. history", "a quark", "ne", "the word \u03b4\u03cc\u03be\u03b1"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5395664869721473}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.11320754716981131, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-4256", "mrqa_squad-validation-5545", "mrqa_squad-validation-5303", "mrqa_squad-validation-7083", "mrqa_squad-validation-8421", "mrqa_squad-validation-6449", "mrqa_squad-validation-10351", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-5724", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-253", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-6783", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4974", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-3215", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-778", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-1686", "mrqa_triviaqa-validation-3095", "mrqa_naturalquestions-validation-5094", "mrqa_naturalquestions-validation-395", "mrqa_hotpotqa-validation-462", "mrqa_newsqa-validation-3199", "mrqa_searchqa-validation-7976", "mrqa_naturalquestions-validation-9323"], "SR": 0.453125, "CSR": 0.5846354166666667, "EFR": 0.9714285714285714, "Overall": 0.7105096726190476}, {"timecode": 12, "before_eval_results": {"predictions": ["pulmonary fibrosis", "Henry Cavendish", "Lower Norfolk County", "melatonin", "90-60's", "the deaths of two friends", "1985", "Ismailiyah, Egypt", "England", "the ability to pursue valued goals", "tentilla", "political support in his struggle against leftists", "$5 million", "Lake George", "Jadaran", "Dwight D. Eisenhower", "decreases", "one", "Secretariat", "1952", "Australian", "September 1901", "America", "The Dragon", "psilocin", "Fundamentalist Church of Jesus Christ of Latter-Day Saints", "Eurasia", "Boyd Gaming", "MGM Resorts International", "james boswell", "Omega SA", "September 14, 1877", "alternate", "Yasir Hussain", "Malayalam movies", "Kennedy Road", "2002", "31", "the corner of North Avenue at Techwood Drive", "Bill Boyd", "Jack Ryan", "Emilia-Romagna Region", "the reigning monarch of the United Kingdom", "322,520", "Chief Strategy Officer", "Dave Lee Travis", "Bedknobs and Broomsticks", "Lady Ella", "William Bradford", "140 million", "\"Beauty and the Beast\"", "Gary Ross", "International Boxing Hall of Fame", "1996 PGA Championship", "Revolver", "Jack Nicklaus", "a transformation change of heart", "Mussolini", "joseph boswell", "off the coast of Dubai", "1918", "butter", "Boston", "rain"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7545296717171717}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, true, false, true, false, true, true, true, true, true, true, false, true, false, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.2, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3639", "mrqa_squad-validation-2657", "mrqa_squad-validation-9565", "mrqa_squad-validation-6108", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-454", "mrqa_hotpotqa-validation-577", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-3606", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-4810", "mrqa_naturalquestions-validation-5851", "mrqa_triviaqa-validation-4173", "mrqa_newsqa-validation-2790"], "SR": 0.6875, "CSR": 0.5925480769230769, "EFR": 1.0, "Overall": 0.7178064903846153}, {"timecode": 13, "before_eval_results": {"predictions": ["the riches of Croesus", "Fred Silverman", "occupational burnout", "Saudi Arabia", "political that surrounds us; this is known as a \"creative plea,\" and will usually be interpreted as a plea of not guilty.", "huge mouths armed with groups of large, stiffened cilia that act as teeth", "$20.4 billion", "twelve residential Houses", "The nobility of England", "Christopher Eccleston recorded special video introductions", "stricter discipline based on their power of expulsion", "killed in a horse-riding accident", "1522", "eight", "Of course [the price of oil] is going to rise", "Terra nullius", "Henry Hudson", "chipmunk", "David II BRUCE", "Melbourne", "Albania", "brown trout", "Mayflower", "Johnny Weissmuller", "lacrimal fluid", "George Best", "ali", "The Great British Bake Off", "aliDB", "Fenn Street School", "the Smiths", "penguin", "The Nobel Prize in Literature", "Pakistan", "The Times", "United States", "Big Fat Gypsy Wedding", "beards", "Andes", "Thor", "The Comitium", "\"Moon River\"", "ali Turner", "SW19", "Lancashire", "The Pacific Ocean", "stockbridge Indians", "John Wheelbarrowman", "climatology", "Charlie Brown", "vinaya", "ali pear", "Black Sea", "glucose", "three times", "The episode typically ends as a cliffhanger showing the first few moments of Sam's next leap", "Abu Dhabi", "Craig William Macneill", "terminal brain cancer", "800,000", "Mount Mazama", "giant slalom", "Serie B", "Saoirse Ronan"], "metric_results": {"EM": 0.5, "QA-F1": 0.5568790584415584}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, true, false, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, false, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.12121212121212123, 0.1111111111111111, 1.0, 1.0, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6913", "mrqa_squad-validation-4621", "mrqa_squad-validation-1126", "mrqa_squad-validation-7811", "mrqa_squad-validation-7112", "mrqa_squad-validation-3730", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2777", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-2989", "mrqa_triviaqa-validation-5760", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7614", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-4951", "mrqa_triviaqa-validation-2335", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-9026", "mrqa_hotpotqa-validation-3607", "mrqa_searchqa-validation-1416", "mrqa_searchqa-validation-15315"], "SR": 0.5, "CSR": 0.5859375, "EFR": 0.96875, "Overall": 0.710234375}, {"timecode": 14, "before_eval_results": {"predictions": ["the Cathedral of Saint John the Divine", "Ruhr", "Hulu", "time complexity", "Muslim Iberia", "10 o'clock", "NYPD Blue", "AAUW", "the Magnetophon tape recorder", "evaded being drafted into the Austro-Hungarian Army in Smiljan", "Rotterdam", "if (as WWF argued), population levels would start to drop to a sustainable level (1/3 of current levels, so about 2 billion people", "Charles Dickens", "force", "the best teachers", "imperfect", "laysan albatross", "fiery", "fiery", "the National Gallery of Art", "cape horn", "cape horn", "neptune", "russellus", "6", "turkeys", "wood grissom", "lionhead", "William", "wood", "1", "wood", "james crouch", "Tim Russert", "russell crouchy", "cocoa butter", "Violent Femmes", "oats", "angels", "a laser", "James Fenimore Cooper", "2016", "fiery", "russell", "cape", "cape dust", "Copenhagen", "crouchy", "martin", "davies", "fiery", "russell crouch", "rylands", "Elizabeth Cochran", "sperm plasma", "world's second most populous country after the People's Republic of China", "Nissan", "wood", "Kind Hearts and Coronets", "2012", "poems", "Cyprus", "honda", "russell crouch"], "metric_results": {"EM": 0.375, "QA-F1": 0.434323489010989}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.7428571428571429, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1234", "mrqa_squad-validation-7632", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10428", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-3019", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-1948", "mrqa_searchqa-validation-9250", "mrqa_searchqa-validation-8283", "mrqa_searchqa-validation-14628", "mrqa_searchqa-validation-10011", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-6931", "mrqa_searchqa-validation-7140", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-1914", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-8607", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-2337", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-5613", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-14502", "mrqa_naturalquestions-validation-2350", "mrqa_naturalquestions-validation-8420", "mrqa_triviaqa-validation-4416", "mrqa_triviaqa-validation-2305", "mrqa_triviaqa-validation-7170"], "SR": 0.375, "CSR": 0.571875, "EFR": 0.975, "Overall": 0.7086718750000001}, {"timecode": 15, "before_eval_results": {"predictions": ["trial division", "Go-Ahead", "three to five", "heavy/highway, heavy civil or heavy engineering", "Osama bin Laden", "September 1944", "oxygen", "criminal investigations", "complexity classes", "1963", "Jamukha", "consultant", "711,988", "the Mumbai Rajdhani Express", "President pro tempore of the Senate", "Hugo Weaving", "It was borrowed by the neighbouring Romans, and became the Latin word autumnus", "the Dursley family", "(Razzle) Dazzle", "the somatic nervous system and the autonomic nervous system", "Shruti Sharma", "Disha Vakani", "Kevin Sumlin", "roofing material", "the beginning of the American colonies", "Canada", "two - stroke engines and chain drive", "the English", "a writ of certiorari", "Luke Evans", "Guant\u00e1namo Bay", "a limited period of time", "the Colony of Virginia", "January 2017", "2013", "Tatsumi", "2017", "the Sunni Muslim family", "the Magnavox Odyssey", "the Internet", "Christianity", "India", "the sperm centriole", "between 1923 and 1925", "Moscazzano", "the stems and roots of certain vascular plants", "Lager", "the most recent Super Bowl champions", "ADP and P", "San Francisco", "Hal Derwin", "a more clearly defined episcopate", "2007", "55", "Robert Boyle", "the solar system", "Ascona", "Ludwig van Beethoven", "Diego Maradona", "Akshay Kumar", "Harriet", "Bananas", "the interstitial carbides", "a centerpiece"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5765462835775336}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, false, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.2857142857142857, 1.0, 1.0, 0.7499999999999999, 0.20000000000000004, 1.0, 0.0, 0.0, 0.0, 0.13333333333333333, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.30769230769230765, 0.5, 0.8, 0.5, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.22222222222222224, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8910", "mrqa_squad-validation-3500", "mrqa_squad-validation-6025", "mrqa_squad-validation-6113", "mrqa_naturalquestions-validation-3416", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-7827", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-144", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-1044", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-1770", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-327", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-4072", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-8412", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-8962", "mrqa_hotpotqa-validation-1409", "mrqa_newsqa-validation-3042", "mrqa_searchqa-validation-5471", "mrqa_searchqa-validation-397"], "SR": 0.453125, "CSR": 0.564453125, "EFR": 1.0, "Overall": 0.7121875}, {"timecode": 16, "before_eval_results": {"predictions": ["512-bit primes", "the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n", "National Broadcasting Company", "Marco Polo", "November 2006 and May 2008", "complex", "the symptoms of the Black Death are not unique (and arguably in some accounts may differ from bubonic plague)", "they circulate and are moved around within plant cells, and occasionally pinch in two to reproduce", "xenoliths", "approximately 80 avulsions", "the political party or coalition with the most seats", "April 1887", "two hamburger patties American cheese, \u201cspecial sauce\u201d (a variant of Thousand Island dressing), iceberg lettuce, pickles, and onions, served in a three-part sesame seed bun", "Rock Follies of \u201977", "Montmorency", "Nut & Honey Crunch", "Elton John", "AdSlogansAdslogans", "David Davis Visits Thanet", "credit issues with government debt and not just credit problems  detonating in the financial systems", "Corfu", "midrib", "Leopold II of Belgium", "8 minutes", "the Federal Reserve System", "four red stars", "Cyclops", "oxygen", "Silent Spring", "resistance of an unknown resistor", "white spirit", "Possumhaw Viburnum", "Harold Wilson", "Denmark", "Anna (Julia Roberts)", "James Mason", "Hypervelocity star", "West Point", "the ostrich", "Moby Dick", "William Golding", "the 5th fret", "Hard Rock from the eighties, also known as Glam, Sleaze and Hair Metal style that was established in legendary pubs mainly in Los Angeles, such as \"Rainbow\" and \"The Troubadour\"", "Kim Clijsters", "Les Dennis", "the A38", "Nicola Walker", "Virgin Group", "1949", "Port Talbot", "precipitation", "Famous Epitaph", "Nicola Adams", "Sax Rohmer", "the EU Data Protection Directive 1995 protection", "May 2010", "Bruce R. Cook", "James II", "federal ocean planning", "police chased him and a gunfight ensued", "the Equator", "Aerosmith", "Cesar Millan", "Princeton University"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5489655802581165}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, true, true, true, false], "QA-F1": [0.6666666666666666, 0.3, 0.0, 1.0, 1.0, 1.0, 0.20689655172413793, 0.3157894736842105, 1.0, 0.5, 0.4444444444444445, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.5, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8976", "mrqa_squad-validation-1708", "mrqa_squad-validation-5605", "mrqa_squad-validation-5001", "mrqa_squad-validation-8560", "mrqa_squad-validation-9357", "mrqa_squad-validation-2852", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-376", "mrqa_triviaqa-validation-1067", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-2385", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-456", "mrqa_triviaqa-validation-678", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7387", "mrqa_triviaqa-validation-5143", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-3606", "mrqa_naturalquestions-validation-3930", "mrqa_hotpotqa-validation-1542", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-1537", "mrqa_hotpotqa-validation-4298"], "SR": 0.453125, "CSR": 0.5579044117647058, "EFR": 1.0, "Overall": 0.7108777573529411}, {"timecode": 17, "before_eval_results": {"predictions": ["after the end of the Mexican War", "deportation of the French-speaking Acadian population from the area", "journalist", "Seventy percent", "the modern hatred of the Jews", "Germany and Austria", "inclusions (or clasts) are found in a formation", "Sweynforkbeard", "the King", "eight", "the Sierra Freeway", "Mickey Mouse", "Rugby School", "norway", "merry & mirthful", "Google", "dance", "prostitutes", "Quebec", "Planet of the Apes", "Prince Edward Island", "bilirubin", "frosted", "Virginia Woolf", "Vasco da Gama", "canter", "Musculus gluteus maximus", "1972", "Arbor Day", "Countrywide Financial", "red light", "Triumph The Insult Comic Dog", "Ohio State", "gwalior", "Nikita Khrushchev", "Other Rooms", "Hair", "black Forest", "Robert Stempel", "joan", "sepoy", "last", "2008", "submarines", "Joan", "turtles", "Trinidad and Tobago", "Vladimir Nabokov", "Oreo", "Peter Pan", "hama", "laser pulses", "Phi Beta Phi", "Joel", "Balaam", "vaud", "Prince Philip", "Athenion", "5.3 million", "pilot", "touma", "Pandora", "Tears for Fears", "paper sales company"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5640624999999999}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, false, false, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, true, false, false, false, false, true, false, false, false, true, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 0.4, 0.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-4260", "mrqa_squad-validation-5121", "mrqa_squad-validation-1092", "mrqa_searchqa-validation-6", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-9389", "mrqa_searchqa-validation-4933", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-15777", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-12536", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-2247", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-2347", "mrqa_naturalquestions-validation-230", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-6259", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-2525", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-6435"], "SR": 0.453125, "CSR": 0.5520833333333333, "EFR": 1.0, "Overall": 0.7097135416666667}, {"timecode": 18, "before_eval_results": {"predictions": ["quantum mechanics", "northern (German) shore of the lake", "Alfred Stevens", "state intervention through taxation could boost broader consumption, create wealth, and encourage a peaceful, tolerant, multipolar world order", "difficulty of factoring large numbers into their prime factors", "third", "1886/1887", "clerical", "The Apollo spacecraft", "Linebacker", "2000", "Richard Street", "Jack Chick", "1926 Paris", "burlesque", "Polk County", "Skyscraper", "a schoolteacher", "Dunlop India Ltd", "David Anthony O'Leary", "a family member", "Tranquebar (Tharangambadi)", "Attorney General and as Lord Chancellor of England", "North Dakota", "fennec", "Norwood, Massachusetts", "1993", "switzerland World Championionship", "liquidambarisella", "Battle of Chester", "Flashback", "Kentucky", "Marco Fu", "Francis the Talking Mule", "Kristin Scott Thomas", "Clark Gable", "Christian", "paternalistic policies", "The Hindu Group", "Kealakekua Bay", "1919", "Julia Verdin", "2013", "Guthred", "Centers for Medicare & Medicaid Services (HCFA)", "Australia", "1912", "1941", "Teatro Carlo Felice", "\"How to Train Your Dragon\"", "pronghorn", "ambassador to Ghana", "Life Is a Minestrone", "Monk's", "Sir Ernest Rutherford", "Bump Ball", "duke Cotton (June Brown)", "France", "at least $20 million", "(Ulysses S. Grant)", "Virgil Tibbs", "The eventual closure of Guantanamo Bay prison and CIA \"black site\"", "CNN", "Michael Arrington"], "metric_results": {"EM": 0.40625, "QA-F1": 0.5410094246031746}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 0.28571428571428575, 1.0, 0.1111111111111111, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.8333333333333334, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.5, 0.0, 0.7499999999999999, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-9286", "mrqa_squad-validation-9888", "mrqa_squad-validation-9085", "mrqa_squad-validation-8026", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-4937", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-1227", "mrqa_hotpotqa-validation-2425", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-996", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4678", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-5094", "mrqa_hotpotqa-validation-2366", "mrqa_hotpotqa-validation-0", "mrqa_naturalquestions-validation-339", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-7209", "mrqa_newsqa-validation-2601", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-12442", "mrqa_newsqa-validation-1114"], "SR": 0.40625, "CSR": 0.544407894736842, "EFR": 0.9473684210526315, "Overall": 0.6976521381578947}, {"timecode": 19, "before_eval_results": {"predictions": ["40,000 plant species", "swimming-plates", "MHC I", "Executive Vice President of Football Operations", "two", "Continental Edison Company in France", "Time magazine", "Stan Lebar", "Warszawa", "Troggs", "schizophrenia", "Cressida", "Tom Osborne", "Arnold Schoenberg", "a shih tzu", "In 1956, she became Foreign Minister", "Fiddler on the Roof", "Monopoly", "Al Czervik", "Stanislaw Leszyzynski", "masks", "the same universe", "Tower of London", "reptiles", "Madonna", "googly", "Walter Alston", "Benazir Bhutto", "Coca-Cola", "googly", "Chaillot", "Ibrahim Petrovich Hannibal", "googly", "grow a beard", "Soup Nazi", "Pyrrhus", "Guatemala", "bonds", "Rue Morgue", "huevos rancheros", "August Wilson", "Sacher Torte", "apartheid", "descend", "Lovebirds", "In Blood Diamond", "strawberries", "Daisy Miller", "the arithmetical machine", "Give Me Liberty or Give Me death", "Frank Sinatra", "the Sonnets", "South Africa", "the standard for the Navy's commissioned ships while in commission", "the Infamy Speech of US President Franklin D. Roosevelt", "Costa Del Sol", "river Stour", "Annales de chimie et deimens", "gull-wing doors", "In denying the show's producers the New Jersey Economic Development Authority's 20% tax credit on TV shows filmed or produced in the state", "Roethlisberger, who led the Steelers to Super Bowl titles in 2006 and 2009,", "1994", "Rod Blagojevich", "38"], "metric_results": {"EM": 0.5, "QA-F1": 0.53203125}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, true, true, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false, false, false, true], "QA-F1": [0.0, 0.0, 1.0, 0.8, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4264", "mrqa_squad-validation-4730", "mrqa_squad-validation-375", "mrqa_squad-validation-1239", "mrqa_searchqa-validation-11901", "mrqa_searchqa-validation-835", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-9769", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-1800", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-3762", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-14453", "mrqa_searchqa-validation-6208", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9809", "mrqa_hotpotqa-validation-4813", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-564"], "SR": 0.5, "CSR": 0.5421875, "EFR": 1.0, "Overall": 0.707734375}, {"timecode": 20, "UKR": 0.693359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1340", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1393", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-1893", "mrqa_hotpotqa-validation-204", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2732", "mrqa_hotpotqa-validation-2885", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3480", "mrqa_hotpotqa-validation-3481", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3734", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3929", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3968", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4085", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4441", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4899", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-5054", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5174", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5854", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-722", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-765", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-928", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10012", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1400", "mrqa_naturalquestions-validation-1435", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-230", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2606", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-339", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4466", "mrqa_naturalquestions-validation-4657", "mrqa_naturalquestions-validation-5447", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6506", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7468", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-9921", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1649", "mrqa_newsqa-validation-1843", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-3042", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-970", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10097", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-10971", "mrqa_searchqa-validation-11248", "mrqa_searchqa-validation-11392", "mrqa_searchqa-validation-12648", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-12952", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13026", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14502", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14666", "mrqa_searchqa-validation-14723", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14852", "mrqa_searchqa-validation-14952", "mrqa_searchqa-validation-1523", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-15702", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16156", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16595", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-2105", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2783", "mrqa_searchqa-validation-3385", "mrqa_searchqa-validation-348", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-4068", "mrqa_searchqa-validation-4072", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-455", "mrqa_searchqa-validation-456", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5063", "mrqa_searchqa-validation-5329", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5583", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-577", "mrqa_searchqa-validation-5920", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6298", "mrqa_searchqa-validation-6531", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-6937", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8385", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9557", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10008", "mrqa_squad-validation-1009", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-10251", "mrqa_squad-validation-10273", "mrqa_squad-validation-10285", "mrqa_squad-validation-10335", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10413", "mrqa_squad-validation-10427", "mrqa_squad-validation-10466", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1095", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-1180", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1255", "mrqa_squad-validation-1312", "mrqa_squad-validation-1316", "mrqa_squad-validation-1338", "mrqa_squad-validation-1461", "mrqa_squad-validation-1552", "mrqa_squad-validation-1554", "mrqa_squad-validation-161", "mrqa_squad-validation-1636", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1706", "mrqa_squad-validation-1808", "mrqa_squad-validation-1949", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2069", "mrqa_squad-validation-2318", "mrqa_squad-validation-2369", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2453", "mrqa_squad-validation-2458", "mrqa_squad-validation-2476", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2717", "mrqa_squad-validation-2768", "mrqa_squad-validation-2780", "mrqa_squad-validation-2832", "mrqa_squad-validation-2888", "mrqa_squad-validation-3046", "mrqa_squad-validation-3138", "mrqa_squad-validation-3153", "mrqa_squad-validation-3197", "mrqa_squad-validation-3217", "mrqa_squad-validation-3223", "mrqa_squad-validation-3243", "mrqa_squad-validation-3326", "mrqa_squad-validation-3346", "mrqa_squad-validation-3363", "mrqa_squad-validation-3381", "mrqa_squad-validation-3415", "mrqa_squad-validation-3475", "mrqa_squad-validation-3497", "mrqa_squad-validation-3500", "mrqa_squad-validation-3551", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3641", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3773", "mrqa_squad-validation-3922", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4210", "mrqa_squad-validation-4226", "mrqa_squad-validation-4240", "mrqa_squad-validation-4256", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4348", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-441", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4631", "mrqa_squad-validation-4666", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4870", "mrqa_squad-validation-4902", "mrqa_squad-validation-4921", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5106", "mrqa_squad-validation-5112", "mrqa_squad-validation-5118", "mrqa_squad-validation-512", "mrqa_squad-validation-5167", "mrqa_squad-validation-5242", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5344", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5624", "mrqa_squad-validation-5714", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5958", "mrqa_squad-validation-6015", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6074", "mrqa_squad-validation-6181", "mrqa_squad-validation-6196", "mrqa_squad-validation-6244", "mrqa_squad-validation-6284", "mrqa_squad-validation-6361", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6511", "mrqa_squad-validation-6512", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-6728", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6920", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7083", "mrqa_squad-validation-7112", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7209", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7398", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7867", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7918", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8190", "mrqa_squad-validation-8233", "mrqa_squad-validation-8243", "mrqa_squad-validation-8295", "mrqa_squad-validation-8312", "mrqa_squad-validation-8436", "mrqa_squad-validation-8452", "mrqa_squad-validation-8480", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8557", "mrqa_squad-validation-8596", "mrqa_squad-validation-8647", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8900", "mrqa_squad-validation-8905", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9176", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9334", "mrqa_squad-validation-9335", "mrqa_squad-validation-9345", "mrqa_squad-validation-9351", "mrqa_squad-validation-9371", "mrqa_squad-validation-9411", "mrqa_squad-validation-9484", "mrqa_squad-validation-9489", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-958", "mrqa_squad-validation-9614", "mrqa_squad-validation-9619", "mrqa_squad-validation-964", "mrqa_squad-validation-9750", "mrqa_squad-validation-9761", "mrqa_squad-validation-9892", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9999", "mrqa_triviaqa-validation-1064", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1114", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1320", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-146", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1747", "mrqa_triviaqa-validation-1771", "mrqa_triviaqa-validation-179", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2357", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-270", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-3133", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3606", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4319", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4583", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4705", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5495", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5560", "mrqa_triviaqa-validation-5948", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6643", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6847", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-7067", "mrqa_triviaqa-validation-708", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7470", "mrqa_triviaqa-validation-7595", "mrqa_triviaqa-validation-7742"], "OKR": 0.8671875, "KG": 0.4625, "before_eval_results": {"predictions": ["keratinocytes and macrophages", "Roone Arledge", "The Muslims in the semu class", "The John W. Weeks Bridge", "9th", "the clinical pharmacy movement initially began inside hospitals and clinics.", "US$3 per barrel", "Trajan's Column", "Indian Ocean", "Barb Wire", "Golda Meir", "xerophyte", "anions", "Uranus", "George III", "Frank Spillane", "Iolani Palace", "Gandalf", "Mungo Park", "Squash", "Hodges", "Iron", "Sam Mendes", "San Juan County", "Emeril Lagasse", "\"Shine,\u201d", "Karl Marx", "an ornamental figure or illustration", "four and a half hours", "norway", "Jamaica", "Skylab", "Sydney", "Peter Purves", "Zephyros", "Frobisher Bay", "Dumbo", "Joanna trollope", "Botany Bay", "Peterborough United", "Chelsea", "albedo", "11", "Washington State", "red", "remnants of very massive stars with gravity", "Groucho Marx", "Lady Bird Johnson", "Prince Eddy", "Algeria", "Spain", "Barry White", "gin", "Dennis C. Stewart", "1966", "guitar feedback", "LA Galaxy", "Stephen Johns", "Veracruz, Mexico", "Peter Jennings", "Simon Garfunkel", "Graham", "2009", "Robert Kimmitt"], "metric_results": {"EM": 0.59375, "QA-F1": 0.62890625}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, false, true, false, true, true, false, true, false, false, true, false, true, false, false, true, true, true, true, false, true, true, true, false, true, false, true, false, true, true], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6567", "mrqa_squad-validation-3635", "mrqa_triviaqa-validation-371", "mrqa_triviaqa-validation-5630", "mrqa_triviaqa-validation-554", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3676", "mrqa_triviaqa-validation-69", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-5088", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-4946", "mrqa_triviaqa-validation-192", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1904", "mrqa_naturalquestions-validation-1008", "mrqa_newsqa-validation-2939", "mrqa_searchqa-validation-13803", "mrqa_newsqa-validation-3091"], "SR": 0.59375, "CSR": 0.5446428571428572, "EFR": 0.9615384615384616, "Overall": 0.7058456387362637}, {"timecode": 21, "before_eval_results": {"predictions": ["spring of 1349", "the center of mass", "July 23, 1963", "very rare", "James E. Webb", "eight", "foreclosure even before construction of the tower began", "modern programming practices and enabled business applications", "February 6, 2005", "development of electronic computers", "159", "an Easter egg", "Andhra Pradesh and Odisha", "1975", "John Vincent Calipari", "winter solstice", "29 - year - old Billie Jean King", "Matthias Schleiden and Theodor Schwann", "in rocks and minerals", "October 2, 2017", "avoid the inconvenience of a pure barter system", "four", "Sardis", "Lykan", "in the pachytene stage of prophase I", "Baltimore", "once Upon a Time in India", "Hank J. Deutschendorf", "a toasted wheat bun", "Dan Stevens", "moral tale", "May 5, 1904", "Albert Einstein", "May 26, 2017", "1992", "restored to life", "Master Christopher Jones", "solve its problem of lack of food self - sufficiency", "bore iv and recovering addict", "Portuguese", "part of the normal flora of the human colon", "the church sexton Robert Newman and Captain John Pulling", "Fox Ranch in Malibu Creek State Park", "Spain", "Dmitri Mendeleev", "treaty", "31", "the disputed 1824 presidential election", "12", "local organization", "bandwidth of 2.048 Mbit / s", "twelve", "Paige O'Hara", "ghee", "\"The Crow\"", "Michael Schumacher", "micronutrient-rich diet", "Dr. Jennifer Arnold and husband", "top designers", "honolulu", "mexican", "lungfield Cathedral", "Connally", "liver"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5743481580838771}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.7000000000000001, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 0.6666666666666666, 0.625, 0.0, 1.0, 0.6666666666666666, 0.5882352941176471, 0.0, 0.625, 0.8571428571428571, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.08695652173913043, 0.0, 0.0, 0.4444444444444445, 1.0, 0.7368421052631579, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.1111111111111111, 1.0, 1.0, 1.0, 0.3636363636363636, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1449", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-2572", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-3705", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-10265", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-5961", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8762", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-8136", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-3300", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-2635", "mrqa_hotpotqa-validation-4181", "mrqa_hotpotqa-validation-5873", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-3054", "mrqa_searchqa-validation-14780", "mrqa_searchqa-validation-7212", "mrqa_triviaqa-validation-5476", "mrqa_triviaqa-validation-4182"], "SR": 0.421875, "CSR": 0.5390625, "EFR": 0.972972972972973, "Overall": 0.7070164695945945}, {"timecode": 22, "before_eval_results": {"predictions": ["machine labor in wealthier nations", "an intuitive understanding", "evenly round the body", "2,869", "president of NBC's entertainment division", "the Wesel-Datteln Canal", "Melanie Griffith", "fowls", "lexicographer", "Islamic Republic", "One Flew Over the Cuckoo's Nest", "Barbecue sauce", "Royal Wives", "John Brown", "conhea", "\"Canterbury Tales\"", "France", "Target", "cone-headed", "Russia", "\"Tom Terrific\" and \"The franchise\"", "magnesium", "\"The Swamp Fox\"", "The New York Times", "German Shepherd", "peanuts", "Xinjiang", "Parker House Rolls", "Damascus", "Missouri Western State University", "a hologram", "Thomas Gibson", "the 1096 quake", "Leo Tolstoy", "mother Vineyard", "Virginia Woolf", "apogee", "Cherry Garcia", "in his magic books", "pressed Steel Car Company", "axiom", "Princeton University", "Eric Knight", "Apple", "The Sound of Music", "Pygmalion", "T.S. Eliot", "Andes", "Emeralds", "Asteroid Redirect", "the Nutcracker", "an earthquake", "Conservative Party", "1933", "a gravy with onions and sometimes other vegetables, such as peas, celery or carrots, and topped with grated cheese", "Falstaff", "redhead", "Republican", "Wojbek (bear)", "Bangor Air Force Base", "2009", "superstardom", "12-hour-plus shifts", "a meeting with the president to discuss her son"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5828869047619047}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, true, true, false, true, true, false, false, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, true], "QA-F1": [0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666665, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7541", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-6525", "mrqa_searchqa-validation-4120", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-13527", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-1640", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12316", "mrqa_searchqa-validation-9788", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15009", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-11263", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-13764", "mrqa_searchqa-validation-9991", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-10625", "mrqa_triviaqa-validation-6403", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-2782", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-4061"], "SR": 0.53125, "CSR": 0.5387228260869565, "EFR": 1.0, "Overall": 0.7123539402173913}, {"timecode": 23, "before_eval_results": {"predictions": ["Department for Culture, Media and Sport", "Kevin Harlan", "Khongirad", "the Solim\u00f5es Basin", "49\u201315", "21st century", "on a lifeboat off the coast of Somalia", "Sen. Joe Biden", "18", "Adidas", "she was going to be on the Olympic medals podium.", "the body of the aircraft", "serfs", "the United States", "Michigan", "serfs", "Two", "Russia", "Saruman", "$106,482,500", "March 31.", "Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment", "misdemeanor", "eight in 10", "tennis", "Toy Story", "Christmas parade", "90", "involved in an Internet broadband deal with a Chinese firm.", "$75", "free laundry service", "in Florida", "Jamaleldine", "18th AK47 rifles, two rocket propelled grenade launchers", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East", "more than 1.2 million people", "the most likely candidate to see through \"knee-jerk, ideological\" perspectives and \"bridge the political divide in Washington.\"", "a new constitution", "serfs", "near Grand Ronde, Oregon", "Seasons of My Heart", "raping and murdering", "150", "Anil Kapoor", "misdemeanor assault charges", "Pope Benedict XVI refused Wednesday to soften the Vatican's ban on condom use", "for a long time, people thought this was a small problem.", "serfs", "a model of sustainability", "Kenyan", "a strong and transparent system for measuring our progress toward meeting the goal we set.", "in a motor motorcycle accident", "the Isthmus of Corinth", "Needtobreathe", "Old Trafford", "Shakyamuni", "Kristina Brown", "Shayne Ward", "Wuornos", "Jermaine Lamarr Cole", "Hong Kong", "Dredge", "Henry Wadsworth", "Queen Charlotte Sound"], "metric_results": {"EM": 0.359375, "QA-F1": 0.4770273109243698}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.8, 0.2222222222222222, 0.3333333333333333, 0.0, 1.0, 0.16, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.9411764705882353, 0.4, 0.4, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.5, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-233", "mrqa_squad-validation-4356", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1039", "mrqa_newsqa-validation-3222", "mrqa_newsqa-validation-2635", "mrqa_newsqa-validation-1054", "mrqa_newsqa-validation-1977", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-2925", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1297", "mrqa_newsqa-validation-2621", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-3111", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-3167", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-2067", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-3731", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2167", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-8272", "mrqa_triviaqa-validation-4569", "mrqa_triviaqa-validation-2919", "mrqa_hotpotqa-validation-641", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5469", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-12527"], "SR": 0.359375, "CSR": 0.53125, "EFR": 1.0, "Overall": 0.710859375}, {"timecode": 24, "before_eval_results": {"predictions": ["John Harvard", "late 1886", "the AKS primality test", "Deficiencies", "Gold footballs", "1967", "John Boyd Dunlop", "XVideos", "Niger\u2013Congo", "Sports Illustrated", "Robert Allen Iger", "Regional League North", "2002", "Harsh Times", "Fade Out: The Calamitous Final Days of MGM", "Restoration Hardware", "Louis Silvie \"Louie\" Zamperini", "Keelung", "Minneapolis, Minnesota", "Idisi", "French composer Ambroise Thomas", "Hans Rosenfeldt", "May 4, 2004.", "Everything Is wrong", "Captain", "Smoothie King Center", "Martin Scorsese", "Viacom Media Networks", "1853", "\"Pimp My Ride\"", "Columbia Records", "Umar S. Israilov", "Derry City", "Fort Hood, Texas", "Bonny Hills", "London", "1999", "2006", "scorer", "Zero Mostel", "October 13, 1980", "Chechen Republic", "House of Commons", "1926", "Nikolai Alexandrovich Morozov", "1968", "Bernd Bertie", "Girl Meets World", "January 15, 1975", "Pansexuality, or omnisexuality", "Javan leopard", "2,463,431", "a Celtic people living in northern Asia Minor", "in Narnia", "Pyeongchang County, South Korea", "british", "Leslie Lynch King", "Chechnya", "Aryan Airlines Flight 1625", "Sen. Barack Obama", "her son has strong values.", "Jay Gillespie", "linda", "Hormuz"], "metric_results": {"EM": 0.625, "QA-F1": 0.7116100045787546}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, true, false, false, false, true, false, false, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.7692307692307693, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-793", "mrqa_hotpotqa-validation-108", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-1730", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-1416", "mrqa_hotpotqa-validation-1457", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1543", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-4818", "mrqa_naturalquestions-validation-2990", "mrqa_naturalquestions-validation-3016", "mrqa_triviaqa-validation-7434", "mrqa_triviaqa-validation-4647", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1892", "mrqa_searchqa-validation-4914", "mrqa_searchqa-validation-8010"], "SR": 0.625, "CSR": 0.5349999999999999, "EFR": 1.0, "Overall": 0.711609375}, {"timecode": 25, "before_eval_results": {"predictions": ["1894", "Effective", "a pointless pursuit", "If the water level drops, such that the temperature of the firebox crown increases significantly", "Northern Rail", "South Korea", "paralysis", "discerning golfer", "rudolph", "Pocahontas", "Matlock", "Washington", "Argentina", "The Blue Boy", "the Holy Scriptures", "liriope", "Goddess of Creation", "Delaware", "Pyrenees", "(See Important Quotations Explained)", "Dutch", "Dramatizing History", "Gryffindor", "Sam Allardyce", "most Memorable Moment", "Nick Hancock", "Edward Yorke", "keeper of the Longstone (Fame Islands) lighthouse", "Lil' Kim", "Superman", "Richard Walter Jenkins", "Ouagadougou", "Billy Cox", "Javier Bardem", "Independence Day", "baryons", "Jordan", "So Solid Crew", "Richard Ripley", "(Magi)", "al", "honours", "Ash", "Ian Botham", "squash", "Leander Club", "Sir Stirling Craufurd", "The Book", "Poland", "Play style", "Texas", "albion", "Authority ( derived from the Latin word auctoritas )", "1 mile ( 1.6 km )", "Steve Valentine", "(Power Rangers Zeo)", "Virgin", "UFC Fight Pass", "the Airbus A330-200", "fill a million sandbags and place 700,000 around our city", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability, but Langevin said it now appears that Congress was misled.", "percipient", "amelia earhart", "Final Cut Pro"], "metric_results": {"EM": 0.5, "QA-F1": 0.573735119047619}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, false, false, true, true, true, false, true, false, false, false, true, true, false, true, false, true, true, false, true, false, false, false, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-3207", "mrqa_triviaqa-validation-2266", "mrqa_triviaqa-validation-2393", "mrqa_triviaqa-validation-5112", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-610", "mrqa_triviaqa-validation-5414", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-1607", "mrqa_triviaqa-validation-991", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-4283", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3924", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-4836", "mrqa_naturalquestions-validation-1255", "mrqa_hotpotqa-validation-5822", "mrqa_newsqa-validation-3605", "mrqa_searchqa-validation-8379", "mrqa_searchqa-validation-5324"], "SR": 0.5, "CSR": 0.5336538461538461, "EFR": 1.0, "Overall": 0.7113401442307692}, {"timecode": 26, "before_eval_results": {"predictions": ["at the narrow end", "Levi's Stadium", "specific catechism questions", "Warcis\u0142aw", "the disk", "2016", "the Islamic prophet Muhammad", "Mel Tillis", "pangaea or Pangea", "Stephen Lang", "2018", "Erika Mitchell Leonard", "two years", "edd Kimber", "The Jewel of the Nile", "chas chandler", "photodiode", "August 30, 2011", "Jesse Frederick James Conaway", "dromedarius", "Dan Stevens", "Ben Fransham", "The player character", "1979", "October 27, 2016", "Authority", "and fear of the Lord", "Luther Ingram", "Jodie Foster", "Matt", "Sanchez Navarro", "low to moderate rates of inflation", "1916", "British Columbia, Canada", "Columbia University", "2007", "2001", "Washington Redskins", "Hebrew Bible", "September 14, 2008", "Consular Report of Birth Abroad", "Pasek & Paul", "Chicago metropolitan area", "Francisco Pizarro", "1940", "Norman", "Mary Rose Foster", "John Smith", "The eighth and final season of the fantasy drama television series", "1603", "neutrality", "he cheated on Miley", "banjo", "rarities", "rudolph do- right", "Taylor Swift", "david hemery", "Michael Crawford", "$15.3 million", "14-day", "flooding", "pisco", "david hemery", "Fiscal Policy"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5667253702180173}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, false, true, true, true, false, false, true, false, false, false, false, true, false, true, true, false, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, true, true, false, true, true, false, true, true, false, true, false, false, true, false, false, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.19999999999999998, 0.125, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.5, 1.0, 0.375, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.20000000000000004, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.2, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.5, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_squad-validation-869", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-7812", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-4288", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-10433", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-5017", "mrqa_naturalquestions-validation-1258", "mrqa_naturalquestions-validation-10138", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-1325", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-7650", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7563", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-511", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-4010", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-2054", "mrqa_searchqa-validation-5461"], "SR": 0.46875, "CSR": 0.53125, "EFR": 0.9705882352941176, "Overall": 0.7049770220588235}, {"timecode": 27, "before_eval_results": {"predictions": ["multiple revisions,", "introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage", "Best Actor", "around 300", "anvil", "1999", "Larry Richard Drake", "she is often regarded as the first to recognise the full potential of a \"computing machine\"", "London", "she was a member of the Hawaii Senate, representing the 13th District since 1996", "Hanford Nuclear Reservation", "Thunderbird of Native American tradition", "Mindy Kaling", "Alonso L\u00f3pez", "the world", "Ginger Rogers", "\"The Fugitive\", which in turn was based on the 1960s television series of the same name, created by Huggins.", "churro", "Christies Beach", "eastern", "Arsenal Football Club", "Don Bluth and Gary Goldman", "torpedo boats", "1969 until 1974,", "skiing and mountaineering", "June 11, 1973", "January 18, 1977", "Protestant Christian", "defensive", "Henry John Kaiser", "Saoirse Ronan", "122,067", "Wandsworth, London", "association football YouTube channel", "Daniel Andre Sturridge ( ; born 1 September 1989)", "USS \"Essex\" (CV-9)", "Ron Cowen and Daniel Lipman", "1991's \"The Changing Scottish Landscape\"", "Captain while retaining the substantive rank of Commodore", "Andrea Maffei", "Tomasz Adamek", "Russell T Davies", "Geraldine Sue Page", "Manchester", "3,000", "Umberto II", "the Canadian province of Ontario", "Daphnis et Chlo\u00e9", "saloon-keeper and Justice of the Peace", "John Lennon", "International Imitation Hemingway Competition", "The Emperor of Japan", "Mary Rose Foster", "1986", "transposed from one scale to another for various purposes, often to accommodate the range of a vocalist", "golden anniversary", "Australian", "stroke", "the troop movement was part of a normal rotation and that Thai soldiers had not gone anywhere they were not permitted to be.", "amanda Knox's aunt", "100,000", "brandy", "The Beatles' Yellow Submarine Album", "North Carolina"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6075469163359788}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true], "QA-F1": [1.0, 0.29629629629629634, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.5714285714285715, 1.0, 0.28571428571428575, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5714285714285715, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.4444444444444445, 0.8, 1.0, 0.8571428571428571, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.125, 1.0, 0.33333333333333337, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.125, 0.0, 1.0, 0.0, 0.6875000000000001, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1672", "mrqa_squad-validation-7819", "mrqa_hotpotqa-validation-3943", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-1559", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-5770", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-5091", "mrqa_hotpotqa-validation-2335", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-3187", "mrqa_hotpotqa-validation-23", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-672", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-4543", "mrqa_naturalquestions-validation-4497", "mrqa_triviaqa-validation-1818", "mrqa_triviaqa-validation-2192", "mrqa_newsqa-validation-310", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-9122"], "SR": 0.453125, "CSR": 0.5284598214285714, "EFR": 1.0, "Overall": 0.7103013392857143}, {"timecode": 28, "before_eval_results": {"predictions": ["left foot", "the USSR", "common flagellated protists", "Mildred", "Adidas", "George W. Bush", "billions of dollars", "one", "The Beatles", "Communist Party of Nepal (Unified Marxist-Leninist)", "Pope Benedict XVI", "around 8 p.m. local time", "Sri Lanka's Tamil rebels", "64", "CNN", "at least 12 months", "Mike Griffin, Disney\\'s vice president for public affairs.", "Adriano", "he stopped farming and trade at gun point", "183", "American Civil Liberties Union", "the deployment of unmanned drones, including possibly the Predator drones used in Iraq and Afghanistan", "40 militants and six Pakistan soldiers", "Liverpool Street Station", "137", "8", "Congressional auditors", "Jacob", "his own country", "Markland Locks and Dam", "4,000", "Oaxacan countryside of southern Mexico", "increase the flow of water passing through its network of dams.", "Catholic League", "August 19, 2007", "10 years", "not guilty", "Japanese officials", "her daughter and granddaughter", "consumer confidence", "he was mad at the U.S. military because of what they had done to Muslims", "six", "nearly 28 years", "July 18, 1994", "Dan Brown", "digging up graves of American soldiers held as slaves by Nazi Germany during World War II.", "agreed to cooperate with interrogators", "Chao Phraya River and its many canals.", "two", "an antihistamine and epinephrine auto-injector", "4,000", "he discussed foreplay, sexual conquests and how he picks up women, all taboo subjects in deeply conservative Saudi Arabia", "Jean F Kernel", "around 10 : 30am", "14th century", "crossword puzzle", "Christian Wulff", "Bajec-Lapajne", "general secretary", "the George Washington Bridge", "Johns Creek", "wasteland", "Aristotle\\'s lantern", "seafood"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5947033955627706}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, false, false, true, false, false, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5454545454545454, 1.0, 1.0, 0.2857142857142857, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.9090909090909091, 0.20000000000000004, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.5, 0.32, 0.4, 0.0, 1.0, 1.0, 1.0, 0.125, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.2, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8652", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-1314", "mrqa_newsqa-validation-960", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-274", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-3530", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-2568", "mrqa_newsqa-validation-3856", "mrqa_newsqa-validation-2651", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-198", "mrqa_newsqa-validation-3264", "mrqa_newsqa-validation-3435", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3290", "mrqa_newsqa-validation-1131", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3640", "mrqa_newsqa-validation-2422", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-203", "mrqa_naturalquestions-validation-5769", "mrqa_naturalquestions-validation-9060", "mrqa_triviaqa-validation-7076", "mrqa_triviaqa-validation-6923", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-2787", "mrqa_searchqa-validation-12506", "mrqa_searchqa-validation-4780"], "SR": 0.46875, "CSR": 0.5264008620689655, "EFR": 1.0, "Overall": 0.7098895474137931}, {"timecode": 29, "before_eval_results": {"predictions": ["Cadeby", "Near Sankt Goarshausen", "10,006,721", "anchovy", "lovebird", "Chicago", "monk seal", "Wilhelm", "albion", "Take Me Out to the Ballgame", "prost", "Morse", "The Antipodes Islands", "St. Erasmus", "Little Tommy Tucker", "H. G. Wells", "Pauline Wayne", "Illegible", "Scrabble", "Mussolini", "valkyries", "rain", "lamb", "Jodie Foster", "Elysian Fields", "Five Easy pieces", "Thomas Edison", "Manhattan Project", "Charles", "Laura Corey", "Enchanted", "Liberty", "mov", "Autobahn", "Destiny's Child", "Byron", "a spoon", "Prednisone", "Margot Fonteyn", "Coral", "\"McMillan and wives\"", "John F. Kennedy", "the Honorary Doctor", "Galileo", "Existentialism", "John Donne", "british", "Annie's", "murder", "Charles Lindbergh", "a queen", "synaptic vesicles", "a candidate state", "James W. Marshall at Sutter's Mill in Coloma, California", "a single, implicitly structured data item", "South Korea", "\"foreigner\"", "M*A*S*H", "The Boeing EA-18G Growler", "5249", "Fleetwood Mac", "around 3.5 percent of global greenhouse emissions.", "reading a novel", "HPV"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5776830808080808}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, false, false, false, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, false, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.7272727272727272, 0.0, 0.0, 0.0, 0.5, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4444444444444445, 0.5, 0.8333333333333333, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_searchqa-validation-10014", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-8042", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2615", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-5788", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-7230", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-13377", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-11420", "mrqa_searchqa-validation-5715", "mrqa_searchqa-validation-14999", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-9472", "mrqa_searchqa-validation-15174", "mrqa_searchqa-validation-10889", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-8563", "mrqa_naturalquestions-validation-2956", "mrqa_triviaqa-validation-4268", "mrqa_triviaqa-validation-935", "mrqa_newsqa-validation-3376", "mrqa_newsqa-validation-1372"], "SR": 0.46875, "CSR": 0.5244791666666666, "EFR": 1.0, "Overall": 0.7095052083333333}, {"timecode": 30, "UKR": 0.681640625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1080", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1263", "mrqa_hotpotqa-validation-1323", "mrqa_hotpotqa-validation-1361", "mrqa_hotpotqa-validation-1409", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1906", "mrqa_hotpotqa-validation-2020", "mrqa_hotpotqa-validation-2064", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2122", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2183", "mrqa_hotpotqa-validation-2222", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2816", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-2937", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-2985", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-3032", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3180", "mrqa_hotpotqa-validation-3206", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3607", "mrqa_hotpotqa-validation-3669", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3797", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3969", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4146", "mrqa_hotpotqa-validation-4166", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-435", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4543", "mrqa_hotpotqa-validation-4662", "mrqa_hotpotqa-validation-471", "mrqa_hotpotqa-validation-472", "mrqa_hotpotqa-validation-474", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-4996", "mrqa_hotpotqa-validation-5045", "mrqa_hotpotqa-validation-508", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5259", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-5303", "mrqa_hotpotqa-validation-5345", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5534", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5894", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-996", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10273", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1165", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1707", "mrqa_naturalquestions-validation-1728", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2438", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2609", "mrqa_naturalquestions-validation-2658", "mrqa_naturalquestions-validation-2956", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3199", "mrqa_naturalquestions-validation-3217", "mrqa_naturalquestions-validation-328", "mrqa_naturalquestions-validation-3499", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3965", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4236", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4369", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4814", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6050", "mrqa_naturalquestions-validation-6052", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6770", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6998", "mrqa_naturalquestions-validation-7035", "mrqa_naturalquestions-validation-7101", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7650", "mrqa_naturalquestions-validation-7811", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8849", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9278", "mrqa_naturalquestions-validation-9311", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1212", "mrqa_newsqa-validation-1275", "mrqa_newsqa-validation-1319", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-1537", "mrqa_newsqa-validation-1665", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1836", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2248", "mrqa_newsqa-validation-2408", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2601", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2870", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2919", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2939", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3199", "mrqa_newsqa-validation-349", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3784", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4017", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4051", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4169", "mrqa_newsqa-validation-527", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-564", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-668", "mrqa_newsqa-validation-820", "mrqa_searchqa-validation-10060", "mrqa_searchqa-validation-10093", "mrqa_searchqa-validation-10173", "mrqa_searchqa-validation-10241", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10507", "mrqa_searchqa-validation-10669", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11465", "mrqa_searchqa-validation-12078", "mrqa_searchqa-validation-1236", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-1289", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13330", "mrqa_searchqa-validation-13569", "mrqa_searchqa-validation-13651", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14468", "mrqa_searchqa-validation-14512", "mrqa_searchqa-validation-14514", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-1529", "mrqa_searchqa-validation-15315", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-15637", "mrqa_searchqa-validation-1565", "mrqa_searchqa-validation-15845", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16233", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-177", "mrqa_searchqa-validation-1823", "mrqa_searchqa-validation-1880", "mrqa_searchqa-validation-2040", "mrqa_searchqa-validation-2162", "mrqa_searchqa-validation-2202", "mrqa_searchqa-validation-2674", "mrqa_searchqa-validation-3014", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-3955", "mrqa_searchqa-validation-429", "mrqa_searchqa-validation-4355", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-457", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4745", "mrqa_searchqa-validation-478", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5368", "mrqa_searchqa-validation-547", "mrqa_searchqa-validation-5574", "mrqa_searchqa-validation-5591", "mrqa_searchqa-validation-5760", "mrqa_searchqa-validation-60", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-6208", "mrqa_searchqa-validation-621", "mrqa_searchqa-validation-628", "mrqa_searchqa-validation-6417", "mrqa_searchqa-validation-668", "mrqa_searchqa-validation-6712", "mrqa_searchqa-validation-7233", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7688", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7896", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8348", "mrqa_searchqa-validation-8401", "mrqa_searchqa-validation-8449", "mrqa_searchqa-validation-8578", "mrqa_searchqa-validation-8900", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-915", "mrqa_searchqa-validation-9151", "mrqa_searchqa-validation-9529", "mrqa_searchqa-validation-9991", "mrqa_squad-validation-10008", "mrqa_squad-validation-10111", "mrqa_squad-validation-10207", "mrqa_squad-validation-1021", "mrqa_squad-validation-10251", "mrqa_squad-validation-10279", "mrqa_squad-validation-10351", "mrqa_squad-validation-10351", "mrqa_squad-validation-10427", "mrqa_squad-validation-10474", "mrqa_squad-validation-1079", "mrqa_squad-validation-1092", "mrqa_squad-validation-1116", "mrqa_squad-validation-1138", "mrqa_squad-validation-121", "mrqa_squad-validation-1219", "mrqa_squad-validation-1241", "mrqa_squad-validation-1449", "mrqa_squad-validation-1461", "mrqa_squad-validation-1636", "mrqa_squad-validation-1681", "mrqa_squad-validation-1856", "mrqa_squad-validation-1951", "mrqa_squad-validation-1973", "mrqa_squad-validation-1982", "mrqa_squad-validation-2005", "mrqa_squad-validation-2194", "mrqa_squad-validation-2318", "mrqa_squad-validation-2434", "mrqa_squad-validation-2506", "mrqa_squad-validation-2569", "mrqa_squad-validation-2609", "mrqa_squad-validation-2670", "mrqa_squad-validation-2768", "mrqa_squad-validation-312", "mrqa_squad-validation-3153", "mrqa_squad-validation-3223", "mrqa_squad-validation-3326", "mrqa_squad-validation-3363", "mrqa_squad-validation-3456", "mrqa_squad-validation-3497", "mrqa_squad-validation-354", "mrqa_squad-validation-3575", "mrqa_squad-validation-3633", "mrqa_squad-validation-3683", "mrqa_squad-validation-3724", "mrqa_squad-validation-375", "mrqa_squad-validation-3752", "mrqa_squad-validation-3904", "mrqa_squad-validation-3922", "mrqa_squad-validation-3930", "mrqa_squad-validation-3998", "mrqa_squad-validation-4110", "mrqa_squad-validation-4226", "mrqa_squad-validation-4264", "mrqa_squad-validation-4294", "mrqa_squad-validation-4343", "mrqa_squad-validation-4357", "mrqa_squad-validation-4361", "mrqa_squad-validation-4458", "mrqa_squad-validation-4491", "mrqa_squad-validation-4595", "mrqa_squad-validation-4614", "mrqa_squad-validation-4621", "mrqa_squad-validation-4631", "mrqa_squad-validation-4631", "mrqa_squad-validation-4729", "mrqa_squad-validation-4730", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4902", "mrqa_squad-validation-4965", "mrqa_squad-validation-4978", "mrqa_squad-validation-50", "mrqa_squad-validation-5098", "mrqa_squad-validation-510", "mrqa_squad-validation-5118", "mrqa_squad-validation-5242", "mrqa_squad-validation-525", "mrqa_squad-validation-5303", "mrqa_squad-validation-5320", "mrqa_squad-validation-5350", "mrqa_squad-validation-5363", "mrqa_squad-validation-5389", "mrqa_squad-validation-5590", "mrqa_squad-validation-5605", "mrqa_squad-validation-5624", "mrqa_squad-validation-5844", "mrqa_squad-validation-5859", "mrqa_squad-validation-5865", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-5954", "mrqa_squad-validation-5973", "mrqa_squad-validation-6025", "mrqa_squad-validation-6181", "mrqa_squad-validation-6284", "mrqa_squad-validation-6286", "mrqa_squad-validation-629", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6394", "mrqa_squad-validation-6467", "mrqa_squad-validation-6518", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6753", "mrqa_squad-validation-6791", "mrqa_squad-validation-680", "mrqa_squad-validation-687", "mrqa_squad-validation-6873", "mrqa_squad-validation-6921", "mrqa_squad-validation-70", "mrqa_squad-validation-7011", "mrqa_squad-validation-7013", "mrqa_squad-validation-7040", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7082", "mrqa_squad-validation-7153", "mrqa_squad-validation-7206", "mrqa_squad-validation-7207", "mrqa_squad-validation-7230", "mrqa_squad-validation-7303", "mrqa_squad-validation-7311", "mrqa_squad-validation-7430", "mrqa_squad-validation-7566", "mrqa_squad-validation-7646", "mrqa_squad-validation-7674", "mrqa_squad-validation-7694", "mrqa_squad-validation-7765", "mrqa_squad-validation-7887", "mrqa_squad-validation-7895", "mrqa_squad-validation-791", "mrqa_squad-validation-7937", "mrqa_squad-validation-8135", "mrqa_squad-validation-8167", "mrqa_squad-validation-8233", "mrqa_squad-validation-8295", "mrqa_squad-validation-8452", "mrqa_squad-validation-85", "mrqa_squad-validation-8516", "mrqa_squad-validation-8596", "mrqa_squad-validation-89", "mrqa_squad-validation-8910", "mrqa_squad-validation-9029", "mrqa_squad-validation-9304", "mrqa_squad-validation-9325", "mrqa_squad-validation-9351", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9512", "mrqa_squad-validation-9562", "mrqa_squad-validation-9565", "mrqa_squad-validation-9578", "mrqa_squad-validation-9614", "mrqa_squad-validation-9895", "mrqa_squad-validation-9895", "mrqa_squad-validation-99", "mrqa_squad-validation-9920", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-134", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1382", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1697", "mrqa_triviaqa-validation-1827", "mrqa_triviaqa-validation-1849", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2030", "mrqa_triviaqa-validation-2054", "mrqa_triviaqa-validation-2056", "mrqa_triviaqa-validation-2075", "mrqa_triviaqa-validation-2080", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2321", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2344", "mrqa_triviaqa-validation-2408", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-255", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2647", "mrqa_triviaqa-validation-2676", "mrqa_triviaqa-validation-2758", "mrqa_triviaqa-validation-2919", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3102", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3473", "mrqa_triviaqa-validation-3476", "mrqa_triviaqa-validation-3751", "mrqa_triviaqa-validation-3876", "mrqa_triviaqa-validation-39", "mrqa_triviaqa-validation-4173", "mrqa_triviaqa-validation-4379", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4944", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5194", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5857", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5978", "mrqa_triviaqa-validation-6091", "mrqa_triviaqa-validation-6187", "mrqa_triviaqa-validation-632", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6403", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6435", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6537", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6805", "mrqa_triviaqa-validation-6842", "mrqa_triviaqa-validation-6923", "mrqa_triviaqa-validation-6939", "mrqa_triviaqa-validation-6972", "mrqa_triviaqa-validation-6979", "mrqa_triviaqa-validation-7295", "mrqa_triviaqa-validation-7360", "mrqa_triviaqa-validation-7390", "mrqa_triviaqa-validation-7473", "mrqa_triviaqa-validation-7474", "mrqa_triviaqa-validation-7742", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-922"], "OKR": 0.8125, "KG": 0.46640625, "before_eval_results": {"predictions": ["30 November 1963,", "1892", "motivated students,", "Nikolai Trubetzkoy", "June 1925", "\"bushwhackers\"", "British", "Argentina", "the Baudot code", "Jacksonville", "the DTM and its successor \u2014 the International Touring Car Championship", "Switzerland", "Maryland", "Tennessee", "John Ford", "Operation Watchtower", "34.9 kilometres", "1 December 1948", "omnisexuality", "Westfield Tea Tree Plaza", "southwest Denver, Colorado", "Atlanta", "Atlanta Braves", "Scunthorpe", "2004", "(born 17 July 1935)", "Towards the Sun", "the heart of the southern (Dolomitic) Alps", "Angus Brayshaw", "An impresario", "Islamic Studies", "January 30, 1930", "Sulla", "The Australian women's national soccer team", "Jaguar Land Rover", "tempo", "Milk Barn", "(born 19 January 1980)", "Timothy Dowling", "London", "Jane", "Patricia Arquette", "Otto Hahn", "AMC", "31", "Robert Paul \"Robbie\" Gould III", "Edward Trowbridge Collins Sr.", "he played Gavin Kossef", "twenty-three", "ethnic, cultural or racial group", "A. R. Rahman", "Akosua Busia", "September 8, 2017", "mid-ocean ridges", "Burbank, California", "shows the feelings and hardships of not just horses from long ago, but even horses now.", "Heisenberg", "the White Sea Canal", "workers agreed to stave off the strike.", "Eintracht Frankfurt", "Republican", "p puppies", "Nickelback", "Will Grace"], "metric_results": {"EM": 0.46875, "QA-F1": 0.588959478021978}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, true, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, false, false, false, false, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.19999999999999998, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.8, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.20000000000000004, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.6666666666666666, 1.0, 0.28571428571428575, 1.0, 0.0, 0.4, 0.0, 0.0, 0.8, 1.0, 0.0, 0.3076923076923077, 0.0, 0.6666666666666666, 0.4, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7744", "mrqa_hotpotqa-validation-458", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1311", "mrqa_hotpotqa-validation-411", "mrqa_hotpotqa-validation-1502", "mrqa_hotpotqa-validation-5311", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5500", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-5503", "mrqa_hotpotqa-validation-2465", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-4732", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-5877", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-4487", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-1629", "mrqa_hotpotqa-validation-4173", "mrqa_hotpotqa-validation-2377", "mrqa_hotpotqa-validation-4389", "mrqa_hotpotqa-validation-3223", "mrqa_hotpotqa-validation-5187", "mrqa_naturalquestions-validation-3287", "mrqa_naturalquestions-validation-6012", "mrqa_triviaqa-validation-1106", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-6433", "mrqa_newsqa-validation-1898", "mrqa_newsqa-validation-2032", "mrqa_searchqa-validation-6730"], "SR": 0.46875, "CSR": 0.5226814516129032, "EFR": 1.0, "Overall": 0.6966456653225805}, {"timecode": 31, "before_eval_results": {"predictions": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "500", "7.63\u00d725mm Mauser", "the Harpe brothers", "French", "1944", "Giuseppina Tuissi", "2007", "Marko Tapani \" Marco\" Hietala", "Shankar", "Cody Miller", "\"Grimjack\"", "video game", "Carson City", "The Nick Cannon Show", "\"Mickey Mouse\"", "ten", "Bergen County", "the 1824 Constitution of Mexico", "Hellenism", "Tomorrowland", "Jaffrey", "Frederick Alexander Lindemann", "Rawhide", "astronomer and composer of German and Czech-Jewish origin", "Don DeLillo", "The Seduction of Hillary Rodham", "balloon Street, Manchester", "9,984", "the Rose Garden", "Spain", "Deep Purple", "Abdul Razzak Yaqoob", "Port Macquarie", "Dan Castellaneta", "Rosie O'Donnell", "Saturday", "Taylor Alison Swift", "Miller Brewing", "Centers for Medicare and Medicaid Services", "Indianapolis Motor Speedway", "Clark County, Nevada", "Predators", "Boba Fett", "the High Court of Admiralty", "\"An All-Colored Vaudeville Show\"", "German", "Lucy Muringo Gichuhi", "Valley Falls", "dice", "Courteney Cox", "Jewish", "JackScanlon", "Leonard Bernstein", "62", "France", "carbonic acid", "a secretary", "five", "a nuclear weapon", "2005", "Beastie Boys", "Wisconsin", "a car"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7296875}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.16666666666666666, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-1267", "mrqa_hotpotqa-validation-2177", "mrqa_hotpotqa-validation-4628", "mrqa_hotpotqa-validation-5326", "mrqa_hotpotqa-validation-1269", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-2744", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-1185", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-5501", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-4528", "mrqa_hotpotqa-validation-5521", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-728", "mrqa_naturalquestions-validation-4995", "mrqa_triviaqa-validation-1534", "mrqa_newsqa-validation-3106", "mrqa_searchqa-validation-10363", "mrqa_searchqa-validation-3835"], "SR": 0.59375, "CSR": 0.52490234375, "EFR": 1.0, "Overall": 0.6970898437499999}, {"timecode": 32, "before_eval_results": {"predictions": ["Hugh Downs", "education", "Till Death Us Do Part", "Laputa", "Leeds", "Hank Snow Tour", "LSD", "Geoffrey Plantagenet", "Albania", "Tombstone", "Travis", "Jaguar Land Rover", "Diego Maradona", "Sudan", "spirit", "football", "multiplayer online role-playing game", "a spirit", "Greece", "(1932-1934)", "Steve Coogan", "Sophie Marceau", "Boston Marathon", "Carl Smith", "Zeppelin", "Jorge Lorenzo", "spirit", "chess", "Terry Dawson", "Arthur, Prince of Wales", "the Grail", "Ronald Reagan", "Charlie Fenton", "climate", "Paris", "Hammer", "the liver", "Guildford Dudley", "Amoco Cadiz", "John Howard", "David", "His Holiness", "12th", "Cornell University", "Love Scottish Islands", "Death & Hells Angels", "sugar", "Lost Weekend", "Stockholm", "Switzerland", "alpine", "fruit", "the senior-most judge of the supreme court", "early Christians of Mesopotamia", "Representatives", "Machine Gun Kelly", "Central Avenue", "middleweight division", "Mokotedi Mpshe, head of the National prosecutinging Authority", "spirit work", "comfort those in mourning,", "Canterbury", "Harold Macmillan", "a marsh"], "metric_results": {"EM": 0.5, "QA-F1": 0.5364583333333333}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, true, false, true, true, true, false, false, false, false, true, false, true, true, true, false, false, true, false, false, false, false, true, true, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, true, true, false, false, true, true, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-6944", "mrqa_triviaqa-validation-5547", "mrqa_triviaqa-validation-2930", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-6683", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-1179", "mrqa_triviaqa-validation-3813", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-4327", "mrqa_triviaqa-validation-414", "mrqa_triviaqa-validation-435", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-7163", "mrqa_triviaqa-validation-7226", "mrqa_triviaqa-validation-5600", "mrqa_triviaqa-validation-1178", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-2330", "mrqa_triviaqa-validation-3239", "mrqa_triviaqa-validation-1798", "mrqa_naturalquestions-validation-3569", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1023", "mrqa_hotpotqa-validation-3988", "mrqa_newsqa-validation-1816", "mrqa_newsqa-validation-4060", "mrqa_searchqa-validation-6833"], "SR": 0.5, "CSR": 0.5241477272727273, "EFR": 1.0, "Overall": 0.6969389204545455}, {"timecode": 33, "before_eval_results": {"predictions": ["Brown v. Board of Education of Topeka", "after midnight one night", "Adidas", "Ennis", "Stratfor", "low prices,", "Jaime Andrade", "673,000", "girls", "possible victims of physical and sexual abuse.", "beach", "gasoline", "vote wozniak", "The plane, an Airbus A320-214", "two Emmys for work on the 'Columbo' series starring Peter Falk.", "a ice jam,", "mikey", "abduction of minors.", "uriah", "navy dress", "Jenny Sanford,", "Florida", "Bhola", "Clifford Harris,", "uriah held favorable views of America, the lowest level among 47 countries surveyed.", "Nirvana", "stephen", "james paulis", "race or its understanding of what the law required it to do.", "vote on hope.", "between June 20 and July 20.", "paul paul fidler", "misdemeanor failure to appear in court warrant", "1.2 million", "100,000", "Heshmat Tehran", "crossfire by insurgent small arms fire,", "2002", "uriko Savoie was given custody of the children and agreed to remain in the United States.", "a \"new chapter\" of improved governance", "Arsene Wenger", "Sunday", "Hamas,", "uriah", "Atlantic Ocean", "movahedi", "Himalayan", "Jiverly Wong,", "sexual assault with a minor", "Louvre", "September 21.", "digging ditches.", "Supplemental oxygen", "Prince Bao", "Narendra Modi", "stephen", "74", "mmorpg", "Musicology", "Steve Buscemi", "Mick Jackson", "West Virginia", "Gary Oldman", "paris"], "metric_results": {"EM": 0.390625, "QA-F1": 0.4379092261904762}, "metric_results_detailed": {"EM": [true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, false, false, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.14285714285714285, 0.8333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1626", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3432", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2086", "mrqa_newsqa-validation-2062", "mrqa_newsqa-validation-3459", "mrqa_newsqa-validation-3438", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3782", "mrqa_newsqa-validation-3830", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-2709", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2200", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1604", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-3936", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-2199", "mrqa_newsqa-validation-2785", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-2953", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-4062", "mrqa_naturalquestions-validation-997", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5093", "mrqa_triviaqa-validation-7079", "mrqa_hotpotqa-validation-4643", "mrqa_searchqa-validation-44"], "SR": 0.390625, "CSR": 0.5202205882352942, "EFR": 1.0, "Overall": 0.6961534926470588}, {"timecode": 34, "before_eval_results": {"predictions": ["most common form of school discipline", "boudins,", "Robert A. Heinlein", "Mumbai", "Indiana", "animals", "the gloved one first unveiled his signature move", "Laos", "Vislor Turlough", "Westminster Abbey", "Battle of Agincourt", "alicyclic", "dennis taylor", "Kent", "\"a handbags\"", "Diptera", "a turkey", "transuranic elements", "Harold Shipman", "River Wyre", "Carson City", "All Things Must Pass", "Hong Kong", "Mercury", "Doctor Who", "North Yorkshire", "George Blake", "Nirvana", "Janis Joplin", "Kenya", "Manchester City F.C.", "Moscow", "Caracas", "Oil of Olay", "hair", "Decoupage", "Bathsheba", "Ennio Morricone", "Dita Von Teese", "collapsible support assembly", "Republicans", "Argentina", "French", "North Dakota", "the internal kidney structures", "a rabbit", "Rocky Marciano", "The Benedictine Order", "Coventry to Leicester Motorway", "dennis taylor", "Jack Lemmon", "four", "1965", "the second half", "a lightning strike", "Danny Glover", "Trey Parker and Matt Stone", "from 140 to 219 passengers", "Hundreds", "Democrats", "31 meters (102 feet)", "dennis Galahad", "Sacramento", "Hawaii"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6489583333333333}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, false, true, true, false, false, true, false], "QA-F1": [0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.5333333333333333, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1924", "mrqa_triviaqa-validation-5998", "mrqa_triviaqa-validation-3979", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-3192", "mrqa_triviaqa-validation-2181", "mrqa_triviaqa-validation-1890", "mrqa_triviaqa-validation-3950", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-3588", "mrqa_triviaqa-validation-5548", "mrqa_triviaqa-validation-2474", "mrqa_triviaqa-validation-331", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-7774", "mrqa_triviaqa-validation-4133", "mrqa_triviaqa-validation-4398", "mrqa_triviaqa-validation-4317", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-475", "mrqa_naturalquestions-validation-10490", "mrqa_hotpotqa-validation-398", "mrqa_newsqa-validation-3976", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-3920"], "SR": 0.609375, "CSR": 0.5227678571428571, "EFR": 1.0, "Overall": 0.6966629464285714}, {"timecode": 35, "before_eval_results": {"predictions": ["late 1970s", "Aristotle", "strawberry daiquiri", "box of Golgotha", "armadillos", "joe burbage", "Danielle Steel", "Absalom", "joe dawne", "Pirate's Treasure Map", "davies roosevelt", "South Africa", "the Seine river", "alcohol", "Alyssa Milano", "if a man bites a dog", "the Star-Spangled Banner", "The Rolling Stones", "Lincoln's Inn Hall", "a knight", "Benjamin Franklin", "\"Judas!\"", "box-shaped container with a handle", "lunar mare that sits within the Tranquillitatis basin on the Moon", "Spain", "Cadillac", "Matt Damon", "a magnum opus", "shalom", "white", "Arthur James Balfour", "a crossword", "Easton", "scrabble", "Iceland", "mosee mercer", "a chamber", "wyre Sinclair", "Stephen Vincent Bent", "Brooke Brooke Hogan", "a war", "Nancy Sinatra", "David", "Pinot noir", "Robert Lowell", "\"Bob ate the pie\"", "Richmond", "Mar 24, 2015", "Amy Tan", "Florence", "Pandora", "Grenada", "Himalayas", "Kusha", "Heroes and Villains", "dennis bieber", "silver", "the result of a complex number raised to the zero power", "2015", "October 20, 2017", "Columbus", "top winds weakened to 110 mph,", "piedad Cordoba", "Martin \"Al\" Culhane,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5828125}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, true, false, false, false, false, false, false, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, false, true, true, true, true, false, true, true, false, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11147", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-1076", "mrqa_searchqa-validation-14070", "mrqa_searchqa-validation-7528", "mrqa_searchqa-validation-2248", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-9007", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-3188", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-5293", "mrqa_searchqa-validation-9559", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-11471", "mrqa_searchqa-validation-15863", "mrqa_searchqa-validation-8076", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-9", "mrqa_searchqa-validation-3703", "mrqa_searchqa-validation-559", "mrqa_searchqa-validation-3922", "mrqa_searchqa-validation-9192", "mrqa_naturalquestions-validation-10026", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-6212", "mrqa_triviaqa-validation-4710", "mrqa_newsqa-validation-2307"], "SR": 0.46875, "CSR": 0.5212673611111112, "EFR": 1.0, "Overall": 0.6963628472222222}, {"timecode": 36, "before_eval_results": {"predictions": ["1085", "Bowe Bergdahl", "people around the world commented, pondered, and paid tribute to pop legend Michael Jackson,", "Zed", "her husband", "the Delta Queen steamboat,", "recall communications", "Tim Clark, Matt Kuchar and Bubba Watson", "a long-range missile on its launch pad,", "75", "Brad Blauser", "male veterans struggling with homelessness and addiction.", "CNN/Opinion Research Corporation", "the world's tallest building,", "the underprivileged.", "Ku Klux Klan", "Felipe Calderon", "137", "1-0", "ancing With the Stars", "\"It shot up the Japanese singles chart, reaching No 4, the highest ever position for a first time enka release.", "Michael Jackson", "military commissions", "Venezuela", "John and Elizabeth Calvert", "the Nazi war crimes suspect who had been ordered deported to Germany,", "a number of calls, and those calls were intriguing, and we're chasing those down now.", "Mandi Hamlin", "Iraq", "Janet Napolitano", "Russian concerns that the defensive shield could be used for offensive aims.", "Bob Dole,", "they can demonstrate they have been satisfactorily treated", "Oklahoma to points east, with 8 to 10 inches of snow possible in some locales,", "people needed to lose 40 [pounds] to over 150.", "Malawi", "246", "the skull", "Six", "Izzat Ibrahim al-Douri,", "eight in 10", "one-shot victory in the Bob Hope Classic on the final hole to join his father as a winner of the tournament.", "in the Muslim north of Sudan", "37", "Clifford Harris,", "Bea Arthur,", "Susan Boyle", "Florida", "UNICEF", "The European Union", "27-year-old", "45", "Sally Phillips", "April 1, 2002", "One Direction", "Runcorn", "oxygen", "Ben R. Guttery", "Preston, Lancashire, UK", "2003 to 1996", "Ecuador", "Halloween", "Gregor Mendel", "Peter & Jane"], "metric_results": {"EM": 0.359375, "QA-F1": 0.5184039829699996}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, false, true, false, false, false, false, true, false, false, true, false, true, false, false, false, true, false, true, false, false, false, true, false, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.1, 1.0, 0.0, 0.8, 0.0, 1.0, 0.2857142857142857, 0.6666666666666666, 0.0, 0.0, 1.0, 0.8, 0.0, 1.0, 0.8, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5333333333333333, 0.35294117647058826, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5217391304347826, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5714285714285715, 0.0, 0.0, 0.6666666666666666, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1174", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1572", "mrqa_newsqa-validation-2245", "mrqa_newsqa-validation-2360", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-3610", "mrqa_newsqa-validation-1149", "mrqa_newsqa-validation-3790", "mrqa_newsqa-validation-744", "mrqa_newsqa-validation-3687", "mrqa_newsqa-validation-3444", "mrqa_newsqa-validation-3130", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2742", "mrqa_newsqa-validation-4202", "mrqa_newsqa-validation-4211", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-1144", "mrqa_newsqa-validation-276", "mrqa_newsqa-validation-1016", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-1302", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-1755", "mrqa_newsqa-validation-152", "mrqa_newsqa-validation-3380", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2078", "mrqa_newsqa-validation-3069", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5006", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-2625", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10010", "mrqa_searchqa-validation-4136"], "SR": 0.359375, "CSR": 0.5168918918918919, "EFR": 0.975609756097561, "Overall": 0.6906097045978906}, {"timecode": 37, "before_eval_results": {"predictions": ["$20 billion.", "Veneto region of Northern Italy", "Preston", "Jean de Florette", "George Orwell", "Eric Allan Kramer", "eight", "Patrick Swayze", "Nineteen Eighty-Four", "Ben Ainslie", "1905", "Sex Drive", "Yoruba", "Archbishop of Canterbury", "Motown music artist Marvin Gaye", "Chrysler", "Portal", "critical quotations", "Terrence Alexander Jones", "\"S&M\"", "one", "Evey", "O", "The Grandmaster", "highland regions of Scotland", "1980", "Nobel Prize in Physics", "Russian Empire", "Philipstown", "Hilary Erhard Duff", "Ogallala", "October 21, 2016", "All of the Lights", "Everything Iswrong", "Massapequa", "1988", "Dan Brandon Bilzerian", "Arctic tundra soil", "1967", "\"lifestyle cities\"", "Macbeth", "band director", "1875", "$10\u201320 million", "Mandarin", "Fester Addams", "March", "The Frog Prince", "Esp\u00edrito Santo Financial Group", "Los Angeles", "The New Yorker", "Walter Egan", "`` Wah - Wah ''", "The First Battle of Bull Run", "Alison Krauss", "Richie McCaw", "eardrum", "mental health and recovery.", "the Bronx", "billions of dollars in", "Diamond", "Simon Legree", "Sideways", "pindaric"], "metric_results": {"EM": 0.40625, "QA-F1": 0.533779761904762}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, false, true, false, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, false, true, false, false, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.5714285714285715, 0.8, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.6666666666666666, 1.0, 0.0, 0.4, 0.0, 1.0, 0.8, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3431", "mrqa_hotpotqa-validation-5342", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-5655", "mrqa_hotpotqa-validation-5425", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-1703", "mrqa_hotpotqa-validation-5651", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-5404", "mrqa_hotpotqa-validation-365", "mrqa_hotpotqa-validation-459", "mrqa_hotpotqa-validation-3722", "mrqa_hotpotqa-validation-3538", "mrqa_hotpotqa-validation-3931", "mrqa_hotpotqa-validation-2813", "mrqa_hotpotqa-validation-1581", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-3877", "mrqa_hotpotqa-validation-65", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-767", "mrqa_triviaqa-validation-6464", "mrqa_triviaqa-validation-3408", "mrqa_newsqa-validation-1312", "mrqa_searchqa-validation-12752", "mrqa_searchqa-validation-8753"], "SR": 0.40625, "CSR": 0.5139802631578947, "EFR": 0.9736842105263158, "Overall": 0.6896422697368421}, {"timecode": 38, "before_eval_results": {"predictions": ["definition of Turing machines", "Nepal", "Everybody Wang Chung", "Panama", "a gastropod shell", "Thailand", "Mary Kies", "graham henry", "Georgie Porgie", "Mork & Mindy", "Catherine de' Medici", "graham taming", "Benito", "Utah", "Fort Leavenworth", "INXS", "Flat", "wildebeest", "Extra-Terrestrial Intelligence", "Arthur", "graham henry", "Clara Barton", "Nine to Five", "coral", "moose", "Winnipeg", "Nicaragua", "Arthur Miller", "Princess Margaret", "75 years", "the Seaweed", "feminism", "Space Coast Convention Center", "the gallbladder", "Good Earth", "midway", "Liechtenstein", "Custer", "the Temple", "salt", "Gloria Steinem", "Queen Louise's sister", "Tonga", "Minos", "Gulliver", "a pina colada", "Sea World", "a coup de grce", "Tyra Banks", "Dick Gephardt", "Bucharest", "Manley", "by fermenting dietary fiber into short - chain fatty acids ( SCFAs )", "attack on Pearl Harbor", "a positive lens", "a cade", "Greek", "the Church of England Diocese of Ely", "Lowndes County", "Zimbabwe", "his son-in-law Cleve Landsberg", "Anjuna beach", "stuck to with remarkably little internal drama. He won it with unparalleled fundraising and an overwhelming ground game. And he won it after various challenges and turning them to", "1,305"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5512094630515683}, "metric_results_detailed": {"EM": [false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, false, false, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6060606060606061, 1.0, 0.5, 0.0, 0.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.0, 0.9824561403508771, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1814", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-948", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-14384", "mrqa_searchqa-validation-8692", "mrqa_searchqa-validation-10873", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-1647", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-8804", "mrqa_searchqa-validation-14900", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-13802", "mrqa_searchqa-validation-2090", "mrqa_searchqa-validation-8233", "mrqa_searchqa-validation-15378", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-5436", "mrqa_searchqa-validation-16431", "mrqa_searchqa-validation-2836", "mrqa_searchqa-validation-13649", "mrqa_naturalquestions-validation-7393", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-6052", "mrqa_triviaqa-validation-2115", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-4989", "mrqa_hotpotqa-validation-4053", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2980", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-3579"], "SR": 0.4375, "CSR": 0.5120192307692308, "EFR": 1.0, "Overall": 0.6945132211538462}, {"timecode": 39, "before_eval_results": {"predictions": ["over the age of 18", "Nalini Negi", "it is illegal to sell alcohol before 1 pm on any sunday", "1984 Summer Olympics", "the IB Primary Years Program", "the medulla oblongata", "Andreas Vesalius", "`` The Crossing ''", "Nicole DuPort", "Brian Johnson", "Palmer Williams Jr.", "After World War I", "a series of prospective studies that examine epidemiology and the long - term effects of nutrition, hormones, environment, and nurses'work - life on health and disease development", "the Stanford Cardinal", "Wake County", "60 by West All - Stars", "RMS Titanic", "Sally Field", "Elizabeth Dean Lail", "Ravi Shastri", "chili con carne, nachos, hard tacos and fajitas", "6 March 1983", "David Kaye", "James Arthur", "DNA", "Antarctica", "during the American Civil War", "Blake Garrett Rosenthal", "to preserve the Union, a cause based on American nationalism", "Sir Ernest Rutherford", "Buddhism", "1889", "diurnal and insectivorous", "Deuteronomy 5 : 4 -- 25", "Sam Nico", "$19.8 trillion", "Sleeping with the Past", "boy", "1820s", "the Chernobyl Nuclear Power Plant", "Vienna", "scientists such as Antoine - Laurent de Lavoisier and John Newlands", "Dalveer Bhandari", "periodic table", "John Ernest Crawford", "July 2014", "\\' Can't Get You Out of My Head", "1924", "Americans", "`` central '' or `` middle '', and gu\u00f3 ( \u570b / \u56fd )", "Sedimentary rock", "\"GEORGES Bizet\"", "a waterfowl", "glass", "Kristofferson", "the Israeli Declaration of Independence", "two Nobel Peace Prizes", "18", "2002", "the Gaslight Theater", "\"M*A*S*H\"", "Johnny Cash", "Louis Rukeyser", "Matt Groening"], "metric_results": {"EM": 0.515625, "QA-F1": 0.59294430535862}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, false, true, false, true, false, false, true, true, true, true, false, false, false, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, true, false, false, true, false, false, true, false, false, true, false, true, true, true, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 0.16, 1.0, 0.2666666666666667, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 0.5, 0.0, 1.0, 0.0, 0.06451612903225806, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.56, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8068", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-3257", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4359", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-10454", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-8637", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-1222", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-1699", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-1327", "mrqa_naturalquestions-validation-5624", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-590", "mrqa_hotpotqa-validation-3871", "mrqa_newsqa-validation-2020", "mrqa_searchqa-validation-14218"], "SR": 0.515625, "CSR": 0.512109375, "EFR": 0.967741935483871, "Overall": 0.6880796370967742}, {"timecode": 40, "UKR": 0.623046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1597", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1781", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2240", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2542", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2664", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3220", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3867", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-388", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3988", "mrqa_hotpotqa-validation-4006", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4299", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4967", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5610", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5708", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-65", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_hotpotqa-validation-803", "mrqa_hotpotqa-validation-855", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-10490", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1236", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2668", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-395", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-458", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4796", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-4869", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6201", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6584", "mrqa_naturalquestions-validation-6637", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7015", "mrqa_naturalquestions-validation-7039", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-75", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-801", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9323", "mrqa_naturalquestions-validation-9330", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9650", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-2454", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2697", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3028", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3075", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3196", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4018", "mrqa_newsqa-validation-4027", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-539", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-779", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10304", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11246", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-1190", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12576", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12740", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14184", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14583", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16545", "mrqa_searchqa-validation-16889", "mrqa_searchqa-validation-2032", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3249", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-3983", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4780", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-532", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5460", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-583", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7109", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7776", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9122", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9559", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1071", "mrqa_squad-validation-1088", "mrqa_squad-validation-1138", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1708", "mrqa_squad-validation-1808", "mrqa_squad-validation-1814", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2609", "mrqa_squad-validation-2888", "mrqa_squad-validation-3086", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-3415", "mrqa_squad-validation-350", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4162", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-434", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4783", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5001", "mrqa_squad-validation-5098", "mrqa_squad-validation-5167", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5318", "mrqa_squad-validation-5374", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5889", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6511", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-6690", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7457", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-7961", "mrqa_squad-validation-806", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8602", "mrqa_squad-validation-8647", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8910", "mrqa_squad-validation-8910", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9085", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9411", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1311", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1336", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1566", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2147", "mrqa_triviaqa-validation-2242", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-308", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3131", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3488", "mrqa_triviaqa-validation-3650", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3939", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4182", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4397", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4534", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5208", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5754", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5942", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6387", "mrqa_triviaqa-validation-6400", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6445", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6746", "mrqa_triviaqa-validation-6761", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-7415", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7567", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-7719", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-822"], "OKR": 0.7734375, "KG": 0.4421875, "before_eval_results": {"predictions": ["the architect or engineer", "Naples", "dengue", "Jefferson", "Rubik's Cube", "kettledrum", "meringue", "let (department manager) go, but can't do it until I have someone to replace him", "one-hand", "the Department of Justice", "Jimmy \" Jimmy\" Doolittle", "John Brown", "# Quiz # Question", "One Hundred Years of Solitude", "Frida Kahlo", "the rulership of a prince", "wodehouse", "Sicily", "(litho), air (atmo), water (hydro), and life (bio)", "William Pitt the Younger", "gourmet", "Madonna", "Welterweight", "yoyo", "Winston-Salem", "\"There Is Nothin' Like A Dame\"", "Edinburgh, Scotland", "(anaplasma, ehrlichia)", "three", "Colorado columbine", "Italy", "kwanzaa", "Woody Guthrie", "Nigeria", "William Jennings Bryan", "The Spiderwick Chronicles", "an amended FEC Form 1", "Chicago", "the Great Pyramid", "Herod", "Alaska", "\"more likely to be killed by a terrorist\"", "Asia", "anaphylaxis", "\"Wendy's Story\"", "Kuwait", "1.o", "\"The Day of the Locust\"", "carmen", "Emilio Estevez", "Call of the Wild", "Gibraltar", "the National League ( NL ) champion Chicago Cubs and the American League ( AL ) champion Cleveland Indians", "1923", "Bahrain", "El Hiero", "Hans Lippershey", "\"Sippin' on Some Sizzurp,\"", "Larry Eustachy,", "Isabella II", "Stanford", "three", "a shortfall in their pension fund", "2018"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5702850877192982}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, false, false, true, true, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, false, true, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 0.5, 1.0, 0.0, 0.20000000000000004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.631578947368421, 1.0, 1.0, 0.5, 1.0, 0.4, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-8782", "mrqa_searchqa-validation-11769", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-4950", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-9762", "mrqa_searchqa-validation-7034", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-58", "mrqa_searchqa-validation-11346", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-16742", "mrqa_searchqa-validation-13023", "mrqa_searchqa-validation-8025", "mrqa_searchqa-validation-13271", "mrqa_searchqa-validation-11968", "mrqa_searchqa-validation-57", "mrqa_searchqa-validation-16298", "mrqa_searchqa-validation-13046", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-12335", "mrqa_searchqa-validation-14691", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-3504", "mrqa_searchqa-validation-5600", "mrqa_searchqa-validation-11661", "mrqa_searchqa-validation-5758", "mrqa_naturalquestions-validation-6197", "mrqa_triviaqa-validation-6424", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4568", "mrqa_newsqa-validation-3554"], "SR": 0.46875, "CSR": 0.5110518292682926, "EFR": 0.9705882352941176, "Overall": 0.6640623879124821}, {"timecode": 41, "before_eval_results": {"predictions": ["$40,000", "the Stockton & Darlington Railway", "aurochs", "Israel", "(Charles) Grimaldi", "Harper", "Fred Astaire", "Humphrey Bogart", "onda", "Gagarin", "Joseph Priestley", "Smiley", "jacks", "Rosslyn Chapel", "Hispaniola", "the Zulus", "blood", "Ironside", "Aristotle", "(Jefferson) Sachs", "South Sudan", "Monday", "the Dannebrog", "Secretary of State William H. Seward", "the Atlantic Ocean and the Pacific Ocean", "Antoine Lavoisier", "Daily Bugle", "Tuscany (Tuscany) Region", "Battle of the Alamo", "Beaujolais", "Edmund Cartwright", "Der Stern", "el hier van Noort", "the popes", "kautta", "(Jefferson) Ramsay", "Wisconsin", "(John) Barbirolli", "Eton College", "Harrods", "Charles Dickens", "(Jefferson) Baxter", "Chiang Kai-shek", "a leaf", "sternum", "Portuguese", "mexico", "Greece", "Ed Miliband", "marriage", "iron lung", "The Mandate of Heaven", "in the fascia surrounding skeletal muscle", "the Major Molineux", "the Distinguished Service Cross", "Indian classical", "1998", "15", "an eye for an eye.", "Arabic, French and English", "Schwalbe", "the owl", "Seinfeld", "Cress"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7107886904761904}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, false, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, true, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.8571428571428571, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-2666", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-2802", "mrqa_triviaqa-validation-2302", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-5810", "mrqa_triviaqa-validation-3390", "mrqa_triviaqa-validation-5940", "mrqa_triviaqa-validation-7332", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-837", "mrqa_triviaqa-validation-3715", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4961", "mrqa_triviaqa-validation-4598", "mrqa_triviaqa-validation-1676", "mrqa_naturalquestions-validation-7945", "mrqa_hotpotqa-validation-1596", "mrqa_searchqa-validation-11859", "mrqa_searchqa-validation-762"], "SR": 0.609375, "CSR": 0.5133928571428572, "EFR": 1.0, "Overall": 0.6704129464285715}, {"timecode": 42, "before_eval_results": {"predictions": ["Sybilla of Normandy", "beta decay", "Norman", "John Prine and Roger Cook", "number 11", "1983", "a virtual reality simulator", "Banquo", "Pakistan", "October 1, 2015", "longwave radio", "Isaiah Amir Mustafa", "negotiates treaties with foreign nations, but treaties enter into force if ratified by two - thirds of the Senate", "Paracelsus", "John C. Reilly", "Strabo", "Gloria ( Lisa Stelly)", "Utah, Arizona, Wyoming, and Oroville, California", "the epidermis", "in serial format in Collier's Weekly magazine ( 27 January -- 16 April 1898 )", "1770 BC", "a total of 360 members who are elected in single - member constituencies using the simple majority ( or first - past - the - post ) system", "a single, implicitly structured data item in a table", "1959", "Gunpei Yokoi", "216", "Justin Bieber", "Australia", "ideology", "160km / hour", "Chinese", "Andrew Garfield", "the 90s", "Gibraltar", "electrons", "by the hip, and under the left shoulder, he carried a crutch, which he managed with wonderful dexterity, hopping about upon it like a bird", "Lulu", "a ranking used in combat sports, such as boxing or mixed martial arts", "Los Angeles", "The 1972 Dolphins were the third NFL team to accomplish a perfect regular season, and won Super Bowl VIII", "Virgil Tibbs", "Jaye P. Morgan", "1961", "passwords, commands and data", "National Industrial Recovery Act ( NIRA)", "adenosine diphosphate", "General George Washington", "Barbara Eve Harris as Colonel O'Donnell", "Lake Wales", "1923", "Johannes Gutenberg of Mainz", "Wichita", "tina turner", "polly", "Henry J. Kaiser", "Marilyn Martin", "SARS", "tax incentives", "linda Siemionow", "over a kilometer (3,281 feet) high.", "neon", "polly and the Prisoner of Azkaban", "the ark of acacia", "island of Basilan"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5687285433971773}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, true, false, true, true, false, true, true, true, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 0.0, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 0.15384615384615385, 0.8, 0.1, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.08333333333333333, 1.0, 0.7272727272727273, 0.0, 0.21052631578947367, 0.0, 0.0, 1.0, 0.0, 0.888888888888889, 0.8, 1.0, 0.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.7272727272727272, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-8137", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4092", "mrqa_naturalquestions-validation-2222", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-2334", "mrqa_naturalquestions-validation-8484", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-10118", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-7701", "mrqa_naturalquestions-validation-4123", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-7553", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-3533", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-5104", "mrqa_naturalquestions-validation-3802", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-3001", "mrqa_naturalquestions-validation-6901", "mrqa_triviaqa-validation-7013", "mrqa_hotpotqa-validation-4021", "mrqa_hotpotqa-validation-153", "mrqa_newsqa-validation-1549", "mrqa_newsqa-validation-1091", "mrqa_searchqa-validation-909", "mrqa_searchqa-validation-7408", "mrqa_newsqa-validation-3408"], "SR": 0.421875, "CSR": 0.5112645348837209, "EFR": 0.972972972972973, "Overall": 0.6645818765713388}, {"timecode": 43, "before_eval_results": {"predictions": ["article 30", "a caramel latte", "Sheffield United", "Microsoft", "Wat Tyler from Kent", "john Wayne", "Scotland", "the Earth", "James Hogg", "Texas", "Rhino", "Pears soap", "Rhine River", "Louis XVI", "(Jefferson) Jefferson", "fifty-three", "Uranus", "Plato", "the chord", "john checker", "There Goes the bride", "romania", "coal", "Henry I", "Relpromax Antitrust Inc.", "eukharistos", "baseball", "Bear Grylls", "jawless fish", "Tanzania", "Val Doonican", "tittle", "E. T. A. Hoffmann", "united Volta", "Edward Knoblock", "an elephant", "the Creel Committee", "New Zealand", "Mendip Hills", "graffiti", "Jane Austen", "\"From the mountains, to the prairies,", "a trademark", "boxing", "Benjamin Disraeli", "I Wanna Be Like You", "(The Great Leap)", "Jan van Eyck", "Rabin", "Shania Twain", "john Nash", "electron donors to electron acceptors via redox", "`` It Ain't Over'til It's Over ''", "a pH indicator, a color marker, and a dye", "Nicolas Winding Refn", "137\u201373", "Elvis' Christmas Album", "restrictions on nighttime raids of Afghan homes and compounds,", "Robert Park", "Nearly eight in 10", "Cairo", "Jackson Pollock", "an elk", "tax incentives for businesses hiring veterans as well as job training"], "metric_results": {"EM": 0.5, "QA-F1": 0.56383547008547}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, false, true, true, true, false, false, true, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, true, true, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.4444444444444445, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077]}}, "before_error_ids": ["mrqa_triviaqa-validation-1389", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-6684", "mrqa_triviaqa-validation-3326", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-931", "mrqa_triviaqa-validation-6272", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-1012", "mrqa_triviaqa-validation-5296", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-2265", "mrqa_triviaqa-validation-7755", "mrqa_triviaqa-validation-2185", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3243", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-722", "mrqa_triviaqa-validation-3855", "mrqa_triviaqa-validation-4759", "mrqa_triviaqa-validation-3105", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-7849", "mrqa_hotpotqa-validation-501", "mrqa_newsqa-validation-2862", "mrqa_newsqa-validation-1551"], "SR": 0.5, "CSR": 0.5110085227272727, "EFR": 1.0, "Overall": 0.6699360795454545}, {"timecode": 44, "before_eval_results": {"predictions": ["199695\u20131696", "Aamir Khan", "Medea", "Alfonso Cuar\u00f3n", "1990", "in Austria, south Germany, German Switzerland, and Slovenia", "June 24, 1935", "Frederick Martin \"Fred\" Mac Murray", "Kauffman Stadium", "concentration camp", "The 2013\u201314 Premier League", "the demarcation line between the newly emerging states, the Second Polish Republic, and the Soviet Union", "1995 to 2012", "wilson", "Rothschild", "united states", "smith", "Gwyneth Paltrow, Ewan McGregor", "alternate uniform", "1874", "carbon", "North Dakota and Minnesota", "smith", "Zambezi river", "smith", "smith", "smith", "Chesley Burnett \"Sully\" Sullenberger III", "Francis", "American League (AL) Central division", "Cleveland Browns", "a fibre optic cable with TOSLINK connectors", "Dutch", "Battle of Prome", "35,000", "eastern shore of the Firth of Clyde, Scotland", "first and only U.S. born world grand prix champion", "2012", "19th district", "smith", "Lev Ivanovich yashin", "carrefour", "smith", "Benjam\u00edn f\u00e9lix", "the Bank of China Tower", "Spanish conquistadors", "Battle of Hightower", "9", "Antiochia", "smith", "200,000", "2,140 kilometres ( 1,330 mi )", "Highlands County, Florida", "honey bees", "squash", "smith", "soy", "18", "smith", "a collapsed ConAgra Foods plant", "Everest", "I.M. Pei", "The Lady with the Lamp", "a fort"], "metric_results": {"EM": 0.3125, "QA-F1": 0.3858580221861472}, "metric_results_detailed": {"EM": [false, false, false, true, false, false, true, false, true, true, true, true, true, false, false, false, false, false, true, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false], "QA-F1": [0.0, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33333333333333337, 1.0, 0.1818181818181818, 0.0, 0.125, 1.0, 1.0, 1.0, 0.6, 0.0, 0.0, 0.28571428571428575, 0.0, 0.8, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1050", "mrqa_hotpotqa-validation-827", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-1836", "mrqa_hotpotqa-validation-2376", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-2244", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-5307", "mrqa_hotpotqa-validation-4797", "mrqa_hotpotqa-validation-5793", "mrqa_hotpotqa-validation-3360", "mrqa_hotpotqa-validation-4053", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-3484", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-4934", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3554", "mrqa_hotpotqa-validation-3729", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-800", "mrqa_hotpotqa-validation-1698", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-1257", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4994", "mrqa_hotpotqa-validation-2715", "mrqa_hotpotqa-validation-3060", "mrqa_naturalquestions-validation-10354", "mrqa_naturalquestions-validation-10098", "mrqa_naturalquestions-validation-8186", "mrqa_triviaqa-validation-3252", "mrqa_triviaqa-validation-3878", "mrqa_newsqa-validation-732", "mrqa_newsqa-validation-3637", "mrqa_newsqa-validation-2766", "mrqa_searchqa-validation-9156", "mrqa_searchqa-validation-16341"], "SR": 0.3125, "CSR": 0.5065972222222221, "EFR": 0.9772727272727273, "Overall": 0.66450836489899}, {"timecode": 45, "before_eval_results": {"predictions": ["Islam", "Spain", "Jesse of Bethlehem", "Oklahoma City", "insulin", "Marrix", "John Mortimer", "John Walsh", "Moldova", "Mnemosyne", "London", "Sunset Boulevard", "Country Life Magazine", "The Lion King", "leading roles,", "Wyoming", "Benedictus", "La's", "Javier Bardem", "8", "Lee Harvey Oswald", "curved mirrors", "Sherlock Holmes", "Bayern Munchen", "Rotherham and Barnsley", "Passover", "Bobby Kennedy", "Skylab", "Portugal", "Rhine", "Confucius", "Japan", "Elvis", "London", "Yves Saint Laurent", "Phoenicia", "bobby Moore", "A Frighteners", "Jerez de la Frontera", "plac\u0113b\u014d", "Sandy Welch's", "FC Porto", "Vinyl", "argument form", "Rochdale", "Portuguese", "Madagascar", "Helsinki", "The Landlord's Game", "myxoma virus", "Sri Lanka", "8.7 %", "along Interstate 20", "Mercedes -Benz GL - Class", "Denmark", "eastern India", "World Famous Gold & Silver Pawn Shop", "Little Rock Nine", "Tim Masters,", "South Africa", "Dental brackets", "ABBA", "Phoenicia", "Tom Coughlin"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5473958333333333}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, false, false, true, true, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, false], "QA-F1": [1.0, 0.0, 0.5, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-4568", "mrqa_triviaqa-validation-3437", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-2213", "mrqa_triviaqa-validation-2324", "mrqa_triviaqa-validation-6095", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-4856", "mrqa_triviaqa-validation-3970", "mrqa_triviaqa-validation-1407", "mrqa_triviaqa-validation-5064", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2776", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-3539", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-2485", "mrqa_triviaqa-validation-2853", "mrqa_naturalquestions-validation-9824", "mrqa_naturalquestions-validation-10353", "mrqa_naturalquestions-validation-1586", "mrqa_hotpotqa-validation-4222", "mrqa_newsqa-validation-3710", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_searchqa-validation-5528", "mrqa_hotpotqa-validation-3195"], "SR": 0.484375, "CSR": 0.5061141304347826, "EFR": 0.9393939393939394, "Overall": 0.6568359889657445}, {"timecode": 46, "before_eval_results": {"predictions": ["several", "Mattel", "stromatolites", "Rugby School", "a modem", "Clinton", "George Herbert Walker Bush", "Pennsylvania State University", "chicago", "Berlusconi", "leviathan", "Mending Wall", "wombat", "a crystal", "thunder", "Josephine Beauharnais", "Louise de La Vallire", "Sony's SonicStage 2.0", "Neptune", "Annie", "Romeo and Juliet", "KLM", "Captain America", "X-Men 2", "the retina", "a goat", "Planet of the Apes", "cheese", "India", "Reading Railroad", "In a 1948 work,", "Mexican cheese traditionally made from skimmed... or Feta cheese", "the Justice Department", "her 1993 album Yes I Am", "Ignace Jan Paderewski", "christian", "Charles M. Schulz", "a striper", "Frida Kahlo", "Jane Austen", "Rikki-Tikki tavi", "mutual fund", "concave polygons", "Tennessee 37214", "lm", "a ferry", "New York Times", "The Oresteian Trilogy", "cereal", "christian kesselring", "the Florida Panthers", "Thomas Mundy Peterson", "USS Chesapeake", "From 1900 to 1946", "Skye terrier", "alligators", "jerees", "London", "John Snow", "Ghanaian national team", "soldiers had not gone anywhere they were not permitted to be.", "Afghanistan", "Tuesday", "1955"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5364583333333333}, "metric_results_detailed": {"EM": [true, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-5523", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-1339", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-1294", "mrqa_searchqa-validation-4703", "mrqa_searchqa-validation-4871", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-2851", "mrqa_searchqa-validation-13179", "mrqa_searchqa-validation-15142", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-9246", "mrqa_searchqa-validation-8593", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7238", "mrqa_searchqa-validation-14169", "mrqa_searchqa-validation-15513", "mrqa_searchqa-validation-370", "mrqa_searchqa-validation-117", "mrqa_searchqa-validation-6124", "mrqa_searchqa-validation-14944", "mrqa_searchqa-validation-2212", "mrqa_searchqa-validation-5800", "mrqa_searchqa-validation-6336", "mrqa_searchqa-validation-12880", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-14657", "mrqa_naturalquestions-validation-4341", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-3147", "mrqa_newsqa-validation-1216"], "SR": 0.484375, "CSR": 0.5056515957446808, "EFR": 0.9696969696969697, "Overall": 0.6628040880883301}, {"timecode": 47, "before_eval_results": {"predictions": ["Kauai", "The Lord Mayor of London", "Shel Silverstein", "wine", "trolley", "Liverpool", "the South Dakota monument", "Cyrus the Younger", "Greece", "Jim Bunning", "George Harrison", "The Last Starfighter", "a woofer", "Cubism", "Dune", "the Panama Canal", "Eragon", "vacuum tubes", "Drug Rehab and Treatment Center", "Darfur", "bicentennial", "midway", "George Gershwin", "alpacas", "the Atlantic Ocean", "Heredity", "Bicentennial Man", "the rod", "heart attack", "Elke Sommer", "Ivan the Terrible", "Flav", "Fulgencio Batista", "The Indianapolis 500", "twist", "(Rabbie) Burns", "the cuckoos", "London", "beetle", "Joan of Arc", "palindromes", "quid", "Rob Van Winkle", "A Night at the Roxbury", "Steinbeck", "Eric Knight", "Heroes", "the Ganges (Ganga) River", "Thomas Mann", "The Rise and Fall Of", "Sing Sing", "Rajendra Prasad", "1945", "an edited version of a film ( or television episode, music video, commercial, or video game )", "Buckinghamshire", "Charles I", "The Lion", "Lord's Resistance Army", "Polynesian", "Netflix", "the immediate release into the United States of 17 Chinese Muslims", "Casa de Campo International Airport", "July", "the greater risk-adjusted return of value stocks over growth stocks"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6720511412607}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, false, false, true, false], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.18181818181818182, 0.0, 1.0, 0.9411764705882353, 0.7692307692307693, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-8918", "mrqa_searchqa-validation-2555", "mrqa_searchqa-validation-7581", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-5857", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-1850", "mrqa_searchqa-validation-12232", "mrqa_searchqa-validation-10161", "mrqa_searchqa-validation-4913", "mrqa_searchqa-validation-10425", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-6008", "mrqa_searchqa-validation-7136", "mrqa_searchqa-validation-2258", "mrqa_searchqa-validation-7103", "mrqa_searchqa-validation-2691", "mrqa_naturalquestions-validation-1664", "mrqa_naturalquestions-validation-3342", "mrqa_triviaqa-validation-7151", "mrqa_triviaqa-validation-1325", "mrqa_triviaqa-validation-344", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-2504", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-3958", "mrqa_hotpotqa-validation-741"], "SR": 0.53125, "CSR": 0.5061848958333333, "EFR": 0.9666666666666667, "Overall": 0.6623046875}, {"timecode": 48, "before_eval_results": {"predictions": ["Jaws 2", "The Dorchester Hotel", "(Robert) Reynolds", "James Patterson", "The Incredibles", "a cheetah", "Charlie Brown", "the god Odin", "Japan", "Sea-Monkeys", "the daffodils", "\"24\"", "Neil Simon", "Voyager 2", "a gulls", "The Nez Perce", "Eva Peron", "the solas", "the Hawkeye", "the 2016 NBA Draft", "Swiffer", "the Huckleberry Hound", "Austria", "Bourne", "Brazil", "The Trojan Horse", "the Chagos", "the Colosseum", "Cambodia", "(John) Lennon and the Medicine Show", "Songs of Innocence", "the Uvula", "a catechism of the Council of Trent", "Genesis 35", "Scrubs", "Cheyenne", "the Black Sea", "King George", "Frank Sinatra", "the Zambezi", "a Mongolian rikishi", "The Foundations of Bible History of Israel", "The Police", "Jamestown", "the band Wild Cherry", "(Robert) Ford", "St. Francis", "the Lemon Meringue pie", "Melville", "Tarzan and Jane", "( Brett) Favre", "1919", "eight years", "Taron Egerton", "Batman", "Stieg Larsson", "marriage", "Tomasz Adamek", "The Thomas Crown Affair", "1866", "new materials -- including ultra-high-strength steel and boron", "Sri Lanka", "The EU naval force", "ruritania"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6688289141414141}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, true, true, true, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.7272727272727273, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-16827", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-9860", "mrqa_searchqa-validation-6975", "mrqa_searchqa-validation-4739", "mrqa_searchqa-validation-7756", "mrqa_searchqa-validation-16716", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-14290", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-8513", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-15461", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-11715", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-15335", "mrqa_triviaqa-validation-6041", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-2895"], "SR": 0.609375, "CSR": 0.5082908163265306, "EFR": 1.0, "Overall": 0.6693925382653061}, {"timecode": 49, "before_eval_results": {"predictions": ["1918", "Venice", "Carmen", "islands of Scilly", "the Temple Mount", "sexual", "fourteen", "the kidneys", "an apple", "Athina Onassis", "nadal", "Apollo 11", "five", "Kirk Douglas", "martin martin", "tin", "longchamp", "Nippon or Nihon", "martin", "a joey", "Maine", "USS Missouri", "the Pyrenees Mountains", "basketball", "Janis Joplin", "Miss Marple", "basketball", "South Africa", "martin wilson", "Ed Miliband", "Scotland", "an aeoline", "martin sewell", "Republic of Upper Volta", "martina nratilova", "40", "75", "(George) VI", "John Masefield", "Rio de Janeiro", "Hezbollah", "Bengalis", "Grayson Perry", "Guatemala", "carousel", "Leicester", "Frank Saul", "Radish", "Lister", "Downton Abbey", "a knife", "Garfield Sobers", "Herman Hollerith", "BETA", "Golden Gate National Recreation Area", "Forbes", "The English Electric Canberra", "Ford", "the New Promised Land: Silicon Valley", "Pakistan's", "a rat", "a tapas", "Maria Callas", "Hern\u00e1n Jorge Crespo"], "metric_results": {"EM": 0.578125, "QA-F1": 0.63984375}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, true, true, false, true, true, true, false, false, true, true, false, false, true, true, true, true, true, true, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-4726", "mrqa_triviaqa-validation-5762", "mrqa_triviaqa-validation-4967", "mrqa_triviaqa-validation-2912", "mrqa_triviaqa-validation-1169", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-7047", "mrqa_triviaqa-validation-4639", "mrqa_triviaqa-validation-1975", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-1066", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-7160", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-3578", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1657", "mrqa_naturalquestions-validation-8483", "mrqa_hotpotqa-validation-3343", "mrqa_newsqa-validation-2457", "mrqa_newsqa-validation-3302", "mrqa_newsqa-validation-1194", "mrqa_searchqa-validation-4559", "mrqa_hotpotqa-validation-3207"], "SR": 0.578125, "CSR": 0.5096875000000001, "EFR": 0.9259259259259259, "Overall": 0.6548570601851853}, {"timecode": 50, "UKR": 0.57421875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1657", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2619", "mrqa_hotpotqa-validation-2759", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-2981", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-331", "mrqa_hotpotqa-validation-3358", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4336", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-464", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-497", "mrqa_hotpotqa-validation-5101", "mrqa_hotpotqa-validation-5117", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5624", "mrqa_hotpotqa-validation-5642", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5703", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10258", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-1067", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2467", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-3959", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-4803", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5624", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5851", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9026", "mrqa_naturalquestions-validation-9099", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1741", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1899", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3541", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3644", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-732", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-11651", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12442", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-1311", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13235", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-14017", "mrqa_searchqa-validation-14149", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-145", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-1642", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16899", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-230", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5532", "mrqa_searchqa-validation-5717", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6349", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-6724", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7087", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7785", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_searchqa-validation-9451", "mrqa_searchqa-validation-9800", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10413", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1312", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1672", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-233", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-3883", "mrqa_squad-validation-3953", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4473", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4857", "mrqa_squad-validation-4921", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5389", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-603", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7459", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1518", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1817", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2045", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2883", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-306", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3110", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-3999", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4426", "mrqa_triviaqa-validation-4443", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4940", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5172", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-532", "mrqa_triviaqa-validation-5325", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-5993", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6113", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-62", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6460", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6786", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6898", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-83", "mrqa_triviaqa-validation-996"], "OKR": 0.7890625, "KG": 0.41484375, "before_eval_results": {"predictions": ["Ross Elliott", "Annette", "1980s", "Nodar Kumaritashvili", "Carpenter", "Dan Stevens", "human colon", "in December 1886", "July 1, 1890", "March 31, 2013", "Manley", "1978", "Judiththia Aline Keppel", "BC Jean and Toby Gad", "in Gangneung, South Korea", "The Walking Dead", "Koine Greek : apokalypsis, meaning `` unveiling '' or `` revelation ''", "1962", "non-ferrous", "the state sector", "a sacroiliac joint or SI joint ( SIJ )", "Joudeh Al - Goudia family", "after World War II", "Cheshire", "The Massachusetts Compromise was a solution reached in a controversy between Federalists and Anti-Federalists over ratification of the United States Constitution", "L.K. Advani", "in a reserve unit in accordance with the military's needs", "Jason Marsden", "Louis XIV", "Ashrita Furman", "St. Augustine renders it as clara notitia cum laude, `` brilliant celebrity with praise ''", "By the early 1960s", "Maricopa County", "the beginning", "2013", "Diego Tinoco", "between two variables is equal to the Pearson correlation between the rank values of those two variables", "in the summer of 2003", "Glenn Close", "Cefal\u00f9, Caen, Durham, and elsewhere", "Johannes Gutenberg", "Dan Stevens", "Charlotte", "douglas ford", "Carolyn Sue Jones", "De pictura ( English : `` On Painting '' ) is a treatise written by the Italian architect and art theorist Leon Battista Alberti", "a symbol of Lord Shiva", "in various submucosal membrane sites of the body, such as the gastrointestinal tract, oral passage, nasopharyngeal tract, thyroid, breast, lung, salivary glands, eye, and skin", "Article 1, Section 2, Clause 3", "birch", "a response to the sensation of food within the esophagus itself", "Dolly Parton", "westminster bridge", "darlington", "Jack Murphy Stadium", "\"Black Abbots\"", "Prince Amedeo, 5th Duke of Aosta", "a real person to talk to,\"", "Suba Kampong township", "2004.", "Laryngitis", "the Pequod", "Calvin Coolidge", "Dan Parris, 25, and Rob Lehr, 26,"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6265432454343745}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, false, false, true, true, false, false, false, false, false, false, false, false, true, true, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.5, 0.28571428571428575, 0.0, 0.0, 0.07999999999999999, 0.14814814814814814, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.12903225806451615, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6399999999999999, 0.5, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-3835", "mrqa_naturalquestions-validation-2743", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-2940", "mrqa_naturalquestions-validation-678", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-1301", "mrqa_naturalquestions-validation-405", "mrqa_naturalquestions-validation-6720", "mrqa_naturalquestions-validation-1090", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-486", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-6810", "mrqa_triviaqa-validation-7304", "mrqa_triviaqa-validation-6353", "mrqa_hotpotqa-validation-5522", "mrqa_hotpotqa-validation-1577", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-3406", "mrqa_newsqa-validation-2294"], "SR": 0.53125, "CSR": 0.5101102941176471, "EFR": 0.9666666666666667, "Overall": 0.6509803921568628}, {"timecode": 51, "before_eval_results": {"predictions": ["Domhnall Gleeson", "Lagaan ( English : Taxation ; also called Lagaa : Once Upon a Time in India )", "Alicia Vikander", "Franklin Roosevelt", "Orange Juice", "1837", "The Vamps,Connor Maynard, Bronnie, Ella Eyre, Sheppard and Louisa Johnson", "22 November 1914", "Shareef Abdur", "2018", "the breast or lower chest of beef or veal", "in the mid - to late 1920s", "Seattle Center, including the Seattle Center Monorail and the Space Needle", "douglas", "2007", "two tablets, which Moses broke in anger with his rebellious nation, and were later rewritten on replacement stones and placed in the ark of the covenant", "prevent any contaminants in the sink from flowing into the potable water system by siphonage", "to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "15 February 1998", "brothers Henry, Jojo and Ringo Garza", "Thomas Alva Edison", "the Greek name `` \u0391\u03bd\u03b4\u03c1\u03ad\u03b1\u03c2 / Andreas ''", "`` Mirror Image ''", "two senators, regardless of its population, serving staggered terms of six years", "the most junior enlisted sailor ( `` E-1 '' ) to the most senior enlisted sailor's pay grade", "1623", "Eduardo", "follows a child with Treacher Collins syndrome trying to fit in", "Kansas", "Efren Manalang Reyes", "Jesse McCartney", "Baseball Writers'Association of America ( or BBWAA ), or the Veterans Committee, which now consists of four subcommittees, each of which considers and votes for candidates from a separate era of baseball", "Herman Hollerith", "ulnar nerve", "December 18, 2017", "the Brewster family", "2008", "Buddhism", "American country music singer - songwriter Rodney Crowell", "Atlanta", "peninsular", "21 June 2007", "president of the organization and the president becomes the chair of the board", "Great Britain and the other Allied powers", "Gamora", "Darlene Cates", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "in Poems : Series 1, a collection of Dickinson's poems assembled and edited by her friends Mabel Loomis Todd and Thomas Wentworth Higginson", "birch", "Dorothy Kilgallen, Arlene Francis, and Bennett Cerf", "Matt Monro", "Joe Willie Kirk", "douino", "Vito Corleone", "blood", "Baugur Group", "Venice", "Hyundai Steel", "at Gaylord Opryland,", "100 percent", "New York City", "Roger Clemens", "Andrew Carnegie", "an independent homeland for the country's ethnic"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5198813432469563}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, false, true, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, false, false, false, true, false, false, true, false, true, false, false, false, false, false, true, false, false, true, false, true, true, true], "QA-F1": [1.0, 0.9090909090909091, 0.5714285714285715, 0.0, 1.0, 1.0, 0.3636363636363636, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.3636363636363636, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.1111111111111111, 0.11764705882352941, 0.0, 0.0, 0.9473684210526316, 1.0, 1.0, 0.4444444444444445, 0.4102564102564102, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 0.10526315789473685, 0.0, 0.0, 1.0, 0.8205128205128205, 0.9302325581395349, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-5215", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-4698", "mrqa_naturalquestions-validation-1823", "mrqa_naturalquestions-validation-7489", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-3037", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-9818", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-4365", "mrqa_naturalquestions-validation-6545", "mrqa_naturalquestions-validation-4759", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-4318", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5348", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-3146", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-407", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-6482", "mrqa_triviaqa-validation-4493", "mrqa_hotpotqa-validation-1001", "mrqa_hotpotqa-validation-1756", "mrqa_newsqa-validation-1052", "mrqa_newsqa-validation-268", "mrqa_searchqa-validation-8208"], "SR": 0.390625, "CSR": 0.5078125, "EFR": 0.8974358974358975, "Overall": 0.6366746794871795}, {"timecode": 52, "before_eval_results": {"predictions": ["Lithuania", "Roc-A-Fella Records and Priority Records", "United States Army", "White Horse", "Serial (Bad) Weddings", "created the American Land-Grant Acts of 1862 and 1890", "Pacific War", "1949", "dark fantasy, science fantasy, horror, and Western", "John Waters", "1945", "Sacramento Kings", "Samsung Galaxy S6", "the Magic Band", "Supergirl", "April 1, 1949", "the Northern Ireland national team", "Standard Oil", "William Harold \"Bill\" Ponsford", "Anatoly Lunacharsky", "Robert Matthew Hurley", "\"Macbeth\"", "Brad Silberling", "1973", "Italy", "Vaisakhi List", "\"Twice in a Lifetime\"", "seventh generation", "Len Wiseman", "31 July 1975", "Texas Tech Red Raiders", "Walldorf", "RCA Victor", "the sarod", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Sarah Winnemucca", "Robert Moses", "Godiva Chocolatier", "Manchester United", "The Simpsons", "Los Alamos National Laboratory", "Russia", "Lush Ltd.", "Telugu", "1952", "the University System of Georgia", "Restoration Hardware", "1942", "Kansas City Chiefs", "Michael Henderson", "C. H. Greenblatt", "Stephen Graham", "Section 1 is a vesting clause that bestows federal legislative power exclusively to Congress", "introverted Sensing ( Si ), Extroverted Thinking ( Te )", "Belgium", "Jiles Perry (JP) Richardson,", "JnotationsREY BROWN", "alwin Landry's supply vessel Damon Bankston", "about 3,000 kilometers (1,900 miles),", "The Casalesi clan", "Linda Darnell", "Scrabble", "Wilbur & Orville", "a leap year"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6172787486206603}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, true, false, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, true, true, false, true, false, true, true, false, false, false, false, false, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.5, 0.5882352941176471, 0.5, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.3636363636363636, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.14285714285714288, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.888888888888889, 0.5, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2916", "mrqa_hotpotqa-validation-1844", "mrqa_hotpotqa-validation-5140", "mrqa_hotpotqa-validation-4005", "mrqa_hotpotqa-validation-5376", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-422", "mrqa_hotpotqa-validation-5795", "mrqa_hotpotqa-validation-5728", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-5735", "mrqa_hotpotqa-validation-1510", "mrqa_hotpotqa-validation-1835", "mrqa_hotpotqa-validation-305", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-920", "mrqa_hotpotqa-validation-850", "mrqa_hotpotqa-validation-2532", "mrqa_hotpotqa-validation-5889", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-1997", "mrqa_naturalquestions-validation-4714", "mrqa_naturalquestions-validation-6706", "mrqa_triviaqa-validation-2582", "mrqa_triviaqa-validation-4933", "mrqa_newsqa-validation-3349", "mrqa_newsqa-validation-2641", "mrqa_searchqa-validation-7977", "mrqa_searchqa-validation-1784"], "SR": 0.515625, "CSR": 0.5079599056603774, "EFR": 1.0, "Overall": 0.6572169811320755}, {"timecode": 53, "before_eval_results": {"predictions": ["$250,000 for Rivers' charity: God's Love We Deliver.", "HPV (human papillomavirus)", "eight-day", "97-year-old", "a delegation of American Muslim and Christian leaders", "18", "Darrel Mohler", "Lance Cpl. Maria Lauterbach", "\"Operation Pipeline Express.\"", "Both Won Sei Hoon, who heads South Korea's National Intelligence Service, and Defense Minister Kim Kwan Jim", "the head with a.40-caliber pistol,", "President Obama", "dock back in Monaco in May next year.", "Grand Ronde, Oregon", "a bag", "suggested returning combat veterans could be recruited by right-wing extremist groups.", "14-day", "the fact that the teens were charged as adults.", "Conway", "The United States Holocaust Memorial Museum, The American Academy of Diplomacy and the United States Institute of Peace.", "rwanda", "Arsenal manager Arsene Wenger", "scored a hat-trick as AC Milan went second in Serie A with a 5-1 win over Torino in the San Siro", "the Genocide Prevention Task Force", "Mohammed Mohsen Zayed", "\"black box\" warning on Cipro and other fluoroquinolones,", "Jacob Zuma", "the return of a fallen U.S. service member", "Sporting Lisbon", "The opposition group,", "Saturday", "Jezebel.com's Crap E-mail", "original member of The Charlie Daniels Band", "Democratic VP candidate", "jackson ford", "united states", "three", "between the ages of 14 to 17.", "President Richard M. Nixon and his Brazilian counterpart, Emilio Medici", "Piedad Cordoba", "buddhism", "most high-profile amalgamation of Indian and western talent yet", "Pakistani territory", "a fight outside of an Atlanta strip club", "\"Britain's Got Talent\"", "Sen. Barack Obama", "the 2009 Swamp Soccer World Championship", "the man facing up, with his arms out to the side.", "stand down.", "a muddy barley field owned by farmer Alan Graham outside Bangor,", "The ACLU", "serves as the physical link between the mRNA and the amino acid sequence of proteins", "Bruno Mars", "2018", "surfer", "a phylum", "white", "November 6, 2018", "1898", "My Beautiful Dark Twisted Fantasy", "Ned Kelly", "Beta Monocerotis", "a fish eagle", "a crust of mashed potato"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6286990620533577}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, true, true, false, true, false, false, true, false, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, true, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 1.0, 0.9523809523809523, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.21052631578947367, 1.0, 0.5454545454545454, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.30769230769230765, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.36363636363636365, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666]}}, "before_error_ids": ["mrqa_newsqa-validation-1370", "mrqa_newsqa-validation-1591", "mrqa_newsqa-validation-2521", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-2777", "mrqa_newsqa-validation-2315", "mrqa_newsqa-validation-2530", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4151", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-2672", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-3990", "mrqa_newsqa-validation-2220", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-3013", "mrqa_newsqa-validation-85", "mrqa_newsqa-validation-1142", "mrqa_newsqa-validation-3097", "mrqa_naturalquestions-validation-7003", "mrqa_naturalquestions-validation-3783", "mrqa_triviaqa-validation-2038", "mrqa_searchqa-validation-12411", "mrqa_searchqa-validation-16162", "mrqa_naturalquestions-validation-10616"], "SR": 0.515625, "CSR": 0.5081018518518519, "EFR": 0.967741935483871, "Overall": 0.6507937574671446}, {"timecode": 54, "before_eval_results": {"predictions": ["Haikou on the Hainan Island", "Squamish, British Columbia, Canada", "2018", "2004", "right of the dinner plate", "illegitimate son of Ned Stark", "Samuel L. Jackson as Lucius Best / Frozone,", "Atlanta Bucks", "Hans Raffert", "31", "Jesse Frederick James Conaway", "an Aldabra giant tortoise", "to stop attacks by its U-boat torpedoed the RMS Lusitania in 1915", "Number 4, Privet Drive, Little Whinging in Surrey, England", "in positions 14 - 15, 146 - 147 and 148 - 149", "2018", "Malibu, California", "desublimation", "eight", "the Anglo - Norman French waleis", "covering his ears", "white blood cell in a vertebrate's immune system", "mitochondrial membrane", "Kansas", "the eventual Super Bowl champion New England Patriots 36 -- 17 in the AFC Championship Game", "Chesapeake Bay", "Thomas Edison's assistants, Fred Ott", "a variety of different relationships ; for example, one might refer to a music group's ex-guitarist", "the body - centered cubic ( BCC ) lattice", "President Lyndon Johnson", "in a Norwegian town circa 1879", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins", "of the topography and the dominant wind direction", "Development of Substitute Materials", "pagan custom", "in various submucosal membrane sites of the body", "2013", "John Ridgely as Jim Merchant", "of a Commonwealth of the Believers", "Lord Irwin", "its absolute temperature", "no longer a fundamental right", "Robert Gillespie Adamson IV", "the end of the 18th century", "1998", "the lungs", "Gladys Knight & the Pips", "Sir Ernest Rutherford", "Hendersonville, North Carolina", "the internal auditory canal of the temporal bone", "1858", "delivered appliances and other goods for department stores", "WrestleMania", "The Kennel Club", "Timothy Dalton", "Grammy awards", "John D Rockefeller", "misdemeanor assault charges", "$106,482,500 to an unidentified telephone bidder,", "improve the military's suicide-prevention programs.\"", "Stone Temple Pilots", "real estate investment trust", "Hubert Humphrey", "Tim Clark, Matt Kuchar and Bubba Watson"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5866160179349518}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, true, false, false, false, true, false, false, true, false, false, true, false, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, false, false, false, false, true, false, false, true], "QA-F1": [0.888888888888889, 1.0, 1.0, 1.0, 0.6666666666666665, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.375, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.4666666666666667, 1.0, 0.5714285714285715, 0.0, 1.0, 0.0, 0.3333333333333333, 1.0, 0.29629629629629634, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444444, 0.0, 0.33333333333333337, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4427", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-309", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-3309", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-9087", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-5330", "mrqa_naturalquestions-validation-5472", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-10707", "mrqa_naturalquestions-validation-7405", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-3721", "mrqa_naturalquestions-validation-4815", "mrqa_naturalquestions-validation-553", "mrqa_naturalquestions-validation-3882", "mrqa_naturalquestions-validation-5703", "mrqa_naturalquestions-validation-365", "mrqa_naturalquestions-validation-2242", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-4974", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-6727", "mrqa_triviaqa-validation-7669", "mrqa_hotpotqa-validation-574", "mrqa_newsqa-validation-3250", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-1887", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3606"], "SR": 0.4375, "CSR": 0.5068181818181818, "EFR": 0.9444444444444444, "Overall": 0.6458775252525253}, {"timecode": 55, "before_eval_results": {"predictions": ["in the Haganah (a precursor to the IDF) for... Yes, Putz, as in a Yiddish saying Westheimer paraphrased recently", "John Updike", "North Rhine-Westphalia", "clouds", "Jericho", "clean a ship's deck", "asteroids", "\"plankton\"", "John Quincy Adams", "Eleanor Roosevelt", "BATTLE of LAKE ERIE", "Bangladesh", "The Secret", "medals", "Seth Rogen", "a laser", "Jamaica Inn", "Walt Disney World", "Mexico", "Artemis", "pink", "the Aladdin Hotel", "9 to 5", "Jan & Dean", "walk the plank", "ice cream", "Huckabee", "catherine the great", "Texas", "constellations", "As I Lay Dying", "Celia", "Ross Perot", "the Black Sea", "C. S. Lewis", "Thomas Paine", "plutonium", "an antelope", "Anne Boleyn", "Q'umarkaj", "Dizzy", "soup", "The ACT", "Fermi", "Daedalus", "suspension bridge", "Tigger", "songs", "marathon", "QWERTY", "Deuteronomy", "collect menstrual flow", "13 May 1787", "cartilage", "Triumph", "Kansas", "a recorder", "UFC 50: The War of '04", "newspapers, television, radio, cable television, and other businesses", "March 17, 2015", "4.6 million", "Tibetan exile leaders,", "Alwin Landry's supply vessel Damon Bankston", "Geoffrey Zakarian"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6006725045787545}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, true, false, true, true, true, true, false, false, false, true, false, true, true, false, false, true, false, true, true, true, true, true, true, true, false, true, true, false, true, false, true, true, false, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, true, true, false, false, true, true, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.7692307692307693, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-14322", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-717", "mrqa_searchqa-validation-8235", "mrqa_searchqa-validation-755", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-3328", "mrqa_searchqa-validation-15379", "mrqa_searchqa-validation-4506", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-16847", "mrqa_searchqa-validation-8846", "mrqa_searchqa-validation-518", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-3845", "mrqa_searchqa-validation-11295", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-5620", "mrqa_searchqa-validation-14266", "mrqa_searchqa-validation-960", "mrqa_searchqa-validation-2219", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-9878", "mrqa_naturalquestions-validation-5113", "mrqa_triviaqa-validation-4151", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-4855", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-2205"], "SR": 0.515625, "CSR": 0.5069754464285714, "EFR": 1.0, "Overall": 0.6570200892857143}, {"timecode": 56, "before_eval_results": {"predictions": ["Pegida", "Tardis", "sugar", "The Potteries", "parliamentary secretary to the Ministry", "iron", "\"Little arrows\"", "Turin", "cats", "Reanne Evans", "Niger", "Battle of Camlann", "Mary Frances Winston Newson", "1905", "british", "british", "Jack London", "Hard Times", "Muhammad Ali", "carbon", "The Bill", "a Late check-Out Service", "Boxing Day", "cheers", "Taliban", "alpestrine", "a toad", "to make something better", "noreg", "skirts", "Australia", "Blucher", "Apollo", "Sachin Tendulkar", "an air guitar", "the River Hull", "tenerife", "South Africa", "bone", "Annie Mae Bullock", "John le Mesurier", "Shinto", "Batley", "The Greater Antilles", "a smokiness", "Pluto", "Jim Branning", "cryonics", "Fleet Street", "Scafell Pike", "baseball", "Speaker of the House of Representatives", "Athens", "iOS", "Les Clark", "American Idol", "\" Realty Bites\"", "Former Mobile County Circuit Judge Herman Thomas", "News of the World", "propofol", "Emmett Kelly", "Julio", "Shakespeare in Love", "a desire to be reckoned with as an openly wounded and unabashedly portentous rock balladeer"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6217524509803921}, "metric_results_detailed": {"EM": [true, false, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0588235294117647, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1622", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-6925", "mrqa_triviaqa-validation-3176", "mrqa_triviaqa-validation-2527", "mrqa_triviaqa-validation-4480", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-2262", "mrqa_triviaqa-validation-5808", "mrqa_triviaqa-validation-4559", "mrqa_triviaqa-validation-7650", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-2141", "mrqa_triviaqa-validation-4669", "mrqa_triviaqa-validation-1589", "mrqa_triviaqa-validation-3696", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2748", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-2374", "mrqa_newsqa-validation-1282", "mrqa_naturalquestions-validation-7270"], "SR": 0.59375, "CSR": 0.5084978070175439, "EFR": 0.9615384615384616, "Overall": 0.6496322537112011}, {"timecode": 57, "before_eval_results": {"predictions": ["against outside influences in next month's run-off election,", "Monday", "eight-week", "london", "coalition forces in Afghanistan", "to gamble in a casino, buy a drink in a pub or see the horror film \"hostel: Part II,\" currently six places below his number one movie on the UK box office chart.", "stratfor", "we feel very empowered.", "ninth", "murder in the beating death of a company boss who fired them.", "kaka", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha", "Islamabad", "Dennis Davern,", "kite surfers and wind surfers to be the fastest wind-powered boat on the planet is rapidly gaining momentum as speeds reach all-time highs.", "\"happy to see that the Brazilian and American positions and views\" were so close.", "anti-government protesters", "Madhav Kumar", "Saturday", "The Eye", "Dube", "11", "france Andrade had just gotten out of the shower when the men came to snatch him.", "\"It's viewed as an anti-Russian device. Well, it's not.\"", "Citizens", "refusal to \"turn it off\"", "returning combat veterans could be recruited by right-wing extremist groups.", "bicycles", "he was one of 10 gunmen who attacked several targets in Mumbai", "Pakistan", "the fact that the teens were charged as adults.", "Siri", "foxes", "10 to 15 percent", "Israel", "london", "The incident Sunday evening", "Landry", "president Bush", "Thebault", "Steven Gerrard", "three", "the Golden Gate Yacht Club of San Francisco", "Veracruz", "Sandy Olssen", "into some of the most hostile war zones,", "2002", "too many unwanted horses in the past year that her resources are stretched to the breaking point.", "the job bill's controversial millionaire's surtax", "seven", "Bin Laden", "in Africa", "Great Britain", "The Long Haul", "2004", "foxes", "Ambassador Bridge", "The University of Liverpool", "Count Schlieffen", "Chillingham Castle", "400th anniversary", "the River Liffey", "scrabble", "Rickie Lee Skaggs"], "metric_results": {"EM": 0.390625, "QA-F1": 0.5163690476190476}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, false, true, false, false, true, false, false, true, false, false, true, false, false, true, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, true, true], "QA-F1": [0.9333333333333333, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714288, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.8333333333333333, 0.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.6, 1.0, 0.0, 0.5714285714285715, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3942", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2865", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-3184", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-1912", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-2674", "mrqa_newsqa-validation-964", "mrqa_newsqa-validation-4121", "mrqa_newsqa-validation-592", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-854", "mrqa_newsqa-validation-844", "mrqa_newsqa-validation-4152", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-421", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-2968", "mrqa_newsqa-validation-3484", "mrqa_newsqa-validation-1448", "mrqa_newsqa-validation-4011", "mrqa_newsqa-validation-2911", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1799", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-246", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-5355", "mrqa_triviaqa-validation-5508", "mrqa_hotpotqa-validation-5455", "mrqa_hotpotqa-validation-3212", "mrqa_searchqa-validation-7178"], "SR": 0.390625, "CSR": 0.5064655172413793, "EFR": 1.0, "Overall": 0.656918103448276}, {"timecode": 58, "before_eval_results": {"predictions": ["\"The Orville\"", "1,467", "1989", "Nicole Kidman", "14", "the National Basketball Development League (NBDL)", "Charlie Wilson", "involuntary euthanasia", "astronaut, naval aviator, test pilot, and businessman", "duck", "The Summer Olympic Games", "Miami Gardens, Florida", "St. Louis Cardinals", "2007", "1993", "The University of Vienna", "Jack Ridley", "The Pennsylvania State University", "Willis Tower", "William Corcoran Eustis", "evangelical Christian", "Hanoi", "ITV", "Australian", "Torrens Road", "Flex-fuel", "The Savannah River Site", "swingman", "\"Haunted\"", "Scotland", "Todd Emmanuel Fisher", "1944", "Suicide Squad", "1883", "the NFL single-season touchdown reception record", "dimensionless quantity", "James Gay-Rees, George Pank, and Paul Bell", "1872", "poetry", "Madonna", "musicologist", "Lauren Alaina", "Prince Amedeo", "Ben Ainslie", "Forbidden Quest", "non-alcoholic", "scratchcard", "\"White Horse\"", "Duncan Kenworthy", "Malayalam movies", "Peter Nowalk", "Annette", "an exultation of spirit", "Meg Optimus", "riyadh", "Lady Gaga", "African violets", "three", "The museum was scheduled to open on the 11th anniversary of the September 11, 2001, terror attacks.", "the Carrousel du Louvre,", "A Tale of Two Cities", "Gabriel", "Isabella", "( Boss) Tweed"], "metric_results": {"EM": 0.5, "QA-F1": 0.5893229166666667}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, false, true, false, true, true, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.7499999999999999, 0.0, 0.4444444444444445, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4434", "mrqa_hotpotqa-validation-4289", "mrqa_hotpotqa-validation-2681", "mrqa_hotpotqa-validation-2928", "mrqa_hotpotqa-validation-4747", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-1456", "mrqa_hotpotqa-validation-3935", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-1674", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-5035", "mrqa_hotpotqa-validation-2051", "mrqa_hotpotqa-validation-5470", "mrqa_hotpotqa-validation-4806", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-943", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-1820", "mrqa_hotpotqa-validation-2144", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-2228", "mrqa_hotpotqa-validation-3420", "mrqa_naturalquestions-validation-9966", "mrqa_naturalquestions-validation-6523", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2955", "mrqa_searchqa-validation-3318", "mrqa_searchqa-validation-7521"], "SR": 0.5, "CSR": 0.5063559322033898, "EFR": 1.0, "Overall": 0.6568961864406779}, {"timecode": 59, "before_eval_results": {"predictions": ["two reservoirs in the eastern Catskill Mountains", "connotations of the passing of the year", "John Barry", "Thespis", "in the Saronic Gulf", "2010", "Coroebus", "Ewan McGregor", "1952", "iron", "Jesse Frederick James Conaway", "autopistas", "supported modern programming practices and enabled business applications to be developed with Flash", "Gene MacLellan", "1957", "certain actions taken by employers or unions that violate the National Labor Relations Act", "The Dewey Decimal Classification", "Have I Told You Lately", "the world's second most populous country", "a naval battle fought between an alliance of Greek city - states under Themistocles and the Persian Empire", "Emily Osment", "April 1979", "season seven finale", "Janie Crawford", "the adoption of the first ten amendments, the Bill of Rights", "2018", "Byzantine Greek culture and Eastern Christianity", "ordain presbyters / bishops and to exercise general oversight", "11 January 1923", "1961", "the Indians", "to condense the steam coming out of the cylinders or turbines", "Jacques Cousteau", "Felix Baumgartner", "1995", "2026", "the Gupta Empire", "Abigail Hawk", "Gary Mitchell", "South Korea", "in the 1970s", "1919", "23 September 1889", "halogenated paraffin hydrocarbons", "November 2016", "three levels", "Richard Crispin Armitage", "the Missouri River", "Kelly Osbourne, Ian `` Dicko '' Dickson", "Jack Barry", "the khimar", "vulpine", "the Andaman & Nicobar Islands", "One Direction", "Delacorte Press", "Drifting", "1927", "Indian film", "Iran", "missile", "the Celsius scale", "South Carolina", "Jonathan Swift", "Linux Format"], "metric_results": {"EM": 0.546875, "QA-F1": 0.626980176109412}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, false, true, false, true, true, true, true, false, false, false, true, false, false, false, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, false, true, false, false, false, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.24000000000000002, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.47058823529411764, 0.15384615384615383, 1.0, 0.7272727272727272, 0.17391304347826086, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-5550", "mrqa_naturalquestions-validation-2238", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-290", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-8420", "mrqa_naturalquestions-validation-5561", "mrqa_naturalquestions-validation-7632", "mrqa_naturalquestions-validation-7737", "mrqa_naturalquestions-validation-4416", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-9765", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-7885", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-7710", "mrqa_naturalquestions-validation-870", "mrqa_triviaqa-validation-6837", "mrqa_triviaqa-validation-4980", "mrqa_newsqa-validation-1877", "mrqa_newsqa-validation-213", "mrqa_searchqa-validation-2403", "mrqa_searchqa-validation-14090", "mrqa_hotpotqa-validation-4642"], "SR": 0.546875, "CSR": 0.50703125, "EFR": 0.9310344827586207, "Overall": 0.6432381465517242}, {"timecode": 60, "UKR": 0.61328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1081", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-1159", "mrqa_hotpotqa-validation-1288", "mrqa_hotpotqa-validation-1321", "mrqa_hotpotqa-validation-1372", "mrqa_hotpotqa-validation-1418", "mrqa_hotpotqa-validation-1505", "mrqa_hotpotqa-validation-1528", "mrqa_hotpotqa-validation-1548", "mrqa_hotpotqa-validation-1579", "mrqa_hotpotqa-validation-1596", "mrqa_hotpotqa-validation-1643", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1760", "mrqa_hotpotqa-validation-1767", "mrqa_hotpotqa-validation-1876", "mrqa_hotpotqa-validation-1935", "mrqa_hotpotqa-validation-1957", "mrqa_hotpotqa-validation-1993", "mrqa_hotpotqa-validation-2008", "mrqa_hotpotqa-validation-2047", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2237", "mrqa_hotpotqa-validation-224", "mrqa_hotpotqa-validation-2312", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2365", "mrqa_hotpotqa-validation-2400", "mrqa_hotpotqa-validation-2472", "mrqa_hotpotqa-validation-2521", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-2589", "mrqa_hotpotqa-validation-2787", "mrqa_hotpotqa-validation-2788", "mrqa_hotpotqa-validation-284", "mrqa_hotpotqa-validation-2890", "mrqa_hotpotqa-validation-2968", "mrqa_hotpotqa-validation-298", "mrqa_hotpotqa-validation-3022", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3075", "mrqa_hotpotqa-validation-3174", "mrqa_hotpotqa-validation-3247", "mrqa_hotpotqa-validation-3359", "mrqa_hotpotqa-validation-338", "mrqa_hotpotqa-validation-3381", "mrqa_hotpotqa-validation-3408", "mrqa_hotpotqa-validation-3577", "mrqa_hotpotqa-validation-3604", "mrqa_hotpotqa-validation-3785", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3995", "mrqa_hotpotqa-validation-4096", "mrqa_hotpotqa-validation-412", "mrqa_hotpotqa-validation-4135", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4246", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4433", "mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-462", "mrqa_hotpotqa-validation-4749", "mrqa_hotpotqa-validation-4754", "mrqa_hotpotqa-validation-4813", "mrqa_hotpotqa-validation-4815", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-4875", "mrqa_hotpotqa-validation-4901", "mrqa_hotpotqa-validation-5243", "mrqa_hotpotqa-validation-5312", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5458", "mrqa_hotpotqa-validation-5469", "mrqa_hotpotqa-validation-5481", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5661", "mrqa_hotpotqa-validation-5667", "mrqa_hotpotqa-validation-5675", "mrqa_hotpotqa-validation-5794", "mrqa_hotpotqa-validation-5817", "mrqa_hotpotqa-validation-5897", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-731", "mrqa_hotpotqa-validation-783", "mrqa_hotpotqa-validation-80", "mrqa_naturalquestions-validation-10029", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10184", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10386", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10597", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-10723", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-1328", "mrqa_naturalquestions-validation-1377", "mrqa_naturalquestions-validation-1398", "mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-2226", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3087", "mrqa_naturalquestions-validation-3099", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3477", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3892", "mrqa_naturalquestions-validation-392", "mrqa_naturalquestions-validation-4090", "mrqa_naturalquestions-validation-4148", "mrqa_naturalquestions-validation-4222", "mrqa_naturalquestions-validation-4240", "mrqa_naturalquestions-validation-4315", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4387", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4498", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-473", "mrqa_naturalquestions-validation-5133", "mrqa_naturalquestions-validation-5297", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-5374", "mrqa_naturalquestions-validation-5553", "mrqa_naturalquestions-validation-559", "mrqa_naturalquestions-validation-5672", "mrqa_naturalquestions-validation-5702", "mrqa_naturalquestions-validation-5864", "mrqa_naturalquestions-validation-6237", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6460", "mrqa_naturalquestions-validation-6474", "mrqa_naturalquestions-validation-6550", "mrqa_naturalquestions-validation-6768", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6927", "mrqa_naturalquestions-validation-6991", "mrqa_naturalquestions-validation-7047", "mrqa_naturalquestions-validation-7162", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7262", "mrqa_naturalquestions-validation-7608", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-808", "mrqa_naturalquestions-validation-8254", "mrqa_naturalquestions-validation-8464", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-8660", "mrqa_naturalquestions-validation-870", "mrqa_naturalquestions-validation-8916", "mrqa_naturalquestions-validation-8948", "mrqa_naturalquestions-validation-8965", "mrqa_naturalquestions-validation-8995", "mrqa_naturalquestions-validation-9191", "mrqa_naturalquestions-validation-9569", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9614", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9967", "mrqa_naturalquestions-validation-997", "mrqa_naturalquestions-validation-9972", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1195", "mrqa_newsqa-validation-1357", "mrqa_newsqa-validation-1564", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1643", "mrqa_newsqa-validation-1676", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-1724", "mrqa_newsqa-validation-1847", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1890", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2015", "mrqa_newsqa-validation-2020", "mrqa_newsqa-validation-2042", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2117", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2236", "mrqa_newsqa-validation-2426", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2507", "mrqa_newsqa-validation-2511", "mrqa_newsqa-validation-2713", "mrqa_newsqa-validation-2748", "mrqa_newsqa-validation-2913", "mrqa_newsqa-validation-2934", "mrqa_newsqa-validation-2976", "mrqa_newsqa-validation-3021", "mrqa_newsqa-validation-3048", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-314", "mrqa_newsqa-validation-3171", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-3557", "mrqa_newsqa-validation-3691", "mrqa_newsqa-validation-3752", "mrqa_newsqa-validation-3972", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-427", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-593", "mrqa_newsqa-validation-613", "mrqa_newsqa-validation-714", "mrqa_newsqa-validation-771", "mrqa_newsqa-validation-97", "mrqa_searchqa-validation-10063", "mrqa_searchqa-validation-10124", "mrqa_searchqa-validation-10247", "mrqa_searchqa-validation-10289", "mrqa_searchqa-validation-10672", "mrqa_searchqa-validation-10771", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11828", "mrqa_searchqa-validation-12110", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12129", "mrqa_searchqa-validation-12144", "mrqa_searchqa-validation-12230", "mrqa_searchqa-validation-12597", "mrqa_searchqa-validation-12623", "mrqa_searchqa-validation-12715", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-12979", "mrqa_searchqa-validation-13012", "mrqa_searchqa-validation-13110", "mrqa_searchqa-validation-13282", "mrqa_searchqa-validation-13771", "mrqa_searchqa-validation-13931", "mrqa_searchqa-validation-13955", "mrqa_searchqa-validation-1418", "mrqa_searchqa-validation-14218", "mrqa_searchqa-validation-1437", "mrqa_searchqa-validation-14849", "mrqa_searchqa-validation-14890", "mrqa_searchqa-validation-14910", "mrqa_searchqa-validation-14930", "mrqa_searchqa-validation-15003", "mrqa_searchqa-validation-15030", "mrqa_searchqa-validation-15243", "mrqa_searchqa-validation-15282", "mrqa_searchqa-validation-15410", "mrqa_searchqa-validation-15469", "mrqa_searchqa-validation-15555", "mrqa_searchqa-validation-15578", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15643", "mrqa_searchqa-validation-15652", "mrqa_searchqa-validation-15881", "mrqa_searchqa-validation-15942", "mrqa_searchqa-validation-16187", "mrqa_searchqa-validation-16447", "mrqa_searchqa-validation-16837", "mrqa_searchqa-validation-2130", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-2347", "mrqa_searchqa-validation-2691", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-2971", "mrqa_searchqa-validation-3122", "mrqa_searchqa-validation-3243", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3920", "mrqa_searchqa-validation-4142", "mrqa_searchqa-validation-4185", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4485", "mrqa_searchqa-validation-4555", "mrqa_searchqa-validation-4602", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4721", "mrqa_searchqa-validation-4848", "mrqa_searchqa-validation-5070", "mrqa_searchqa-validation-5105", "mrqa_searchqa-validation-5167", "mrqa_searchqa-validation-5324", "mrqa_searchqa-validation-5461", "mrqa_searchqa-validation-5528", "mrqa_searchqa-validation-5725", "mrqa_searchqa-validation-5817", "mrqa_searchqa-validation-6319", "mrqa_searchqa-validation-6367", "mrqa_searchqa-validation-6506", "mrqa_searchqa-validation-685", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7002", "mrqa_searchqa-validation-7279", "mrqa_searchqa-validation-7408", "mrqa_searchqa-validation-7616", "mrqa_searchqa-validation-7739", "mrqa_searchqa-validation-7828", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7864", "mrqa_searchqa-validation-7875", "mrqa_searchqa-validation-7906", "mrqa_searchqa-validation-815", "mrqa_searchqa-validation-8229", "mrqa_searchqa-validation-8365", "mrqa_searchqa-validation-846", "mrqa_searchqa-validation-8600", "mrqa_searchqa-validation-8632", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8866", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9113", "mrqa_searchqa-validation-9123", "mrqa_searchqa-validation-9133", "mrqa_searchqa-validation-9192", "mrqa_searchqa-validation-929", "mrqa_searchqa-validation-9323", "mrqa_squad-validation-10260", "mrqa_squad-validation-10279", "mrqa_squad-validation-10474", "mrqa_squad-validation-1160", "mrqa_squad-validation-1219", "mrqa_squad-validation-1338", "mrqa_squad-validation-161", "mrqa_squad-validation-1808", "mrqa_squad-validation-1982", "mrqa_squad-validation-2145", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2506", "mrqa_squad-validation-2888", "mrqa_squad-validation-3196", "mrqa_squad-validation-3207", "mrqa_squad-validation-350", "mrqa_squad-validation-3575", "mrqa_squad-validation-3752", "mrqa_squad-validation-3865", "mrqa_squad-validation-4117", "mrqa_squad-validation-4232", "mrqa_squad-validation-4294", "mrqa_squad-validation-4316", "mrqa_squad-validation-4341", "mrqa_squad-validation-4348", "mrqa_squad-validation-4356", "mrqa_squad-validation-447", "mrqa_squad-validation-4562", "mrqa_squad-validation-4666", "mrqa_squad-validation-4795", "mrqa_squad-validation-4965", "mrqa_squad-validation-5098", "mrqa_squad-validation-5303", "mrqa_squad-validation-5310", "mrqa_squad-validation-5407", "mrqa_squad-validation-5590", "mrqa_squad-validation-5630", "mrqa_squad-validation-5638", "mrqa_squad-validation-566", "mrqa_squad-validation-5758", "mrqa_squad-validation-5844", "mrqa_squad-validation-5846", "mrqa_squad-validation-5978", "mrqa_squad-validation-6025", "mrqa_squad-validation-6072", "mrqa_squad-validation-6113", "mrqa_squad-validation-6196", "mrqa_squad-validation-6286", "mrqa_squad-validation-6316", "mrqa_squad-validation-6361", "mrqa_squad-validation-6393", "mrqa_squad-validation-6408", "mrqa_squad-validation-6645", "mrqa_squad-validation-6658", "mrqa_squad-validation-7144", "mrqa_squad-validation-7303", "mrqa_squad-validation-7428", "mrqa_squad-validation-7474", "mrqa_squad-validation-7571", "mrqa_squad-validation-7632", "mrqa_squad-validation-7852", "mrqa_squad-validation-7867", "mrqa_squad-validation-8227", "mrqa_squad-validation-8421", "mrqa_squad-validation-8436", "mrqa_squad-validation-8576", "mrqa_squad-validation-8647", "mrqa_squad-validation-8971", "mrqa_squad-validation-901", "mrqa_squad-validation-9022", "mrqa_squad-validation-9029", "mrqa_squad-validation-9226", "mrqa_squad-validation-9286", "mrqa_squad-validation-9333", "mrqa_squad-validation-9360", "mrqa_squad-validation-9740", "mrqa_squad-validation-9750", "mrqa_squad-validation-9818", "mrqa_squad-validation-9895", "mrqa_triviaqa-validation-1035", "mrqa_triviaqa-validation-1259", "mrqa_triviaqa-validation-1318", "mrqa_triviaqa-validation-1331", "mrqa_triviaqa-validation-1360", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1475", "mrqa_triviaqa-validation-1603", "mrqa_triviaqa-validation-1692", "mrqa_triviaqa-validation-1868", "mrqa_triviaqa-validation-2103", "mrqa_triviaqa-validation-2154", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-2335", "mrqa_triviaqa-validation-2399", "mrqa_triviaqa-validation-2411", "mrqa_triviaqa-validation-2624", "mrqa_triviaqa-validation-274", "mrqa_triviaqa-validation-2796", "mrqa_triviaqa-validation-2974", "mrqa_triviaqa-validation-2980", "mrqa_triviaqa-validation-2994", "mrqa_triviaqa-validation-3004", "mrqa_triviaqa-validation-3086", "mrqa_triviaqa-validation-3095", "mrqa_triviaqa-validation-3170", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3195", "mrqa_triviaqa-validation-3313", "mrqa_triviaqa-validation-3332", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3642", "mrqa_triviaqa-validation-380", "mrqa_triviaqa-validation-3810", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-4028", "mrqa_triviaqa-validation-4145", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4172", "mrqa_triviaqa-validation-4189", "mrqa_triviaqa-validation-4197", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-450", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4611", "mrqa_triviaqa-validation-4647", "mrqa_triviaqa-validation-4933", "mrqa_triviaqa-validation-5044", "mrqa_triviaqa-validation-5082", "mrqa_triviaqa-validation-5128", "mrqa_triviaqa-validation-5182", "mrqa_triviaqa-validation-5336", "mrqa_triviaqa-validation-5370", "mrqa_triviaqa-validation-5393", "mrqa_triviaqa-validation-5394", "mrqa_triviaqa-validation-5402", "mrqa_triviaqa-validation-5408", "mrqa_triviaqa-validation-5426", "mrqa_triviaqa-validation-5464", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-552", "mrqa_triviaqa-validation-5591", "mrqa_triviaqa-validation-5595", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-5644", "mrqa_triviaqa-validation-5720", "mrqa_triviaqa-validation-5743", "mrqa_triviaqa-validation-5750", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6002", "mrqa_triviaqa-validation-6125", "mrqa_triviaqa-validation-6149", "mrqa_triviaqa-validation-6159", "mrqa_triviaqa-validation-6318", "mrqa_triviaqa-validation-6404", "mrqa_triviaqa-validation-6554", "mrqa_triviaqa-validation-6561", "mrqa_triviaqa-validation-6564", "mrqa_triviaqa-validation-663", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-6907", "mrqa_triviaqa-validation-695", "mrqa_triviaqa-validation-719", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-725", "mrqa_triviaqa-validation-7429", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7519", "mrqa_triviaqa-validation-7530", "mrqa_triviaqa-validation-7659", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-7707", "mrqa_triviaqa-validation-790", "mrqa_triviaqa-validation-806", "mrqa_triviaqa-validation-822", "mrqa_triviaqa-validation-996"], "OKR": 0.82421875, "KG": 0.4640625, "before_eval_results": {"predictions": ["Fulgencio Batista", "St. Vincent", "5,042", "Mandalay Entertainment", "Debbie Reynolds", "(1963\u201393)", "Mike Holmgren", "2,627", "Scanian soil, in the former Danish provinces along the border with Sweden and in Northern Germany", "Sparky", "Frederick Louis, Prince of Wales", "American", "Virgin", "October 21, 2016", "Kiss", "Ferdinand Magellan", "Sun Records founder Sam Phillips", "the Corps of Discovery", "receive the benefits of the Morrill Acts of 1862 and 1890", "the Crab Orchard Mountains", "Miss Universe 2010", "Maryland", "\"Take a Bow\"", "democracy and personal freedom", "Days of our Lives", "French Canadians", "1964 to 1974 model years", "The National League, currently named the Vanarama National League", "City Mazda Stadium", "Continental Army", "Matt Roller", "1994", "Vancouver", "The Lego Group", "Thomas Mawson", "Tony Aloupis", "various registries", "North Dakota", "Francis Nethersole", "The Panther", "British", "Fainaru Fantaj\u012b Tuerubu", "California State University system", "City of Onkaparinga", "October 31, 1999", "thirteen", "Princes Park", "The Bye Bye Man", "Germany", "\"Europop\"", "1698", "orbit", "the Constitution of India came into effect on 26 January 1950", "Raza Jaffrey", "lizz Winstead", "\"acts of the greatest heroism or of the most conspicuous courage in circumstances of extreme danger.", "ArcelorMittal Orbit", "Government Accountability Office", "Joe Harn", "$249", "high and dry", "An American Tail", "Cats", "Peru"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6646887400793651}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, true, true, false, true, false, true, true, true, false, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, true, false, false, false, true, true, false, true, true, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.125, 1.0, 0.22222222222222224, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.7499999999999999, 0.25, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4114", "mrqa_hotpotqa-validation-3918", "mrqa_hotpotqa-validation-2954", "mrqa_hotpotqa-validation-5489", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-2582", "mrqa_hotpotqa-validation-5586", "mrqa_hotpotqa-validation-753", "mrqa_hotpotqa-validation-4674", "mrqa_hotpotqa-validation-401", "mrqa_hotpotqa-validation-631", "mrqa_hotpotqa-validation-344", "mrqa_hotpotqa-validation-3039", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-2964", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3970", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1396", "mrqa_hotpotqa-validation-241", "mrqa_hotpotqa-validation-4986", "mrqa_hotpotqa-validation-2635", "mrqa_naturalquestions-validation-10257", "mrqa_triviaqa-validation-7718", "mrqa_triviaqa-validation-2096", "mrqa_newsqa-validation-4098"], "SR": 0.578125, "CSR": 0.5081967213114754, "EFR": 1.0, "Overall": 0.681951844262295}, {"timecode": 61, "before_eval_results": {"predictions": ["Blades", "Ronnie Biggs", "james stewart", "trout", "jon stewart", "javanese", "France", "Manchester", "sky", "Susan Bullock", "Jordan Spieth", "November", "jon stewart", "Alan Ladd", "Genghis Khan", "Kofi Annan", "Constance and Parkin", "left", "Istanbul", "karkul lamb", "Space Oddity", "collie", "14", "copepods", "cercopithecuss", "Mike Hammer", "stewart", "Dame Evelyn Glennie", "a heart", "Zaragoza", "David Bowie", "Billy Wilder", "\"Mr Loophole\"", "corset", "4.4 million", "jon stewart", "Westminster Abbey", "Ralph Lauren", "Pentecost", "Morgan Spurlock", "Oliver", "Debbie Reynolds", "Caroline Aherne", "cations", "George Santayana", "Rudolf Nureyev", "Brian Gabbitas", "Van cat", "apple", "argos", "Rodgers & Hammerstein", "in San Francisco, California as a nomination speech for presidential candidate Senator Barry Goldwater ; Richard Nixon gave that nomination speech", "By 1770 BC", "The United States Secretary of State", "2", "Jordan WikiLeaks", "C. J. Cherryh", "autonomy.", "Heshmatollah Attarzadeh", "Mark Obama Ndesandjo", "abandoned and looted", "the Louvre", "Kansas City, Missouri", "YIVO"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5765624999999999}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true, true, false, true, false, false, false, false, true, false, false, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true, true, false, false, true, false, false, false, true, false, false, false, false, true, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 1.0, 0.6666666666666665, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.5, 0.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-4210", "mrqa_triviaqa-validation-7044", "mrqa_triviaqa-validation-2081", "mrqa_triviaqa-validation-5522", "mrqa_triviaqa-validation-6699", "mrqa_triviaqa-validation-336", "mrqa_triviaqa-validation-1385", "mrqa_triviaqa-validation-1824", "mrqa_triviaqa-validation-732", "mrqa_triviaqa-validation-3702", "mrqa_triviaqa-validation-4536", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-502", "mrqa_triviaqa-validation-4872", "mrqa_triviaqa-validation-6779", "mrqa_triviaqa-validation-600", "mrqa_triviaqa-validation-6810", "mrqa_triviaqa-validation-2389", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-6339", "mrqa_triviaqa-validation-6342", "mrqa_triviaqa-validation-2487", "mrqa_triviaqa-validation-6210", "mrqa_triviaqa-validation-4021", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-6224", "mrqa_hotpotqa-validation-1073", "mrqa_hotpotqa-validation-4178", "mrqa_hotpotqa-validation-2955", "mrqa_newsqa-validation-2489", "mrqa_searchqa-validation-4730", "mrqa_searchqa-validation-5842"], "SR": 0.484375, "CSR": 0.5078125, "EFR": 1.0, "Overall": 0.681875}, {"timecode": 62, "before_eval_results": {"predictions": ["paul boudreau", "Gabriel Jesus Iglesias", "The Snowman", "Vikram Bhatt", "Helsinki, Finland", "Robert Plant", "Tommy Cannon", "Scottish national team", "203", "Ward Bond", "Illinois's 15", "Rochester", "7,500 and 40,000", "5,112", "Prof Media", "Timmy Sanders", "four months in jail", "Michael Redgrave", "Sturt", "Taylor Swift", "Robin David Segal", "Europe", "Trilochanapala", "deadpan sketch group", "larger than a subcompact car but smaller than a mid-size car", "Mexican", "DJ Khaled", "14,000", "in photographs, film and television", "37", "Teachta D\u00e1la", "137th", "Mr. Nice Guy", "Charkhi Dadri mid-air collision", "professional wrestling", "Bury St Edmunds, Suffolk, England", "Loretta Lynn", "Ford Island", "barcode", "American filmmaker and video game writer", "The United States of America", "Lerotholi Polytechnic", "Ribhu Dasgupta", "Peter Thiel", "orange", "Memphis", "relations", "Delaware", "Sophie Charlene Akland Monk", "Reinhard Heydrich", "\"lo Stivale\" (the Boot)", "the Tigris and Euphrates rivers", "during his first year", "Woodrow Wilson", "oliver Twist", "lion", "Volkswagen", "Teresa Hairston", "Pope Benedict XVI", "in St. Louis, Missouri.", "pearls", "sarsaparilla", "overbite", "trying to build nuclear bombs,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5920510912698412}, "metric_results_detailed": {"EM": [false, false, true, false, true, false, true, false, false, true, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, false, false, false, true, false, true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, false, false, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false], "QA-F1": [0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.28571428571428575, 0.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.16666666666666666, 0.0, 0.0, 0.6666666666666666, 0.888888888888889, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7499999999999999, 1.0, 1.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.8333333333333333]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3221", "mrqa_hotpotqa-validation-1707", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-956", "mrqa_hotpotqa-validation-5428", "mrqa_hotpotqa-validation-614", "mrqa_hotpotqa-validation-2581", "mrqa_hotpotqa-validation-5442", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-1093", "mrqa_hotpotqa-validation-5623", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-2278", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-1821", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-1675", "mrqa_hotpotqa-validation-2213", "mrqa_hotpotqa-validation-4655", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-4554", "mrqa_hotpotqa-validation-234", "mrqa_hotpotqa-validation-2204", "mrqa_hotpotqa-validation-290", "mrqa_hotpotqa-validation-2492", "mrqa_hotpotqa-validation-5585", "mrqa_hotpotqa-validation-1330", "mrqa_hotpotqa-validation-4604", "mrqa_naturalquestions-validation-5897", "mrqa_triviaqa-validation-6807", "mrqa_triviaqa-validation-3842", "mrqa_newsqa-validation-2278", "mrqa_newsqa-validation-353", "mrqa_searchqa-validation-6948", "mrqa_newsqa-validation-2662"], "SR": 0.453125, "CSR": 0.5069444444444444, "EFR": 0.9714285714285714, "Overall": 0.6759871031746031}, {"timecode": 63, "before_eval_results": {"predictions": ["Indian", "A Claude Monet pastel drawing of London's Waterloo Bridge", "Brazil", "Jacob Zuma,", "apartment building in Cologne, Germany,", "July", "2005 & 2006 Acura MDX", "Ryan Adams.", "The forehead and chin", "Greece, the birthplace of the Olympics,", "27-year-old's", "next week", "April 26, 1913,", "9-1", "Brazil began guaranteeing free anti-retroviral treatment to HIV/AIDS patients.", "next year", "\"Swamp Soccer\"", "Savoie", "The Falklands, known as Las Malvinas in Argentina,", "\"People can use solar and renewable energy at home everyday,\"", "Roger Federer", "tennis", "two", "in the 1950s,", "Gary Player", "12 million", "Rin Tin: The Life and the Legend", "The Lost Trailers have also partnered with Keep America Beautiful, a national organization dedicated to litter reduction and recycling.", "President Bill Clinton", "an average of 25 percent", "The crash destroyed four homes and killed two people who lived in at least one of the homes,", "800,000", "Sporting Lisbon", "President Sheikh Sharif Sheikh Ahmed", "2005", "his decision to join the U.S. military,", "The two men work,", "Israel", "Sunday's", "between government soldiers and Taliban militants in the Swat Valley.", "Jeffrey Jamaleldine", "The Rev. Alberto Cutie", "9 a.m.-1 p.m.", "The TNT series resume Monday night where it left off in September with what Sedgwick called \"a fantastic five episodes.\"", "Struck by the abject chaos surrounding him and seeing bystand children scooting along the ground,", "that in May her husband had knocked her down, held a loaded gun to her head and then threatened to commit suicide,", "recite her poetry", "in the head", "U.S. troops.", "neck", "the stylish Sansibar (80 H\u00f6rnumer Str., Rantum; 49-4651/964-656; dinner for two $92)", "Andrew Garfield", "Minneapolis, Minnesota", "interstitial fluid in the `` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals )", "gold", "The Mystery of Edwin Drood", "Bligh", "Melbourne", "1998", "23 July 1989", "Tuesday", "Evian", "Tampa", "Kind Hearts and Coronets"], "metric_results": {"EM": 0.5, "QA-F1": 0.6496560844047258}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, false, false, true, true, true, false, false, true, true, false, false, false, true, true, true, false, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, true, true, false, true], "QA-F1": [0.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.16666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.19999999999999998, 1.0, 1.0, 0.6666666666666666, 0.25, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5714285714285715, 0.9090909090909091, 0.45454545454545453, 0.8, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 0.0, 0.0, 0.0, 0.923076923076923, 0.13333333333333333, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4347826086956522, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1873", "mrqa_newsqa-validation-792", "mrqa_newsqa-validation-3245", "mrqa_newsqa-validation-1681", "mrqa_newsqa-validation-1159", "mrqa_newsqa-validation-2995", "mrqa_newsqa-validation-4070", "mrqa_newsqa-validation-2807", "mrqa_newsqa-validation-1902", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-3275", "mrqa_newsqa-validation-3732", "mrqa_newsqa-validation-1570", "mrqa_newsqa-validation-1346", "mrqa_newsqa-validation-2680", "mrqa_newsqa-validation-2361", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-2044", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2240", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-104", "mrqa_newsqa-validation-1147", "mrqa_newsqa-validation-1393", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2423", "mrqa_newsqa-validation-3434", "mrqa_naturalquestions-validation-8963", "mrqa_naturalquestions-validation-3261", "mrqa_triviaqa-validation-2862", "mrqa_searchqa-validation-5963"], "SR": 0.5, "CSR": 0.5068359375, "EFR": 0.96875, "Overall": 0.6754296875}, {"timecode": 64, "before_eval_results": {"predictions": ["\"Den V\u00e6gelsindede\"", "Max Martin and Shellback", "William J. Weaver", "6,396", "Reinhard Heydrich", "Standard Oil", "40 million albums", "Lieutenant Colonel Horace Meek Hickam", "Charles Russell", "May 1, 2011", "Parapsychologist Konstant\u012bns Raudive", "South West Peninsula League", "Transporter 3", "1983", "December 13, 1920", "Gaelic", "more than 265 million business records", "2004", "Eisenhower Executive Office Building", "Big 12 Conference", "Thocmentony", "age thirteen", "Robert \"Bobby\" Bunda", "New Jersey", "Black Panther Party", "Walt Disney and Ub Iwerks", "\"Queen In-hyun's Man\"", "Woodsy owl", "Daniel Louis Castellaneta", "other individuals, teams, or entire organizations", "1,467 rooms", "Ian Rush", "John Alexander", "The 2009 UEFA Champions League Final was played on 27 May 2009 at the Stadio Olimpico in Rome, Italy", "Kramer Guitars", "Coahuila, Mexico", "1968", "Holston River", "July 10, 2017", "London", "jazz homeland section of New Orleans and on that part of the South in particular", "Neon City", "Stephen Mangan", "largest Mission Revival Style building in the United States", "George Balanchine", "The Terminator", "Samoa", "The single became Lamar's second number-one single on the US \"Billboard\" Hot 100 after \"Bad Blood\"", "Timo Hildebrand", "Univision", "first flume ride in Ireland", "in the five - year time jump for her brother's wedding to Serena van der Woodsen", "The Statue of Freedom", "the Mishnah ( Hebrew : \u05de\u05e9\u05e0\u05d4, c. 200 CE ), a written compendium of Rabbinic Judaism's Oral Torah ; and the Gemara ( c. 500 CE )", "Mexico", "Julie Andrews", "Captain Mark Phillips", "Democratic VP candidate", "$75 for full-day class,", "\"Nu au Plateau de Sculpteur,\"", "cigar", "The Bridges of Madison County", "Thomas Jefferson", "In finance, a foreign exchange option ( commonly shortened to justFX option or currency option )"], "metric_results": {"EM": 0.625, "QA-F1": 0.7287122873471558}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.8, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473684, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.8]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1692", "mrqa_hotpotqa-validation-2758", "mrqa_hotpotqa-validation-1818", "mrqa_hotpotqa-validation-1515", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-2171", "mrqa_hotpotqa-validation-861", "mrqa_hotpotqa-validation-1461", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-3096", "mrqa_hotpotqa-validation-5242", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-3120", "mrqa_hotpotqa-validation-2802", "mrqa_hotpotqa-validation-5557", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-5191", "mrqa_naturalquestions-validation-7286", "mrqa_naturalquestions-validation-4073", "mrqa_triviaqa-validation-1508", "mrqa_newsqa-validation-901", "mrqa_searchqa-validation-12600", "mrqa_searchqa-validation-1518", "mrqa_naturalquestions-validation-8414"], "SR": 0.625, "CSR": 0.5086538461538461, "EFR": 1.0, "Overall": 0.6820432692307692}, {"timecode": 65, "before_eval_results": {"predictions": ["prophets and beloved religious leaders", "John Ernest Crawford", "Justin Timberlake", "cells", "Indo - Pacific", "the leaves of the plant species Stevia rebaudiana", "Gustav Bauer", "Universal Pictures", "July 31, 2010", "T - Bone Walker", "visited paid monument in the world", "Bobby Darin", "Alex Skuby", "All four volumes were illustrated by E.H. Shepard", "James Rodr\u00edguez", "Lou Rawls", "Andrew Garfield", "Juliet", "Payaya Indians", "The Whig Party's colorful Log Cabin Campaign in the 1840 United States presidential election", "Robert Irsay", "runoff", "two early wars", "the base of the right ventricle", "Puente Hills Mall", "February 15, 1974", "a status line", "June 1992", "the National Health Service ( NHS )", "28 July 1914", "the X Window System", "the year 1", "October 27, 1904", "December 25", "lizards", "timomatic and Geri Halliwell", "the final scene of the fourth season", "The 2017 SEC Championship Game was played on December 2, 2017 at Mercedes - Benz Stadium in Atlanta, Georgia", "during meiosis", "the world's longest - running radio soap opera", "Yuzuru Hanyu", "Italian Agostino Bassi", "Rachel Sarah Bilson", "grass and sedges", "Jonathan Cheban", "2015", "computers or in an organised paper filing system", "bicameral Congress", "Missouri River", "sport utility vehicles", "March 2, 2016", "Fred Perry", "\"Raging Bull,\"", "Gretel", "Get Him to the Greek", "Netflix", "Union Hill section of Kansas City, Missouri", "three", "New York Post's Page 6 gossip column.", "fifth", "\"Let him die\"", "Panic", "arne de France", "\"Salve, Corneli\""], "metric_results": {"EM": 0.46875, "QA-F1": 0.5649544155390122}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, false, false, false, false], "QA-F1": [0.7499999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.5714285714285715, 0.0, 0.0, 0.2222222222222222, 1.0, 1.0, 1.0, 1.0, 0.0, 0.08695652173913043, 1.0, 1.0, 0.0, 0.6666666666666666, 0.42857142857142855, 0.0, 1.0, 1.0, 0.0, 0.6, 0.0, 0.1818181818181818, 1.0, 0.0, 0.0, 0.0, 1.0, 0.10526315789473684, 0.5714285714285715, 0.0, 1.0, 0.8, 1.0, 0.2727272727272727, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5105", "mrqa_naturalquestions-validation-1622", "mrqa_naturalquestions-validation-9972", "mrqa_naturalquestions-validation-5188", "mrqa_naturalquestions-validation-9992", "mrqa_naturalquestions-validation-1719", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-368", "mrqa_naturalquestions-validation-4552", "mrqa_naturalquestions-validation-4219", "mrqa_naturalquestions-validation-5589", "mrqa_naturalquestions-validation-4018", "mrqa_naturalquestions-validation-3895", "mrqa_naturalquestions-validation-8612", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-4413", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-8171", "mrqa_naturalquestions-validation-1698", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-5599", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-8294", "mrqa_naturalquestions-validation-8733", "mrqa_naturalquestions-validation-9162", "mrqa_naturalquestions-validation-5751", "mrqa_triviaqa-validation-5315", "mrqa_hotpotqa-validation-5271", "mrqa_hotpotqa-validation-3806", "mrqa_newsqa-validation-2388", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-9384", "mrqa_searchqa-validation-13572", "mrqa_triviaqa-validation-2486"], "SR": 0.46875, "CSR": 0.5080492424242424, "EFR": 0.9705882352941176, "Overall": 0.676039995543672}, {"timecode": 66, "before_eval_results": {"predictions": ["Braille", "huggins and Stephen J. Cannell", "360 degrees", "Steely Dan", "Strictly Come Dancing", "c Clement Richard Attlee", "north of the Isle of Man", "hladetina", "The bookplate of the Book Society,", "The Stone Age", "Justin Bieber", "Russia", "F. Scott Fitzgerald's 1925 novel", "The Gunpowder Plot of 1605", "Moldovan", "Sydney", "Edwina Currie", "sprite", "IKEA", "Pablo Picasso", "Some Like It Hot", "j. S. Bach", "Tony Blair", "Pickwick", "360", "Caracas", "Ireland", "The Donington Grand Prix Collection includes several of the distinctive red and white machines which Ayrton Senna and Alain Prost battled in, as well as the 1993 machine,", "Jim Peters", "racing", "onion", "Pat Houston", "1948", "narwhal", "Sikhism", "giraffe", "kabuki", "The first web page went live on August 6, 1991", "Zachary Taylor", "indigo", "Friday", "\u201cFor gallantry;\u201d", "Swindon Town", "cricket", "jordan", "Myanma", "Tottenham Court Road", "hongi", "defense", "grumpy", "Italy", "Zane Lowe's show on BBC Radio 1 in June 2010, at the Rockstar offices in New York in July 2010, and at the Spike Video Game Awards in December 2010", "Buddhist missionaries first reached Han China via the maritime or overland routes of the Silk Road", "eukaryotic cell", "Hechingen", "1986", "Charles L. Clifford", "Eleven", "Joe Pantoliano", "Robert Barnett", "Get Smart", "Bridges of Madison County", "Paraguay", "HackThis Site"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6075867200328408}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, false, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, true, false, false, false, false, true, true, false, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275865, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.05714285714285715, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6066", "mrqa_triviaqa-validation-728", "mrqa_triviaqa-validation-5772", "mrqa_triviaqa-validation-4405", "mrqa_triviaqa-validation-5239", "mrqa_triviaqa-validation-7706", "mrqa_triviaqa-validation-1833", "mrqa_triviaqa-validation-2806", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-5264", "mrqa_triviaqa-validation-4211", "mrqa_triviaqa-validation-7168", "mrqa_triviaqa-validation-7503", "mrqa_triviaqa-validation-1585", "mrqa_triviaqa-validation-1283", "mrqa_triviaqa-validation-2567", "mrqa_triviaqa-validation-7085", "mrqa_triviaqa-validation-2096", "mrqa_triviaqa-validation-7545", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-23", "mrqa_triviaqa-validation-7525", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-10355", "mrqa_hotpotqa-validation-2378", "mrqa_newsqa-validation-334", "mrqa_newsqa-validation-2030", "mrqa_hotpotqa-validation-1714"], "SR": 0.546875, "CSR": 0.5086287313432836, "EFR": 0.9655172413793104, "Overall": 0.6751416945445188}, {"timecode": 67, "before_eval_results": {"predictions": ["Yann Martel", "archers", "Tiffany and Co.", "estonia", "Cambridge", "estonia", "1825", "Lorraine", "sneeose", "Geoffrey Chaucer", "sports agent", "rough collie", "Sen. Edward M. Kennedy", "james garner", "red squirrel", "Richard Lester", "estonia", "skye", "gooseberry", "sneege w. Bush", "The Color Purple", "lincoln skye", "Il Divo", "Barack Obama", "1984", "skye", "chicago", "quito", "victoria", "Roy Plomley", "victoria skye", "360", "jeremy shumann", "1123", "victoria williams", "Sparta", "Hyundai", "pearls", "Julian Fellowes", "wood-smoked haddock", "Yemen", "George Miller", "estonia", "Nowhere Boy", "victoria", "head and neck", "quant pole", "Edward Lear", "35", "Frank Sinatra", "Wales", "Meri", "South Asia", "Uralic languages", "New York City", "1882", "a reward for ability or finding an easy way out of an unpleasant situation by dishonest means", "Larry King", "Noida.", "rebecca", "Super Bowl XI", "the Mediterranean", "Queen Isabella", "Turing"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5588541666666667}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, true, false, false, true, false, true, false, true, false, true, true, true, true, true, false, false, false, true, false, false, false, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.13333333333333333, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3632", "mrqa_triviaqa-validation-3849", "mrqa_triviaqa-validation-329", "mrqa_triviaqa-validation-1176", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-2115", "mrqa_triviaqa-validation-616", "mrqa_triviaqa-validation-6969", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-7485", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-5658", "mrqa_triviaqa-validation-4792", "mrqa_triviaqa-validation-2139", "mrqa_triviaqa-validation-5753", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3996", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-467", "mrqa_hotpotqa-validation-2623", "mrqa_hotpotqa-validation-3709", "mrqa_hotpotqa-validation-1641", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-177", "mrqa_searchqa-validation-13672", "mrqa_searchqa-validation-1508"], "SR": 0.484375, "CSR": 0.5082720588235294, "EFR": 1.0, "Overall": 0.6819669117647058}, {"timecode": 68, "before_eval_results": {"predictions": ["Siri", "Philippines", "heavy turbulence about 02:15 a.m. local time Monday", "Brian Smith", "Tim Clark, Matt Kuchar and Bubba Watson", "there were problems with the well and he should move his ship away.", "Ricardo Valles de la Rosa", "Elin Nordegren", "\"We Found Love\"", "immediate release into the United States of 17 Chinese", "millionaire's surtax,", "\"E! News\"", "about 50", "two-state solution", "yusuf Saad Kamel", "Glasgow, Scotland", "his father", "Iran", "south africans", "the insurgency", "Arlington National Cemetery", "Rosie Show", "Ricardo Valles de la Rosa", "March 24,", "changed Hollywood.", "mouth.", "100", "concentration camps,", "The EU naval force", "five", "Joel \"Taz\" DiGregorio", "robert may", "man-made island shaped like a date palm tree", "Somali coast of Somalia", "municipal police officers", "hiring veterans as well as job training for all service members leaving the military.", "shock", "northwestern Montana", "launch", "without bail", "February 12", "general astonishment", "a place for another non-European Union player in Frank Rijkaard's squad.", "Spanish", "separated", "Democratic VP candidate", "martial arts,", "poor, older than 55, rural residents or racial minorities,", "The Tupolev Tu-160 strategic bombers landed at Venezuela's Libertador military airfield and \"will spend several days carrying out training flights over neutral waters, after which they will return to the base,\"", "1944,", "devastating", "sovereignty over some or all of the current territory of the U.S. state of Texas", "warning signs", "Eurasian Plate", "horse", "elberta", "Brooklyn", "tetrahydrogestrinone", "Real Madrid and the Spain national team", "Brea, California", "Titanic", "Bongoyo Island", "dualism", "William Wordsworth"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5852923067537038}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false, false, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.4, 1.0, 1.0, 0.05, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.4444444444444445, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.8571428571428571, 0.3529411764705882, 0.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.0, 0.15384615384615383, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.787878787878788, 0.3076923076923077, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-670", "mrqa_newsqa-validation-2204", "mrqa_newsqa-validation-1561", "mrqa_newsqa-validation-1449", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-1375", "mrqa_newsqa-validation-2265", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-1941", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-3768", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-645", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-1548", "mrqa_newsqa-validation-83", "mrqa_newsqa-validation-3677", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-946", "mrqa_newsqa-validation-76", "mrqa_newsqa-validation-169", "mrqa_newsqa-validation-1363", "mrqa_newsqa-validation-4185", "mrqa_newsqa-validation-1421", "mrqa_newsqa-validation-2499", "mrqa_newsqa-validation-3164", "mrqa_naturalquestions-validation-1357", "mrqa_naturalquestions-validation-10271", "mrqa_naturalquestions-validation-1139", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-884", "mrqa_searchqa-validation-6786"], "SR": 0.453125, "CSR": 0.5074728260869565, "EFR": 0.9714285714285714, "Overall": 0.6760927795031055}, {"timecode": 69, "before_eval_results": {"predictions": ["1902", "Max Martin and Shellback", "Americana Manhasset", "Mayfair", "Taoiseach", "January 1864", "Arab", "Doggerland", "Larry Drake", "The Bad Hemingway Contest", "Culiac\u00e1n, Sinaloa, in the northwest of Mexico", "villanelle", "Ezo", "Taylor Swift's single \"Back to December\", Lana Del Rey's \"Born to Die\" and Mystery Jets' \"Dreaming of Another World\"", "Heather Elizabeth Langenkamp", "two Nobel Peace Prizes", "Derry", "Daniel Craig", "Hamburger SV", "Four Weddings and a Funeral", "Eisstadion Davos", "Mulberry", "Edward Longshanks and the Hammer of the Scots", "12th Century", "Christopher McCulloch", "novel", "The Daily Stormer", "Fort Saint Anthony", "IT products and services,", "Japan", "1919", "George of the Jungle", "the western end of the National Mall in Washington, D.C., across from the Washington Monument", "Len Wiseman", "Stephen Crawford Young", "\"My Backyard\" in Jacksonville, Florida,", "John Hume", "\"The Patriot\"", "Girls' Generation", "Bob Hurley", "September 1901", "Saturday", "anabolic\u2013androgenic steroids", "eastern Cheshire", "I", "\"Kismet\"", "Virginia", "1961", "1896", "2000", "Donald Sterling", "20 - year period", "Saint Peter", "mining", "the Earth", "diamonds", "horses", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "sumo wrestling", "Russian bombers", "the Juilliard School", "Dinosaurs", "Boy Scouts of America", "Inuit"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6498590818903319}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.8, 0.5, 1.0, 0.8, 1.0, 0.4444444444444444, 0.5, 0.0, 0.2727272727272727, 0.8, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.2857142857142857, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 0.5, 0.0, 0.5, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2863", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-4552", "mrqa_hotpotqa-validation-429", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-5240", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-512", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-2639", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-1572", "mrqa_hotpotqa-validation-3446", "mrqa_hotpotqa-validation-95", "mrqa_hotpotqa-validation-5845", "mrqa_hotpotqa-validation-5725", "mrqa_hotpotqa-validation-4767", "mrqa_hotpotqa-validation-265", "mrqa_hotpotqa-validation-2431", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-5518", "mrqa_hotpotqa-validation-215", "mrqa_hotpotqa-validation-1900", "mrqa_hotpotqa-validation-3597", "mrqa_hotpotqa-validation-4284", "mrqa_hotpotqa-validation-5021", "mrqa_hotpotqa-validation-375", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-1122", "mrqa_searchqa-validation-7074", "mrqa_searchqa-validation-11439", "mrqa_searchqa-validation-4320"], "SR": 0.484375, "CSR": 0.5071428571428571, "EFR": 1.0, "Overall": 0.6817410714285714}, {"timecode": 70, "UKR": 0.68359375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1687", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-176", "mrqa_hotpotqa-validation-1838", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2045", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2257", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2665", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3201", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-346", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3937", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4222", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-482", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5063", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-9", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1336", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2090", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2981", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-4014", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4731", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-6800", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6913", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7165", "mrqa_naturalquestions-validation-7182", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9560", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1194", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1661", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1928", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2820", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2990", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3085", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3786", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-466", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10145", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11024", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14821", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2540", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2622", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-30", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6150", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-6961", "mrqa_searchqa-validation-6977", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9096", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10335", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-1959", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2437", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2717", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5272", "mrqa_squad-validation-5492", "mrqa_squad-validation-5590", "mrqa_squad-validation-5686", "mrqa_squad-validation-5874", "mrqa_squad-validation-5889", "mrqa_squad-validation-60", "mrqa_squad-validation-6091", "mrqa_squad-validation-6255", "mrqa_squad-validation-629", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6524", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-6831", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8436", "mrqa_squad-validation-8662", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8872", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9484", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1088", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1272", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2610", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-305", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3630", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4584", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-4745", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4887", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6738", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935"], "OKR": 0.802734375, "KG": 0.46796875, "before_eval_results": {"predictions": ["Arkansas", "during the early 1970s", "Paris", "875 acre", "every aspect of public and private life", "Maria von Trapp", "\"From Here to Eternity\"", "12", "port city of Aden", "Scott Eastwood", "United States", "Patricia Valeria Bannister", "David Michael Bautista Jr.", "2 March 1972", "Tie Domi", "Mika H\u00e4kkinen", "Princess Jessica", "Queensland, Australia", "\"master builder\" of mid-20th century New York City", "Honolulu", "St. Louis County", "Badfinger", "performances of \"khyal\", \"thumri\", and \"bhajans\"", "XVideos", "the Salzburg Festival", "political correctness", "devotional literature", "Martin Joseph O'Malley", "1891", "Secret Intelligence Service", "Jane Eyre", "the Raiders", "mermaid", "850 m", "DeskMate", "Athenion", "Adolfo Rodr\u00edguez Sa\u00e1", "The Beatles", "Czech", "ninth", "Hanna, Alberta, Canada", "Manchester Victoria station", "Seb Rochford", "Ahn Jae-hyun", "Captain James Cook", "George I", "Seventeen", "37", "Velvet Revolver", "Citizens for a Sound Economy", "Barbara Feldon", "H CO", "prophets", "Bill Russell", "Andre Agassi", "Vienna", "Phillies", "fill a million sandbags and place 700,000 around our city,\"", "Caster Semenya", "extend the usage of morphine sulfate oral solution 20 mg/ml.", "Cuyahoga River", "Uranium", "Peter Sellers", "the river Elbe"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6904761904761905}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, false, false, false, true, false, false, true, true, true, true, true, false, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false], "QA-F1": [1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.4, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.5, 1.0, 0.0, 0.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-247", "mrqa_hotpotqa-validation-1566", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-1871", "mrqa_hotpotqa-validation-3926", "mrqa_hotpotqa-validation-626", "mrqa_hotpotqa-validation-2198", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-4212", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-4925", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-1791", "mrqa_hotpotqa-validation-425", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-249", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-4015", "mrqa_hotpotqa-validation-1819", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1991", "mrqa_naturalquestions-validation-9220", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-105", "mrqa_newsqa-validation-1065", "mrqa_searchqa-validation-10027", "mrqa_triviaqa-validation-4324"], "SR": 0.5625, "CSR": 0.5079225352112676, "EFR": 1.0, "Overall": 0.6924438820422535}, {"timecode": 71, "before_eval_results": {"predictions": ["CNN/Opinion Research Corporation", "Marie-Therese Walter", "led from a Los Angeles grand jury room after her indictment in the 1969 \"Manson murders.\"", "Russian air force,", "2nd Lt. Holley Wimunc.", "three out of four", "Goa", "acquire nuclear weapons are \"not far away, not at all, to what Hitler did to the Jewish people just 65 years ago,\"", "100 percent", "Kenyan and Somali governments", "Susan Atkins", "Casa de Campo International Airport in the Dominican Republic", "\"Operation Crank Call,\"", "228", "North Carolina base", "National September 11 Memorial Museum", "Harlem, New York", "\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\"", "\"Goldstone Report\"", "Elisabeth", "1959.", "his", "269,000", "issued his first military orders as leader of North Korea", "\"Toy Story\"", "\"People of Miami's Jackson Memorial Hospital Burn Center. He suffered second- and third-degree burns over about two-thirds of his body,", "Six", "kase Ng,", "27-year-old", "outside influences in next month's run-off", "\"border states\"", "\"A Whiter Shade of Pale\"", "security breach", "$250,000", "combat veterans", "$1.5 million.", "resources", "$10 billion", "Christopher Savoie", "United States, NATO member states, Russia", "1,500", "trading goods and services without exchanging money", "Charman Sinkfield, 30; Demario Ware, 20; and Jquante Crews, 25", "Fiona MacKeown", "Sen. Barack Obama", "\"The Real Housewives of Atlanta,\"", "\"Peace House,\"", "the shelling of the compound", "Guinea, Myanmar, Sudan and Venezuela.", "pine beetles", "Zoe's Ark", "Aspirin", "either February 28 or March 1", "Indo - Pacific", "mining", "Montezuma", "Maryland", "2012", "Acela Express", "Crackle", "a porcupine", "oxygen", "the Bird of Prey", "the Dixiecrat"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6359701062826063}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, true, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 0.4444444444444445, 1.0, 0.6, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4615384615384615, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1303", "mrqa_newsqa-validation-3714", "mrqa_newsqa-validation-1399", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-3753", "mrqa_newsqa-validation-2232", "mrqa_newsqa-validation-1857", "mrqa_newsqa-validation-2533", "mrqa_newsqa-validation-1750", "mrqa_newsqa-validation-2904", "mrqa_newsqa-validation-2053", "mrqa_newsqa-validation-2778", "mrqa_newsqa-validation-2617", "mrqa_newsqa-validation-3440", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-3066", "mrqa_newsqa-validation-3939", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4157", "mrqa_newsqa-validation-25", "mrqa_newsqa-validation-4163", "mrqa_newsqa-validation-401", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-933", "mrqa_naturalquestions-validation-4809", "mrqa_triviaqa-validation-2418", "mrqa_searchqa-validation-9135", "mrqa_searchqa-validation-14427"], "SR": 0.5625, "CSR": 0.5086805555555556, "EFR": 1.0, "Overall": 0.692595486111111}, {"timecode": 72, "before_eval_results": {"predictions": ["Jesse Triplett", "1997 ( Act No. 33 of 1997 )", "Sharyans Resources", "a marketing term for a vehicle that is both four - wheel - drive and primarily a road car", "Justice Lawrence John Wargrave, a retired judge, known as a `` hanging judge '' for liberally awarding the death penalty in murder cases", "Texas A&M University", "inflammatory cells", "a book of the Old Testament", "Anatomy ( Greek anatom\u0113, `` dissection '' )", "a maritime signal, indicating that the vessel flying it is about to leave", "President Lyndon Johnson", "Anarchists, in an alliance of convenience with the Anarchists and Communists, fought against the Nationalists, a Falangist, Carlist, Catholic, and largely aristocratic conservative group led by General Francisco Franco", "Olivia Olson", "Eukarya -- called eukaryotes", "Mara Jade", "Christopher Norris", "tenderness of meat", "Edward IV of England", "Ashrita Furman ( born Keith Furman, September 16, 1954 )", "a 30 - something man ( XXXX )", "Jean Fernel", "2007 and 2008", "October 1980", "erosion", "an English occupational name for one who obtained his living by fishing or living by a fishing weir", "1960", "Ronald Reagan", "Rolf L\u00f8vland ( Norway, 1985 and 1995 ) and Brendan Graham ( Ireland, 1994 and 1996 )", "showed your ass '' and `` sucks to be you right now ''", "a prohibition of blasphemy", "England and Wales", "1996", "1000 AD", "Idaho", "early Christians of Mesopotamia", "eight hours ( UTC \u2212 08 : 00 )", "Dr. Rajendra Prasad", "Carroll O'Connor", "Jay Baruchel", "John Garfield", "bachata music", "Butter Island off North Haven, Maine in the Penobscot Bay", "end of the 18th century,", "1890s Klondike Gold Rush", "secure communication over a computer network", "3", "1939", "the BBC", "Ticket to Ride", "in soils", "Felicity Huffman", "John of Gaunt", "75", "J29", "Montana State University", "Sun Valley, Idaho", "president of Guggenheim Partners", "either heavy flannel or wool -- fabrics that would not be transparent when wet -- and covered the entire body from neck to toe.", "doctors", "destroyed four homes and killed two people who lived in at least one of the homes,", "the Congo", "an online education management platform", "the Crow", "asylum in Britain."], "metric_results": {"EM": 0.515625, "QA-F1": 0.680420070871921}, "metric_results_detailed": {"EM": [false, false, false, false, false, true, false, false, false, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, true, true, false, false, false, false, true, false, true, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, true], "QA-F1": [0.0, 0.2857142857142857, 0.5, 0.0, 0.3636363636363636, 1.0, 0.5, 0.28571428571428575, 0.3333333333333333, 1.0, 1.0, 0.07692307692307693, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.4, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11764705882352941, 0.15384615384615383, 0.0, 0.5, 1.0, 0.8, 1.0, 1.0, 0.6, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.29629629629629634, 0.888888888888889, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.32, 1.0, 0.967741935483871, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4426", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-7948", "mrqa_naturalquestions-validation-10066", "mrqa_naturalquestions-validation-1782", "mrqa_naturalquestions-validation-6918", "mrqa_naturalquestions-validation-10687", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-1375", "mrqa_naturalquestions-validation-6859", "mrqa_naturalquestions-validation-284", "mrqa_naturalquestions-validation-6248", "mrqa_naturalquestions-validation-1206", "mrqa_naturalquestions-validation-1971", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-7492", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-9875", "mrqa_naturalquestions-validation-6435", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-484", "mrqa_naturalquestions-validation-9492", "mrqa_naturalquestions-validation-8474", "mrqa_triviaqa-validation-5787", "mrqa_hotpotqa-validation-1509", "mrqa_newsqa-validation-3504", "mrqa_newsqa-validation-3544", "mrqa_searchqa-validation-10906"], "SR": 0.515625, "CSR": 0.5087756849315068, "EFR": 0.967741935483871, "Overall": 0.6861628990830756}, {"timecode": 73, "before_eval_results": {"predictions": ["Brazil", "Fall Guy", "Crown", "Maria Montessori", "for Fugitive", "Arthur George Walker", "Rendezvous with Rama", "March of the Penguin", "Patrick Ewing", "Lieutenant William Bligh", "an ambulance", "Condoleezza Rice", "Pakistan", "China", "liquor", "Texas", "a Condors", "John James Audubon", "Pilate", "Bruce Rauner", "neurons", "a halfpipe", "Jackie Collins", "carioca", "Freakonomics", "George Washington Carver", "the Devonian period", "Champagne", "Red Heat", "freedom", "Haiti", "a carrel", "a flop", "Prince William and Kate Middleton", "Sherlock Holmes", "a UP-shaped", "Orion", "Bangladesh", "carbon monoxide", "King John", "UP-Fi Smart socket", "UPominable", "Thailand", "manslaughter", "programming", "the Tennessee River", "Hipparchus", "Billy Idol", "Missouri Compromise", "a Rat", "Tom Hanks", "to encounter antigens passing through the mucosal epithelium", "$582.4 million", "to be married", "Conrad Murray", "Gryffendor", "czech Republic", "Sochi, Russia", "two years", "Manchester Airport", "President Obama", "two weeks", "American Civil Liberties Union", "January 2000"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6286458333333333}, "metric_results_detailed": {"EM": [false, true, true, true, false, false, false, false, true, false, true, false, true, true, true, true, false, false, false, false, true, true, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, true, true, false, false, true, true, false, false, false, true, false, true, true, true, false, false, true, false, true, true, true, false, true, false, false, false], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_searchqa-validation-7908", "mrqa_searchqa-validation-13288", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-9645", "mrqa_searchqa-validation-9337", "mrqa_searchqa-validation-377", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-5326", "mrqa_searchqa-validation-2696", "mrqa_searchqa-validation-16907", "mrqa_searchqa-validation-8097", "mrqa_searchqa-validation-16646", "mrqa_searchqa-validation-6326", "mrqa_searchqa-validation-15708", "mrqa_searchqa-validation-11318", "mrqa_searchqa-validation-284", "mrqa_searchqa-validation-14344", "mrqa_searchqa-validation-702", "mrqa_searchqa-validation-3189", "mrqa_searchqa-validation-14970", "mrqa_searchqa-validation-10515", "mrqa_searchqa-validation-15757", "mrqa_naturalquestions-validation-4547", "mrqa_naturalquestions-validation-10093", "mrqa_triviaqa-validation-5472", "mrqa_hotpotqa-validation-4724", "mrqa_newsqa-validation-397", "mrqa_newsqa-validation-131", "mrqa_hotpotqa-validation-1233"], "SR": 0.53125, "CSR": 0.5090793918918919, "EFR": 0.9666666666666667, "Overall": 0.6860085867117116}, {"timecode": 74, "before_eval_results": {"predictions": ["saccharides", "Angela Rippon", "Anna Eleanor Roosevelt", "liver", "Private Eye", "Gibraltar", "Jack Ruby", "javelin throw", "British Airways", "Bachelor of Science", "cirencester", "Pete Best", "Bonnie and Clyde", "Avatar", "Concepcion", "St Moritz", "Edmund Cartwright", "Par-5", "Zeus", "Japanese silvergrass", "April", "George Conan Doyle", "Wolfgang Amadeus Mozart", "Honeybee", "Sun Hill", "Nutcracker", "(Port) Moresby", "Blarney", "Sesame Street", "photography", "Roy Plomley", "Samuel Johnson", "Science & Literature", "Toms", "ganga", "tabloid", "lock", "Melbourne", "Wonder of the Ancient World", "India", "Shangri-La", "The Tempest", "Diana Ross", "Mansion House", "Ishmael", "repechage", "Crusades", "Dame Kiri Te Kanawa", "Churchill Downs", "Upstairs Downstairs", "One Direction", "ulnar nerve", "Gibraltarian", "111", "Merck & Co.", "second-round", "Vietnam War", "\"It feels great to be back at work,\"", "Amnesty International.", "after Wood went missing off Catalina Island,", "When Harry Met Sally", "Breckenridge", "The Fray", "Sonia Sotomayor"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6034944581280788}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, true, false, false, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.13793103448275862, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2843", "mrqa_triviaqa-validation-3733", "mrqa_triviaqa-validation-861", "mrqa_triviaqa-validation-5009", "mrqa_triviaqa-validation-3628", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-977", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-730", "mrqa_triviaqa-validation-2356", "mrqa_triviaqa-validation-5243", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-5412", "mrqa_triviaqa-validation-7118", "mrqa_triviaqa-validation-45", "mrqa_triviaqa-validation-643", "mrqa_triviaqa-validation-7620", "mrqa_triviaqa-validation-4938", "mrqa_triviaqa-validation-2993", "mrqa_triviaqa-validation-5461", "mrqa_triviaqa-validation-170", "mrqa_naturalquestions-validation-3958", "mrqa_hotpotqa-validation-3058", "mrqa_newsqa-validation-75", "mrqa_newsqa-validation-3966", "mrqa_searchqa-validation-14621", "mrqa_newsqa-validation-2566"], "SR": 0.5625, "CSR": 0.5097916666666666, "EFR": 1.0, "Overall": 0.6928177083333333}, {"timecode": 75, "before_eval_results": {"predictions": ["Fitzroya", "Martin O'Neill", "2012", "3730 km", "Kind Hearts and Coronets", "Maine", "Japan", "hiphop", "erotic thriller", "Eumolpus", "Backspacer", "John Churchill", "Sir William McMahon", "Hopi", "Western District of Victoria", "Australia", "Annie Ida Jenny No\u00eb Haesendonck", "sixth year", "Brazil", "1954", "Newcastle upon Tyne, England", "four", "Eunice Kennedy Shriver", "Vaudevillains", "Chinese Coffee", "Love and Theft", "Adelaide", "4145 ft", "University of Georgia", "just over 1 million", "cigar", "Last of the Mohicans", "Centennial Olympic Stadium", "\"media for the 65.8 million,\"", "William Randolph Hearst", "1 April 1985", "The Maersk Mc- Kinney M\u00f8ller", "Fayetteville", "spirit", "The Books", "twenty kilometers south of the city on the banks of the Presidio River", "Danish", "London, England", "17 October 1945", "1959", "Telugu and Tamil", "Arizona Health Care Cost Containment System", "Laura Jeanne Reese Witherspoon", "Koch Industries", "Billy J. Kramer", "Mindy Kaling", "1992", "September 21, 2016", "state - of - the - art photography of the band's performance and outdoor session pictures", "ear", "Diego Garcia", "cuckoo", "$2 billion", "in an east-facing sitting room called the Morning Room.", "forcibly injecting them with psychotropic drugs while trying to shuttle them out of the country during their deportation.", "St. Patrick", "the Tomb of the Unknown Soldier", "Mount Vesuvius", "in a park in a residential area of Mexico City,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.5582299933862434}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, true, false, false, false, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 0.0, 0.0, 0.5, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.4, 0.14285714285714288, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.25, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.14814814814814817, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5394", "mrqa_hotpotqa-validation-758", "mrqa_hotpotqa-validation-2783", "mrqa_hotpotqa-validation-550", "mrqa_hotpotqa-validation-4455", "mrqa_hotpotqa-validation-3398", "mrqa_hotpotqa-validation-1238", "mrqa_hotpotqa-validation-3219", "mrqa_hotpotqa-validation-4570", "mrqa_hotpotqa-validation-4307", "mrqa_hotpotqa-validation-808", "mrqa_hotpotqa-validation-3155", "mrqa_hotpotqa-validation-1307", "mrqa_hotpotqa-validation-5895", "mrqa_hotpotqa-validation-4368", "mrqa_hotpotqa-validation-1731", "mrqa_hotpotqa-validation-5070", "mrqa_hotpotqa-validation-1297", "mrqa_hotpotqa-validation-5688", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5526", "mrqa_hotpotqa-validation-5597", "mrqa_hotpotqa-validation-3280", "mrqa_hotpotqa-validation-4421", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-2049", "mrqa_hotpotqa-validation-2971", "mrqa_naturalquestions-validation-661", "mrqa_naturalquestions-validation-3556", "mrqa_triviaqa-validation-534", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2632", "mrqa_newsqa-validation-140", "mrqa_searchqa-validation-9147", "mrqa_searchqa-validation-13410"], "SR": 0.453125, "CSR": 0.509046052631579, "EFR": 0.9714285714285714, "Overall": 0.68695429981203}, {"timecode": 76, "before_eval_results": {"predictions": ["pet sounds", "Battle of Culloden", "\u201cA Metro\u2013Goldwyn\u2013Mayer Picture\u201d", "Johann Strauss II", "James Callaghan", "cedars", "price", "Dublin", "el Paso de Mahoma", "leprosy", "left", "Sid James", "avocado", "Anne of Aragon", "Double", "Relpromax Antitrust Inc.", "Supertramp", "Saturn's rings", "Augustus", "All I Really Want To Do", "Heston Blumenthal", "Arkansas", "IT Crowd", "Some Like It Hot", "\"Mr Loophole\"", "Ken Purdy", "Wolf Hall", "Roger Federer", "Alberto Juantorena", "graffiti art", "Friedrich Nietzsche", "Caffari", "cheese", "Annie", "Kristiania", "piano player", "Herman Melville\u2019s Moby Dick", "moss", "scotlandhistory", "Changing Places", "the pea", "Dr Tamseel", "the Sea of Galilee", "one", "Helen of Troy", "caffeine", "The Firm", "1966", "an even break", "31536000", "Jordan", "bilaterally symmetrical", "desperation", "2018", "Colorado Rockies", "Maxwell Smart", "Las Vegas Strip in Paradise, Nevada", "head for Italy.", "Rev. Alberto Cutie", "Michelle Obama", "an alto", "270", "place", "the American Red Cross"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5241003787878789}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, false, true, false, true, true, false, true, false, true, false, true, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, true, false, false, true, true, false, true, false, false], "QA-F1": [1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8]}}, "before_error_ids": ["mrqa_triviaqa-validation-3328", "mrqa_triviaqa-validation-1696", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-6685", "mrqa_triviaqa-validation-4566", "mrqa_triviaqa-validation-1698", "mrqa_triviaqa-validation-166", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-4225", "mrqa_triviaqa-validation-4313", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-3671", "mrqa_triviaqa-validation-4362", "mrqa_triviaqa-validation-4857", "mrqa_triviaqa-validation-5439", "mrqa_triviaqa-validation-5429", "mrqa_triviaqa-validation-2179", "mrqa_triviaqa-validation-3590", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-3468", "mrqa_triviaqa-validation-6466", "mrqa_triviaqa-validation-5120", "mrqa_triviaqa-validation-1026", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-5819", "mrqa_hotpotqa-validation-71", "mrqa_newsqa-validation-1261", "mrqa_searchqa-validation-4422", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-14139"], "SR": 0.46875, "CSR": 0.5085227272727273, "EFR": 0.9117647058823529, "Overall": 0.674916861631016}, {"timecode": 77, "before_eval_results": {"predictions": ["Gary Player,", "remains committed to British sovereignty and the UK maintains a military presence on the islands.", "Kirchner", "iPods", "45 minutes, five days a week", "\"We have duty to keep cases under continuous review, and following expert evidence from a psychiatrist it was suggested no useful purpose would be served by Mr Thomas being detained and treated in a psychiatric hospital,\"", "Kris Allen,", "jordan caff", "Efraim Kam,", "U.S. and Britain.", "Harry Nicolaides,", "Zhanar Tokhtabayeba,", "April 2010.", "mammoth", "e-mails", "environmental", "his father", "Iran", "head injury.", "\"Antichrist.\"", "President kgalema Motlanthe", "Hugo Chavez", "seven", "Frank's diary.", "The Lost Symbol", "Procol Harum", "Rawalpindi", "Colorado Attorney General John Suthers", "eastern Afghan province of Logar,", "Jonathan Breeze,", "removal of his diamond-studded braces.", "Ireland.", "United States", "Arabic, Russian and Mandarin", "Hamas", "two pages -- usually high school juniors who serve Congress as messengers", "at least 40", "four", "Kurt Cobain", "84-year-old", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "three", "undergoing renovation.", "Naples home.", "radioactive sludge.", "November 26,", "sportswear", "city of southern China.", "protest child trafficking and shout anti-French slogans", "get better skin, burn fat and boost her energy.", "urged industrialized nations to honor aid pledge to developing nations despite the recession.", "three preteen boys named Ed, Edd ( called `` Double D '' to avoid confusion with Ed ), and Eddy -- collectively known as `` the Eds ''", "meditation", "katrughan", "India", "allergic", "a lie detector", "electronic rock, electropop and R&B", "1963", "\"Black Abbots\"", "a nurse bag", "Argentina", "Charles Baudelaire", "Sleepy Hollow"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6460468640156141}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, false, false, false, true, true, true, false, true, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, true, false, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, false, false, false, true, true, false, true, true, false, true, true, true], "QA-F1": [1.0, 0.15384615384615383, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.28571428571428575, 0.1818181818181818, 0.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.3076923076923077, 1.0, 1.0, 0.0, 1.0, 0.923076923076923, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.14285714285714285, 0.15384615384615383, 0.14814814814814814, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3698", "mrqa_newsqa-validation-3174", "mrqa_newsqa-validation-1270", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-1512", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-1955", "mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-2150", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-2509", "mrqa_newsqa-validation-3321", "mrqa_newsqa-validation-3201", "mrqa_newsqa-validation-3523", "mrqa_newsqa-validation-1960", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-2448", "mrqa_newsqa-validation-2493", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-3329", "mrqa_newsqa-validation-3403", "mrqa_naturalquestions-validation-8951", "mrqa_naturalquestions-validation-7058", "mrqa_triviaqa-validation-1806", "mrqa_hotpotqa-validation-4133", "mrqa_searchqa-validation-5877"], "SR": 0.5625, "CSR": 0.5092147435897436, "EFR": 1.0, "Overall": 0.6927023237179487}, {"timecode": 78, "before_eval_results": {"predictions": ["\"O...\"", "the Silk Road", "norway", "(George) Rogers Clark", "a mole", "a coach dog", "Sexual oppression in Sweden", "Volleyball", "John Alden", "Ghost World", "a cat", "a map", "Alaska", "Madison Avenue", "Job", "standard pitch", "art deco", "Spiderman", "Lumbini", "Elie Wiesel", "Anna Friel", "Tremain", "lieutenant", "the Jefferson Building", "Nostradamus", "Madrid", "Yuma", "Antarctica", "Ian Fleming", "the NAACP", "Moscow", "a lapanese boss", "beaux", "Mormon Tabernacle Choir", "'A'", "Griffith", "Bangkok", "Dixon", "positron", "Lyndon B. Johnson", "Jefferson", "Jerusalem", "Pushing Daisies", "Cranberry", "Falafel", "Shaugnhnessy", "a health care", "a sharlotka", "canali", "Abraham", "a self-appointed or mob-operated tribunal", "Iran, and cultures such as the Persians relied on sheep's wool for trading", "Rachel Kelly Tucker", "bridal shop with Anita, the girlfriend of her brother, Bernardo", "London", "kermadec Islands", "Julius Caesar", "Greek mythology, the Titaness daughter of the earth goddess Gaia and the sky god Uranus, and sister and wife to Cronus", "\"The Danny Kaye Show\"", "2012", "The drama of the action in-and-around the golf course", "TV's rabbit-ears era.", "Victor Mejia Munera", "The oceans"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5516225961538461}, "metric_results_detailed": {"EM": [false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, false, true, false, false, false, false, true, true, false, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, false, false, false, false, false, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.15384615384615385, 1.0, 0.5, 1.0, 0.0, 1.0, 0.19999999999999998, 1.0, 1.0, 0.0, 0.7499999999999999, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15154", "mrqa_searchqa-validation-11290", "mrqa_searchqa-validation-9709", "mrqa_searchqa-validation-15571", "mrqa_searchqa-validation-2424", "mrqa_searchqa-validation-14872", "mrqa_searchqa-validation-10281", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3139", "mrqa_searchqa-validation-14996", "mrqa_searchqa-validation-8585", "mrqa_searchqa-validation-9928", "mrqa_searchqa-validation-2662", "mrqa_searchqa-validation-16214", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-11682", "mrqa_searchqa-validation-1423", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-4445", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-10164", "mrqa_searchqa-validation-11473", "mrqa_searchqa-validation-3481", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-14159", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-5241", "mrqa_triviaqa-validation-3594", "mrqa_hotpotqa-validation-516", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-1424", "mrqa_newsqa-validation-875"], "SR": 0.46875, "CSR": 0.5087025316455696, "EFR": 1.0, "Overall": 0.6925998813291139}, {"timecode": 79, "before_eval_results": {"predictions": ["12.65 m ( 41.5 ft )", "DeWayne Warren", "a solitary figure who is not understood by others", "Doug Pruzan", "twelve", "byte - level operations", "Rich Mullins", "September 19, 2017", "a marriage officiant", "1624", "Hermann Ebbinghaus", "Agostino Bassi", "a batter is judged to have reached base solely because of a fielder's mistake", "Magnetically soft ( low coercivity ) iron", "Marty J. Walsh", "British Columbia, Canada", "$66.5 million", "Middle Eastern alchemy", "the `` 0 '' trunk code", "14.0 -- 9.1 ( M )", "the Chicago Cubs", "Stephen Chbosky", "Bill Russell", "Conrad Lewis", "Ernest Rutherford", "Fa Ze YouTubers", "10 June 1940", "the citizens", "a nominating committee composed of rock and roll historians selects names for the `` Performers '' category ( singers, vocal groups, bands, and instrumentalists of all kinds )", "Amanda Fuller", "The Forever People", "1997", "the mitochondrial inner membrane", "in the late 1980s", "Michael Phelps", "William DeVaughn", "Virginia Dare", "1960s", "Aidan Gallagher", "2002", "Elizabeth Banks as Gail Abernathy", "Pangaea", "Selena Gomez", "April and Andy", "the dress shop", "6,259 km", "September 18, 2009", "1916", "February 16, 2016", "the Gemara", "the internal reproductive anatomy ( such as the uterus in females ), and the external genitalia", "Brundisium", "France", "Ukrainian", "England", "April 1, 1949", "CBS", "\"green-card warriors\"", "India in Mumbai", "Brian David Mitchell,", "the Netherlands", "Florence", "Tiger Woods", "try and reduce the cost of auto repairs and insurance premium"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6201501623376623}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, false, true, false, false, true, false, true, true, true, false, false, true, true, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.26666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5714285714285715, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1714285714285714, 1.0, 0.0, 1.0, 0.8, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 0.5, 1.0, 0.5, 0.0, 0.0, 0.3333333333333333, 1.0, 0.16666666666666669, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.8181818181818182]}}, "before_error_ids": ["mrqa_naturalquestions-validation-9454", "mrqa_naturalquestions-validation-3119", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-1285", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6977", "mrqa_naturalquestions-validation-5001", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-7685", "mrqa_naturalquestions-validation-10225", "mrqa_naturalquestions-validation-3385", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-8500", "mrqa_naturalquestions-validation-7733", "mrqa_naturalquestions-validation-180", "mrqa_naturalquestions-validation-8685", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-5499", "mrqa_naturalquestions-validation-3208", "mrqa_naturalquestions-validation-9386", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-335", "mrqa_naturalquestions-validation-5665", "mrqa_naturalquestions-validation-10188", "mrqa_naturalquestions-validation-1916", "mrqa_naturalquestions-validation-9005", "mrqa_triviaqa-validation-6581", "mrqa_triviaqa-validation-4862", "mrqa_newsqa-validation-150", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-454"], "SR": 0.484375, "CSR": 0.5083984375, "EFR": 0.9696969696969697, "Overall": 0.6864784564393939}, {"timecode": 80, "UKR": 0.6796875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-1856", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-307", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3313", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4247", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4510", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-5372", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10182", "mrqa_naturalquestions-validation-1027", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1649", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3930", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6204", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6901", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7410", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7629", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7856", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-7957", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1547", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2231", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-4110", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-478", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-632", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10532", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11216", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11425", "mrqa_searchqa-validation-1173", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-1218", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13116", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-14542", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-15365", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16043", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16346", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-1966", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2481", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4305", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-5477", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6256", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6782", "mrqa_searchqa-validation-6814", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1116", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2443", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3633", "mrqa_squad-validation-3823", "mrqa_squad-validation-3953", "mrqa_squad-validation-4110", "mrqa_squad-validation-4430", "mrqa_squad-validation-4595", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7082", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7744", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1165", "mrqa_triviaqa-validation-1183", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1894", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3700", "mrqa_triviaqa-validation-3739", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-4891", "mrqa_triviaqa-validation-4923", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5440", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5496", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5735", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-599", "mrqa_triviaqa-validation-6269", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7349", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-935", "mrqa_triviaqa-validation-938"], "OKR": 0.828125, "KG": 0.49765625, "before_eval_results": {"predictions": ["Agatha Christie", "Miranda v. Arizona", "Oscar Wilde", "Vancouver Island", "violin", "Utrecht", "Vietnam", "Austen's", "georgia fox", "senior Training Manager", "Leadbetter", "Mikhail Gorbachev", "CBS", "jazz", "Earthquake", "jungle book", "(Roger) Rush", "neoclassic designs of Robert Adam", "agallon", "a Great Dane", "the natural world and mysticism", "Cambodia", "jujitsu", "Hunger Games", "head", "11 years and 302 days", "New Zealand", "Prussian 2nd Army", "(Andrew) Warne", "Whisky Galore", "Tunisia", "50", "Sen. Edward M. Kennedy", "Egremont", "penguin", "Google", "shoulder", "Persian", "Downton Abbey", "bird", "Rudyard Kipling", "Backgammon", "jacques", "Albert Einstein", "Germany", "Daniel Barenboim", "personal chronological beginning of the main characters'exploits on the Island", "ear", "Thornless", "Imola Circuit", "trout", "Martin Lawrence", "Doc '' Brown", "North Atlantic Ocean", "1961", "fascism", "Lord Chancellor of England", "\"Britain's Got Talent\"", "Ashley \"A.J.\" Jewell,", "19-year-old woman whose hospitalization exposed a shocking Austrian incest case", "Nebraska", "Tweedledee", "the Library of Congress", "Aung San Suu Kyi"], "metric_results": {"EM": 0.546875, "QA-F1": 0.618080357142857}, "metric_results_detailed": {"EM": [false, true, true, true, true, false, true, false, false, false, true, true, true, false, false, true, false, false, false, true, false, true, false, true, false, true, true, false, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, true, false, false, true, false, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.05714285714285714, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6362", "mrqa_triviaqa-validation-7623", "mrqa_triviaqa-validation-6256", "mrqa_triviaqa-validation-1024", "mrqa_triviaqa-validation-6973", "mrqa_triviaqa-validation-7029", "mrqa_triviaqa-validation-28", "mrqa_triviaqa-validation-5170", "mrqa_triviaqa-validation-3763", "mrqa_triviaqa-validation-2539", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-7592", "mrqa_triviaqa-validation-6858", "mrqa_triviaqa-validation-3465", "mrqa_triviaqa-validation-672", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-2309", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-2551", "mrqa_triviaqa-validation-6781", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-4730", "mrqa_triviaqa-validation-1936", "mrqa_naturalquestions-validation-10238", "mrqa_naturalquestions-validation-4771", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-3317"], "SR": 0.546875, "CSR": 0.5088734567901234, "EFR": 0.9655172413793104, "Overall": 0.6959718896338868}, {"timecode": 81, "before_eval_results": {"predictions": ["angelina Jolie", "worcester", "lincoln van", "van rijn", "Illinois", "belgium", "Paul Maskey", "(University of) Serbia", "tartar sauce", "three Graces", "satyrs", "Daniel Fran\u00e7ois Esprit Auber", "kedushah", "martin van buren", "l Leeds", "ludwig", "webber", "white", "george z", "lincoln", "honda", "runcorn", "Vietnam", "monaco", "vincent van gogh", "monaco", "Croatia", "NBA", "steel", "(University of) lincoln", "Henri Paul", "lincoln potter", "bird", "(University of) Abissinia", "cox", "belgium", "Victor Hugo", "endosperm", "adriatic", "heartburn", "(University of) Nigeria", "HMS Conqueror", "richard attenborough", "webber", "richford", "Cynthia Nixon", "lincoln", "wat tyler", "Patrick Henry", "126 mph", "belgium", "Eddie Murphy", "England", "lincoln attenborough", "Thorgan Hazard", "senior men's Lithuanian national team", "(University of) Kentucky's Center for Applied Energy Research", "almost 100", "sex harassment", "in critical condition", "Superman", "(University of) Norway", "Towering Inferno", "member states"], "metric_results": {"EM": 0.375, "QA-F1": 0.44652777777777775}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false], "QA-F1": [0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.33333333333333337, 0.0, 0.8, 0.5, 0.0, 1.0, 0.0, 1.0, 0.4444444444444445]}}, "before_error_ids": ["mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-4599", "mrqa_triviaqa-validation-522", "mrqa_triviaqa-validation-6615", "mrqa_triviaqa-validation-4852", "mrqa_triviaqa-validation-1779", "mrqa_triviaqa-validation-4099", "mrqa_triviaqa-validation-5620", "mrqa_triviaqa-validation-2449", "mrqa_triviaqa-validation-1256", "mrqa_triviaqa-validation-6970", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-6586", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-2842", "mrqa_triviaqa-validation-3826", "mrqa_triviaqa-validation-3622", "mrqa_triviaqa-validation-3338", "mrqa_triviaqa-validation-481", "mrqa_triviaqa-validation-6356", "mrqa_triviaqa-validation-5499", "mrqa_triviaqa-validation-2556", "mrqa_triviaqa-validation-2521", "mrqa_triviaqa-validation-6467", "mrqa_triviaqa-validation-883", "mrqa_triviaqa-validation-7737", "mrqa_triviaqa-validation-486", "mrqa_triviaqa-validation-2287", "mrqa_triviaqa-validation-4207", "mrqa_naturalquestions-validation-8944", "mrqa_naturalquestions-validation-1909", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-4927", "mrqa_hotpotqa-validation-1039", "mrqa_newsqa-validation-2080", "mrqa_newsqa-validation-2843", "mrqa_newsqa-validation-1829", "mrqa_searchqa-validation-5224", "mrqa_naturalquestions-validation-10495"], "SR": 0.375, "CSR": 0.5072408536585367, "EFR": 1.0, "Overall": 0.7025419207317073}, {"timecode": 82, "before_eval_results": {"predictions": ["Nic\u00e9phore Ni\u00e9pce", "ukraine", "tarn", "GM Motors", "Sheffield", "Messina", "piano", "Louis XVIII", "Pat Cash", "Santiago", "Wild Atlantic Way", "Kyoto Protocol", "charleston", "repechage", "stuart biko", "john leguizamo", "peacock", "rita hayworth", "Miss Trunchbull", "imola", "Albania", "antelope", "animals", "boreas", "Ivan Basso", "bullfighting", "10", "Playboy", "ukraine", "Peter Ackroyd", "walford", "Sven Goran Eriksson", "ad uceline", "Robert Adams", "death penalty", "Nick Clegg", "14", "Bangladesh", "tithonos", "Papua New Guinea", "Lady Gaga", "Sunset Boulevard", "reel life", "ars gratia artis", "baloney", "All Things Must Pass", "fire", "tet", "Arabah", "d\u00e9j\u00e0-vu", "gerry and Sylvia Anderson", "energy moves from producers ( plants ) to primary consumers ( herbivores ) and then to secondary consumers ( predators )", "September 2, 1945", "the British rock group Coldplay", "Greg Gorman and Helmut Newton", "American real estate developer, philanthropist and sports team owner", "Isabella II", "Mexico", "Marines or sons of Marines", "Arizona", "Franois Chopin", "Indiana Jones", "Batavia", "Cosmopolitan"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6416666666666667}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, false, true, true, false, true, true, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2664", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-465", "mrqa_triviaqa-validation-545", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-1367", "mrqa_triviaqa-validation-1233", "mrqa_triviaqa-validation-4554", "mrqa_triviaqa-validation-4147", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-5020", "mrqa_triviaqa-validation-6576", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-1916", "mrqa_triviaqa-validation-6011", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3013", "mrqa_naturalquestions-validation-5396", "mrqa_hotpotqa-validation-4838", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-784", "mrqa_searchqa-validation-5866", "mrqa_searchqa-validation-16678", "mrqa_hotpotqa-validation-668"], "SR": 0.609375, "CSR": 0.5084713855421688, "EFR": 1.0, "Overall": 0.7027880271084338}, {"timecode": 83, "before_eval_results": {"predictions": ["jennifer gervais", "The Green Arrow", "a parable", "Romeo", "Spinal Tap", "Tennessee", "Detroit", "Ferris B Mueller", "the United States", "Giza", "Ruth Bader", "the Boers", "the sensation of stretch", "Old Fashioned", "the Osmonds", "Bonnie and Clyde", "a Crustaceans", "the Indian School", "a chimp", "Indian reservations", "John Updike", "the Ganges", "netherlands", "Bright Lights", "tokyo", "coelacanth", "Pride", "Cheers", "david david Spyri", "Crosby", "Matt Leinart", "Group O", "jennifer jerkowitz", "an albatross", "Falklands", "taro", "a quip", "a lighthouse", "the optic nerve goes through the back wall of the eye, carrying", "Dan Rather", "Georgia", "Buffalo Bill", "Creation", "pig", "Harvard", "neurons", "Hawaii", "the brain", "a mean dog", "dragonflies", "Bill Cosby", "May 19, 2017", "Bachendri Pal", "James Corden", "Pentecost", "jennifer jordan", "jennifer jerkowitz", "Hale\u02bb iwa", "Armidale, New South Wales", "1992", "\"It was quite surprising to learn of the request,\"", "Steven Chu", "Stella McCartney,", "Rwanda"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6019345238095238}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, true, false, true, true, false, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, false, false, true, true, true, true, true, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, true, false, false, false, false, true, false, true, false, false], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13516", "mrqa_searchqa-validation-4520", "mrqa_searchqa-validation-6842", "mrqa_searchqa-validation-13936", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-1455", "mrqa_searchqa-validation-4318", "mrqa_searchqa-validation-12971", "mrqa_searchqa-validation-2790", "mrqa_searchqa-validation-7320", "mrqa_searchqa-validation-6498", "mrqa_searchqa-validation-2738", "mrqa_searchqa-validation-11967", "mrqa_searchqa-validation-8018", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5868", "mrqa_searchqa-validation-2457", "mrqa_searchqa-validation-9304", "mrqa_searchqa-validation-15128", "mrqa_searchqa-validation-7144", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-4821", "mrqa_searchqa-validation-2946", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-4", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-5245", "mrqa_newsqa-validation-1948", "mrqa_newsqa-validation-3058", "mrqa_newsqa-validation-3660"], "SR": 0.53125, "CSR": 0.5087425595238095, "EFR": 1.0, "Overall": 0.7028422619047618}, {"timecode": 84, "before_eval_results": {"predictions": ["in the 1970s", "Steveston Outdoor pool in Richmond, BC", "originated first associated with celebrations of Easter in the United States sometime during the 1930s", "Lenny Jacobson", "the status line", "each team", "a major victory of the Civil Rights Movement, and a model for many future impact litigation cases", "in the weeks before the release of Xscape", "circular", "approximately 230 million kilometres ( 143,000,000 mi ), and its orbital period is 687 ( Earth ) days", "traditional dance", "previous year's Palm Sunday celebrations", "Castleford", "the fourth C key from left on a standard 88 - key piano keyboard", "L.K. Advani, an Indian politician who served as the Deputy Prime Minister of India from 2002 to 2004, and was the Leader of the Opposition in the 15th Lok Sabha", "winter", "Samuel Taylor Coleridge's poem", "Robber Barons", "2001", "Spencer Treat Clark", "transition from summer to winter, in September ( Northern Hemisphere ) or March ( Southern Hemisphere )", "2004", "Renhe Sports Management Ltd", "Americans who served in the armed forces and as civilians", "Michael Crawford", "420 mg", "gastrocnemius muscle", "a biocidal effect of metals, especially heavy metals, that occurs even in low concentrations", "Peter Cetera", "Austin, Texas", "1945", "California State Route 1", "New Delhi", "midpiece", "Burj Khalifa", "Pangaea or Pangea", "mitochondrial membrane in eukaryotes or the plasma membrane in bacteria", "Johnny Cash", "Andrew Lincoln", "a virtual reality simulator accessible by players using visors and haptic technology such as gloves", "Kevin Spacey", "Human anatomy", "Natural - language processing", "10 years", "2026", "eleven", "18", "early 20th century", "Fred E. Ahlert", "Joanna Moskawa", "1962", "Loch Ness", "Lingerie Football League", "a griffin", "Mick Jackson", "Queenston Delta", "15", "Michelle Obama", "Consumer Product Safety Commission Tuesday,", "\"That's ridiculous!\"", "a horse", "A Thousand Darknesses: Lies and Truth in", "Dwight D. Eisenhower", "Joel \"Taz\" DiGregorio,"], "metric_results": {"EM": 0.4375, "QA-F1": 0.600715945790743}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, false, true, true, false, false, true, false, false, true, true, false, false, true, true, false, false, false, true, true, false, true, true, true, false, true, true, false, false, false, false, false, false], "QA-F1": [0.6666666666666666, 0.5, 0.2666666666666667, 0.0, 1.0, 1.0, 0.1379310344827586, 0.0, 0.0, 0.5, 0.0, 0.5333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8181818181818181, 1.0, 1.0, 1.0, 0.47058823529411764, 0.0, 1.0, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.5, 0.6153846153846153, 1.0, 0.0, 0.35294117647058826, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 0.0, 0.0, 0.5, 0.8]}}, "before_error_ids": ["mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-7172", "mrqa_naturalquestions-validation-5483", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-10090", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-10140", "mrqa_naturalquestions-validation-9897", "mrqa_naturalquestions-validation-259", "mrqa_naturalquestions-validation-7017", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-7818", "mrqa_naturalquestions-validation-2208", "mrqa_naturalquestions-validation-735", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-1295", "mrqa_naturalquestions-validation-3363", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-6190", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-7679", "mrqa_naturalquestions-validation-5838", "mrqa_naturalquestions-validation-6797", "mrqa_naturalquestions-validation-960", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-2065", "mrqa_hotpotqa-validation-4692", "mrqa_newsqa-validation-1869", "mrqa_newsqa-validation-1826", "mrqa_searchqa-validation-7897", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-4132", "mrqa_newsqa-validation-3992"], "SR": 0.4375, "CSR": 0.5079044117647059, "EFR": 0.9166666666666666, "Overall": 0.6860079656862745}, {"timecode": 85, "before_eval_results": {"predictions": ["Rolex", "Vincent Motorcycle Company", "cycle racing", "ganga", "gerry adams", "aryl group", "Roy Rogers", "Steve Jobs", "Tommy Lee Jones", "nirvana", "Donna Summer", "heel", "geese", "a special messenger of Jesus Christ", "Sheryl Crow", "Colonel Lacey", "the largest", "Franklin Delano Roosevelt", "neurons", "Prisoner and Escort", "ossan", "Swordfish", "earwax", "george Best", "a faggot", "second and Third Herd", "parson brown", "Australia", "pascal", "British Airways", "five", "Challenger", "The World is Not Enough", "Genoa", "Vienna", "glee", "David Hockney", "iron", "Japan", "bayern munich", "American actress and a former fashion model Megan Richards", "Italy", "New Mexico", "May", "chili", "Madagascar", "Beaujolais", "Angus Robertson", "kolkata", "Strictly Come Dancing", "David Bowie", "Nick Sager", "Forbes Burnham", "Game 1", "Dra\u017een Petrovi\u0107", "Costa del Sol", "early Romantic period", "first grand Slam", "propofol.", "make people anxious.", "Treaty of Versailles", "Zinedine Zidane", "Augustus", "a newt"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6441220238095238}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-980", "mrqa_triviaqa-validation-1334", "mrqa_triviaqa-validation-3601", "mrqa_triviaqa-validation-7627", "mrqa_triviaqa-validation-2920", "mrqa_triviaqa-validation-2336", "mrqa_triviaqa-validation-7526", "mrqa_triviaqa-validation-2477", "mrqa_triviaqa-validation-490", "mrqa_triviaqa-validation-6140", "mrqa_triviaqa-validation-5484", "mrqa_triviaqa-validation-77", "mrqa_triviaqa-validation-7750", "mrqa_triviaqa-validation-960", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-3610", "mrqa_triviaqa-validation-7158", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-7097", "mrqa_triviaqa-validation-7660", "mrqa_triviaqa-validation-1961", "mrqa_naturalquestions-validation-8849", "mrqa_hotpotqa-validation-1634", "mrqa_newsqa-validation-1154", "mrqa_newsqa-validation-1946", "mrqa_searchqa-validation-4261", "mrqa_searchqa-validation-350"], "SR": 0.5625, "CSR": 0.5085392441860466, "EFR": 1.0, "Overall": 0.7028015988372093}, {"timecode": 86, "before_eval_results": {"predictions": ["Switzerland", "John Monash", "tempo", "photographs, film and television", "Arthur Freed", "alt-right", "Runaways", "\"50 best cities to live in.\"", "La Liga", "Best Prom Ever", "June 13, 1960", "Iran", "short interspersed nuclear elements", "capital crimes", "London", "My G girlfriend", "quantum mechanics", "king Duncan", "February 12, 2014", "New South Wales", "Anne and Georges", "David Villa", "Double Agent", "Orange Bowl", "\"White Horse\"", "Diamond Rio", "Quentin Coldwater", "Andrew Johnson", "humble case of Benjamin Button", "Martha Wainwright", "Leafcutter John", "moth", "Final Fantasy XII", "Jim Thorpe", "De La Soul", "\"Numb\"", "Shropshire Union Canal", "1670", "a skerry", "Oliver Parker", "The Strain", "kamehameha I", "Colorado Buffaloes", "Kunta Kinte", "Maxim Gorky", "William Scott Elam", "\"The Jeffersons\"", "Franz Ferdinand", "prevent the opposing team from scoring goals", "Cody Miller", "1907", "Maidstone Studios in Maidstone, Kent", "strings of eight bits ( known as bytes )", "Majo to Hyakkihei 2", "nathan leopold Jr.", "Bill Haley", "george Carey", "Amanda Knox", "\"No. 1\"", "Jeddah, Saudi Arabia,", "William Strunk Jr.,", "Andrew Jackson", "Thomas Jefferson", "Willa Cather"], "metric_results": {"EM": 0.546875, "QA-F1": 0.5911401098901099}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, false, false, true, false, true, true, false, false, true, true, true, false, true, true, false, true, false, false, true, true, false, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4615384615384615, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-2434", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-533", "mrqa_hotpotqa-validation-1527", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-4836", "mrqa_hotpotqa-validation-1032", "mrqa_hotpotqa-validation-4566", "mrqa_hotpotqa-validation-665", "mrqa_hotpotqa-validation-3242", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-4774", "mrqa_hotpotqa-validation-4507", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-4756", "mrqa_hotpotqa-validation-4326", "mrqa_hotpotqa-validation-4198", "mrqa_hotpotqa-validation-4109", "mrqa_hotpotqa-validation-1656", "mrqa_hotpotqa-validation-3788", "mrqa_naturalquestions-validation-3329", "mrqa_triviaqa-validation-5287", "mrqa_triviaqa-validation-5380", "mrqa_newsqa-validation-3212", "mrqa_newsqa-validation-2051", "mrqa_newsqa-validation-2558", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-1530"], "SR": 0.546875, "CSR": 0.5089798850574713, "EFR": 1.0, "Overall": 0.7028897270114942}, {"timecode": 87, "before_eval_results": {"predictions": ["Edward R. Murrow", "Vision of the Future", "in 1754", "8 November 1978", "Hamlet", "Erick Avari", "Milwaukee Bucks", "McLaren-Honda", "Armin Shimerman", "Set in the Spiderwick Estate in New England", "American reality documentary television series", "Tricia Helfer", "Qualcomm", "water", "10-metre platform event", "Cincinnati Bengals", "on the shore", "his work on \"Nanny McPhee\", \"The Legend of Tarzan\"", "November 15, 1903", "Bury St Edmunds, Suffolk, England", "Rothschild", "Mr. Church", "\"Billboard\" Hot 100, \"Rich Girl\"", "Thomas Christopher Ince", "Matt Serra", "public", "Los Angeles", "\"Me and You and everyone We Know\"", "Vyd\u016bnas", "al-Qaeda", "the Darling River", "Baldwin", "2 April 1977", "House of Commons", "William Finn", "\"Love Letter\"", "Indian", "German", "Barnoldswick", "the late 12th Century", "\"Gibby\"", "The S7 series", "729", "tenure", "Professor Frederick Lindemann,", "Robert Jenrick", "Somerset County, Pennsylvania", "Salford, Lancashire", "British Conservative", "The Division of Cook", "\"Peshwa\"", "Prafulla Chandra Ghosh", "innermost in the eye", "United States", "Western Samoa", "The De Lorean DMC-12", "comets", "his comments while Saudi authorities discuss whether he should be charged with a crime,", "\"The New Promised Land: Silicon Valley.\"", "laid 11 healthy eggs and, this week, all 11 of them hatched -- the last one on Wednesday.", "a snowmobile", "a comets", "porcelain", "vasoconstriction of most blood vessels"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5668030753968254}, "metric_results_detailed": {"EM": [true, true, false, true, true, false, false, true, false, false, false, false, true, false, true, true, false, false, true, false, false, true, false, true, false, true, true, false, false, true, true, true, false, true, true, true, false, false, true, true, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, true, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.22222222222222224, 0.888888888888889, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.4, 1.0, 0.7499999999999999, 0.5, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.4, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 0.5, 0.6, 0.0, 0.14285714285714285, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1722", "mrqa_hotpotqa-validation-4079", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-103", "mrqa_hotpotqa-validation-4988", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-3951", "mrqa_hotpotqa-validation-2121", "mrqa_hotpotqa-validation-4345", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-4064", "mrqa_hotpotqa-validation-908", "mrqa_hotpotqa-validation-958", "mrqa_hotpotqa-validation-870", "mrqa_hotpotqa-validation-4483", "mrqa_hotpotqa-validation-3464", "mrqa_hotpotqa-validation-3842", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4751", "mrqa_hotpotqa-validation-3843", "mrqa_hotpotqa-validation-2811", "mrqa_hotpotqa-validation-4163", "mrqa_hotpotqa-validation-5168", "mrqa_hotpotqa-validation-354", "mrqa_naturalquestions-validation-6579", "mrqa_naturalquestions-validation-6340", "mrqa_naturalquestions-validation-6888", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-6935", "mrqa_newsqa-validation-3305", "mrqa_newsqa-validation-4026", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5276", "mrqa_searchqa-validation-10831", "mrqa_naturalquestions-validation-836"], "SR": 0.4375, "CSR": 0.5081676136363636, "EFR": 0.9722222222222222, "Overall": 0.6971717171717171}, {"timecode": 88, "before_eval_results": {"predictions": ["Barack Obama", "\"La Mome Piaf,\"", "Ulysses S. Grant", "Apollo", "Richard Wagner", "Atticus Finch", "The Peter Principle", "copper and zinc", "hammertone", "Dunfermline", "bison", "Edmund Cartwright", "Mary Poppins", "leicestershire", "Black Wednesday", "Samoa", "John Gorman", "The Daily Mirror", "copper", "Olympus Mons on Mars", "Poland", "dee caffari", "Labyrinth", "Belize", "kennets", "Chester", "prawns", "James Hogg", "mUD", "Fermanagh", "Colombia", "Kevin Painter", "llyn Padarn", "katherine parr", "Muhammad Ali", "Carmen Miranda", "Sandi Toksvig", "John McEnroe", "August 10, 1960", "estonia", "Sarajevo", "gluten", "epi", "Peter Duck", "muthia Murlitharan", "Ridley Scott", "four", "Futurama", "adrian Edmondson", "63 to 144 inches", "1979", "September 29, 2017", "Walter Brennan", "from 13 to 22 June 2012", "1909 Cuban-American Major League Clubs Series", "2015", "Spanish", "Lashkar-e-Tayyiba (LeT)", "\"our unflinching support.\"", "\"Rightwing Extremism: Current Economic and Political Climate Fueling Resurgence in Radicalization and Recruitment.\"", "devil's food cake", "Michelangelo", "Missouri", "the Jetsons"], "metric_results": {"EM": 0.671875, "QA-F1": 0.7131628787878788}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, false, true, true, true, false, false, true, true, true, true, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4589", "mrqa_triviaqa-validation-176", "mrqa_triviaqa-validation-3142", "mrqa_triviaqa-validation-7041", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-7133", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-1988", "mrqa_triviaqa-validation-3242", "mrqa_triviaqa-validation-1813", "mrqa_triviaqa-validation-500", "mrqa_triviaqa-validation-3426", "mrqa_triviaqa-validation-2443", "mrqa_triviaqa-validation-2876", "mrqa_triviaqa-validation-5205", "mrqa_triviaqa-validation-7516", "mrqa_triviaqa-validation-867", "mrqa_triviaqa-validation-7523", "mrqa_naturalquestions-validation-3589", "mrqa_newsqa-validation-161", "mrqa_searchqa-validation-6490"], "SR": 0.671875, "CSR": 0.5100070224719101, "EFR": 1.0, "Overall": 0.703095154494382}, {"timecode": 89, "before_eval_results": {"predictions": ["3", "a human interface", "Scottie Pippen", "Vaseline", "savings rate", "silver", "Gone with the Wind", "large", "Nelly", "Saint Telemachus", "Finding Nemo", "the hyoid horns", "the Kite Runner", "a shark", "egypt", "Oprah Winfrey", "the Dixie Chicks", "pear tart", "California", "a pair", "the Ionian Sea", "Pope John Paul II", "Newburg", "Yemen", "David Geffen", "chariots", "Pablo Neruda", "the fifth amendment", "a mite", "Saturn", "the Nanny Diaries", "liquid crystals", "Robert Frost", "a dictum", "brown Butter sauce", "Crete", "Father Brown", "reuben", "The Outsiders", "the waltz", "ellantern", "Jane Austen", "Wisconsin", "a story of Two Cities", "Q", "When Harry Met Sally", "mexican", "pumice", "John Molson", "Jan and Dean", "an American physician and novelist", "Janis Joplin", "all transmissions", "Sir Hugh Beaver", "andorra", "Michael Faraday", "g Gerald R. Ford", "1992", "\"The King of Chutzpah\"", "Niger\u2013Congo", "an upper respiratory infection,", "Fernando Gonzalez", "at least 14", "eight years,"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7122395833333333}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, true, true, false, true, false, false, true, false, false, true, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, true, true, true, false, false], "QA-F1": [0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.75, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.3333333333333333]}}, "before_error_ids": ["mrqa_searchqa-validation-13576", "mrqa_searchqa-validation-1293", "mrqa_searchqa-validation-16479", "mrqa_searchqa-validation-10986", "mrqa_searchqa-validation-9196", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-349", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-946", "mrqa_searchqa-validation-14490", "mrqa_searchqa-validation-833", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6422", "mrqa_searchqa-validation-14125", "mrqa_searchqa-validation-12573", "mrqa_searchqa-validation-2779", "mrqa_searchqa-validation-15735", "mrqa_searchqa-validation-550", "mrqa_naturalquestions-validation-9419", "mrqa_triviaqa-validation-1115", "mrqa_newsqa-validation-795", "mrqa_newsqa-validation-3145"], "SR": 0.65625, "CSR": 0.5116319444444444, "EFR": 1.0, "Overall": 0.7034201388888889}, {"timecode": 90, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-106", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2650", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2908", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2971", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-3998", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4084", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-5872", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-10610", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2148", "mrqa_naturalquestions-validation-2291", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3560", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4645", "mrqa_naturalquestions-validation-468", "mrqa_naturalquestions-validation-4736", "mrqa_naturalquestions-validation-4885", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-5927", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8424", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9814", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-161", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1867", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2480", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_newsqa-validation-946", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-10776", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15709", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-3092", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-731", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-8986", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10466", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-4870", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7162", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1125", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2458", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3172", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3812", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-524", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5944", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6332", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6428", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6662", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6853", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-7668", "mrqa_triviaqa-validation-7669", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.80859375, "KG": 0.47421875, "before_eval_results": {"predictions": ["the Harpe brothers", "McComb, Mississippi", "\"Loch Lomond\"", "American reality television series", "Gweilo", "The Royal Family", "The Ninth Gate", "James G. Kiernan", "emperor", "Erreway", "Protestant Christian", "\u00c6thelred I", "Bellagio and The Mirage", "The Los Angeles Dance Theater", "Johnnie Ray", "Hampton University", "The Clash of Triton", "Jenji Kohan", "1", "God Save the Queen", "Scottish Premiership club Hibernian", "Oklahoma City", "Vincent Landay", "Randall Boggs", "October 22, 2012", "rock music", "his eldest daughter Patricia, 2nd Countess Mountbatten", "\"Slaughterhouse-Five\"", "Harry F. Sinclair", "Ghana Technology University College", "the Michigan Bigfoot", "Cyclic Defrost", "Commonwealth of England, Scotland, and Ireland", "Loretta", "Worcester", "1972", "Ang Lee", "Brad Silberling", "Blue (Da Ba Dee)", "Ubba", "La Scala, Milan", "Orson Welles", "1987", "Schaffer", "Ali B.", "Melbourne's City Centre", "Lincoln Riley", "private equity, credit and hedge fund", "Enigma", "University of Nevada, Reno", "Mission Inn Hotel & Spa", "Muhammad", "under the age of 18", "Harlem River", "Turkish", "orange", "sulfuric and nitric acids", "1913.", "Del Potro.", "Amsterdam, in the Netherlands, to Ankara, Turkey", "Tolkien", "Jaguar", "Wheat smut", "semi-autonomous organisational units within the National Health Service in England"], "metric_results": {"EM": 0.609375, "QA-F1": 0.699516369047619}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, false, true, true, false, true, true, false, true, true, false, false, true, true, true, false, true, false, false, false, true, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, false, true, false, false, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.25, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 1.0, 0.4, 1.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.6666666666666666, 0.2857142857142857, 0.0, 1.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2588", "mrqa_hotpotqa-validation-4240", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-3627", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-3260", "mrqa_hotpotqa-validation-788", "mrqa_hotpotqa-validation-1429", "mrqa_hotpotqa-validation-1305", "mrqa_hotpotqa-validation-4252", "mrqa_hotpotqa-validation-295", "mrqa_hotpotqa-validation-2635", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4479", "mrqa_hotpotqa-validation-2708", "mrqa_hotpotqa-validation-1025", "mrqa_hotpotqa-validation-1568", "mrqa_naturalquestions-validation-5516", "mrqa_triviaqa-validation-7763", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-1471", "mrqa_newsqa-validation-1150", "mrqa_newsqa-validation-2098", "mrqa_searchqa-validation-9281", "mrqa_searchqa-validation-5567"], "SR": 0.609375, "CSR": 0.512706043956044, "EFR": 0.96, "Overall": 0.6901662087912087}, {"timecode": 91, "before_eval_results": {"predictions": ["Terry Reid", "Friedman Billings Ramsey", "Robber Barons", "Phillip Schofield and Christine Bleakley", "the manifestation of God's presence as perceived by humans according to the Abrahamic religions", "LED illuminated display", "Bart Howard", "transmission", "Bob Dylan", "brain, muscles, and liver", "USS Chesapeake, or the Battle of Boston Harbor", "mid-1980s", "fortified complex at the heart of Moscow, overlooking the Moskva River to the south, Saint Basil's Cathedral and Red Square to the east, and the Alexander Garden to the west", "Charles Darwin and Alfred Russel Wallace", "locations in Google Maps", "Richard Stallman", "January 2004", "during the 1930s", "to check the president's power to commit the United States to an armed conflict without the consent of the U.S. Congress", "an evaluation by an individual and can affect the perception of a decision, action, idea, business, person, group, entity, or other whenever concrete data is generalized or influences ambiguous information", "heat", "Spain", "peptide bonds", "New England Patriots", "people in the 20th century who used obscure languages as a means of secret communication during wartime", "Zhu Yuanzhang", "1980 Summer Olympics", "Heather Stebbins", "within a dorsal root ganglion", "drizzle, rain, sleet, snow, graupel and hail", "Karen Gillan", "2017 season", "Julie Adams", "1881", "Mike Higham, who had previously worked with Sondheim on Sweeney Todd : The Demon Barber of Fleet Street", "a proverbial phrase referring to one of the seven heavenly virtues typically said to date back to `` Psychomachia, '' an epic poem written in the fifth century", "600,000 cu mi", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "Theodore Roosevelt", "1937", "non-voters", "a December 28, 1975 NFL playoff game between the Dallas Cowboys and the Minnesota Vikings", "Payson, Lauren, and Kaylie", "2015, 2017", "Dr. Lexie Grey", "February 27, 2007", "Claims adjuster ( claim adjuster ), or claims handler ( claim handler )", "Taron Egerton", "1990", "smen", "T'Pau", "Fort Nelson near Portsmouth", "playing cards", "Sparta", "World Famous Gold & Silver Pawn Shop", "Darkroom", "Louis \"Louie\" Zamperini", "Former Mobile County Circuit Judge Herman Thomas", "death of cardiac arrest", "20th place.", "Madison", "Babel", "20th century", "Juan Ponce de Len"], "metric_results": {"EM": 0.375, "QA-F1": 0.5544978630983479}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, false, true, false, false, false, false, false, false, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, false, true, false, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.2857142857142857, 1.0, 0.0, 1.0, 0.4, 0.4444444444444445, 0.6666666666666666, 0.07142857142857142, 0.5, 0.3333333333333333, 1.0, 0.6666666666666666, 0.0, 0.875, 0.16666666666666669, 0.0, 1.0, 0.5, 1.0, 0.18750000000000003, 1.0, 1.0, 0.0, 0.5714285714285715, 0.25, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4827586206896552, 0.0, 1.0, 0.14285714285714288, 0.5, 0.0, 0.375, 0.4, 0.6666666666666666, 1.0, 1.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.4, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-522", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9316", "mrqa_naturalquestions-validation-754", "mrqa_naturalquestions-validation-9757", "mrqa_naturalquestions-validation-7704", "mrqa_naturalquestions-validation-2098", "mrqa_naturalquestions-validation-5758", "mrqa_naturalquestions-validation-4905", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-4265", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-8028", "mrqa_naturalquestions-validation-10331", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-7415", "mrqa_naturalquestions-validation-2652", "mrqa_naturalquestions-validation-3419", "mrqa_naturalquestions-validation-2448", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-2865", "mrqa_naturalquestions-validation-5154", "mrqa_naturalquestions-validation-2176", "mrqa_naturalquestions-validation-3469", "mrqa_naturalquestions-validation-4524", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-3187", "mrqa_naturalquestions-validation-2949", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-8161", "mrqa_triviaqa-validation-5161", "mrqa_triviaqa-validation-1101", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-1518", "mrqa_searchqa-validation-2818", "mrqa_searchqa-validation-5579"], "SR": 0.375, "CSR": 0.5112092391304348, "EFR": 0.825, "Overall": 0.662866847826087}, {"timecode": 92, "before_eval_results": {"predictions": ["Miller Lite beer", "beetle", "the Saskatchewan River", "Kentmere Valley", "electronic junk mail", "Tahrir Square", "David Frost", "Newbury Racecourse", "torture-linked rendition program", "knutsford", "peno Verde", "Spongebob", "The Bespoke Overcoat", "China", "Maine", "Thomas Cranmer", "ford administration", "the federal district of Washington, D.C.", "Jack Sprat", "Ronnie Kray", "conclave", "Dublin port", "The Mayor of Casterbridge", "feet", "Mumbai", "John Lennon", "Lusitania", "Anne Boleyn", "Australia", "mammal", "Portugal", "Botswana", "Philippines", "blood left at crime scenes", "Spain", "Marilyn Monroe", "Jupiter Mining Corporation", "dry rot", "Isambard Kingdom Brunel", "Canada", "bomber", "Jinnah International", "indus", "anne fordred", "Peter Paul Rubens", "john ford", "six", "Mendip Hills", "burma", "Charles Taylor", "Pancho Villa", "for the purpose of changing display or audio settings quickly, such as brightness, contrast, or volume, and is held down in conjunction with the appropriate key to change the settings", "Jerry Leiber and Mike Stoller for The Coasters", "Total Drama World Tour", "Karl Johan Schuster", "Worcester County", "Blue Ridge Parkway", "Lucky Dube,", "the Middle East and North Africa,", "Michael Partain,", "Beauty and the Beast", "Luxembourg", "Hammurabi", "Lobotomy"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6844975490196079}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, false, true, false, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, true, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, true, true, true, true, true], "QA-F1": [1.0, 1.0, 0.5, 0.0, 0.8, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9411764705882353, 0.8333333333333333, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-6124", "mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-7342", "mrqa_triviaqa-validation-3295", "mrqa_triviaqa-validation-7096", "mrqa_triviaqa-validation-729", "mrqa_triviaqa-validation-5356", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-4148", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-572", "mrqa_triviaqa-validation-1000", "mrqa_triviaqa-validation-3952", "mrqa_triviaqa-validation-3217", "mrqa_triviaqa-validation-7596", "mrqa_triviaqa-validation-413", "mrqa_triviaqa-validation-7461", "mrqa_triviaqa-validation-4632", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-6872", "mrqa_naturalquestions-validation-1587", "mrqa_naturalquestions-validation-7278", "mrqa_hotpotqa-validation-2017", "mrqa_hotpotqa-validation-4122"], "SR": 0.59375, "CSR": 0.5120967741935484, "EFR": 1.0, "Overall": 0.6980443548387096}, {"timecode": 93, "before_eval_results": {"predictions": ["American", "keeper of the Great Seal of Scotland", "1776", "Meghan Markle", "U.S. Bancorp", "Justin Adler", "Aston Martin Racing", "Coahuila, Mexico", "green", "methylenedioxy meth", "Colin Vaines", "Orange County", "racehorse breeder and owner", "Jim Kelly", "Australia", "the Wye river", "Travis, the chimpanzee who attacked Charla Nash in 2009", "Miracle", "Erich Maria Remarque", "Scott Mosier", "A Scholar Under Siege", "Dutch", "1999", "\"Saw II\"", "1947", "Fianna \u00c9ireann", "Tuesday, January 24, 2012", "John Monash", "edward the Elder", "Riverside Stadium", "rap parts from Darryl,RB Djan and Ryan Babel", "5,112 feet", "Jefferson Memorial", "May 1, 2011", "four", "Red and Assiniboine Rivers", "200", "15 mi", "February 18, 1965", "brothers Malcolm and Angus Young", "the Goddess of Pop", "125 lb (57 kg) and below", "chocolate-colored Labrador Retriever", "1966", "March 14, 2000", "1927", "Gregg Popovich", "Princess Anne", "Neighbours", "Hall & Oates", "January 16, 2013", "northwest Washington", "1830", "Lake Powell", "gulls", "chariot", "Louisiana", "\"stressed and tired force\" made vulnerable by multiple deployments,", "Islamists", "Tuesday", "The African Queen", "cats", "Gibraltar", "acidity or basicity"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6691220238095239}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, true, false, false, true, false, true, true, true, false, false, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, true, false, true, false, false, true, false, true, false, false, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, false], "QA-F1": [1.0, 0.8333333333333334, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.5, 0.0, 0.7000000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.33333333333333337, 1.0, 0.8, 0.5, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-694", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-347", "mrqa_hotpotqa-validation-424", "mrqa_hotpotqa-validation-5608", "mrqa_hotpotqa-validation-1425", "mrqa_hotpotqa-validation-4254", "mrqa_hotpotqa-validation-761", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-3264", "mrqa_hotpotqa-validation-5264", "mrqa_hotpotqa-validation-955", "mrqa_hotpotqa-validation-5641", "mrqa_hotpotqa-validation-2300", "mrqa_hotpotqa-validation-3152", "mrqa_hotpotqa-validation-2728", "mrqa_hotpotqa-validation-5878", "mrqa_hotpotqa-validation-4802", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-1810", "mrqa_hotpotqa-validation-5879", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-2886", "mrqa_hotpotqa-validation-1077", "mrqa_triviaqa-validation-1582", "mrqa_newsqa-validation-2886", "mrqa_newsqa-validation-2066", "mrqa_searchqa-validation-7854", "mrqa_naturalquestions-validation-8652"], "SR": 0.53125, "CSR": 0.5123005319148937, "EFR": 1.0, "Overall": 0.6980851063829787}, {"timecode": 94, "before_eval_results": {"predictions": ["Villa Park", "Guinea", "new Plymouth", "four", "Guardian", "tartan", "toy Story", "Pontiac", "lungs", "Zeitschrift", "Left", "Chile", "Columba", "Donald Sutherland", "New Orleans", "Ethiopia", "Wales", "sternum", "pressure", "Murdoch", "Chicago", "water", "bach", "Squeeze", "The Rolling Stones", "The Sensational Space Shifters", "Jerry Seinfeld", "stern tube", "kia", "lemurs", "Sir Robert Walpole", "eight", "Andorra", "mugs", "williams", "Lone Ranger", "Great Paul", "66", "Formula One", "squash", "Mary Decker", "Godwin Austen", "France", "Birdman of Alcatraz", "Bernardo Bertolucci", "Christopher Columbus", "the buck", "lady Godiva", "Britannia", "boots", "a palla", "1940s", "0.30 in ( 7.6 mm )", "digestive systems", "Neymar", "Parliamentarians (\" Roundheads\") and Royalists (\"Cavaliers\")", "5.3 million", "6-4", "Kenyan", "UNICEF", "The B", "The Voice of Florence Nightingale", "Saturn", "globalization"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6911458333333333}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, false, true, false, false, true, true, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, false, true, false, true, true, true, true, false, false, true, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-5528", "mrqa_triviaqa-validation-990", "mrqa_triviaqa-validation-2018", "mrqa_triviaqa-validation-7026", "mrqa_triviaqa-validation-1733", "mrqa_triviaqa-validation-2708", "mrqa_triviaqa-validation-2378", "mrqa_triviaqa-validation-5898", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-2256", "mrqa_triviaqa-validation-1469", "mrqa_triviaqa-validation-5559", "mrqa_triviaqa-validation-6084", "mrqa_triviaqa-validation-4356", "mrqa_triviaqa-validation-5458", "mrqa_triviaqa-validation-2214", "mrqa_triviaqa-validation-3908", "mrqa_naturalquestions-validation-6832", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2229", "mrqa_searchqa-validation-12145", "mrqa_searchqa-validation-9012"], "SR": 0.640625, "CSR": 0.5136513157894738, "EFR": 1.0, "Overall": 0.6983552631578948}, {"timecode": 95, "before_eval_results": {"predictions": ["American actor, producer, and director", "its air-cushioned sole", "local South Australian and Australian produced content", "Oryzomyini", "Eric Whitacre", "2010", "Statutory List of Buildings of Special Architectural or Historic Interest", "pubs, bars and restaurants", "2004", "Hobart", "Jim Kelly", "Oderturm", "Edward James Olmos", "Girls' Generation", "June 12, 2017", "two or three", "Dra\u017een Petrovi\u0107", "prussia", "David Wells", "the north bank of the North Esk", "two-time", "Argentine cuisine", "13th century", "Pru Goward", "team", "Matthew Abraham \"Matt\" Groening", "Hazel Keech", "Minami-Tori-shima", "1909", "Jesus", "Sulla", "binaural", "Larry Gatlin & the Gatlin Brothers", "left-arm fast bowler", "black nationalism", "simpsons", "Bayern", "Deftones", "\"Gangsta's Paradise\"", "Clitheroe Football Club", "Green Lantern", "The Birds", "The Fault in our Stars", "Liesl", "robert", "twin-faced sheepskin", "White Horse", "The Pogues' last single to chart in the UK Top 50 before frontman Shane MacGowan left the group in 1991,", "Yellow fever", "Elise Marie Stefanik", "Francis Schaeffer", "extends 2,000 kilometres ( 1,200 mi ) down the Australian northeast coast", "between 3.9 and 5.5 Griffith / L ( 70 to 100 mg / dL )", "the heads of federal executive departments who form the Cabinet of the United States", "willnard", "victors of Quebec", "cold Comfort Farm", "red", "thunderstorms", "final moments of the late Libyan strongman's life.", "Guernsey", "Southern Christian Leadership Conference", "Berlin", "brain and spinal cord"], "metric_results": {"EM": 0.421875, "QA-F1": 0.525889574579832}, "metric_results_detailed": {"EM": [false, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, false, false, false, true, false, false, false, false, false, true, true, false, false, false, true, true, false], "QA-F1": [0.0, 0.375, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.5, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.8571428571428572, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-3989", "mrqa_hotpotqa-validation-4357", "mrqa_hotpotqa-validation-2662", "mrqa_hotpotqa-validation-5335", "mrqa_hotpotqa-validation-1668", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-3848", "mrqa_hotpotqa-validation-4283", "mrqa_hotpotqa-validation-3920", "mrqa_hotpotqa-validation-4069", "mrqa_hotpotqa-validation-5050", "mrqa_hotpotqa-validation-2870", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-2926", "mrqa_hotpotqa-validation-2969", "mrqa_hotpotqa-validation-4676", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-2819", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-1949", "mrqa_hotpotqa-validation-2671", "mrqa_hotpotqa-validation-5178", "mrqa_hotpotqa-validation-1250", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-1745", "mrqa_naturalquestions-validation-4710", "mrqa_naturalquestions-validation-9076", "mrqa_naturalquestions-validation-8982", "mrqa_triviaqa-validation-4415", "mrqa_triviaqa-validation-4334", "mrqa_newsqa-validation-2340", "mrqa_newsqa-validation-2382", "mrqa_searchqa-validation-1780", "mrqa_naturalquestions-validation-7342"], "SR": 0.421875, "CSR": 0.5126953125, "EFR": 1.0, "Overall": 0.6981640625}, {"timecode": 96, "before_eval_results": {"predictions": ["maximum speed 160 km / h", "year of the conception or birth of Jesus of Nazareth", "1987", "360", "Pradyumna", "Carol Ann Susi", "the pyloric valve", "Ben Fransham", "Ephesus", "the Gaither Vocal Band", "Cha - Ka -- Phillip Paley", "Germany", "Robert Andrews Millikan", "1830", "positions 14 - 15, 146 - 147 and 148 - 149", "100", "James Madison", "Woodrow Strode", "Baaghi", "Taylor Michel Momsen", "Panning", "16 December 1908", "$66.5 million", "pathology", "April 3, 1973", "keratinization", "her abusive husband", "United Nations", "people who jointly oversee the activities of an organization", "swine - origin influenza virus ( S - OIV )", "take - it - or - leave - it contract", "1612", "The show", "American country music duo Brooks & Dunn", "May 31, 2012", "1,228 km / h ( 763 mph )", "October 27, 2017", "Kida", "~ 55 - 75", "`` G Gower's Diet Beer, '' developed in 1967 by Joseph L. Owades, PhD, highlighting the fact that their beer was called `` Bud Light, '' as `` everything else is just a light", "Oona Castilla Chaplin", "The speech compares the world to a stage and life to a play, and catalogues the seven stages of a man's life, sometimes referred to as the seven ages of man", "Lulu", "the host community", "Steve Russell", "thirteen British colonies that declared independence from the Kingdom of Great Britain, and became the first states in the U.S. Nicknames for the flag", "Profit maximization", "Melbourne", "early morning of April 1", "city of San Antonio", "801,200", "Michael Phelps", "royal oak", "Krankies", "France", "Province of Syracuse", "1986", "3-2", "200", "Republican", "reshit", "Deere", "gusts", "curfew"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6750546618843377}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, true, true, true, false, true, false, false, true, false, false, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.8235294117647058, 0.0, 0.2608695652173913, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.06451612903225806, 1.0, 0.0, 1.0, 0.9189189189189189, 1.0, 1.0, 0.5, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1382", "mrqa_naturalquestions-validation-10550", "mrqa_naturalquestions-validation-7463", "mrqa_naturalquestions-validation-8662", "mrqa_naturalquestions-validation-8638", "mrqa_naturalquestions-validation-3623", "mrqa_naturalquestions-validation-1186", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3112", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-3494", "mrqa_naturalquestions-validation-5586", "mrqa_naturalquestions-validation-3970", "mrqa_naturalquestions-validation-2201", "mrqa_naturalquestions-validation-8962", "mrqa_naturalquestions-validation-5739", "mrqa_naturalquestions-validation-2844", "mrqa_naturalquestions-validation-3243", "mrqa_naturalquestions-validation-3918", "mrqa_naturalquestions-validation-199", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-4953", "mrqa_hotpotqa-validation-3107", "mrqa_hotpotqa-validation-848", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-2327", "mrqa_searchqa-validation-16252", "mrqa_searchqa-validation-12334"], "SR": 0.5625, "CSR": 0.513208762886598, "EFR": 0.9285714285714286, "Overall": 0.6839810382916054}, {"timecode": 97, "before_eval_results": {"predictions": ["Tchaikovsky", "dark places", "Peter Paul", "the boll weevil", "right-clicks", "Google Groups", "The Sundance Kid", "Buddhism", "Mozart", "Jonathan Swift", "Tiger lily", "ice cream", "Algeria", "Edgar Allan Poe", "(Sergey) Brin", "Sanders", "The Smashing Pumpkins", "bread", "Yale", "Napoleon", "Paris", "the Black Forest", "you created the fresco walls in the Stanza della Segnatura,", "an ant", "Birkenstock", "Firebird", "Hafnium", "flax plant", "the Muse", "the Wachowski brothers", "Rumpole of the Bailey", "John Quincy Adams", "Steve Austin", "Kurt Warner", "size zero", "a small retail store", "Belle, Gaston & Mrs. Potts", "Ratatouille", "pro bono", "a bear", "The Office", "The Oprah Show", "Bigfoot", "Jackson Pollock", "glow", "Mona Lisa", "Vietnamese", "Crayola", "The Man in the Gray Flannel Suit", "Assimilation", "orange", "Isaiah Amir Mustafa", "14 November 2001", "Americans acting under orders", "mike hammer", "The Crow", "Hartley", "Tumzabt", "the European Champion Clubs' Cup", "second largest", "North Korea,", "Al alcohol", "It wasn't appreciated how much of an impact [IBS] can have on a patient's quality of life,\"", "AMC"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6260416666666666}, "metric_results_detailed": {"EM": [false, true, false, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, true, true, true, true, false, false, false, true, true, false, false, true, false, false, true, true, false, false, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, false], "QA-F1": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.06666666666666667, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-6269", "mrqa_searchqa-validation-14299", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-4631", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-10747", "mrqa_searchqa-validation-12166", "mrqa_searchqa-validation-8538", "mrqa_searchqa-validation-1971", "mrqa_searchqa-validation-10545", "mrqa_searchqa-validation-9435", "mrqa_searchqa-validation-684", "mrqa_searchqa-validation-8764", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-942", "mrqa_searchqa-validation-4705", "mrqa_searchqa-validation-7743", "mrqa_searchqa-validation-8775", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-11006", "mrqa_searchqa-validation-13416", "mrqa_searchqa-validation-16144", "mrqa_searchqa-validation-4924", "mrqa_triviaqa-validation-6442", "mrqa_hotpotqa-validation-3553", "mrqa_newsqa-validation-2406", "mrqa_newsqa-validation-1760", "mrqa_newsqa-validation-96", "mrqa_hotpotqa-validation-2138"], "SR": 0.546875, "CSR": 0.5135522959183674, "EFR": 1.0, "Overall": 0.6983354591836735}, {"timecode": 98, "before_eval_results": {"predictions": ["Florence", "Pierre Trudeau", "Redblush", "a chargeback", "Millard", "the cornea", "The Go", "Rumpole", "the croissant", "the light bulb", "Spider-Man", "Atlanta", "Chile", "Dick Tracy", "Queen Latifah", "Van Allen", "beer", "Zen", "El", "Zenith", "baboon", "wine", "The Sopranos", "The Q- tip", "natural selection", "Massachusetts", "the densely wooded Ardennes region of Belgium", "Shaft", "The Merry", "the Two Sicilies", "Victory", "a constitutional innovation", "Golden Hind", "the Final Jeopardy answer for Winter", "Einstein", "The Devil", "the pituitary", "Alfred Hitchcock", "Mays", "reconnaissance", "Florida", "Ectoplasm", "Thomas Jefferson", "Mercury", "Dante", "Columbus", "(Joseph) Haydn", "Meringue", "Babe Zaharias", "the FBI", "calcium", "four", "Charles Lyell", "961", "The Netherlands", "a wish", "Mary Jane Grant", "Orchard Central", "Fort Hood, Texas", "Andr\u00e9 3000", "the iPods", "suspend all", "The two-hour finale.", "Charles Frederickson"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5618055555555556}, "metric_results_detailed": {"EM": [true, true, false, false, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, true, true, false, false, false, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, true, false, false], "QA-F1": [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-13080", "mrqa_searchqa-validation-2078", "mrqa_searchqa-validation-3729", "mrqa_searchqa-validation-718", "mrqa_searchqa-validation-3054", "mrqa_searchqa-validation-14517", "mrqa_searchqa-validation-16225", "mrqa_searchqa-validation-11920", "mrqa_searchqa-validation-319", "mrqa_searchqa-validation-12158", "mrqa_searchqa-validation-12700", "mrqa_searchqa-validation-8929", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-4192", "mrqa_searchqa-validation-11670", "mrqa_searchqa-validation-15328", "mrqa_searchqa-validation-16265", "mrqa_searchqa-validation-6181", "mrqa_searchqa-validation-6625", "mrqa_searchqa-validation-1366", "mrqa_searchqa-validation-6444", "mrqa_searchqa-validation-5926", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1848", "mrqa_searchqa-validation-221", "mrqa_searchqa-validation-9945", "mrqa_searchqa-validation-5756", "mrqa_searchqa-validation-6821", "mrqa_naturalquestions-validation-307", "mrqa_triviaqa-validation-7667", "mrqa_triviaqa-validation-1611", "mrqa_hotpotqa-validation-2679", "mrqa_newsqa-validation-2040", "mrqa_naturalquestions-validation-6711"], "SR": 0.46875, "CSR": 0.5130997474747474, "EFR": 1.0, "Overall": 0.6982449494949494}, {"timecode": 99, "UKR": 0.615234375, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1039", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-1099", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1247", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1292", "mrqa_hotpotqa-validation-13", "mrqa_hotpotqa-validation-1301", "mrqa_hotpotqa-validation-1473", "mrqa_hotpotqa-validation-1490", "mrqa_hotpotqa-validation-150", "mrqa_hotpotqa-validation-1540", "mrqa_hotpotqa-validation-1544", "mrqa_hotpotqa-validation-1630", "mrqa_hotpotqa-validation-1641", "mrqa_hotpotqa-validation-1653", "mrqa_hotpotqa-validation-1691", "mrqa_hotpotqa-validation-1736", "mrqa_hotpotqa-validation-181", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-189", "mrqa_hotpotqa-validation-1986", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2111", "mrqa_hotpotqa-validation-2113", "mrqa_hotpotqa-validation-2126", "mrqa_hotpotqa-validation-2212", "mrqa_hotpotqa-validation-2241", "mrqa_hotpotqa-validation-228", "mrqa_hotpotqa-validation-2324", "mrqa_hotpotqa-validation-2342", "mrqa_hotpotqa-validation-2378", "mrqa_hotpotqa-validation-2449", "mrqa_hotpotqa-validation-2469", "mrqa_hotpotqa-validation-2478", "mrqa_hotpotqa-validation-2496", "mrqa_hotpotqa-validation-2519", "mrqa_hotpotqa-validation-2590", "mrqa_hotpotqa-validation-2604", "mrqa_hotpotqa-validation-2873", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2903", "mrqa_hotpotqa-validation-2944", "mrqa_hotpotqa-validation-2952", "mrqa_hotpotqa-validation-2984", "mrqa_hotpotqa-validation-3114", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3245", "mrqa_hotpotqa-validation-3323", "mrqa_hotpotqa-validation-3334", "mrqa_hotpotqa-validation-3364", "mrqa_hotpotqa-validation-3374", "mrqa_hotpotqa-validation-3428", "mrqa_hotpotqa-validation-3515", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-3793", "mrqa_hotpotqa-validation-381", "mrqa_hotpotqa-validation-393", "mrqa_hotpotqa-validation-3934", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-4038", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4123", "mrqa_hotpotqa-validation-4236", "mrqa_hotpotqa-validation-4249", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-4277", "mrqa_hotpotqa-validation-4312", "mrqa_hotpotqa-validation-4356", "mrqa_hotpotqa-validation-4391", "mrqa_hotpotqa-validation-4401", "mrqa_hotpotqa-validation-4484", "mrqa_hotpotqa-validation-450", "mrqa_hotpotqa-validation-4500", "mrqa_hotpotqa-validation-4612", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4658", "mrqa_hotpotqa-validation-4708", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4766", "mrqa_hotpotqa-validation-4818", "mrqa_hotpotqa-validation-4841", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-4897", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-494", "mrqa_hotpotqa-validation-5172", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-5323", "mrqa_hotpotqa-validation-5358", "mrqa_hotpotqa-validation-5370", "mrqa_hotpotqa-validation-542", "mrqa_hotpotqa-validation-5427", "mrqa_hotpotqa-validation-5523", "mrqa_hotpotqa-validation-5531", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-5666", "mrqa_hotpotqa-validation-5719", "mrqa_hotpotqa-validation-5772", "mrqa_hotpotqa-validation-5835", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-5866", "mrqa_hotpotqa-validation-66", "mrqa_hotpotqa-validation-727", "mrqa_hotpotqa-validation-76", "mrqa_hotpotqa-validation-884", "mrqa_hotpotqa-validation-916", "mrqa_hotpotqa-validation-975", "mrqa_naturalquestions-validation-10114", "mrqa_naturalquestions-validation-10159", "mrqa_naturalquestions-validation-10417", "mrqa_naturalquestions-validation-10493", "mrqa_naturalquestions-validation-10554", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1797", "mrqa_naturalquestions-validation-1824", "mrqa_naturalquestions-validation-1846", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1920", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-214", "mrqa_naturalquestions-validation-2379", "mrqa_naturalquestions-validation-2515", "mrqa_naturalquestions-validation-2544", "mrqa_naturalquestions-validation-2620", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-289", "mrqa_naturalquestions-validation-2908", "mrqa_naturalquestions-validation-2934", "mrqa_naturalquestions-validation-2971", "mrqa_naturalquestions-validation-3186", "mrqa_naturalquestions-validation-3205", "mrqa_naturalquestions-validation-3425", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3589", "mrqa_naturalquestions-validation-3627", "mrqa_naturalquestions-validation-3692", "mrqa_naturalquestions-validation-373", "mrqa_naturalquestions-validation-3783", "mrqa_naturalquestions-validation-3942", "mrqa_naturalquestions-validation-3958", "mrqa_naturalquestions-validation-4073", "mrqa_naturalquestions-validation-4156", "mrqa_naturalquestions-validation-4307", "mrqa_naturalquestions-validation-4319", "mrqa_naturalquestions-validation-4341", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4435", "mrqa_naturalquestions-validation-4471", "mrqa_naturalquestions-validation-4563", "mrqa_naturalquestions-validation-4740", "mrqa_naturalquestions-validation-4976", "mrqa_naturalquestions-validation-5053", "mrqa_naturalquestions-validation-5072", "mrqa_naturalquestions-validation-5199", "mrqa_naturalquestions-validation-5331", "mrqa_naturalquestions-validation-5366", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5425", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5503", "mrqa_naturalquestions-validation-5509", "mrqa_naturalquestions-validation-5516", "mrqa_naturalquestions-validation-5722", "mrqa_naturalquestions-validation-5798", "mrqa_naturalquestions-validation-5808", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-6035", "mrqa_naturalquestions-validation-6069", "mrqa_naturalquestions-validation-6094", "mrqa_naturalquestions-validation-6216", "mrqa_naturalquestions-validation-6353", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-6940", "mrqa_naturalquestions-validation-7097", "mrqa_naturalquestions-validation-7225", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-749", "mrqa_naturalquestions-validation-757", "mrqa_naturalquestions-validation-7595", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7767", "mrqa_naturalquestions-validation-7806", "mrqa_naturalquestions-validation-7849", "mrqa_naturalquestions-validation-7892", "mrqa_naturalquestions-validation-8633", "mrqa_naturalquestions-validation-8659", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9130", "mrqa_naturalquestions-validation-9551", "mrqa_naturalquestions-validation-9608", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9871", "mrqa_naturalquestions-validation-9878", "mrqa_newsqa-validation-1013", "mrqa_newsqa-validation-1105", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1129", "mrqa_newsqa-validation-1183", "mrqa_newsqa-validation-1301", "mrqa_newsqa-validation-1351", "mrqa_newsqa-validation-1365", "mrqa_newsqa-validation-1406", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1714", "mrqa_newsqa-validation-175", "mrqa_newsqa-validation-1792", "mrqa_newsqa-validation-183", "mrqa_newsqa-validation-1854", "mrqa_newsqa-validation-1996", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2022", "mrqa_newsqa-validation-203", "mrqa_newsqa-validation-2030", "mrqa_newsqa-validation-2307", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2558", "mrqa_newsqa-validation-2856", "mrqa_newsqa-validation-2861", "mrqa_newsqa-validation-2884", "mrqa_newsqa-validation-2899", "mrqa_newsqa-validation-2992", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3054", "mrqa_newsqa-validation-3091", "mrqa_newsqa-validation-310", "mrqa_newsqa-validation-3179", "mrqa_newsqa-validation-3474", "mrqa_newsqa-validation-3520", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-3596", "mrqa_newsqa-validation-3639", "mrqa_newsqa-validation-3715", "mrqa_newsqa-validation-3780", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-3992", "mrqa_newsqa-validation-406", "mrqa_newsqa-validation-4060", "mrqa_newsqa-validation-454", "mrqa_newsqa-validation-502", "mrqa_newsqa-validation-54", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-61", "mrqa_newsqa-validation-698", "mrqa_newsqa-validation-855", "mrqa_newsqa-validation-895", "mrqa_searchqa-validation-10167", "mrqa_searchqa-validation-10330", "mrqa_searchqa-validation-10597", "mrqa_searchqa-validation-10616", "mrqa_searchqa-validation-10754", "mrqa_searchqa-validation-11247", "mrqa_searchqa-validation-11294", "mrqa_searchqa-validation-11861", "mrqa_searchqa-validation-11898", "mrqa_searchqa-validation-1196", "mrqa_searchqa-validation-12085", "mrqa_searchqa-validation-12127", "mrqa_searchqa-validation-12151", "mrqa_searchqa-validation-12185", "mrqa_searchqa-validation-12360", "mrqa_searchqa-validation-12402", "mrqa_searchqa-validation-12782", "mrqa_searchqa-validation-1290", "mrqa_searchqa-validation-12976", "mrqa_searchqa-validation-13014", "mrqa_searchqa-validation-13765", "mrqa_searchqa-validation-13789", "mrqa_searchqa-validation-13803", "mrqa_searchqa-validation-14285", "mrqa_searchqa-validation-14307", "mrqa_searchqa-validation-14387", "mrqa_searchqa-validation-14471", "mrqa_searchqa-validation-1497", "mrqa_searchqa-validation-15064", "mrqa_searchqa-validation-152", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-15725", "mrqa_searchqa-validation-16016", "mrqa_searchqa-validation-16162", "mrqa_searchqa-validation-16311", "mrqa_searchqa-validation-16354", "mrqa_searchqa-validation-16865", "mrqa_searchqa-validation-16895", "mrqa_searchqa-validation-16910", "mrqa_searchqa-validation-1829", "mrqa_searchqa-validation-191", "mrqa_searchqa-validation-1950", "mrqa_searchqa-validation-2189", "mrqa_searchqa-validation-2204", "mrqa_searchqa-validation-2388", "mrqa_searchqa-validation-2591", "mrqa_searchqa-validation-2673", "mrqa_searchqa-validation-273", "mrqa_searchqa-validation-2898", "mrqa_searchqa-validation-2943", "mrqa_searchqa-validation-3081", "mrqa_searchqa-validation-324", "mrqa_searchqa-validation-327", "mrqa_searchqa-validation-3303", "mrqa_searchqa-validation-3381", "mrqa_searchqa-validation-3405", "mrqa_searchqa-validation-3455", "mrqa_searchqa-validation-3565", "mrqa_searchqa-validation-3618", "mrqa_searchqa-validation-3838", "mrqa_searchqa-validation-419", "mrqa_searchqa-validation-4320", "mrqa_searchqa-validation-4380", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4609", "mrqa_searchqa-validation-4702", "mrqa_searchqa-validation-4878", "mrqa_searchqa-validation-5060", "mrqa_searchqa-validation-550", "mrqa_searchqa-validation-5729", "mrqa_searchqa-validation-5971", "mrqa_searchqa-validation-6122", "mrqa_searchqa-validation-6127", "mrqa_searchqa-validation-6137", "mrqa_searchqa-validation-6697", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-6829", "mrqa_searchqa-validation-6948", "mrqa_searchqa-validation-714", "mrqa_searchqa-validation-7162", "mrqa_searchqa-validation-7186", "mrqa_searchqa-validation-7322", "mrqa_searchqa-validation-7521", "mrqa_searchqa-validation-7741", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-7786", "mrqa_searchqa-validation-784", "mrqa_searchqa-validation-7880", "mrqa_searchqa-validation-7913", "mrqa_searchqa-validation-7932", "mrqa_searchqa-validation-8166", "mrqa_searchqa-validation-8331", "mrqa_searchqa-validation-8481", "mrqa_searchqa-validation-8648", "mrqa_searchqa-validation-8691", "mrqa_searchqa-validation-8941", "mrqa_searchqa-validation-8951", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-9284", "mrqa_searchqa-validation-9348", "mrqa_searchqa-validation-9438", "mrqa_searchqa-validation-9679", "mrqa_squad-validation-10067", "mrqa_squad-validation-1023", "mrqa_squad-validation-10483", "mrqa_squad-validation-1071", "mrqa_squad-validation-1215", "mrqa_squad-validation-1251", "mrqa_squad-validation-1312", "mrqa_squad-validation-1856", "mrqa_squad-validation-2098", "mrqa_squad-validation-2434", "mrqa_squad-validation-2458", "mrqa_squad-validation-2888", "mrqa_squad-validation-3202", "mrqa_squad-validation-343", "mrqa_squad-validation-3551", "mrqa_squad-validation-356", "mrqa_squad-validation-3823", "mrqa_squad-validation-4110", "mrqa_squad-validation-5112", "mrqa_squad-validation-512", "mrqa_squad-validation-5590", "mrqa_squad-validation-5874", "mrqa_squad-validation-60", "mrqa_squad-validation-6255", "mrqa_squad-validation-6316", "mrqa_squad-validation-6324", "mrqa_squad-validation-6373", "mrqa_squad-validation-6393", "mrqa_squad-validation-6539", "mrqa_squad-validation-6657", "mrqa_squad-validation-6690", "mrqa_squad-validation-687", "mrqa_squad-validation-7068", "mrqa_squad-validation-7144", "mrqa_squad-validation-7209", "mrqa_squad-validation-7937", "mrqa_squad-validation-805", "mrqa_squad-validation-8747", "mrqa_squad-validation-8761", "mrqa_squad-validation-8807", "mrqa_squad-validation-8881", "mrqa_squad-validation-9154", "mrqa_squad-validation-9578", "mrqa_squad-validation-9761", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-1030", "mrqa_triviaqa-validation-1076", "mrqa_triviaqa-validation-1101", "mrqa_triviaqa-validation-1123", "mrqa_triviaqa-validation-1157", "mrqa_triviaqa-validation-1335", "mrqa_triviaqa-validation-1355", "mrqa_triviaqa-validation-1441", "mrqa_triviaqa-validation-1467", "mrqa_triviaqa-validation-1657", "mrqa_triviaqa-validation-170", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-1835", "mrqa_triviaqa-validation-1907", "mrqa_triviaqa-validation-1938", "mrqa_triviaqa-validation-1948", "mrqa_triviaqa-validation-1965", "mrqa_triviaqa-validation-2168", "mrqa_triviaqa-validation-2239", "mrqa_triviaqa-validation-2518", "mrqa_triviaqa-validation-2653", "mrqa_triviaqa-validation-2729", "mrqa_triviaqa-validation-2798", "mrqa_triviaqa-validation-2811", "mrqa_triviaqa-validation-287", "mrqa_triviaqa-validation-2911", "mrqa_triviaqa-validation-3025", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3036", "mrqa_triviaqa-validation-3064", "mrqa_triviaqa-validation-3093", "mrqa_triviaqa-validation-3101", "mrqa_triviaqa-validation-3104", "mrqa_triviaqa-validation-3152", "mrqa_triviaqa-validation-3201", "mrqa_triviaqa-validation-3263", "mrqa_triviaqa-validation-3288", "mrqa_triviaqa-validation-3300", "mrqa_triviaqa-validation-3314", "mrqa_triviaqa-validation-344", "mrqa_triviaqa-validation-3483", "mrqa_triviaqa-validation-3625", "mrqa_triviaqa-validation-3631", "mrqa_triviaqa-validation-3708", "mrqa_triviaqa-validation-3865", "mrqa_triviaqa-validation-393", "mrqa_triviaqa-validation-3948", "mrqa_triviaqa-validation-401", "mrqa_triviaqa-validation-4086", "mrqa_triviaqa-validation-4110", "mrqa_triviaqa-validation-4146", "mrqa_triviaqa-validation-4167", "mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-4436", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4450", "mrqa_triviaqa-validation-4512", "mrqa_triviaqa-validation-4549", "mrqa_triviaqa-validation-4605", "mrqa_triviaqa-validation-4614", "mrqa_triviaqa-validation-4625", "mrqa_triviaqa-validation-4710", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-4814", "mrqa_triviaqa-validation-4862", "mrqa_triviaqa-validation-4871", "mrqa_triviaqa-validation-5079", "mrqa_triviaqa-validation-5101", "mrqa_triviaqa-validation-5265", "mrqa_triviaqa-validation-5306", "mrqa_triviaqa-validation-5415", "mrqa_triviaqa-validation-543", "mrqa_triviaqa-validation-5469", "mrqa_triviaqa-validation-5507", "mrqa_triviaqa-validation-5702", "mrqa_triviaqa-validation-5715", "mrqa_triviaqa-validation-5787", "mrqa_triviaqa-validation-5818", "mrqa_triviaqa-validation-5923", "mrqa_triviaqa-validation-5950", "mrqa_triviaqa-validation-609", "mrqa_triviaqa-validation-6136", "mrqa_triviaqa-validation-6289", "mrqa_triviaqa-validation-6346", "mrqa_triviaqa-validation-6352", "mrqa_triviaqa-validation-6353", "mrqa_triviaqa-validation-6364", "mrqa_triviaqa-validation-6504", "mrqa_triviaqa-validation-6599", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-6654", "mrqa_triviaqa-validation-6702", "mrqa_triviaqa-validation-6788", "mrqa_triviaqa-validation-6864", "mrqa_triviaqa-validation-6872", "mrqa_triviaqa-validation-703", "mrqa_triviaqa-validation-7039", "mrqa_triviaqa-validation-7062", "mrqa_triviaqa-validation-7079", "mrqa_triviaqa-validation-7083", "mrqa_triviaqa-validation-7153", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-7319", "mrqa_triviaqa-validation-749", "mrqa_triviaqa-validation-7506", "mrqa_triviaqa-validation-7557", "mrqa_triviaqa-validation-7581", "mrqa_triviaqa-validation-915", "mrqa_triviaqa-validation-938", "mrqa_triviaqa-validation-980"], "OKR": 0.794921875, "KG": 0.465625, "before_eval_results": {"predictions": ["J.M. Barrie's The Admirable Crichton", "Andrea Brooks", "July 14, 2017", "2018", "Neuropsychology", "Hem Chandra Bose", "potential of hydrogen", "Peking", "Bart Howard", "2013", "Ozzie Smith", "the largest Greek island in the Saronic Gulf, about 1 nautical mile ( 2 km ) off - coast from Piraeus and about 16 kilometres ( 10 miles ) west of Athens", "George Harrison", "Persian", "Sarah Silverman", "Sophia Akuffo", "January 17, 1899", "IIII", "2014", "Natural - language processing ( NLP )", "six", "a request line", "$2 million", "three", "Sohrai", "The Monsoons from the south atlantic ocean arrives in central Nigeria in July bringing with it high humidity, heavy cloud cover and heavy rainfall", "the celebrity alumna Cecil Lockhart", "Michael Madhusudan Dutta", "257,083", "March 23, 2018", "starting quarterback", "public sector ( also called the state sector )", "Carpenter", "2018", "1992", "Dan Stevens", "`` Killer Within ''", "Jethalal Gada", "Nickelback", "1999", "King Willem - Alexander", "is a song recorded, written, and produced by American musician Lenny Kravitz for his second studio album, Mama Said ( 1991 )", "the ark of the covenant", "a circular movement of an object around a center ( or point ) of rotation", "Ren\u00e9 Georges Hermann", "H.L. Hunley", "bird nests created by edible - nest swiftlets using solidified saliva", "John Bull", "1973", "1996", "Fawcett", "Charlie Chaplin", "Francis Matthews", "Hymenaeus", "33rd most visited tourist attraction in the world", "1776", "Stapleton Cotton", "London transit bombings", "eight-day journey", "102 new jobs for a minimum of nine weeks.", "Spain", "Elijah", "Geneva", "27-year-old's"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6894427750145835}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, true, false, true, true, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, false, false, false, false, true, false, true, false, false, true, true, false, false, true, false, false, false, false, true, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9361702127659575, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.42857142857142855, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.14285714285714288, 0.8571428571428571, 0.4, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3384", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-6117", "mrqa_naturalquestions-validation-819", "mrqa_naturalquestions-validation-1195", "mrqa_naturalquestions-validation-2953", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-2212", "mrqa_naturalquestions-validation-1471", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-10367", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-10310", "mrqa_naturalquestions-validation-4192", "mrqa_naturalquestions-validation-5025", "mrqa_naturalquestions-validation-10618", "mrqa_naturalquestions-validation-4844", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-3692", "mrqa_triviaqa-validation-2334", "mrqa_hotpotqa-validation-849", "mrqa_hotpotqa-validation-111", "mrqa_newsqa-validation-894", "mrqa_newsqa-validation-2494", "mrqa_newsqa-validation-2803", "mrqa_searchqa-validation-3524"], "SR": 0.578125, "CSR": 0.5137499999999999, "EFR": 1.0, "Overall": 0.67790625}]}