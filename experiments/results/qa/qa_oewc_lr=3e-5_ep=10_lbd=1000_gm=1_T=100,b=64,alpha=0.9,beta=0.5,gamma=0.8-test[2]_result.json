{"method_class": "online_ewc", "base_model_args": "Namespace(base_model_path='out/mrqa_squad_bart-base_1029_upstream_model//best-model.pt', model_type='facebook/bart-base')", "debugger_args": "Namespace(adam_epsilon=1e-08, ckpt_dir='experiments/ckpt_dirs/qa/owc/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]', diff_loss_weight=0, ewc_gamma=1.0, ewc_lambda=1000.0, gradient_accumulation_steps=1, kg_eval_freq=10, kg_eval_mode='metric', kr_eval_freq=10, kr_eval_mode='metric', learning_rate=3e-05, max_grad_norm=0.1, num_epochs=10.0, okr_sample_seed=1337, okr_sample_size=512, save_ckpt_freq=10, skip_instant_eval=False, total_steps=10000, warmup_steps=0, weight_decay=0.01)", "data_args": "Namespace(accumulate_eval_freq=-1, append_another_bos=1, do_lowercase=False, heldout_submission_data='experiments/eval_data/qa/heldout_eval.jsonl', max_input_length=888, max_output_length=50, max_timecode=100, num_beams=3, predict_batch_size=48, result_file='experiments/results/qa/qa_oewc_lr=3e-5_ep=10_lbd=1000_gm=1_T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test[2]_result.json', stream_id=2, submission_stream_data='experiments/eval_data/qa/submission_stream.T=100,b=64,alpha=0.9,beta=0.5,gamma=0.8-test.json', task_name='mrqa', train_batch_size=8, upstream_data_path='data/mrqa_squad/mrqa_squad_train.jsonl', upstream_eval_data='experiments/eval_data/qa/upstream_eval.jsonl')", "model_update_steps": 4040, "online_eval_results": [{"timecode": 0, "UKR": 0.802734375, "KG": 0.3125, "before_eval_results": {"predictions": ["Jelme and Bo'orchu", "gauge bosons", "consumer prices", "Albert C. Outler", "a computational problem", "1521", "River Tyne", "Boston", "San Jose Marriott", "illegal boycotts", "Mitochondria", "bilaterians", "Alexandre Yersin", "Methodists today", "Beyonc\u00e9", "the Rhine and its downstream extension", "7\u20134\u20132\u20133", "Horniman Museum", "400 AD to 1914", "early 1526", "The individual is the final judge of right and wrong", "five", "Battle of B\u1ea1ch \u0110\u1eb1ng", "Time Lady", "oxygen-16", "The Day of the Doctor", "Sierra Sky Park", "James Clerk Maxwell", "Bill Clinton", "in areas its forces occupied in Eastern Europe", "20,000", "Queen Elizabeth II", "The Daleks", "gas turbines", "Newton", "Miasma theory", "Ealy", "several medals", "remaining in black and white", "computability theory", "autoimmune", "American Sweetgum", "Pleistocene epoch", "Feynman diagrams", "orange", "oxygen compounds", "four", "Fort Caroline", "counties or powiats", "chemical bonds", "2015", "France's claim to the region was superior to that of the British", "double", "helps many proteins bind the polypeptide", "Islamism", "lines or a punishment essay", "mercuric oxide", "released Islamists from prison and welcomed home exiles", "Washington and Thomas Gage", "The individual", "Bruno Mars", "the public", "Thomas Edison and George Westinghouse", "a freshwater lake"], "metric_results": {"EM": 0.796875, "QA-F1": 0.8040364583333334}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, false, true, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.125, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6848", "mrqa_squad-validation-9923", "mrqa_squad-validation-100", "mrqa_squad-validation-9434", "mrqa_squad-validation-6966", "mrqa_squad-validation-7725", "mrqa_squad-validation-8538", "mrqa_squad-validation-1703", "mrqa_squad-validation-3511", "mrqa_squad-validation-3597", "mrqa_squad-validation-9567", "mrqa_squad-validation-6973", "mrqa_squad-validation-2025"], "SR": 0.796875, "CSR": 0.796875, "EFR": 1.0, "Overall": 0.8984375}, {"timecode": 1, "before_eval_results": {"predictions": ["the General Sejm", "232", "New Holland", "the \"Rhine knee\"", "the U.S. South", "the Schmalkaldic League", "January 1985", "an Executive Committee", "King Sancho VI of Navarre", "the Arizona Cardinals", "36", "Chloroplasts", "Sydney", "the Panic of 1901", "Muslim medicine", "the Silk Road", "silicon dioxide", "statocyst", "Several thousand", "the Fourth Intercolonial War and the Great War for the Empire", "medieval", "30\u201360% of Europe's total population", "the laws of physics", "the Ten Commandments", "the San Fernando Valley", "Roger NFL", "Hugh L. Dryden", "metals", "1.5 gigatons", "Denver Broncos", "\u00a31 of capital", "the 2010 series", "megaprojects", "1024-bit primes", "the portrait of Fran\u00e7ois, Duc d'Alen\u00e7on by Fran\u00e7ois Clouet, Gaspard Dughet", "Africa", "the Electorate of Saxony", "ice-sheets", "the lion, leopard, buffalo, rhinoceros, and elephant", "A plea of no contest is sometimes regarded as a compromise between the two", "seven", "Demaryius Thomas", "Napoleon's", "the Santa Clara Marriott", "kinematic measurements", "shipping", "12th", "helical thylakoid model", "the A69", "14%", "Thomas Edison", "Toshiba", "detention", "antigen presentation", "hunter's garb", "The Harvard Crimson competes in 42 intercollegiate sports in the NCAA Division I Ivy League", "British Gas plc", "Demaryius Thomas", "Homebrewing", "Joe Scarborough", "became a politician", "Gareth Jones", "Sivakumar, S. V. Subbaiah, Jayachitra, Srividya, Shubha, Kamal Haasan and Jayasudha", "Teresa Hairston"], "metric_results": {"EM": 0.75, "QA-F1": 0.7791752518315018}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, false, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.25000000000000006, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14285714285714288, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.3076923076923077, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-9196", "mrqa_squad-validation-1117", "mrqa_squad-validation-3664", "mrqa_squad-validation-85", "mrqa_squad-validation-4482", "mrqa_squad-validation-8978", "mrqa_squad-validation-5490", "mrqa_squad-validation-8446", "mrqa_squad-validation-8278", "mrqa_squad-validation-6914", "mrqa_squad-validation-7155", "mrqa_hotpotqa-validation-4002", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_newsqa-validation-2275"], "SR": 0.75, "CSR": 0.7734375, "EFR": 1.0, "Overall": 0.88671875}, {"timecode": 2, "before_eval_results": {"predictions": ["Ugali with vegetables, sour milk, meat, fish or any other stew", "TFEU article 294", "over $20 billion", "was a major source of water pollution", "unity of God", "all war", "1000 and 1900", "Gamal Abdul Nasser", "viniculture and tourism", "minor", "1162", "was lost in the 5th Avenue laboratory fire of March 1895", "ABC Cable News", "22 May 2006", "Germany and Austria", "Golden Gate Bridge", "the Welsh", "acquiring nutrients", "Muslims in the semu class", "the Chinese", "temperature and light", "12 May 1999", "1852", "the development of safety lamps", "stabilize the rest of the chloroplast genome", "The Mongols' extensive West Asian and European contacts", "24%", "Milton Friedman Institute", "Donald Davies", "three", "student motivation and attitudes towards school", "1560", "1891", "Lutheran views", "electron", "fear of their lives", "Science", "John Pell, Lord of Pelham Manor", "Cam Newton", "Osama bin Laden", "international drug suppliers", "President", "expelled Jews", "arid and semi-arid areas with near-desert landscapes", "Yosemite Freeway", "Annan and his UN-backed panel and African Union chairman Jakaya", "The Warsaw Stock Exchange", "it becomes an Act of the Scottish Parliament", "certification by a recognized body", "a chain or screw stoking mechanism", "Battle of B\u1ea1ch \u0110\u1eb1ng (1288)", "silver", "1947", "1860", "The club will participate in the Premier League, FA Cup, EFL Cup (as holders), UEFA Champions League and UEFA Super Cup.", "Detroit, Michigan", "the Emancipation Proclamation", "26,000", "Pakistan A", "Ricky Skaggs", "Saturday Night Live", "last living pilot of the X-15 program", "Two new U.S. representatives are teaming up with CNN.com to report their \"Freshman Year\" experience through videos and commentaries", "250,000"], "metric_results": {"EM": 0.765625, "QA-F1": 0.8239718614718615}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, false, true, true, false, true, true, true, false, true, true, false, true, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 0.9523809523809523, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.4, 1.0, 0.1818181818181818, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-9343", "mrqa_squad-validation-9093", "mrqa_squad-validation-9388", "mrqa_squad-validation-5157", "mrqa_squad-validation-8399", "mrqa_squad-validation-4562", "mrqa_squad-validation-8383", "mrqa_squad-validation-9499", "mrqa_squad-validation-8222", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-4018", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-368"], "SR": 0.765625, "CSR": 0.7708333333333334, "EFR": 1.0, "Overall": 0.8854166666666667}, {"timecode": 3, "before_eval_results": {"predictions": ["10 February 1763", "good, clear laws, fairly and democratically", "shaping ideas about the free market", "SAP Center in San Jose", "older", "university and military academy", "Foreign Protestants Naturalization Act", "inequality", "jigg TV", "three", "permafrost", "Silas B. Cobb", "the traditional salute of a knight winning a bout", "Jane Kim", "the Presiding Officer", "lipophilic alkaloid toxins", "one", "William Rainey Harper", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "June", "Huguenot", "US President Barack Obama chose not to visit the country", "Ralph Woodward", "Susan Foreman", "clinical pharmacists", "teleforce", "British failures in North America, combined with other failures in the European theater", "300", "66 million years ago", "Hong Kong", "in commerce, schooling and government", "Krak\u00f3w", "France", "three", "power outage", "easier and more efficient than anywhere else", "Muslim and Chinese", "free trade", "15,100", "Cuba", "high pressure shock waves", "28,000", "21 to 11", "Cam Newton", "128,843", "Van Gend en Loos v Nederlandse Administratie der Belastingen", "four years", "Howard Keel", "Pangaea (or Pangea)", "Mel Gibson", "George Washington", "John Uhler Lemmon III", "six", "ccoli", "Charles, Eric Clapton, Bob Dylan and Johnny Cash", "Henry Kelly", "hedgehogs", "George IV", "The Time Machine", "the Granite City", "the natural world and mysticism", "more funds", "Brad Blauser", "$1.45 billion"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7401697261072261}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, true, true, true, false, true, true, true, true, true, false, false, true, true, true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, false, false, false, true, false, false, true, true, false, false, false, true], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.07692307692307691, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.4, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-1831", "mrqa_squad-validation-4971", "mrqa_squad-validation-5975", "mrqa_squad-validation-6223", "mrqa_squad-validation-9570", "mrqa_squad-validation-3044", "mrqa_squad-validation-8326", "mrqa_squad-validation-1830", "mrqa_squad-validation-4065", "mrqa_squad-validation-973", "mrqa_triviaqa-validation-3713", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-2135", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-1148"], "SR": 0.6875, "CSR": 0.75, "EFR": 1.0, "Overall": 0.875}, {"timecode": 4, "before_eval_results": {"predictions": ["vote clerk", "estimated $200,000", "carbohydrates", "redistributive taxation", "the League of Nations", "two", "\"The Time of the Doctor\"", "Nairobi", "Missy", "free", "7:00 to 9:00 a.m.", "Professor Richard ( Dick) Geary", "lipid monolayer", "2009", "whether a state or threat of war existed", "the European Commission", "Jin", "greater equality but not per capita income", "John Houghton", "carbohydrates", "in both houses of Congress", "America's Funniest Home Videos", "42%", "19", "specialised education and training", "layered basaltic lava flows", "October 2007", "Robert Maynard Hutchins", "Shoushi Li", "clerical marriage", "40%", "Kevin Harlan", "about 200 Troupes de la marine and 30 Indians.", "Half", "Independence Day: Resurgence", "duty", "complex silicates (in silicate minerals)", "Worldvision Enterprises", "hunter's garb", "in the Channel Islands", "\"Blue Harvest\" and \"420\"", "1225", "georgie", "george jackson", "jackson", "\"Hey there Delilah, I know... God speed your love to me\"", "Mark Antony", "Cuba", "his father", "filius vocabat Marcum", "on the right side", "george hopp", "george hanger", "billy jackson", "jackson King", "billy jackson", "preston's men stormed the shores of this Barbary state at Derna and... the first verse of the Marines' Hymn: \"From the Halls of Montezuma", "Topix", "London", "Britomart", "Casey Beane", "Islamabad", "The Jefferson Memorial", "murder"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6343487394957983}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2491", "mrqa_squad-validation-3948", "mrqa_squad-validation-3932", "mrqa_squad-validation-8234", "mrqa_squad-validation-10162", "mrqa_squad-validation-3507", "mrqa_squad-validation-1131", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-9286", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-2118", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-14197", "mrqa_naturalquestions-validation-5040", "mrqa_newsqa-validation-839"], "SR": 0.59375, "CSR": 0.71875, "EFR": 0.9615384615384616, "Overall": 0.8401442307692308}, {"timecode": 5, "before_eval_results": {"predictions": ["Justifying Grace", "neuronal dendrites", "an electrical generator", "Doctor Who", "coronary thrombosis", "his grandson", "San Francisco", "consumer prices", "2016", "as soon as they enter into force", "colonizing empires", "\"hockey stick graph\"", "OpenTV", "intuition", "1720", "around 300", "2001", "The Chase", "cortisol and catecholamines", "Economist Intelligence Unit", "the Decalogue (the Ten Commandments) and the Lord's Prayer", "Paramount Pictures", "Thomas Coke", "The Neighbors", "waldzither", "the United States", "\u20ac53,423", "the Helicosproidia", "build their own dedicated networks", "2001", "the \"Rhine knee\"", "Justin Tucker", "Colorado Springs", "2004", "Newton", "26", "University College London", "Jerricho Cotchery", "a talking horse", "the cube root of a negative number", "Willa Cather", "The third law of thermodynamics", "Lewis and Clark", "Truman", "pope", "manganese", "a Fokker", "Richardter", "(2008)", "Ian Fleming", "The Caresse D'Eole Secret Duo", "The Thing", "manganese", "Ely", "the Mensheviks", "Charlotte Russe cake", "Schiller", "The Tale of Genji", "\"The Daily Show\"", "Ant & Dec", "\"Unchampram\"", "Cherokee Nation", "said Rivers.The victory means $250,000 for Rivers' charity: God's Love We Deliver.", "UFC 50: The War of '04"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6400222173659673}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, false, false, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.15384615384615385, 0.7499999999999999]}}, "before_error_ids": ["mrqa_squad-validation-6267", "mrqa_squad-validation-9865", "mrqa_squad-validation-7827", "mrqa_squad-validation-2391", "mrqa_squad-validation-4874", "mrqa_squad-validation-9199", "mrqa_squad-validation-5214", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-8713", "mrqa_triviaqa-validation-3042", "mrqa_newsqa-validation-1583", "mrqa_hotpotqa-validation-1190"], "SR": 0.59375, "CSR": 0.6979166666666667, "EFR": 1.0, "Overall": 0.8489583333333334}, {"timecode": 6, "before_eval_results": {"predictions": ["ten", "", "Von Miller", "a downward pressure on wages", "Catholic", "nine", "11:28", "chest pains", "March 1896", "T cells", "economically", "private networks were often connected via gateways to the public network to reach locations not on the private network", "the college", "Yes\u00fcgei", "research", "toward the end of his life", "Bill Clinton", "Ollie Treiz", "Sufism", "San Andreas Fault", "30%\u201350%", "a double coronation", "ESPN", "p", "plantar fasciitis", "Peter Capaldi", "6000 Da", "Queen Victoria and Prince Albert", "cortisol and catecholamines", "Manakintown", "1985", "a stronger, tech-oriented economy", "stream capture", "the general number field sieve", "identity documents", "The Bronx County District Attorneys Office", "a woman", "Nothing But Love", "a man's lifeless, naked body", "the Sri Lankan cricket team", "a comprehensive detainees policy", "Richard Findley", "8 p.m. local time", "\"Jersey Shore\"", "diabetes and hypertension", "6-4", "\"wow.\"", "Silvan Shalom", "2009", "\"It has never been the policy of this president or this administration to torture.\"", "Himalayan", "Wednesday", "2,000 euros", "Siri", "88", "Charles Bukowski", "two", "Russia", "House of Representatives", "watensis", "Argentinian", "lion", "Department of Homeland Security", "Bessarabia"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6382575757575758}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, true, true, false, false, false, true, true, false, false, false, true, false, false, false, false, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.18181818181818182, 0.0, 0.8, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10413", "mrqa_squad-validation-6008", "mrqa_squad-validation-2122", "mrqa_squad-validation-9213", "mrqa_squad-validation-6696", "mrqa_squad-validation-3193", "mrqa_squad-validation-2835", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-4207", "mrqa_newsqa-validation-990", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3491", "mrqa_triviaqa-validation-2240", "mrqa_hotpotqa-validation-985", "mrqa_searchqa-validation-10525", "mrqa_searchqa-validation-9754", "mrqa_searchqa-validation-8198"], "SR": 0.578125, "CSR": 0.6808035714285714, "EFR": 1.0, "Overall": 0.8404017857142857}, {"timecode": 7, "before_eval_results": {"predictions": ["Blaydon Race", "The Central Region", "ten", "viral pathogens", "100\u2013150", "1886", "from \u00a318m to \u00a334m per year", "Alvaro Martin", "more efficient solutions", "Schedule 5", "BBC 1", "victory at Fort Niagara successfully cut off the French frontier forts further to the west and south.", "cantatas", "January 30", "seven", "\u20ac25,000 per year", "St. Bartholomew's Day massacre", "9th", "principle of equivalence", "The Entertainment Channel", "when they improve society as a whole, including the poorest members", "1725", "the incentive for the democratic changes", "St. Johns River", "Life", "priest", "Jan Andrzej Menich", "Jane Kim", "the global last ice age maximum", "biomass", "1562", "9 a.m.-1 p.m.", "Empire of the Sun", "22", "four", "Ross Perot", "Baghdad", "$1.45 billion", "her home", "more than 2,800", "in a tenement in the Mumbai suburb of Chembur", "the contraband is then moved through an elaborate series of drop points", "it, a crime that triggered a nationwide manhunt and search for the girl", "July 4.", "Evan Bayh", "Hu Jintao", "April 24", "the president would legally be able to intervene in the case if it is transferred from a judge in the eastern city of Abeche,", "scientific reasons", "around 1918 and 1919", "the first or second week in April", "Pakistan", "Pakistan", "jazz", "appealed against the punishment", "the used-luxury market", "Steve Williams", "Ali", "Eid-al-Adha", "Mickey's PhilharMagic", "Sugar Ray", "Oedipus Rex", "guitar feedback", "hyperaccumulators"], "metric_results": {"EM": 0.625, "QA-F1": 0.7339839541585865}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, true, false, false, false, false, false, true, false, false, false, false, true, false, false, false, true, true, false, true, false, true, false, true, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.846153846153846, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.923076923076923, 0.0, 0.11764705882352941, 1.0, 0.0, 0.6666666666666666, 0.5714285714285715, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6561", "mrqa_squad-validation-2923", "mrqa_squad-validation-10269", "mrqa_squad-validation-2419", "mrqa_squad-validation-7564", "mrqa_squad-validation-2422", "mrqa_squad-validation-9144", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-691", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1311", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-2809", "mrqa_triviaqa-validation-955", "mrqa_hotpotqa-validation-3972"], "SR": 0.625, "CSR": 0.673828125, "EFR": 1.0, "Overall": 0.8369140625}, {"timecode": 8, "before_eval_results": {"predictions": ["\"nolo contendere\"", "5,984", "Jacksonville", "fish stocks to collapse", "applied force", "DuMont Television Network", "Amtrak San Joaquins", "teaching", "the traditional salute of a knight", "a ribosome in the cytosol", "British", "1887", "February 7, 2016", "Cargill Meat Solutions and Foster Farms", "Henry Plitt", "1978", "Von Miller", "about 0.7%", "water", "Electronic Frontier Foundation", "Tyndale Bible", "pharmacists know about the mode of action of a particular drug, and its metabolism and physiological effects on the human body", "Il milione", "Guo Shoujing", "1908", "1560", "reason", "only \"essentials\"", "30,000", "a Taliban member who had come for the talks about peace and reconciliation, and detonated the explosives", "she sent a letter to Goa's chief minister asking for India's Central Bureau of Investigation to look into the case.", "Keating Holland.", "California-based Current TV", "Behar.", "his business dealings", "summer", "the Dalai Lama's current \"middle way approach,\"", "the U.S. Holocaust Memorial Museum,", "a body", "Brian Mabry", "iTunes, which completely changed the business of music,", "more than 4,000", "consumer confidence", "autonomy", "1996", "1831", "Russia", "a ruthless cartel", "clothes that are consistent and accessible.", "Arizona", "Muslim festival of Eid al-Adha.", "BET", "FBI recordings of his phone calls.", "torture and indefinite detention", "African President Thabo Mbeki,", "Italian and six Africans", "an impromptu memorial for the late singer", "China", "100mph,", "Prime Minister Margaret Thatcher", "Mason-Dixon Line Segment.", "1898", "11 : 40 p.m. ship's time", "18th century"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6708193542568542}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, false, false, false, true, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, false, false, false, false, false, false, false, true, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.8, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.0909090909090909, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.1212121212121212, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.0, 0.8, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.16]}}, "before_error_ids": ["mrqa_squad-validation-4326", "mrqa_squad-validation-2048", "mrqa_squad-validation-8960", "mrqa_squad-validation-7552", "mrqa_squad-validation-6284", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-1415", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2622", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-776", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-1352", "mrqa_naturalquestions-validation-8279", "mrqa_triviaqa-validation-2510", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-3505"], "SR": 0.578125, "CSR": 0.6631944444444444, "EFR": 1.0, "Overall": 0.8315972222222222}, {"timecode": 9, "before_eval_results": {"predictions": ["the Yassa", "An attorney", "The Quasiturbine", "\u22122, \u22124,...", "creates immunological memory", "three", "coal", "pseudorandom", "average teacher salaries", "the same message routing methodology as developed by Baran", "the Mongols beyond the Middle Kingdom saw them as too Chinese.", "Temecula and Murrieta", "antibodies", "Korean", "the revolution could only succeed in Russia as part of a world revolution.", "William of Orange", "the Horn of Africa", "lipid monolayer", "\"Hymn for the Weekend\"", "Colonel Monckton", "Gymnosperms", "7 January 1943", "1.7 million", "ABC News Now", "Boomer Esiason and Dan Fouts", "Eintracht Frankfurt", "former U.S. secretary of state", "1,073 immigration detainees", "10", "the 84-year-old Mugabe has been the country's only ruler.", "Islamabad", "Haleigh Cummings,", "90", "Zimbabwe President Robert Mugabe", "the 12th on the Blue Monster course at Doral", "terminal brain cancer.", "death squad killings", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Leaders of more than 30 Latin American and Caribbean nations", "the Beatles", "Citizens are picking members of the lower house of parliament,", "the 6.2-mile Moffat Tunnel,", "(l-r)", "the 50-year-old King of Pop has agreed to a series of summer concerts at the O2.", "led the weekend box office, grossing $55.7 million during its first weekend.", "The planned Kingdom City project", "DBG", "United Arab Emirates", "Bobby Jindal", "Polo because \"it was the sport of kings.", "pro-democracy activists", "haute, bandeau-style little numbers", "Aniston, Demi Moore and Alicia Keys", "pesos ($193 million)", "Larry King,", "Afghanistan", "Landon Jones", "Transvaginal ultrasonography", "Wikia", "Ben Hogan", "400", "\"Nebo Zovyot\"", "the Chesapeake Bay", "the list of dos & don'ts,"], "metric_results": {"EM": 0.5625, "QA-F1": 0.640086163379749}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, false, false, false, false, false, false, true, false, false, false, false, false, true, false, false, true, false, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.36363636363636365, 1.0, 0.125, 1.0, 0.6153846153846153, 0.0, 0.0, 0.631578947368421, 0.5555555555555556, 0.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10128", "mrqa_squad-validation-3304", "mrqa_squad-validation-9912", "mrqa_squad-validation-8880", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-3827", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-2128", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-8116", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-2016", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-2219"], "SR": 0.5625, "CSR": 0.653125, "EFR": 1.0, "Overall": 0.8265625}, {"timecode": 10, "UKR": 0.78515625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1190", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1337", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3412", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4048", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-532", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-4150", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8582", "mrqa_naturalquestions-validation-9707", "mrqa_newsqa-validation-100", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1110", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1148", "mrqa_newsqa-validation-1218", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1352", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1837", "mrqa_newsqa-validation-1886", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2070", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2133", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-215", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2275", "mrqa_newsqa-validation-2351", "mrqa_newsqa-validation-2434", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2645", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2844", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3126", "mrqa_newsqa-validation-3159", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-324", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3518", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-368", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-3796", "mrqa_newsqa-validation-3816", "mrqa_newsqa-validation-3838", "mrqa_newsqa-validation-3897", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-733", "mrqa_newsqa-validation-743", "mrqa_newsqa-validation-832", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-990", "mrqa_searchqa-validation-10755", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11790", "mrqa_searchqa-validation-11883", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14197", "mrqa_searchqa-validation-1438", "mrqa_searchqa-validation-14427", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16704", "mrqa_searchqa-validation-2057", "mrqa_searchqa-validation-2219", "mrqa_searchqa-validation-2405", "mrqa_searchqa-validation-3149", "mrqa_searchqa-validation-3592", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-8001", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8507", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9754", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10128", "mrqa_squad-validation-10155", "mrqa_squad-validation-10162", "mrqa_squad-validation-10167", "mrqa_squad-validation-1018", "mrqa_squad-validation-10198", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10269", "mrqa_squad-validation-10272", "mrqa_squad-validation-1029", "mrqa_squad-validation-103", "mrqa_squad-validation-10310", "mrqa_squad-validation-10315", "mrqa_squad-validation-10326", "mrqa_squad-validation-10345", "mrqa_squad-validation-1036", "mrqa_squad-validation-10380", "mrqa_squad-validation-10413", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10476", "mrqa_squad-validation-1048", "mrqa_squad-validation-1053", "mrqa_squad-validation-1088", "mrqa_squad-validation-1097", "mrqa_squad-validation-1119", "mrqa_squad-validation-1131", "mrqa_squad-validation-1197", "mrqa_squad-validation-1222", "mrqa_squad-validation-1231", "mrqa_squad-validation-1255", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-139", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1521", "mrqa_squad-validation-1537", "mrqa_squad-validation-1546", "mrqa_squad-validation-1561", "mrqa_squad-validation-1592", "mrqa_squad-validation-1611", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1831", "mrqa_squad-validation-1834", "mrqa_squad-validation-1876", "mrqa_squad-validation-1940", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-20", "mrqa_squad-validation-2048", "mrqa_squad-validation-2048", "mrqa_squad-validation-2087", "mrqa_squad-validation-2116", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2188", "mrqa_squad-validation-2235", "mrqa_squad-validation-2250", "mrqa_squad-validation-2374", "mrqa_squad-validation-239", "mrqa_squad-validation-2391", "mrqa_squad-validation-2403", "mrqa_squad-validation-2419", "mrqa_squad-validation-2422", "mrqa_squad-validation-2447", "mrqa_squad-validation-2462", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2580", "mrqa_squad-validation-2640", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2723", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-2797", "mrqa_squad-validation-282", "mrqa_squad-validation-2835", "mrqa_squad-validation-2848", "mrqa_squad-validation-2870", "mrqa_squad-validation-2873", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-30", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3048", "mrqa_squad-validation-3084", "mrqa_squad-validation-3086", "mrqa_squad-validation-3141", "mrqa_squad-validation-316", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3299", "mrqa_squad-validation-3304", "mrqa_squad-validation-3309", "mrqa_squad-validation-3319", "mrqa_squad-validation-3358", "mrqa_squad-validation-3368", "mrqa_squad-validation-3390", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3511", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3680", "mrqa_squad-validation-3687", "mrqa_squad-validation-3849", "mrqa_squad-validation-3932", "mrqa_squad-validation-3948", "mrqa_squad-validation-4032", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4165", "mrqa_squad-validation-4176", "mrqa_squad-validation-4186", "mrqa_squad-validation-4248", "mrqa_squad-validation-4265", "mrqa_squad-validation-4274", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4413", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4482", "mrqa_squad-validation-4488", "mrqa_squad-validation-4493", "mrqa_squad-validation-4562", "mrqa_squad-validation-4611", "mrqa_squad-validation-4623", "mrqa_squad-validation-4627", "mrqa_squad-validation-465", "mrqa_squad-validation-4698", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-4971", "mrqa_squad-validation-4976", "mrqa_squad-validation-501", "mrqa_squad-validation-506", "mrqa_squad-validation-5079", "mrqa_squad-validation-5113", "mrqa_squad-validation-5133", "mrqa_squad-validation-5150", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5214", "mrqa_squad-validation-5230", "mrqa_squad-validation-5295", "mrqa_squad-validation-5343", "mrqa_squad-validation-5355", "mrqa_squad-validation-5457", "mrqa_squad-validation-5478", "mrqa_squad-validation-5490", "mrqa_squad-validation-5499", "mrqa_squad-validation-55", "mrqa_squad-validation-5544", "mrqa_squad-validation-5563", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5642", "mrqa_squad-validation-5664", "mrqa_squad-validation-567", "mrqa_squad-validation-5698", "mrqa_squad-validation-5708", "mrqa_squad-validation-5762", "mrqa_squad-validation-5820", "mrqa_squad-validation-5835", "mrqa_squad-validation-586", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5978", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6008", "mrqa_squad-validation-6011", "mrqa_squad-validation-6079", "mrqa_squad-validation-6109", "mrqa_squad-validation-6124", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-616", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6223", "mrqa_squad-validation-6247", "mrqa_squad-validation-6267", "mrqa_squad-validation-6273", "mrqa_squad-validation-6284", "mrqa_squad-validation-6350", "mrqa_squad-validation-6362", "mrqa_squad-validation-6382", "mrqa_squad-validation-6421", "mrqa_squad-validation-6452", "mrqa_squad-validation-6475", "mrqa_squad-validation-6509", "mrqa_squad-validation-6535", "mrqa_squad-validation-6561", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6643", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6869", "mrqa_squad-validation-6879", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-7021", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7062", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7250", "mrqa_squad-validation-7306", "mrqa_squad-validation-7474", "mrqa_squad-validation-7521", "mrqa_squad-validation-7540", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7591", "mrqa_squad-validation-7592", "mrqa_squad-validation-7598", "mrqa_squad-validation-7653", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7733", "mrqa_squad-validation-7738", "mrqa_squad-validation-7751", "mrqa_squad-validation-7758", "mrqa_squad-validation-7775", "mrqa_squad-validation-778", "mrqa_squad-validation-7827", "mrqa_squad-validation-7842", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7937", "mrqa_squad-validation-7941", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8023", "mrqa_squad-validation-8028", "mrqa_squad-validation-8066", "mrqa_squad-validation-813", "mrqa_squad-validation-8132", "mrqa_squad-validation-8174", "mrqa_squad-validation-8213", "mrqa_squad-validation-8221", "mrqa_squad-validation-8222", "mrqa_squad-validation-824", "mrqa_squad-validation-8298", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8436", "mrqa_squad-validation-8446", "mrqa_squad-validation-8458", "mrqa_squad-validation-8466", "mrqa_squad-validation-8475", "mrqa_squad-validation-85", "mrqa_squad-validation-8505", "mrqa_squad-validation-8507", "mrqa_squad-validation-8533", "mrqa_squad-validation-8538", "mrqa_squad-validation-855", "mrqa_squad-validation-8557", "mrqa_squad-validation-8576", "mrqa_squad-validation-8606", "mrqa_squad-validation-8636", "mrqa_squad-validation-8656", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8790", "mrqa_squad-validation-8790", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8836", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8880", "mrqa_squad-validation-890", "mrqa_squad-validation-8941", "mrqa_squad-validation-8960", "mrqa_squad-validation-8962", "mrqa_squad-validation-8978", "mrqa_squad-validation-9008", "mrqa_squad-validation-9101", "mrqa_squad-validation-9144", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9297", "mrqa_squad-validation-9308", "mrqa_squad-validation-9343", "mrqa_squad-validation-9388", "mrqa_squad-validation-9431", "mrqa_squad-validation-9470", "mrqa_squad-validation-9499", "mrqa_squad-validation-9567", "mrqa_squad-validation-9638", "mrqa_squad-validation-9661", "mrqa_squad-validation-9692", "mrqa_squad-validation-973", "mrqa_squad-validation-9768", "mrqa_squad-validation-9819", "mrqa_squad-validation-9865", "mrqa_squad-validation-9912", "mrqa_squad-validation-9923", "mrqa_squad-validation-9935", "mrqa_squad-validation-9975", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1452", "mrqa_triviaqa-validation-1517", "mrqa_triviaqa-validation-1723", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-2638", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3266", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-955"], "OKR": 0.908203125, "KG": 0.40859375, "before_eval_results": {"predictions": ["France", "Turkey", "League of Augsburg", "Marburg Colloquy", "1951", "nearly three hundred years", "enter the priesthood", "32%", "Southwest Fresno", "receptions, gatherings or exhibition purposes", "marry secretly and keep quiet about the matter.", "prime numbers", "a six membraned chloroplast", "Timucua people", "capturing prey", "the Presiding Officer", "1521", "5 million", "a supervisory church body", "the member state cannot enforce conflicting laws", "Ed Asner", "bitstrings", "\"Quiet Nights,\"", "Friday,", "\"momentous discovery\"", "\"We Found Love\"", "five dead bodies", "\"It was a comment that shouldn't have been made and certainly one that he wished he didn't make.\"", "Karl Kr\u00f8yer", "Austin Wuennenberg,", "133", "\"Barbarian Queen\" and \" Amazon Women on the Moon.\"", "\"Mad Men\"", "the area where the single-engine Cessna 206 went down", "water continues flow through the river channel and not spread out over land.", "series like \"Rent,\" \"Cabaret\" and \" Proof,\"", "Mutassim,", "Jennifer Arnold and husband Bill Klein,", "\"the incitement of sectarian hatred or involved in the acts of violence\"", "ambassadors", "\"the evidence and investigatory effort has minimized the likelihood that Haleigh's disappearance is the work of a strangers.\"", "the picturesque Gamla Vaster neighborhood", "17", "said she also told FBI agents Lisa's parents never mentioned anyone wanting to harm them.\"", "series \"Friends\" and Kristin Hahn, who was the executive producer of \"The Departed.\"", "make an emotional connection to their lost loved ones.", "e-mails", "North vs. South, black vs. white, Jew vs. Christian, industrial vs. agrarian.", "Fullerton, California,", "the BBC's central London offices", "45 minutes, five days a week.", "\"Tiger Woods will be speaking to a small group of friends, colleagues and close associates,\"", "Arab Emirates", "Lashkar-e-Tayyiba", "July in the Philippines", "a U.S. military helicopter", "San Antonio", "February 6, 2005", "Falkland Islands", "water", "\"Lions for Lambs\"", "Carver Dana Andrews", "an improvement of 160 SAT points or 4 ACT points on your score,", "an American funk rock band"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6401823970985155}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, false, false, false, true, false, false, true, false, false, true, false, false, false, true, true, true, false, true, false, false, false, false, true, false, true, true, true, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.15999999999999998, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8333333333333333, 0.0, 0.0, 1.0, 0.923076923076923, 0.0, 1.0, 0.08333333333333333, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.10526315789473682, 0.0, 0.6666666666666666, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2587", "mrqa_squad-validation-9047", "mrqa_squad-validation-2468", "mrqa_squad-validation-4272", "mrqa_squad-validation-1676", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-2195", "mrqa_newsqa-validation-3961", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3634", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-437", "mrqa_newsqa-validation-2268", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-976", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-3405", "mrqa_naturalquestions-validation-1479", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-15716"], "SR": 0.578125, "CSR": 0.6463068181818181, "EFR": 0.9629629629629629, "Overall": 0.7422445812289562}, {"timecode": 11, "before_eval_results": {"predictions": ["manned lunar landings", "The Better Jacksonville Plan", "the perceived difficulty of its tune", "zero", "\"Hymn for the Weekend\"", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "\"Blue Harvest\" and \"420\",", "$105 billion", "Samarkand", "27", "an occupancy permit", "1774", "a constant bit rate and latency between nodes", "TEU articles 4 and 5", "plastoglobulus", "Pakistan", "San Jose State", "the mouth and pharynx", "allowed local area networks to be established ad hoc without the requirement for a centralized modem or server", "erosion", "Kim Clijsters", "African National Congress Deputy President", "12-hour-plus shifts", "Kit of Elsinore", "28", "\"intense nervous shock and internal bleeding in the chest cavity.\"", "Kim Clijsters", "not feelMisty Cummings has told them everything she knows.", "Sub-Saharan Africa", "a long-range missile test.", "the man was dead,", "Dr. Jennifer Arnold and husband Bill Klein,", "drought, continual armed conflicts in central and southern Somalia and high inflation on food and fuel.", "not the one to be dealt with by us.", "his parents", "International Polo Club Palm Beach", "an antihistamine and an epinephrine auto-injector", "UNICEF", "Casey Anthony,", "a skilled hacker", "Leo Frank,", "in the west African nation", "Cash for Clunkers", "Turkey", "teary Native Americans", "the Atlantic,", "41,280 pounds", "Natalie Cole", "not", "Fayetteville, North Carolina,", "25", "Rolling Stone", "shark River Park", "\"Stagecoach\" (John Ford, 1939)", "Chinese nationals.", "The Man", "ensure party discipline in a legislature", "the driver", "La Toya Jackson", "Lincoln Memorial University", "Ant Timpson, Ted Geoghegan and Tim League.", "raven owl", "'s", "20 - year period"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7044053819444445}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, false, false, true, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 0.3333333333333333, 0.4, 0.4444444444444445, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5, 0.6666666666666666, 1.0, 0.5, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6248", "mrqa_squad-validation-4744", "mrqa_squad-validation-4789", "mrqa_newsqa-validation-809", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-1386", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-2405", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-1825", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1391", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3035", "mrqa_searchqa-validation-16774", "mrqa_searchqa-validation-2726"], "SR": 0.5625, "CSR": 0.6393229166666667, "EFR": 0.9642857142857143, "Overall": 0.7411123511904762}, {"timecode": 12, "before_eval_results": {"predictions": ["green spaces", "it was developed to explore alternatives to the early ARPANET design and to support network research generally", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "15 June 1899", "Ismail El Gizouli", "by department", "Rhine-Ruhr region", "the trans-Atlantic wireless telecommunications facility", "a bachelor's degree", "two", "Death wish Coffee", "The French Protestant Church of London", "Edward Poynter", "Francisco de Orellana", "\"missing self.\"", "Bruno Mars", "Sybilla of Normandy", "420,000", "plate tectonics", "the liver and kidneys", "InterContinental Hotels Group", "Walter Brennan", "Warren Hastings", "in Poems : Series 1", "April 1917", "the New Testament", "Aristotle", "in a thousand years", "De' Andre Hunter", "2015", "April 2, 2018", "they would learn to be nonviolent in any relationship", "Zoe Zebra", "16 June", "Alaska", "Thomas Jefferson", "1927", "HTTP / 1.1", "Labour Party", "Roger Dean Stadium", "May 2002", "northern Europe", "a ranking used in combat sports", "the portal tomb", "the team", "a mountainous, peninsular mainland", "159", "Missouri River", "Brazil", "318", "Kim Basinger", "Domhnall Gleeson", "the chest, back, shoulders, torso and / or legs", "inner epithelia", "Hindu musical theatre styles", "Corey Pavin", "duck", "Super Junior", "Nanyue", "18", "the Acre/ Haifa area in northern Israel", "hockey", "fire to Palestinian fields and orchards", "\"howling\""], "metric_results": {"EM": 0.59375, "QA-F1": 0.677540586677815}, "metric_results_detailed": {"EM": [false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, false, true, false, false, true, true, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, true, true, true, true, true, true, false, false, false, true, true, false, false, true, false, true, false, false], "QA-F1": [0.6666666666666666, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.3846153846153846, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-1040", "mrqa_squad-validation-4786", "mrqa_squad-validation-1384", "mrqa_squad-validation-5422", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-9387", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-6214", "mrqa_naturalquestions-validation-10283", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-976", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-4596", "mrqa_newsqa-validation-3741", "mrqa_searchqa-validation-3679", "mrqa_searchqa-validation-9108"], "SR": 0.59375, "CSR": 0.6358173076923077, "EFR": 0.9615384615384616, "Overall": 0.7398617788461539}, {"timecode": 13, "before_eval_results": {"predictions": ["the Ominde Commission", "2,000", "2005", "the university's off-campus rental policies", "seven", "Thomas Edison", "southern Suriname", "dispensing substandard products", "automobiles", "landed on the Moon", "Cam Newton", "KGPE", "the difference in potential energy", "prime elements", "Julia Butterfly Hill", "five", "Surficial", "Vienna", "40", "Sir John Nott", "Spice Girls", "the hose", "Sandi Toksvig", "Salvador Allende", "Paris", "Arkansas", "\" Crash,\"", "Hyposmia", "Dennis Potter", "Burma", "Peregrines", "a foliar spray of ethephon", "a wild bird", "MauritaniaMauritania", "a Chopin prelude in a public concert by the end of the year,", "Hudsyn", "James Carville", "Charlie Sheen", "an orange to grey rind", "sheep", "Concepcion", "the Republic of Upper Volta", "Laurie Lee", "Karl Marx and Friedrich Engels", "John Mortimer", "Beaujolais", "Humphrey Bogart", "Bon Jovi", "Kansas", "Amy", "Carl Sagan and his wife and co-writer, Ann Druyan", "Alex Turner", "a St. Tropez drag-show nightclub", "commitment", "Paul Monti", "2001", "\"Back to December\"", "Michael Lewis Greenwell", "an independent homeland", "a pool of blood beneath his head", "tabby", "a doctor that specializes in", "a 1992 American action-thriller film directed by Andrew Davis and written by J.F. Lawton", "International Boxing Federation"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6052353896103897}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, true, false, true, true, true, true, true, true, false, false, true, false, true, true, true, true, true, true, false, false, true, true, true, false, false, false, false, false, false, true, false, true, true, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3636363636363636, 1.0, 0.4, 0.0, 0.0, 0.5, 1.0, 0.4, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_squad-validation-1419", "mrqa_squad-validation-6342", "mrqa_squad-validation-3899", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-4263", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6138", "mrqa_triviaqa-validation-2118", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-6158", "mrqa_triviaqa-validation-546", "mrqa_triviaqa-validation-7464", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-4630", "mrqa_naturalquestions-validation-9499", "mrqa_naturalquestions-validation-9953", "mrqa_hotpotqa-validation-3507", "mrqa_newsqa-validation-367", "mrqa_newsqa-validation-530", "mrqa_searchqa-validation-14224", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-47"], "SR": 0.5625, "CSR": 0.6305803571428572, "EFR": 0.9285714285714286, "Overall": 0.7322209821428571}, {"timecode": 14, "before_eval_results": {"predictions": ["Iberia", "that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "as an auditor", "Victoria", "erosion", "Cricket", "2", "Gabriel Zwilling", "The Day of the Doctor", "through confirmation", "in the city of Deabolis", "Wijk bij Duurstede", "June 6, 1951", "24 September 2007", "18", "1648 - 51 war against Khmelnytsky Uprising in Ukraine", "Teri Garr", "Malayalam", "lamb", "the red - bed country of its watershed", "DNA was a repeating set of identical nucleotides", "Steve Russell", "Africa", "Lord's", "Dalveer Bhandari", "boy", "Joan Alison", "Michael Jackson and Lionel Richie", "6 March 1983", "chilell ( Ed Begley )", "Glenn Close", "Andreas Vesalius", "1959", "in a forest", "in the year 2026", "1901", "Uttar Pradesh", "electors", "1834", "Indian Standard Time", "New York University", "song rose to number 4 on Billboard's Pop Singles chart and number one for two weeks on the R&B Singles charts", "Sophia Akuffo", "Death Eaters", "2", "April 29, 2009", "regulatory site", "the ball is fed into the gap between the two forward packs", "that country's surprise attack on Pearl Harbor the prior day", "Wembley Stadium", "1992", "Jonathan Cheban", "green", "4", "lile-Oxley", "Arabah", "May 27, 2016", "Pattugliatore Polivalente d' Altura", "papillomavirus", "Lonnie", "fish", "Felicity", "Lake Placid, New York", "Russell T Davies"], "metric_results": {"EM": 0.5, "QA-F1": 0.5867845695970695}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, true, true, false, true, false, false, false, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.09523809523809523, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.4, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.923076923076923, 0.15384615384615383, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4921", "mrqa_squad-validation-10078", "mrqa_squad-validation-1076", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-5818", "mrqa_naturalquestions-validation-9390", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-5996", "mrqa_naturalquestions-validation-7575", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-6469", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-9944", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-5611", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-4998", "mrqa_triviaqa-validation-4059", "mrqa_triviaqa-validation-3551", "mrqa_hotpotqa-validation-3337", "mrqa_hotpotqa-validation-3977", "mrqa_newsqa-validation-1372", "mrqa_searchqa-validation-14433", "mrqa_hotpotqa-validation-2357"], "SR": 0.5, "CSR": 0.621875, "EFR": 0.9375, "Overall": 0.732265625}, {"timecode": 15, "before_eval_results": {"predictions": ["The Writers Guild of America", "every five years", "NFIL3", "826", "Treaty of Logstown", "mid-Cambrian", "Cabot Science Library, Lamont Library, and Widener Library", "Palestine", "PNU and ODM camps", "The waxy cuticle of many leaves, the exoskeleton of insects, the shells and membranes of externally deposited eggs, and skin", "2,200", "KMBC-TV and KQTV", "whether he stood by their contents", "cloven", "Hudson River", "sumbios", "a doll", "jordan", "Southern elephant seal", "Sally Field,", "1907-1993", "Captain Nemo", "ruby slippers", "jedoublen/jeopardy", "Atreus", "jedoublen/jeopardy", "to raise money for the Muscular Dystrophy Association", "sambal", "Boeing", "Saskatchewan", "Kareem Abdul-Jabbar", "mountain plateau", "Wladimir Klitschko", "Claude Wheeler,", "jEWELRY", "Keith Urban", "French", "deer", "mine resistant", "lack of sleep sooner than", "change to a boat", "parrots, gorillas, and tarantulas", "Rick Springfield", "Lubnan", "70% isopropyl alcohol", "sweet, musical, or pleasant to hear", "Tecumseh,", "an insect pest, Pulex irritans", "sefirot", "jordan", "piedmont", "Lgion d' Honneur", "jOHANNA", "Rent", "Camping World Stadium in Orlando, Florida", "John Cooper Clarke", "Woodrow Wilson", "Nicola Adams", "1943", "Pearl Jam", "\"It is not acceptable,\"", "there is not a process to ensure that auto owners comply with recalls.", "two", "16\u201321"], "metric_results": {"EM": 0.40625, "QA-F1": 0.44968393874643875}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, false, false, false, false, false, true, true, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, true, true, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.1111111111111111, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.14814814814814817, 0.15384615384615385, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4802", "mrqa_squad-validation-6435", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-12272", "mrqa_searchqa-validation-9570", "mrqa_searchqa-validation-8796", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-6050", "mrqa_searchqa-validation-10319", "mrqa_searchqa-validation-14466", "mrqa_searchqa-validation-4536", "mrqa_searchqa-validation-1446", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-16715", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-4207", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-13578", "mrqa_searchqa-validation-1624", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5505", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-2200", "mrqa_searchqa-validation-4470", "mrqa_searchqa-validation-14789", "mrqa_hotpotqa-validation-1238", "mrqa_newsqa-validation-3609", "mrqa_newsqa-validation-2358", "mrqa_hotpotqa-validation-5438"], "SR": 0.40625, "CSR": 0.6083984375, "EFR": 1.0, "Overall": 0.7420703125}, {"timecode": 16, "before_eval_results": {"predictions": ["within the Church of England", "Lenin", "a qualified majority vote, if not consensus", "36 cameras", "Brough Park in Byker", "2012", "Stress", "the physics", "Jonathan Stewart", "George Westinghouse", "a human", "significantly increased British military resources in the colonies", "Aquitaine", "( Bilbo) Baggins", "a Native American", "a grizzly is brown bear", "the Netherlands", "Marriott International", "drink wine", "mask", "(J. Bullock)", "the Sons of Liberty", "movie house", "National Security Agency", "Ugly Betty", "( Ambrose) Bierce", "the Key deer", "the All-New Blue Ribbon Cookbook", "flowers", "Pheonix", "Joey buttafuoco", "A Portrait of the Artist as a Young Man", "a tumbler", "guttural", "polio", "Meg Tilly", "Mausolus", "(George) III", "Annie Braddock", "(Pope) Francis", "the Firmament", "(Jedoublen)", "(Jerry) Maguire", "Wendy Beckett", "(Ferris) B Mueller's Day Off", "(Jedoublen)", "\"Bewitched, Bothered and Bewildered\"", "CNN", "Samuel Goldwyn", "Annika Sorenstam", "(Asparagus)", "Thurman Munson", "Washington", "anthro Hyphen", "the International Border ( IB )", "1783", "chilis", "Ray Robinson", "McComb, Mississippi", "Dorothy Zbornak", "Tutsi and Hutu", "last week.", "18", "\"Twilight\""], "metric_results": {"EM": 0.375, "QA-F1": 0.5289062499999999}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, false, true, true, false, false, false, true, false, false, true, true, true, false, false, false, false, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, false, false, true, false, false, true, false, false, true, false, true], "QA-F1": [1.0, 1.0, 0.08333333333333333, 0.6666666666666666, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.0, 0.3333333333333333, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4216", "mrqa_squad-validation-586", "mrqa_squad-validation-5456", "mrqa_squad-validation-10388", "mrqa_squad-validation-10158", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-10863", "mrqa_searchqa-validation-1961", "mrqa_searchqa-validation-11028", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-10263", "mrqa_searchqa-validation-11531", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-16832", "mrqa_searchqa-validation-4426", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-12484", "mrqa_searchqa-validation-15593", "mrqa_searchqa-validation-366", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-1013", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-9957", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-2405", "mrqa_naturalquestions-validation-1169", "mrqa_triviaqa-validation-7054", "mrqa_triviaqa-validation-2627", "mrqa_hotpotqa-validation-959", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-608"], "SR": 0.375, "CSR": 0.5946691176470589, "EFR": 1.0, "Overall": 0.7393244485294118}, {"timecode": 17, "before_eval_results": {"predictions": ["49\u201315", "113", "protein structure prediction", "Deformational events", "August 2004", "Department of State Affairs", "Prague", "They were put on a bus and taken to the Nye County seat of Tonopah, Nevada", "governments", "M\u00f6ngke Khan", "by using net wealth (adding up assets and subtracting debts),", "Tiger Woods", "snow", "( Cyril J. O'Brien", "Romeo and Juliet", "Jane Addams", "The Rand McNally & Company", "Dean Acheson", "the pound sterling", "Auguste Rodin", "the Andes", "Sherlock Holmes", "the Taj Mittal", "trampoline", "an axe", "rice", "Constantine", "Inouye", "Argon", "a Buddhist monastery", "Kung Fu", "The Star-Crossed Stars of Showgirls, Crossroads, and Glitter", "an ice cream", "The GNTCE", "Schlitz", "silver", "Bangkok", "the Soviet Union", "amaretto", "the Shakespeare play", "a doses", "Frank Sinatra", "Christopher Columbus", "the King of the Hill", "Private Benjamin", "Stephen King", "Lord Byron", "the Japanese", "Joan of Arc", "Jaguar", "an orange skin and green hair", "Cerberus", "R. Stanton Avery", "U.S. Bank Stadium", "The Golden Gate Bridge", "Sir Humphrey", "Jim Branning", "1976", "(IATA: VNO, ICAO: EYVI)", "Cipro, Levaquin, Avelox, Noroxin and Floxin", "murder", "Parlophone Records", "five times", "known as La R\u00e9sistance"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6406013257575758}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, false, true, false, false, false, false, true, true, false, false, false, false, true, true, true, true, false, false, false, true, true, false, true, true, false, true, false, true, false, true, true, false, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.13333333333333333, 1.0, 1.0, 0.0, 0.8, 1.0, 0.8, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-6702", "mrqa_squad-validation-7554", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-7825", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-4288", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-6283", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-9343", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-6327", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-2546", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-5491", "mrqa_naturalquestions-validation-5674", "mrqa_triviaqa-validation-7009", "mrqa_hotpotqa-validation-3728", "mrqa_newsqa-validation-57", "mrqa_hotpotqa-validation-5499"], "SR": 0.546875, "CSR": 0.5920138888888888, "EFR": 1.0, "Overall": 0.7387934027777778}, {"timecode": 18, "before_eval_results": {"predictions": ["hunter's garb", "1763", "two forces", "a computational problem where a single output (of a total function) is expected for every input", "10 to 15 million people", "17", "Establishing \"natural borders\"", "Houston Street lab", "vary by geographic area and subject taught", "\"Turks\" (Muslims) and Catholics", "provides the public with financial information about a nonprofit organization", "the Northeast Monsoon", "April 3, 1973", "Fa Ze YouTubers", "July 14, 1969", "Krypton", "English", "in the bone marrow", "Ukraine", "Coldplay", "Yugoslavia", "head coach", "May 19, 2017", "T - Bone Walker", "April 2, 2018", "Rose Stagg ( Valene Kane )", "Doug Diemoz", "Iran", "southern Anatolia", "classical architecture", "the pyloric valve", "1546", "100,000 writes", "the chryselephantine statue of Athena Parthenos", "16 seasons", "Long Island", "A lacteal", "the Beldam / Other Mother", "1987", "Jim Capaldi, Paul Carrack, and Peter Vale", "Kent", "Jikji", "Panning", "the RAF", "Pepsi", "Detective Superintendent Dave Kelly", "Isabela Moner", "Ray Henderson", "provide information about advance health care directives to adult patients upon their admission to the healthcare facility", "New Orleans", "the Outback", "an unmasked and redeemed Anakin Skywalker", "Russell Huxtable", "Namibia", "balsamico", "all-time leader in total passing yards, touchdowns, and completions", "Rawlings", "eight", "Los Angeles", "North by Northwest", "Naples", "Etna", "King Edward VI", "North Rhine-Westphalia"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5849110958485958}, "metric_results_detailed": {"EM": [true, false, false, false, true, true, true, true, false, true, true, false, false, false, true, false, false, true, true, false, true, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, false, true, false, false, false, true, false, false, false, true, false, false, true, true, false, false, false, false, false, true, true, true, true, true, true, false, false], "QA-F1": [1.0, 0.2222222222222222, 0.6666666666666666, 0.2666666666666667, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5714285714285715, 0.5, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 0.15384615384615385, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6153846153846153, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-10121", "mrqa_squad-validation-10395", "mrqa_squad-validation-1600", "mrqa_squad-validation-2054", "mrqa_naturalquestions-validation-774", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3392", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-2758", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9253", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-2248", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-3658", "mrqa_naturalquestions-validation-613", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-3727", "mrqa_hotpotqa-validation-466", "mrqa_searchqa-validation-3761", "mrqa_searchqa-validation-9438"], "SR": 0.484375, "CSR": 0.5863486842105263, "EFR": 1.0, "Overall": 0.7376603618421054}, {"timecode": 19, "before_eval_results": {"predictions": ["Thomas Vasey and Richard Whatcoat", "reminding their countrymen of injustice", "sex offenders register", "Kenya became a republic under the name \"Republic of Kenya\"", "the violence that subsequently engulfed the country", "aristocracy", "1905", "Saul Alinsky", "local government, sport and the arts, transport, training, tourism, research and statistics and social work", "addition, subtraction, multiplication, and division are represented by the +, -, *, and / keys, respectively", "February 1834", "Jonathan Goldstein", "Yahya Khan", "milk", "BC Jean and Toby Gad", "Debbie Reynolds, Gene Kelly and Donald O'Connor", "Anatomy", "in Pyeongchang County, Gangwon Province, South Korea", "Jacqueline MacInnes Wood", "Maggie Ringwald", "2017", "a child with Treacher Collins syndrome trying to fit in", "Andaman and Nicobar Islands -- Port Blair   Chandigarh   Dadra and Nagar Haveli -- Silvassa   Daman and Diu -- Daman", "interstitial fluid in the ` interstitial compartment '' ( surrounding tissue cells and bathing them in a solution of nutrients and other chemicals ), blood plasma and lymph in the `` intravascular compartment ''", "Pakistan, India, and Bangladesh are among the largest individual contributors with around 8,000 units each", "in the town of Carcassonne in Aude, France", "Rocinante", "1978", "May 1, 2018", "Hans Christian Andersen", "Leon Battista Alberti", "Cheitharol Kummaba", "1", "alveolar process", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F ) at Vostok Station", "October 14, 2017", "16 for females and 18 for males", "July 21, 1861", "a candidate state must be a free market democracy", "45 % of the light is in the photosynthetically active wavelength range", "in the bible", "bypasses, to cross major bridges, and to provide direct intercity connections", "Sally Dworsky", "Robert E. Lee", "Clarence L. Tinker", "Soviet Russia defaulted on all of Imperial Russia's commitments to the Triple Entente alliance", "the church at Philippi", "a federal republic", "Thomas Edison", "mitosis", "an oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke", "pit road speed during the warm - up laps", "Pyeongchang County, Gangwon Province, South Korea", "Glory Be", "Gianni Versace", "David Simon", "1929", "Madrid's Barajas International Airport", "in Seoul", "a chalk", "pours tea", "three", "the Dalai Lama", "Christopher Savoie"], "metric_results": {"EM": 0.453125, "QA-F1": 0.6127228223729531}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, false, false, true, true, false, false, true, false, false, false, false, false, false, true, false, false, false, false, true, false, true, false, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, true, false, true, true, false, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.4444444444444445, 0.6666666666666666, 0.923076923076923, 0.0, 0.0, 0.6666666666666666, 1.0, 0.5454545454545454, 0.43750000000000006, 0.0, 0.0, 1.0, 0.0, 1.0, 0.35294117647058826, 0.5454545454545454, 0.0, 1.0, 1.0, 0.8421052631578948, 1.0, 0.25, 1.0, 1.0, 0.19999999999999998, 0.0, 0.0, 1.0, 0.7499999999999999, 0.4615384615384615, 0.09523809523809525, 1.0, 1.0, 1.0, 1.0, 0.4210526315789474, 0.6, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8370", "mrqa_squad-validation-9640", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-10617", "mrqa_naturalquestions-validation-5600", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-9760", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-4961", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-2421", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-5984", "mrqa_triviaqa-validation-2196", "mrqa_hotpotqa-validation-474", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-84", "mrqa_searchqa-validation-10274", "mrqa_searchqa-validation-2516"], "SR": 0.453125, "CSR": 0.5796875, "EFR": 0.9714285714285714, "Overall": 0.7306138392857143}, {"timecode": 20, "UKR": 0.748046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1705", "mrqa_hotpotqa-validation-2034", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-4018", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4329", "mrqa_hotpotqa-validation-4390", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-466", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4982", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10571", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10625", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-12", "mrqa_naturalquestions-validation-1436", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1557", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-2100", "mrqa_naturalquestions-validation-2146", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-2897", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3048", "mrqa_naturalquestions-validation-3108", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3376", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3950", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4798", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5236", "mrqa_naturalquestions-validation-5526", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5656", "mrqa_naturalquestions-validation-588", "mrqa_naturalquestions-validation-5885", "mrqa_naturalquestions-validation-6141", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6662", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-6789", "mrqa_naturalquestions-validation-6917", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7589", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7736", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-8116", "mrqa_naturalquestions-validation-8279", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-839", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-859", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8903", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9306", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9809", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1583", "mrqa_newsqa-validation-1624", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2", "mrqa_newsqa-validation-2097", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-220", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2383", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2604", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2827", "mrqa_newsqa-validation-283", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3170", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3405", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3527", "mrqa_newsqa-validation-3578", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3765", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3950", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4061", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-481", "mrqa_newsqa-validation-523", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-606", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-875", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-912", "mrqa_newsqa-validation-923", "mrqa_newsqa-validation-976", "mrqa_searchqa-validation-10708", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-10993", "mrqa_searchqa-validation-11360", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11598", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-12817", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13511", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14247", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15560", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16636", "mrqa_searchqa-validation-1680", "mrqa_searchqa-validation-1924", "mrqa_searchqa-validation-1928", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2887", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3031", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3866", "mrqa_searchqa-validation-3899", "mrqa_searchqa-validation-4133", "mrqa_searchqa-validation-4245", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4750", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-494", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5183", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5424", "mrqa_searchqa-validation-5553", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-5671", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6233", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-7976", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8394", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9561", "mrqa_searchqa-validation-9570", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10004", "mrqa_squad-validation-10158", "mrqa_squad-validation-10162", "mrqa_squad-validation-10198", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-10418", "mrqa_squad-validation-10425", "mrqa_squad-validation-10471", "mrqa_squad-validation-1076", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1188", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1330", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1424", "mrqa_squad-validation-1506", "mrqa_squad-validation-1540", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1611", "mrqa_squad-validation-1703", "mrqa_squad-validation-1751", "mrqa_squad-validation-179", "mrqa_squad-validation-1830", "mrqa_squad-validation-1834", "mrqa_squad-validation-1908", "mrqa_squad-validation-1976", "mrqa_squad-validation-2015", "mrqa_squad-validation-2025", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2111", "mrqa_squad-validation-2122", "mrqa_squad-validation-213", "mrqa_squad-validation-2250", "mrqa_squad-validation-2395", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2532", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-283", "mrqa_squad-validation-2848", "mrqa_squad-validation-3001", "mrqa_squad-validation-3044", "mrqa_squad-validation-3048", "mrqa_squad-validation-3193", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-331", "mrqa_squad-validation-3368", "mrqa_squad-validation-3449", "mrqa_squad-validation-3493", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3626", "mrqa_squad-validation-3647", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-3948", "mrqa_squad-validation-4065", "mrqa_squad-validation-4158", "mrqa_squad-validation-4159", "mrqa_squad-validation-4176", "mrqa_squad-validation-4248", "mrqa_squad-validation-4248", "mrqa_squad-validation-4272", "mrqa_squad-validation-4274", "mrqa_squad-validation-4301", "mrqa_squad-validation-435", "mrqa_squad-validation-4386", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4623", "mrqa_squad-validation-4686", "mrqa_squad-validation-4698", "mrqa_squad-validation-4765", "mrqa_squad-validation-4789", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4956", "mrqa_squad-validation-4975", "mrqa_squad-validation-501", "mrqa_squad-validation-5133", "mrqa_squad-validation-5157", "mrqa_squad-validation-5214", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-55", "mrqa_squad-validation-5616", "mrqa_squad-validation-562", "mrqa_squad-validation-5664", "mrqa_squad-validation-5715", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5897", "mrqa_squad-validation-5908", "mrqa_squad-validation-5909", "mrqa_squad-validation-594", "mrqa_squad-validation-5956", "mrqa_squad-validation-5991", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-620", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6251", "mrqa_squad-validation-6253", "mrqa_squad-validation-6264", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6435", "mrqa_squad-validation-6452", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-6966", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7039", "mrqa_squad-validation-7041", "mrqa_squad-validation-7043", "mrqa_squad-validation-7127", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7191", "mrqa_squad-validation-7226", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7552", "mrqa_squad-validation-7564", "mrqa_squad-validation-7592", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7751", "mrqa_squad-validation-7775", "mrqa_squad-validation-7844", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7889", "mrqa_squad-validation-7932", "mrqa_squad-validation-7952", "mrqa_squad-validation-7985", "mrqa_squad-validation-8010", "mrqa_squad-validation-8019", "mrqa_squad-validation-8199", "mrqa_squad-validation-8213", "mrqa_squad-validation-826", "mrqa_squad-validation-8278", "mrqa_squad-validation-8298", "mrqa_squad-validation-830", "mrqa_squad-validation-8303", "mrqa_squad-validation-8312", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8383", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-861", "mrqa_squad-validation-8612", "mrqa_squad-validation-8636", "mrqa_squad-validation-8665", "mrqa_squad-validation-8701", "mrqa_squad-validation-8786", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-8844", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9213", "mrqa_squad-validation-9308", "mrqa_squad-validation-9315", "mrqa_squad-validation-9322", "mrqa_squad-validation-9388", "mrqa_squad-validation-9405", "mrqa_squad-validation-9431", "mrqa_squad-validation-9495", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9640", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9865", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1646", "mrqa_triviaqa-validation-2016", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3034", "mrqa_triviaqa-validation-3042", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4508", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5140", "mrqa_triviaqa-validation-5834", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6531", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-841", "mrqa_triviaqa-validation-88"], "OKR": 0.875, "KG": 0.48125, "before_eval_results": {"predictions": ["the dukes", "He opposed banning the publication of the Qur'an, wanting it exposed to scrutiny.", "Ed Lee", "40%", "post-World War I", "10,000 LPs", "can produce both eggs and sperm at the same time", "\u00d6gedei Khan", "June 24, 1935", "Kohlberg K Travis Roberts", "John McClane", "I Write What I Like", "1910s", "between 11 or 13 and 18", "Song Il-gon", "Let's Make Sure We Kiss Goodbye", "Martin McCann", "an American astronaut, naval aviator, test pilot, and businessman", "Donald Trump's presidential campaign team", "December 17, 1974", "The Royal Family", "Sir Seretse Khama", "Mike Holmgren", "S7", "S. F. Newcombe", "Don DeLillo", "the Raiders", "a co-op of grape growers,", "South America", "five", "Kramer", "Bank of China Building", "Kane Meadows", "Bisexuality", "innie the Pooh", "Excalibur Hotel and Casino", "Rigoletto", "Knoxville, Tennessee", "Americana Manhasset", "Omega SA", "1978", "Pim Fortuyn", "Firth of Clyde, Scotland", "Todd McFarlane", "M. Night Shyamalan", "Magdalen College", "Eric Allan Kramer", "1894", "Province of New York", "a royal residence", "Ghana", "Michael Seater", "1933", "an English expression meaning `` mind your manners '', `` mindyour language '',`` be on your best behaviour '' or similar", "Charles Darwin", "Osborne Road", "Katherine Parr", "his past and his future", "Buenos Aires", "Engelbert Humperdinck", "Colorado River", "John Denver", "The Princess Bride", "a republic in W Africa"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6359561011904762}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, true, false, false, false, false, false, true, true, false, true, false, false, false, true, false, true, false, true, false, false, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, true, false, true, true, true, true, false], "QA-F1": [1.0, 0.625, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 0.7857142857142856, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-2293", "mrqa_squad-validation-5236", "mrqa_hotpotqa-validation-97", "mrqa_hotpotqa-validation-4511", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-4606", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-1218", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-5467", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-999", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-891", "mrqa_naturalquestions-validation-4109", "mrqa_triviaqa-validation-7452", "mrqa_newsqa-validation-3889", "mrqa_triviaqa-validation-6564"], "SR": 0.53125, "CSR": 0.5773809523809523, "EFR": 0.9666666666666667, "Overall": 0.7296688988095238}, {"timecode": 21, "before_eval_results": {"predictions": ["a interception", "ended the true Islamic system, something for which it blames \"the disbelieving (Kafir) colonial powers\" working through Turkish modernist Mustafa Kemal Atat\u00fcrk.", "pharmacy practice science and applied information science", "thought it may have been a combination of anthrax and other pandemics", "1912", "rubisco", "the Huguenot rebellions", "Psych", "Romance language", "last August 11, 1946", "Protestant Christian", "Queens, New York", "Erreway", "Rochester Hills, Michigan", "Madeleine L' Engle", "FAI Junior Cup", "1966 US tour", "last February 18, 1965", "Sydney", "Cuban descent", "1898", "Tricia Helfer", "Mickey's PhilharMagic", "Eielson Air Force Base", "Taylor Swift", "Italy", "Tel Aviv University", "the Donny & Marie Showroom, at the Flamingo Las Vegas", "John of Gaunt", "William Clark Gable", "Kmart", "C. J. Cherryh", "Spanish", "channel 33", "Guns N' Roses", "Daniil Shafran", "Soha Ali Khan", "1912", "four", "13 October 1958", "70", "3 May 1958", "2014 Sochi Games", "Vancouver", "Marlborough", "Fountains of Wayne", "mid-ninth-century Viking chieftain", "sulfur mustard", "The Saturdays", "the Chick tract of the same name", "southwestern", "Campbell Soup Company", "The Gang", "2010", "The vascular cambium", "A", "dark, spicy", "Rod Blagojevich", "Iraq", "Hatha", "Harley-Davidson", "first home series defeat on Australia in almost 16 years", "$60 billion", "last-ever near-total face transplant in the United States,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5497574083052024}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, true, false, true, false, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, false, false, false, false, true, false, false, true, true, false, false, true, false, true, false, false, true, true, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.5185185185185185, 1.0, 0.7058823529411764, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.4, 0.8571428571428571, 1.0, 0.6666666666666666, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 0.28571428571428575, 0.0, 0.0, 0.33333333333333337, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.28571428571428575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-48", "mrqa_squad-validation-9918", "mrqa_squad-validation-5029", "mrqa_hotpotqa-validation-2271", "mrqa_hotpotqa-validation-2823", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-5569", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-5844", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-3144", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-4192", "mrqa_hotpotqa-validation-2796", "mrqa_hotpotqa-validation-2994", "mrqa_hotpotqa-validation-1852", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-1287", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-4174", "mrqa_hotpotqa-validation-51", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1435", "mrqa_triviaqa-validation-2246", "mrqa_triviaqa-validation-2301", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-1784", "mrqa_searchqa-validation-7453", "mrqa_searchqa-validation-2193", "mrqa_newsqa-validation-4085", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-1093"], "SR": 0.421875, "CSR": 0.5703125, "EFR": 1.0, "Overall": 0.7349218750000001}, {"timecode": 22, "before_eval_results": {"predictions": ["one of the daughters of former King of Thebes, Oedipus", "environmental determinism", "1110 AM", "Articles 106 and 107", "corpses", "Cobb Lecture Hall", "Cortina d'Ampezzo", "Eric Edward Whitacre", "1983", "EQT Plaza in Pittsburgh, Pennsylvania", "Ruth Westheimer", "Giotto", "Nickelodeon", "Brad Wilk", "Bobby Hurley", "11", "Journal for Writers and Readers", "March 19, 2017", "Disney California Adventure", "Anah\u00ed", "Nicholas John \"Nic\" Cester", "Anne Elizabeth Alice Louise", "Harry Robbins \"Bob\" Haldeman", "264,152", "tales of various deities, beings, and heroes derived from numerous sources from both before and after the pagan period, including medieval manuscripts, archaeological representations, and folk tradition", "directed several episodes of the popular sitcom \"Friends\"", "40 million", "Seattle, Washington", "Africa", "William Cavendish, 7th Duke of Devonshire", "20 March to 1 May 2003", "23 July 1989", "Kinnairdy Castle", "Javed Miandad", "\"The Catcher in the Rye\"", "Indianapolis Motor Speedway", "Marjorie Jacqueline \"Marge\" Simpson", "May 4, 1924November 22, 1993", "Transporter 3", "Richard Street", "\"Queen In-hyun's Man\"", "Steve Carell", "Green Chair", "The Frog Prince", "24 April 1882", "German", "November 10, 2017", "CTV Television Network", "Australia", "Rafael Palmeiro", "Eric Allan Kramer", "\"Orchard County\"", "Orographic lift", "Reproductive system", "Coroebus of Elis", "\u201cMy Favorite Martian,\u201d", "Disraeli", "Jennifer Ellison", "Sunday,", "5 1/2-year-old", "one day,", "supersonic", "the Supreme Court", "Pledge of Allegiance"], "metric_results": {"EM": 0.578125, "QA-F1": 0.7166846678187404}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, true, true, true, true, false, false, false, false, false, true, false, false, true, true, true, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, true, true, true, true, true, false, true, false, false, false, false, false, true, true], "QA-F1": [0.42857142857142855, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 0.32258064516129037, 0.2, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7499999999999999, 1.0, 0.8, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-6640", "mrqa_squad-validation-4772", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5636", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-1236", "mrqa_hotpotqa-validation-1077", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-1658", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-2117", "mrqa_hotpotqa-validation-4436", "mrqa_hotpotqa-validation-5791", "mrqa_hotpotqa-validation-4879", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-3629", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-4189", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-544", "mrqa_newsqa-validation-1175", "mrqa_searchqa-validation-13691"], "SR": 0.578125, "CSR": 0.5706521739130435, "EFR": 1.0, "Overall": 0.7349898097826087}, {"timecode": 23, "before_eval_results": {"predictions": ["Article 17(3)", "his brother", "Department of Justice", "tidal currents", "punt", "The Eleventh Doctor", "Malware", "Claims adjuster", "1995", "Caleb", "Arunachal Pradesh", "1998", "Richard of Shrewsbury, Duke of York", "the Roman Empire", "1995", "Chaka Khan", "18", "to form a higher alkane", "Percy Jackson", "Gil", "last night", "UMBC", "deposited or cashed by the recipient", "New Croton Reservoir", "Elena Anaya", "January 2018", "a prison", "small packs, and in larger and smaller sizes", "Woodrow Wilson", "Commander in Chief of the United States Armed Forces", "Waylon Jennings", "the Italian / Venetian John Cabot", "the Boston Red Sox", "assemble a stable, protective protein shell to protect the genome from lethal chemical and physical agents", "Turducken", "a balance sheet", "San Francisco Bay", "Mickey Mantle", "left sleeve pocket flap", "1956", "around 2011", "A trustee who stands in for the condemned during execution rehearsals and sells snacks to prisoners and guards", "2014 Olympic Games in Sochi, Krasnodar Krai, Russia", "David Gahan", "the town of Acolman, just north of Mexico City", "South Asia", "deletion of three nucleotides spanning positions 507 and 508 of the CFTR gene on chromosome 7", "stuffing", "note number 60", "New England Patriots", "Tristan Rogers", "novella", "Mike Brady, a widowed architect with sons Greg, Peter, and Bobby,", "Oliver Stone", "Mexico", "Apsley George Benet Cherry-Garrard", "the Ecumenical Award", "Brig Gen Augustine Warner Robins", "Florida", "Secretary Janet Napolitano", "Elisabeth, 43,", "Colorado", "gusto", "Ulysses"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6266016458824851}, "metric_results_detailed": {"EM": [true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, false, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, false, false, true, false, false, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.2, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.782608695652174, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.2222222222222222, 0.5, 0.0, 1.0, 0.6976744186046512, 0.0, 0.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-8230", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-4667", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-8744", "mrqa_naturalquestions-validation-2657", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-4135", "mrqa_naturalquestions-validation-9707", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-1838", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-4412", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-3686", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-4865", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-10680", "mrqa_naturalquestions-validation-3242", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-10586", "mrqa_triviaqa-validation-3114", "mrqa_newsqa-validation-4156", "mrqa_newsqa-validation-2908"], "SR": 0.546875, "CSR": 0.5696614583333333, "EFR": 0.9655172413793104, "Overall": 0.7278951149425288}, {"timecode": 24, "before_eval_results": {"predictions": ["the first two series", "Doctor in Bible", "c1750", "60%", "a deterministic Turing machine", "Henry III of England", "Henkel", "Bowie", "King George II", "Pol Pot", "the Pyrenees Mountains", "cars, jewelry, stamps, art, wines, pens, antiques, cigars, even sneakers", "Motel 6", "a downtown restaurant", "Virginia", "Esau", "a crystal ball", "Houyhnhnm", "the gallbladder", "his deputy Thabo Mbeki", "1921", "a dog", "Robert Schumann", "the Benedictine Order", "brash evans", "translations", "King County Executive", "Scotland", "The Penguin", "The Great Victoria Desert", "Mata Hari", "rings", "the Brisbane River", "The Aidensfield Arms", "armada", "Liechtenstein", "I was born in a cross-fire hurricane", "Rodney", "Robert Shapiro", "the UK", "Prokofiev", "horses", "\"Stutter Rap (No Sleep til Bedtime)\"", "Kansas", "Australia", "EGBDF", "\"Little Red Rented Rowboat\"", "smell", "Jesse of Bethlehem", "driving Miss Daisy", "the Isles of the Blessed", "Anthropocene", "Edward Kenway ( Matt Ryan ), a Welsh privateer - turned - pirate and eventual member of the Assassin Order", "Darren McGavin", "Sir Henry Cole", "Scottish", "Sarah Hurst", "1916", "treats United are said to be willing to cash in at the right price with Spanish giants Barcelona and Real Madrid also monitoring the situation.", "Jason Chaffetz", "the martyrs of their tribe", "an analog watch", "brady bunch", "Alexander Haig Jr."], "metric_results": {"EM": 0.53125, "QA-F1": 0.6183035714285714}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, true, true, true, false, true, false, false, true, false, false, false, false, false, false, true, true, false, true, true, false, false, true, false, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 0.3, 1.0, 0.5714285714285715, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8]}}, "before_error_ids": ["mrqa_squad-validation-1819", "mrqa_triviaqa-validation-5569", "mrqa_triviaqa-validation-1660", "mrqa_triviaqa-validation-1829", "mrqa_triviaqa-validation-4306", "mrqa_triviaqa-validation-5245", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-7328", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-4466", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1583", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-1595", "mrqa_triviaqa-validation-3388", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-2406", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-1702", "mrqa_naturalquestions-validation-3189", "mrqa_hotpotqa-validation-5274", "mrqa_newsqa-validation-1260", "mrqa_newsqa-validation-2385", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-11092", "mrqa_searchqa-validation-1228"], "SR": 0.53125, "CSR": 0.568125, "EFR": 1.0, "Overall": 0.7344843750000001}, {"timecode": 25, "before_eval_results": {"predictions": ["The next architect to work at the museum was Colonel (later Major General) Henry Young Darracott Scott, also of the Royal Engineers", "prime number p with n < p < 2n \u2212 2, for any natural number n > 3", "QuickBooks", "Westinghouse Electric", "The Bachelor", "The Statue of Freedom, also known as Armed Freedom or simply Freedom, is a bronze statue designed by Thomas Crawford ( 1814 -- 1857 ) that, since 1863, has crowned the dome of the U.S. Capitol building", "2018", "Auburn Tigers", "Honor\u00e9 Mirabeau", "first - class and List A cricket", "Eleanor Roosevelt", "small fission systems or radioactive decay for electricity or heat", "1908", "honey bees", "Samantha Jo `` Mandy '' Moore", "Edward Douglass White, Charles Evans Hughes, Harlan Fiske Stone, and William Rehnquist", "Mangal Pandey of the 34th BNI, angered by the recent actions of the East India Company, declared that he would rebel against his commanders", "Woodrow Strode", "a joint session of Congress", "four", "The intermaxillary segment in an embryo is a mass of tissue formed by the merging of tissues in the vicinity of the nose", "season two", "January 17, 1899", "The astronomical predictions of Ptolemy's geocentric model were used to prepare astrological and astronomical charts for over 1500 years", "16 best - selling religious novels by Tim LaHaye and Jerry B. Jenkins, dealing with Christian dispensationalist End Times : the pretribulation, premillennial, Christian eschatological interpretation of the Biblical apocalypse", "British Ultra code - breaking intelligence", "602", "inverted", "5.7 million", "Swedien and Jones", "Steve Russell, in collaboration with Martin Graetz and Wayne Wiitanen, and programmed by Russell with assistance from others including Bob Saunders and Steve Piner", "Janis Joplin", "1871", "September 9, 2012", "Ahmad Givens ( Real )", "Haiti", "a piece of foam insulation broke off from the Space Shuttle external tank and struck the left wing of the orbiter", "October 14, 2017", "2026", "2018", "The Outback", "Deputy Speaker of the Lok Sabha or in his absence, the Deputy - Chairman of the Rajya Sabha", "2004", "The western coast of Australia", "eleven", "Master Christopher Jones", "Around 1200", "12.65 m ( 41.50 ft ) long", "RMS Titanic ( / ta\u026a\u02c8t\u00e6n\u026ak / )", "2017", "in the North Cascades range of, Washington", "2011", "Italy", "music (to be performed) in a fiery manner", "Culture Club", "These Are Special Times", "Indian", "Sofia the First", "software magnate", "Hurricane Gustav", "21", "Joseph Heller", "drake", "uranium"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5506549325514842}, "metric_results_detailed": {"EM": [false, false, true, true, true, false, true, true, false, false, true, false, true, false, true, false, false, true, false, false, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, true, true, false, true, true, false, true, false, true, true, false, false, false, false, false, true, false, false, true, false, false, true, true, true, false, false, false, true], "QA-F1": [0.5925925925925926, 0.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 1.0, 0.0, 0.14285714285714285, 1.0, 0.9523809523809523, 1.0, 0.0, 1.0, 0.0, 0.3448275862068965, 1.0, 0.0, 0.0, 0.45454545454545453, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.15384615384615385, 0.33333333333333337, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.28571428571428575, 0.8, 0.0, 0.6, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-5270", "mrqa_squad-validation-8964", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-1455", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-8186", "mrqa_naturalquestions-validation-8004", "mrqa_naturalquestions-validation-4097", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-6149", "mrqa_naturalquestions-validation-1103", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-9867", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-7750", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-2781", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-7128", "mrqa_hotpotqa-validation-3423", "mrqa_hotpotqa-validation-3842", "mrqa_newsqa-validation-1234", "mrqa_searchqa-validation-3483", "mrqa_searchqa-validation-9088"], "SR": 0.4375, "CSR": 0.5631009615384616, "EFR": 0.9722222222222222, "Overall": 0.7279240117521368}, {"timecode": 26, "before_eval_results": {"predictions": ["Soviet", "Thomas Commerford Martin", "seven", "labor inputs", "1996", "a mental disorder characterized by at least two weeks of low mood that is present across most situations", "Indraprastha", "John Quincy Adams", "third season", "Christopher Columbus", "trying to fit in", "an intensive week - long initiation process in which the teaching of the ritual skills and moral behavior occurs informally and nonverbally", "The United States Secretary of State", "administrative supervision over all courts and the personnel thereof ''", "in a Saiyan's \u014czaru ( \u5927 \u733f, lit. `` Great Ape '' ) form, he made the hair more `` wild '' and covered Frieza's body in red fur", "Mandy", "King Harold Godwinson", "during meiosis", "the Miracles", "Spanish / Basque origin", "cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep", "a relationship", "January 2, 1971", "Hirschman", "in the books of Exodus and Deuteronomy", "Lucknow", "Neuropsychology", "The User State Migration Tool", "Effy", "May 1, 2018", "291 episodes in Japan", "the Naturalization Act of 1790", "flawed democracy", "Pashto", "last Ice Age", "Ren\u00e9 Descartes", "in people and animals that collects and stores urine from the kidneys before disposal by urination", "Confederate", "Tumble", "Revenge of the Wars", "Germany", "London, United Kingdom", "Jeff East", "President Yahya Khan", "vaskania", "6th century AD", "Arnold Schoenberg", "~ 3.5 million years old from Idaho, USA", "111", "when a population temporarily exceeds the long term carrying capacity of its environment", "Mike Leeson and Peter Vale", "a duty to the deceased to take care", "Joan Crawford", "Andorra", "Ned Sherrin", "Band-e Amir National Park", "Cartoon Network", "Chrysler", "Kabul in the eastern Afghan province of Logar,", "more than 100", "to comfort those in mourning, to offer healing and \"the blessing of your voice, your chaste touch.\"", "Roanoke", "with the New York City Ballet", "apples"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6878030823343324}, "metric_results_detailed": {"EM": [false, true, true, true, false, true, false, true, true, true, false, false, false, true, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, false, true, true, false, false, true, true, true, true, true, true, false, true, false, true, false, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6153846153846153, 0.9, 0.6666666666666665, 1.0, 0.0, 1.0, 0.0, 0.5714285714285715, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4, 0.28571428571428575, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.8, 0.6666666666666666, 0.3333333333333333, 1.0, 0.7272727272727273, 1.0, 1.0, 0.5714285714285715, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30769230769230765, 1.0, 0.4, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3771", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-9812", "mrqa_naturalquestions-validation-6224", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-10218", "mrqa_naturalquestions-validation-1052", "mrqa_naturalquestions-validation-5938", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9675", "mrqa_naturalquestions-validation-2206", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-5960", "mrqa_naturalquestions-validation-9568", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-3422", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-5579", "mrqa_naturalquestions-validation-2151", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-4399", "mrqa_naturalquestions-validation-1694", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-3089", "mrqa_searchqa-validation-7787"], "SR": 0.515625, "CSR": 0.5613425925925926, "EFR": 0.967741935483871, "Overall": 0.7266762806152928}, {"timecode": 27, "before_eval_results": {"predictions": ["eight", "after sustaining an injury which would be fatal to most other species", "the Western Atlantic ctenophore Mnemiopsis leidyi was accidentally introduced into the Black Sea and Sea of Azov via the ballast tanks of ships,", "chloroplast", "Naples", "black, red or white,", "2009", "cowardly lion", "Diego Maradona", "Harkat-ul-Jihad al-Islami ( HuJi)", "\"I'm certainly not nearly as good of a speaker as he is.\"", "\"Sesame Street\"", "golf", "AOL Autos", "Floxin", "Copts", "February 12", "Roberto Micheletti,", "in the last few months,", "Shiza Shahid,", "the chief executive officer, the one on the very top,", "two awards.", "African-Americans", "environmental and political events", "\"I'm going to deny that motion,\"", "three empty vodka bottles,", "Euna Lee,", "Angela Merkel", "the Ku Klux Klan", "some dental work done, including removal of his diamond-studded teeth.", "Manuel Mejia Munera", "requires police to question people if there's reason to suspect they're in the United States illegally.", "UNICEF", "club managers,", "used", "\"I've got a good group of Marines that are behind me, so I'm real excited about the deployment,\"", "\"I'm certainly not nearly as good of a speaker as he is.\"", "suicides", "$15 billion in 2008 and is projected to grow by 10 percent, according to PricewaterhouseCoopers.", "Karen Floyd", "Leo Frank,", "a skilled hacker could disrupt the system and cause a blackout.", "Cogentin and Haldol,", "$2 billion", "Dan Parris, 25, and Rob Lehr, 26,", "Krishna Rajaram,", "flooding was so fast that the thing flipped over,\"", "the Scudetto", "Dubai", "up three of the last four months.", "Sunday,", "12-hour-plus", "the Eagles have appeared in the Super Bowl three times, losing in their first two appearances but winning the third, in 2018", "Canada", "Iowa", "Homo sapiens", "potash", "Something In The Air,", "March 1987", "Peter Kay's Car Share", "Cyclic Defrost", "pfeffernuesse", "school", "14"], "metric_results": {"EM": 0.484375, "QA-F1": 0.555286337014912}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, true, false, true, false, false, false, true, false, true, false, true, true, false, true, false, false, true, true, false, true, true, true, true, false, false, false, true, true, false, false, false, true, false, true, false, false, false, true, false, true, false, false, true, false, true, true, false, true, true, true, false, false, false, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.5714285714285715, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.06896551724137931, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.42857142857142855, 0.07692307692307691, 1.0, 1.0, 0.0, 0.0, 0.19999999999999998, 1.0, 0.23529411764705882, 1.0, 0.0, 0.4, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 0.0, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-4648", "mrqa_squad-validation-8704", "mrqa_newsqa-validation-2646", "mrqa_newsqa-validation-23", "mrqa_newsqa-validation-858", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-2326", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1202", "mrqa_newsqa-validation-223", "mrqa_newsqa-validation-873", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2325", "mrqa_newsqa-validation-1878", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-2298", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-2754", "mrqa_newsqa-validation-2456", "mrqa_naturalquestions-validation-6991", "mrqa_triviaqa-validation-7394", "mrqa_triviaqa-validation-5378", "mrqa_hotpotqa-validation-1223", "mrqa_searchqa-validation-2197"], "SR": 0.484375, "CSR": 0.55859375, "EFR": 1.0, "Overall": 0.7325781250000001}, {"timecode": 28, "before_eval_results": {"predictions": ["deserts", "political parties", "flew the first Earth orbital test mission Apollo 7,", "William Goldman", "Cherokee Nation", "Flamingo Las Vegas", "Gran Sasso d'Italia,", "Bantu", "2013", "Luis Edgardo Resto", "the Salzburg Festival", "Jay Park", "Blackpool Football Club", "New Orleans Saints", "2012", "\"Charmed\"", "Dundalk, County Louth, Ireland", "Ashley Jensen", "Syracuse University", "Dame Eileen June Atkins, DBE", "Mollie Elizabeth King", "Esteban Ocon", "the flags of dependent territories", "Ouse and Foss", "Emilia-Romagna Region in Northern Italy", "Casablanca", "The Go-Go's", "1943", "Sleepy Hollow", "Ronnie Schell", "Wandsworth, London", "Christopher Lloyd Smalling (born 22 November 1989)", "Chevron Corporation", "World Music Awards", "La Liga", "Australian", "Floyd Nathaniel \"Nate\" Hills", "Fort Hood, Texas", "Michael Phelps", "Lauren Alaina", "February 21, 1961", "Droga5", "Preston, Lancashire, UK", "Prudential Center in Newark, New Jersey", "\"The Clash of Triton\"", "the Mach number (M or Ma)", "1945 to 1951", "Mexico", "Chevy", "wooden roller coaster", "Disco", "Theodor W. Adorno", "re-education", "Sir Rowland Hill", "A complex sentence", "Jimmy Carter", "John Logie Baird", "25 September", "Joan Rivers", "\"She was briefly hospitalized this summer for \"scheduled testing,\"", "auction off one of the earliest versions of the Magna Carta later this year,", "the Wall of the Moon", "Windows 7,", "kids"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6579117063492064}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, false, true, false, true, true, true, true, true, true, false, true, true, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.4, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.8571428571428571, 0.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2791", "mrqa_squad-validation-4060", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-4673", "mrqa_hotpotqa-validation-2699", "mrqa_hotpotqa-validation-1997", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-3833", "mrqa_hotpotqa-validation-1782", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-3787", "mrqa_hotpotqa-validation-5493", "mrqa_hotpotqa-validation-1926", "mrqa_hotpotqa-validation-548", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-3821", "mrqa_hotpotqa-validation-4102", "mrqa_hotpotqa-validation-1593", "mrqa_hotpotqa-validation-1667", "mrqa_triviaqa-validation-4504", "mrqa_newsqa-validation-2548", "mrqa_newsqa-validation-2606", "mrqa_searchqa-validation-9671", "mrqa_searchqa-validation-14497"], "SR": 0.5625, "CSR": 0.5587284482758621, "EFR": 0.9642857142857143, "Overall": 0.7254622075123154}, {"timecode": 29, "before_eval_results": {"predictions": ["some teachers and parents", "July 1969", "a glaucophyte", "God", "Mercury Records", "Evgeni Platov", "Nye County", "in the series \"Runaways\"", "\"Supergirl\"", "ten", "White Knights of the Ku Klux Klan", "the Chechen Republic", "The Riddler's Revenge", "Arrowhead Stadium", "March 16, 1927", "English", "Food and Agriculture Organization", "Dallas", "Jeff Meldrum", "Crossed: Dead or Alive", "Romance language", "Philip K. Dick", "over 80%", "English", "Cartoon Network Too", "David Starkey", "Cherokee River", "pop music and popular culture", "Field Marshal Lord Gort", "Hopeless Records", "Razor Ramon", "Godspell", "8 August 1907", "Donald J. Trump's", "7.63\u00d725mm Mauser", "Bangkok", "51st", "his exploration and settlement", "August 28, 1774", "Afghanistan", "British", "Potomac River", "the Netherlands", "Love the Way You Lie", "Rio Gavin Ferdinand", "Boston", "Las Vegas", "actor, producer, and director", "Rockbridge County", "St. Louis, Missouri", "Tsung-Dao Lee", "Bay Ridge, Brooklyn", "Human anatomy", "Lydia", "The crossing of Highway 68 ( Holman Highway / Sunset Drive )", "AFC Wimbledon", "The Duke of Plaza Toro", "2", "Meira Kumar", "The U.S. Food and Drug Administration", "bartering", "a beetle", "a malted", "Iceland"], "metric_results": {"EM": 0.625, "QA-F1": 0.7000525210084033}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, true, false, false, true, false, true, true, false, false, false, true, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7499999999999999, 1.0, 1.0, 0.33333333333333337, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.11764705882352941, 1.0, 0.28571428571428575, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5717", "mrqa_hotpotqa-validation-2088", "mrqa_hotpotqa-validation-3749", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5167", "mrqa_hotpotqa-validation-4508", "mrqa_hotpotqa-validation-4023", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-992", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-2600", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-5306", "mrqa_naturalquestions-validation-9660", "mrqa_naturalquestions-validation-3362", "mrqa_triviaqa-validation-6131", "mrqa_triviaqa-validation-4462", "mrqa_searchqa-validation-11933"], "SR": 0.625, "CSR": 0.5609375, "EFR": 1.0, "Overall": 0.7330468750000001}, {"timecode": 30, "UKR": 0.70703125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1052", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1079", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1268", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1313", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1411", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1430", "mrqa_hotpotqa-validation-1437", "mrqa_hotpotqa-validation-1524", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1541", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-1825", "mrqa_hotpotqa-validation-1920", "mrqa_hotpotqa-validation-1941", "mrqa_hotpotqa-validation-208", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2258", "mrqa_hotpotqa-validation-2314", "mrqa_hotpotqa-validation-2398", "mrqa_hotpotqa-validation-2423", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2702", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2823", "mrqa_hotpotqa-validation-2847", "mrqa_hotpotqa-validation-2861", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2955", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-303", "mrqa_hotpotqa-validation-3095", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3136", "mrqa_hotpotqa-validation-3382", "mrqa_hotpotqa-validation-3388", "mrqa_hotpotqa-validation-3401", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-363", "mrqa_hotpotqa-validation-3671", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3710", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3789", "mrqa_hotpotqa-validation-3906", "mrqa_hotpotqa-validation-3921", "mrqa_hotpotqa-validation-3950", "mrqa_hotpotqa-validation-4014", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4058", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4097", "mrqa_hotpotqa-validation-4301", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4334", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4575", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-4687", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4888", "mrqa_hotpotqa-validation-4953", "mrqa_hotpotqa-validation-498", "mrqa_hotpotqa-validation-5024", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5074", "mrqa_hotpotqa-validation-5141", "mrqa_hotpotqa-validation-5197", "mrqa_hotpotqa-validation-5206", "mrqa_hotpotqa-validation-5215", "mrqa_hotpotqa-validation-5313", "mrqa_hotpotqa-validation-5314", "mrqa_hotpotqa-validation-5465", "mrqa_hotpotqa-validation-5499", "mrqa_hotpotqa-validation-5551", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5559", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-5870", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-646", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-719", "mrqa_hotpotqa-validation-872", "mrqa_hotpotqa-validation-932", "mrqa_hotpotqa-validation-959", "mrqa_hotpotqa-validation-999", "mrqa_naturalquestions-validation-1001", "mrqa_naturalquestions-validation-1015", "mrqa_naturalquestions-validation-10307", "mrqa_naturalquestions-validation-10421", "mrqa_naturalquestions-validation-10461", "mrqa_naturalquestions-validation-10604", "mrqa_naturalquestions-validation-1068", "mrqa_naturalquestions-validation-1169", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1300", "mrqa_naturalquestions-validation-1528", "mrqa_naturalquestions-validation-1640", "mrqa_naturalquestions-validation-1694", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1850", "mrqa_naturalquestions-validation-1863", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2067", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-287", "mrqa_naturalquestions-validation-3041", "mrqa_naturalquestions-validation-3297", "mrqa_naturalquestions-validation-3319", "mrqa_naturalquestions-validation-3362", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3483", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3505", "mrqa_naturalquestions-validation-3569", "mrqa_naturalquestions-validation-3677", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-4005", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4432", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4880", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-508", "mrqa_naturalquestions-validation-5282", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-5556", "mrqa_naturalquestions-validation-5596", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-564", "mrqa_naturalquestions-validation-5984", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6264", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-643", "mrqa_naturalquestions-validation-6500", "mrqa_naturalquestions-validation-6555", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-672", "mrqa_naturalquestions-validation-7004", "mrqa_naturalquestions-validation-7020", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7124", "mrqa_naturalquestions-validation-7309", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7844", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8075", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-8277", "mrqa_naturalquestions-validation-8433", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8728", "mrqa_naturalquestions-validation-8766", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8857", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9155", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-951", "mrqa_naturalquestions-validation-954", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9761", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9809", "mrqa_naturalquestions-validation-9867", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1104", "mrqa_newsqa-validation-1160", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1175", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-1299", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1339", "mrqa_newsqa-validation-1372", "mrqa_newsqa-validation-1385", "mrqa_newsqa-validation-1486", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1582", "mrqa_newsqa-validation-1615", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-1900", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-197", "mrqa_newsqa-validation-1989", "mrqa_newsqa-validation-2128", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2436", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2461", "mrqa_newsqa-validation-2575", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2590", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2792", "mrqa_newsqa-validation-2809", "mrqa_newsqa-validation-2815", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2964", "mrqa_newsqa-validation-2983", "mrqa_newsqa-validation-3089", "mrqa_newsqa-validation-3101", "mrqa_newsqa-validation-3105", "mrqa_newsqa-validation-3243", "mrqa_newsqa-validation-3390", "mrqa_newsqa-validation-3451", "mrqa_newsqa-validation-3604", "mrqa_newsqa-validation-3608", "mrqa_newsqa-validation-3631", "mrqa_newsqa-validation-364", "mrqa_newsqa-validation-3678", "mrqa_newsqa-validation-3741", "mrqa_newsqa-validation-3760", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3979", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4122", "mrqa_newsqa-validation-4179", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-477", "mrqa_newsqa-validation-538", "mrqa_newsqa-validation-562", "mrqa_newsqa-validation-591", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-719", "mrqa_newsqa-validation-818", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-885", "mrqa_newsqa-validation-958", "mrqa_searchqa-validation-10108", "mrqa_searchqa-validation-10732", "mrqa_searchqa-validation-11595", "mrqa_searchqa-validation-11633", "mrqa_searchqa-validation-11634", "mrqa_searchqa-validation-11855", "mrqa_searchqa-validation-11985", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-1228", "mrqa_searchqa-validation-12744", "mrqa_searchqa-validation-13337", "mrqa_searchqa-validation-13489", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14046", "mrqa_searchqa-validation-14058", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14224", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14493", "mrqa_searchqa-validation-14936", "mrqa_searchqa-validation-15779", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1828", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2516", "mrqa_searchqa-validation-2580", "mrqa_searchqa-validation-2726", "mrqa_searchqa-validation-2994", "mrqa_searchqa-validation-3269", "mrqa_searchqa-validation-3966", "mrqa_searchqa-validation-4544", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5607", "mrqa_searchqa-validation-576", "mrqa_searchqa-validation-6523", "mrqa_searchqa-validation-7116", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7891", "mrqa_searchqa-validation-8270", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8667", "mrqa_searchqa-validation-8713", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-9108", "mrqa_searchqa-validation-9315", "mrqa_searchqa-validation-9354", "mrqa_searchqa-validation-9491", "mrqa_searchqa-validation-9613", "mrqa_squad-validation-0", "mrqa_squad-validation-100", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10260", "mrqa_squad-validation-10326", "mrqa_squad-validation-10340", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1222", "mrqa_squad-validation-1237", "mrqa_squad-validation-129", "mrqa_squad-validation-1379", "mrqa_squad-validation-1384", "mrqa_squad-validation-1408", "mrqa_squad-validation-1419", "mrqa_squad-validation-1424", "mrqa_squad-validation-1546", "mrqa_squad-validation-1592", "mrqa_squad-validation-16", "mrqa_squad-validation-1600", "mrqa_squad-validation-1751", "mrqa_squad-validation-1819", "mrqa_squad-validation-1908", "mrqa_squad-validation-1943", "mrqa_squad-validation-1976", "mrqa_squad-validation-2025", "mrqa_squad-validation-2106", "mrqa_squad-validation-2122", "mrqa_squad-validation-2250", "mrqa_squad-validation-2419", "mrqa_squad-validation-2447", "mrqa_squad-validation-2468", "mrqa_squad-validation-2491", "mrqa_squad-validation-2579", "mrqa_squad-validation-2640", "mrqa_squad-validation-2668", "mrqa_squad-validation-2677", "mrqa_squad-validation-2755", "mrqa_squad-validation-2768", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-2797", "mrqa_squad-validation-2819", "mrqa_squad-validation-282", "mrqa_squad-validation-2848", "mrqa_squad-validation-2923", "mrqa_squad-validation-2997", "mrqa_squad-validation-3001", "mrqa_squad-validation-3103", "mrqa_squad-validation-3229", "mrqa_squad-validation-3262", "mrqa_squad-validation-327", "mrqa_squad-validation-3449", "mrqa_squad-validation-3571", "mrqa_squad-validation-3578", "mrqa_squad-validation-3664", "mrqa_squad-validation-3680", "mrqa_squad-validation-4065", "mrqa_squad-validation-4132", "mrqa_squad-validation-4159", "mrqa_squad-validation-4216", "mrqa_squad-validation-4248", "mrqa_squad-validation-4274", "mrqa_squad-validation-4472", "mrqa_squad-validation-4488", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4698", "mrqa_squad-validation-4736", "mrqa_squad-validation-4765", "mrqa_squad-validation-4772", "mrqa_squad-validation-4789", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4874", "mrqa_squad-validation-4878", "mrqa_squad-validation-4907", "mrqa_squad-validation-4960", "mrqa_squad-validation-5157", "mrqa_squad-validation-5173", "mrqa_squad-validation-5270", "mrqa_squad-validation-5295", "mrqa_squad-validation-5296", "mrqa_squad-validation-5343", "mrqa_squad-validation-541", "mrqa_squad-validation-5478", "mrqa_squad-validation-55", "mrqa_squad-validation-5796", "mrqa_squad-validation-5835", "mrqa_squad-validation-5881", "mrqa_squad-validation-5908", "mrqa_squad-validation-5999", "mrqa_squad-validation-6079", "mrqa_squad-validation-6157", "mrqa_squad-validation-6158", "mrqa_squad-validation-6206", "mrqa_squad-validation-6233", "mrqa_squad-validation-6247", "mrqa_squad-validation-6248", "mrqa_squad-validation-6267", "mrqa_squad-validation-6350", "mrqa_squad-validation-6382", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-6511", "mrqa_squad-validation-6579", "mrqa_squad-validation-6589", "mrqa_squad-validation-66", "mrqa_squad-validation-6702", "mrqa_squad-validation-6720", "mrqa_squad-validation-6914", "mrqa_squad-validation-701", "mrqa_squad-validation-7021", "mrqa_squad-validation-7028", "mrqa_squad-validation-7043", "mrqa_squad-validation-7152", "mrqa_squad-validation-7184", "mrqa_squad-validation-7217", "mrqa_squad-validation-7395", "mrqa_squad-validation-7540", "mrqa_squad-validation-7564", "mrqa_squad-validation-7653", "mrqa_squad-validation-7733", "mrqa_squad-validation-7775", "mrqa_squad-validation-7850", "mrqa_squad-validation-7851", "mrqa_squad-validation-7869", "mrqa_squad-validation-7937", "mrqa_squad-validation-8010", "mrqa_squad-validation-8023", "mrqa_squad-validation-826", "mrqa_squad-validation-8298", "mrqa_squad-validation-8312", "mrqa_squad-validation-8326", "mrqa_squad-validation-8332", "mrqa_squad-validation-8370", "mrqa_squad-validation-8466", "mrqa_squad-validation-8487", "mrqa_squad-validation-85", "mrqa_squad-validation-8576", "mrqa_squad-validation-8612", "mrqa_squad-validation-8665", "mrqa_squad-validation-887", "mrqa_squad-validation-8952", "mrqa_squad-validation-8986", "mrqa_squad-validation-8986", "mrqa_squad-validation-9093", "mrqa_squad-validation-9162", "mrqa_squad-validation-9199", "mrqa_squad-validation-9308", "mrqa_squad-validation-9499", "mrqa_squad-validation-9594", "mrqa_squad-validation-9638", "mrqa_squad-validation-9918", "mrqa_squad-validation-9925", "mrqa_squad-validation-9975", "mrqa_triviaqa-validation-1206", "mrqa_triviaqa-validation-1212", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2121", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2221", "mrqa_triviaqa-validation-2369", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3246", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3634", "mrqa_triviaqa-validation-3827", "mrqa_triviaqa-validation-4097", "mrqa_triviaqa-validation-4191", "mrqa_triviaqa-validation-4504", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4957", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5873", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-6014", "mrqa_triviaqa-validation-6173", "mrqa_triviaqa-validation-6227", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6727", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7388", "mrqa_triviaqa-validation-7452", "mrqa_triviaqa-validation-7711", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-88"], "OKR": 0.818359375, "KG": 0.45, "before_eval_results": {"predictions": ["the State Department", "immediately", "a second Gleichschaltung", "the International Hotel", "the Recording Industry Association of America", "between 7,500 and 40,000 words", "mountaineer", "Belgian", "Eve Hewson", "\"Slaughterhouse-Five\"", "the Royal Automobile Club's Tourist Trophy", "William Jefferson Clinton (born William Jefferson Blythe III; August 19, 1946) is an American politician who served as the 42nd President of the United States from 1993 to 2001.", "Oldham County", "sandstone", "Channel 4", "Christmas Day, December 25, 2009", "Father Austin Purcell in \" Think Fast, Father Ted\"", "punk rock", "the Lufthansa heist", "Lord Byron", "Laura Elizabeth Dern", "Carrefour", "American burlesque", "Venancio Flores", "Forever Living Products", "FBI", "The Saturdays", "Indianapolis", "French", "1968", "Edinburgh", "Charles Bronson", "Oklahoma Sooners", "Orson Welles", "Sharyn McCrumb", "Robert Digges Wimberly Connor (September 26, 1878 \u2013 February 25, 1950) was an American historian and the first Archivist of the United States, 1934-1941", "1.23 million", "Ford", "J. K. Rowling", "University of Kentucky College of Pharmacy", "Kiyone Kotetsu in \"Bleach\", Zola in the \"Blue Dragon\" series", "England", "January 28, 2016", "Martin Scorsese", "1979", "John Malkovich", "Merrimack County", "RAF Tangmere, West Sussex", "\"Brotherly Leader\"", "Suicide Kings", "North Kesteven, Lincolnshire, England", "A stolperstein", "Earl ( John Doe )", "Montgomery", "Bart Howard", "South Park", "a pest", "Andre Agassi", "10 below", "Asashoryu", "heavy turbulence", "gary pizzarelli", "gary", "The Secret"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6518801702368513}, "metric_results_detailed": {"EM": [true, true, true, false, true, false, true, true, true, true, false, false, false, true, true, false, false, false, false, true, false, true, true, false, false, false, true, true, false, true, true, true, true, true, false, false, false, false, true, false, false, true, true, true, true, false, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 1.0, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.0, 0.20689655172413793, 0.8, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.32, 0.0, 0.5, 1.0, 0.7272727272727272, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5494", "mrqa_hotpotqa-validation-5496", "mrqa_hotpotqa-validation-2473", "mrqa_hotpotqa-validation-61", "mrqa_hotpotqa-validation-1588", "mrqa_hotpotqa-validation-4546", "mrqa_hotpotqa-validation-3846", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-3367", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-4470", "mrqa_hotpotqa-validation-2837", "mrqa_hotpotqa-validation-4950", "mrqa_hotpotqa-validation-5674", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-1682", "mrqa_hotpotqa-validation-2626", "mrqa_hotpotqa-validation-2743", "mrqa_hotpotqa-validation-5278", "mrqa_hotpotqa-validation-1310", "mrqa_hotpotqa-validation-2268", "mrqa_hotpotqa-validation-3282", "mrqa_hotpotqa-validation-3442", "mrqa_naturalquestions-validation-7912", "mrqa_triviaqa-validation-7167", "mrqa_newsqa-validation-1077", "mrqa_searchqa-validation-12237", "mrqa_searchqa-validation-8942"], "SR": 0.546875, "CSR": 0.560483870967742, "EFR": 1.0, "Overall": 0.7071748991935485}, {"timecode": 31, "before_eval_results": {"predictions": ["the Pac-12 Conference", "1985", "Royal Lieutenant of Ireland", "2006", "25 million", "15", "American composer, conductor, orchestrator and musician", "The Bye Bye Man", "Draco Gingold", "GameStop Corp.", "Fort Albany", "Thorgan Ganael Francis Hazard", "Robert Marvin \"Bobby\" Hull, OC", "Love Actually", "Larnelle Steward Harris", "Queensland", "Southbank", "the Commanding General", "1976", "Sean Yseult", "1998", "Benjamin Andrew \" Ben\" Stokes", "newspapers, television, radio, cable television, and other businesses", "Joseph Ruttenberg", "Royal Navy", "The Land of Enchantment", "$10\u201320 million", "the Sydney central business district in the local government area of City of Blacktown", "Formula E", "many artists' lofts and art galleries", "Province of Canterbury", "the Anhaltisches Theater", "Alemannic", "1932", "128", "Telugu", "1937", "Windermere Hotel", "Curtis Martin", "Marco Fu Ka-chun", "a 2003 South Korean horror film", "Isabella (Belle) Baumfree", "Kate Millett", "the American comedy-drama series \"Gilmore Girls\"", "Premier League", "Aqua", "St. Louis, Missouri", "Dan Castellaneta", "Bury St Edmunds, Suffolk", "Philip K. Dick", "Labour Party", "Bury F.C.", "Toby Keith", "Rigg", "August 2, 1990", "Kenya", "Macbeth", "Sir William Hamilton", "Las Vegas", "\"Larry King Live\"", "\"unnamed international terror group\"", "the Church of Christ, Scientist", "Ronald Reagan Presidential Library", "East Germany"], "metric_results": {"EM": 0.546875, "QA-F1": 0.7149191086691087}, "metric_results_detailed": {"EM": [false, true, false, true, true, true, false, true, false, true, true, true, false, true, false, true, true, true, true, true, false, false, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, false, false, false, true, false, false, true], "QA-F1": [0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.15384615384615385, 1.0, 0.8, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.7692307692307693, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 0.8, 0.888888888888889, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4, 0.8571428571428571, 1.0, 0.6666666666666666, 0.3333333333333333, 0.5, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 1.0, 0.4, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2793", "mrqa_hotpotqa-validation-4613", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-599", "mrqa_hotpotqa-validation-96", "mrqa_hotpotqa-validation-4113", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-4855", "mrqa_hotpotqa-validation-5224", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-2960", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-1680", "mrqa_hotpotqa-validation-3815", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-3798", "mrqa_hotpotqa-validation-3777", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-5482", "mrqa_naturalquestions-validation-8446", "mrqa_triviaqa-validation-2828", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-4128", "mrqa_searchqa-validation-1275", "mrqa_searchqa-validation-1396"], "SR": 0.546875, "CSR": 0.56005859375, "EFR": 1.0, "Overall": 0.7070898437500001}, {"timecode": 32, "before_eval_results": {"predictions": ["Licensed Local Pastor", "a power outage", "13", "Hebrew", "Blenheim Palace", "David chose five smooth stones", "Edith Louisa Cavell", "County of Cotentin", "De Lorean DMC-12", "Soviet Union and the United States", "Action Comics", "Queen Elizabeth II", "William Shakespeare's The Merchant of Venice", "Northwestern University", "curling", "Cole Porter", "Colorado", "Google", "Aviva plc", "oil", "Project Gutenberg", "surf", "Dr John Sentamu", "Kiel Canal", "General Sir Herbert Kitchener", "Cevennes", "eggs Benedict", "Luigi Pirandello", "Sheffield United", "R. White's Lemonade", "Flatiron District", "Eddy Shah", "Hugh Laurie", "a cappella", "Netherlands", "New Kids On The Block", "Red squirrels", "near the port city of Karachi", "Adam Smith", "Model T", "Spice Girls", "Brian Blessed", "Michael Caine", "Sebastian Beach", "pig", "Bank of England", "Isaac Newton", "Brian Wilson's", "the monarch", "Bangladesh", "St Clements", "Castor", "Nalini Negi", "his last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "Germany", "2008 to 2010", "University of Mississippi", "The Rebirth", "\"Theoneste Bagosora, 67,", "Apple", "Akio Toyoda", "Hamlet", "a reddish-orange nose", "chicken Kiev"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6245907738095238}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, false, false, false, true, true, false, false, false, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.7499999999999999, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4572", "mrqa_triviaqa-validation-2721", "mrqa_triviaqa-validation-958", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-3902", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4709", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-2333", "mrqa_triviaqa-validation-601", "mrqa_triviaqa-validation-2039", "mrqa_triviaqa-validation-6905", "mrqa_triviaqa-validation-178", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-6366", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-907", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-1538", "mrqa_hotpotqa-validation-217", "mrqa_newsqa-validation-408", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-250", "mrqa_searchqa-validation-16717", "mrqa_searchqa-validation-10619"], "SR": 0.5625, "CSR": 0.5601325757575757, "EFR": 1.0, "Overall": 0.7071046401515152}, {"timecode": 33, "before_eval_results": {"predictions": ["Administrator Webb", "Duval", "Atlantic", "Richard Branson", "ohm", "tibet", "yorkshire", "1720", "king Tutankhamun", "Morgan Spurlock", "iris", "Massachusetts", "elff Graff", "Jane Austen", "Dutch", "Bruce", "iridescent nacre", "yellow", "tbilisi", "Mrs Merton", "cricketer", "Wyoming", "Catherine Cookson", "Hugh Quarshie", "Bud Flanagan", "elvis preley", "lord sugar", "9", "Henri Paul", "Red Sea", "Helen Gurley Brown", "Wash", "salt", "bar\u00e7a", "Mark Carney", "Cassidy", "dijon", "Utah", "Toy Story", "lord", "Italy", "lord Nelson", "George Osborne", "1982", "Apollo", "Gentlemen Prefer Blondes", "elvis", "Harry Shearer", "Paul Gauguin", "tibet", "hydrogen", "Demi Moore", "from 28 July 1914 to 11 November 1918", "butch or Killer", "between 2004 and 2007", "Ellie Kemper", "7 June 1954", "Oryzomyini", "Ryder Russell", "NATO's Membership Action Plan,", "The Washington Post", "manhattan", "yorkshire", "Pearl Jam"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5953124999999999}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, false, false, true, false, true, false, false, true, true, false, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, false, false, true, false, true, true, true, false, true, false, true, true, true, true, false, true, false, false, false, true, false, false, false, true, false, true, true, false, true, false, false, true], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.9333333333333333, 0.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3929", "mrqa_triviaqa-validation-2523", "mrqa_triviaqa-validation-4196", "mrqa_triviaqa-validation-2483", "mrqa_triviaqa-validation-1429", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-3919", "mrqa_triviaqa-validation-4304", "mrqa_triviaqa-validation-3327", "mrqa_triviaqa-validation-5307", "mrqa_triviaqa-validation-3127", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-6355", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-5636", "mrqa_triviaqa-validation-3245", "mrqa_triviaqa-validation-5934", "mrqa_triviaqa-validation-284", "mrqa_triviaqa-validation-1375", "mrqa_triviaqa-validation-3888", "mrqa_triviaqa-validation-7345", "mrqa_triviaqa-validation-5950", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-8908", "mrqa_hotpotqa-validation-5041", "mrqa_newsqa-validation-852", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-15915"], "SR": 0.53125, "CSR": 0.5592830882352942, "EFR": 0.9666666666666667, "Overall": 0.7002680759803922}, {"timecode": 34, "before_eval_results": {"predictions": ["San Diego\u2013Tijuana", "petroleum", "Pepsi-Cola", "nonesuch", "Dan", "if", "GIGO", "pawn", "three's Company", "silk", "bamboo", "Arthur C. Clarke", "rice", "zorro", "people who didn't fly airplanes", "lloyd smith", "scoop", "Led Zeppelin", "Alderney", "Charles Lindbergh", "River Phoenix", "eva", "aston villa", "Krntnertor Theatre", "Jason", "if", "the marine", "AbeBooks.com Community Forum", "florence nightingale", "Profiles in Courage", "bogota", "Atonement", "maude", "Glenn smith", "Naples", "aston villa", "coal", "Jean Foucault", "Hanna Glawari", "humerus", "Harriet Tubman", "a horse", "Louisa May Alcott", "scoop", "Hugh Williams", "Margaret Atwood", "cacique", "Khartoum", "Joaquin Phoenix", "Winslow Homer", "Moby Dick", "The Hot Chick", "iOS, watchOS", "on Mars Hill", "Andrew Michael Harrison", "Gwyneth Paltrow", "smeagol", "aromatherapy", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "140 million", "1923", "Saturday", "9-1", "Karen Floyd"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5125}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, true, true, true, true, false, true, true, false, false, false, false, true, false, false, true, false, false, false, true, false, false, false, true, true, false, true, false, false, true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, false, true, true, false, false, false, false, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-2831", "mrqa_searchqa-validation-10168", "mrqa_searchqa-validation-16300", "mrqa_searchqa-validation-6843", "mrqa_searchqa-validation-1570", "mrqa_searchqa-validation-4738", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-4524", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-12487", "mrqa_searchqa-validation-15135", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-2049", "mrqa_searchqa-validation-7837", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-12354", "mrqa_searchqa-validation-15325", "mrqa_searchqa-validation-16288", "mrqa_searchqa-validation-8940", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-1882", "mrqa_searchqa-validation-2976", "mrqa_searchqa-validation-2929", "mrqa_searchqa-validation-1564", "mrqa_searchqa-validation-7832", "mrqa_naturalquestions-validation-2748", "mrqa_naturalquestions-validation-6046", "mrqa_triviaqa-validation-4753", "mrqa_hotpotqa-validation-4810", "mrqa_hotpotqa-validation-30", "mrqa_newsqa-validation-3872", "mrqa_newsqa-validation-2996"], "SR": 0.46875, "CSR": 0.5566964285714286, "EFR": 1.0, "Overall": 0.7064174107142858}, {"timecode": 35, "before_eval_results": {"predictions": ["an arrow", "Chicago Bears", "Floridians", "green and yellow", "Central-Eastern Europe", "Regional Rural Bank", "M2M", "American 3D computer-animated comedy", "extreme nationalist, and nativist ideologies, as well as authoritarian tendencies", "Division of Cook", "July 16, 1971", "13 May 2018", "Kentucky River", "Barbara Niven", "Kramer's caddy Stan", "Yellowcraig", "Salta, Chaco, Santa Fe, C\u00f3rdoba, Catamarca and Tucum\u00e1n", "a super-regional shopping mall", "Messiah Part II", "Abbey Road", "Mel Blanc", "the Czech Kingdom", "March 14, 2000", "Lamar Wyatt", "Alfred Preis", "Terry Malloy", "Alan Ada King-Noel, Countess of Lovelace (\"n\u00e9e\" Byron; 10 December 1815 \u2013 27 November 1852) was an English mathematician", "The interview", "various registries", "20th episode", "Lord Lucan", "January 15, 1975", "Chiwetel Ejiofor", "Appleby-in-Westmorland", "27 November 1956", "Charles de Gaulle Airport", "The St Andrews Agreement", "the Seasiders", "Victorian College of the Arts and Melbourne Conservatorium of Music", "north", "Nick Cassavetes", "Cate Blanchett", "John David Souther", "January 28, 2016", "Hopi", "John Meston", "Romeo Montague", "\u00c6thelred I of Northumbria", "an Albanian political party in Montenegro", "the University of Keele", "Battle of the Rosebud", "Jaffrey, New Hampshire", "1998", "Australia", "Sara Gilbert", "Elkie Brooks", "an abecedarius", "blue", "Asashoryu", "processing data, requiring that all flight-plan information be processed through a facility in Salt Lake City, Utah,", "December 7, 1941", "Peter Pan", "beryl", "Luxor Las Vegas"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6547991071428572}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, false, false, true, true, true, false, false, true, false, true, false, true, false, true, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8571428571428571, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2222222222222222, 1.0, 0.4, 0.2222222222222222, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.25, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2222222222222222, 0.0, 0.0, 0.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-59", "mrqa_hotpotqa-validation-1905", "mrqa_hotpotqa-validation-2673", "mrqa_hotpotqa-validation-5286", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-2921", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3780", "mrqa_hotpotqa-validation-5632", "mrqa_hotpotqa-validation-4319", "mrqa_hotpotqa-validation-3435", "mrqa_hotpotqa-validation-1426", "mrqa_hotpotqa-validation-4231", "mrqa_hotpotqa-validation-5130", "mrqa_hotpotqa-validation-212", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-3644", "mrqa_hotpotqa-validation-3867", "mrqa_naturalquestions-validation-6452", "mrqa_triviaqa-validation-7224", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-1457", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-15743"], "SR": 0.578125, "CSR": 0.5572916666666667, "EFR": 1.0, "Overall": 0.7065364583333335}, {"timecode": 36, "before_eval_results": {"predictions": ["in an H+ or hydrogen ion gradient to generate ATP energy", "the Caesars Palace Grand Prix", "A compact car", "Benjam\u00edn Arellano F\u00e9lix", "Koch Industries", "Enigma", "Yellow fever", "Julia Compton Moore", "Lord's Resistance Army", "the Workers' Party", "Yasiin Bey", "1763\u20131791", "Bulgaria", "(Mark van Bommel)", "George A. Romero's 1978 film", "Oldham County, Kentucky", "The Captain Matchbox Whoopee Band", "Alec Berg", "wild boar, and red, fallow and roe deer", "Ghana", "Nikolai Alexandrovich Morozov", "Rabies", "Switzerland", "Tennessee", "Godiva", "February 5, 2017", "August 1973", "Lawrence of Arabia", "The Ansonia Hotel", "1937", "Government of Ireland", "Leona Lewis", "John Robert Cocker", "$7.3 billion", "Angus Brayshaw", "his most brilliant student.", "the \"Black Abbots\"", "Sarah Kerrigan, the Queen of Blades", "German", "Katy Perry", "Bharat Ratna", "Wilderness Road", "75 mi southeast", "\"Nebo Zovyot\"", "\"Orchard County\"", "The Kree", "more than 110 films", "The shortest player ever to play in the National Basketball Association", "The authorship of Titus Andronicus", "Denmark", "Patricia Arquette", "James Corden", "Audrey II ''", "the end of the 2015 season", "red", "sakyamuni", "Jim Braddock", "his son would get back to his \"German roots.\"", "co-wrote its signature song,\"The Devil Went Down to Georgia.\"", "Brazilian supreme court judge", "the Yankees", "the Washington Redskins", "Coleridge", "h2g2"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5829529539088363}, "metric_results_detailed": {"EM": [false, true, false, false, true, true, false, true, false, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, false, true, true, true, false, false, true, false, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 1.0, 0.4, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 0.18181818181818182, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.11764705882352941, 0.0, 0.8, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.4, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.13333333333333333, 0.19999999999999998, 0.4, 0.5, 0.0, 0.5, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-8903", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-655", "mrqa_hotpotqa-validation-5337", "mrqa_hotpotqa-validation-757", "mrqa_hotpotqa-validation-1873", "mrqa_hotpotqa-validation-4571", "mrqa_hotpotqa-validation-4343", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-1716", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-823", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-987", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-4926", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-667", "mrqa_hotpotqa-validation-3692", "mrqa_hotpotqa-validation-4004", "mrqa_hotpotqa-validation-475", "mrqa_hotpotqa-validation-1872", "mrqa_naturalquestions-validation-7614", "mrqa_triviaqa-validation-4569", "mrqa_newsqa-validation-151", "mrqa_newsqa-validation-3991", "mrqa_newsqa-validation-4184", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-2512", "mrqa_triviaqa-validation-768"], "SR": 0.484375, "CSR": 0.5553209459459459, "EFR": 1.0, "Overall": 0.7061423141891893}, {"timecode": 37, "before_eval_results": {"predictions": ["the plain moraine plateau", "Latin", "leaves", "Nizhny Novgorod", "James Bond", "money", "keeper of the Longstone (Fame Islands) lighthouse", "\"Carlos the Jackal\"", "Australia", "Annelies Marie Frank", "Belgium", "Sufjan Stevens", "the town and castle of Gibraltar", "Benny Hill", "Roddy Doyle", "Kevin Spacey", "Alexandria", "the Republic of Chad", "1215", "the neck", "David Hockney", "Rudyard Kipling", "lactic acid", "Fleet Street", "the Netherlands", "fractal geometry", "the National Geographic channel", "the duck-billed platypus", "Aquaman", "Jean-Paul Sartre", "sense of an Ending", "bugeye", "the Esmeralda's Barn night  club", "Scotch whisky", "Switzerland", "sheep", "trumpet", "Lou Gehrig", "(James Thurber)", "lamas", "Heston Blumenthal", "New York", "U2", "a peasant's wife", "Germany", "Charlie Sheen", "Shirley Caesar", "Canada", "Buster Edwards", "Chief Inspector of Prisons", "Henley Royal Regatta", "Paul Lynde", "Mayor Hudnut", "Cairo, Illinois", "the Lazio region", "Chuck Schumer", "SKUM", "TNT", "HSH Nordbank Arena", "October 2007", "Newellie Tay Chloe Ross", "lanculescu", "the Alaskan Malamute", "pronghorn"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5416666666666667}, "metric_results_detailed": {"EM": [false, false, false, false, true, false, false, false, true, false, true, true, false, true, true, true, true, false, true, true, true, true, true, true, false, true, false, false, true, false, false, false, false, false, true, true, true, true, false, false, true, false, true, false, false, false, false, true, true, true, true, true, false, true, true, false, true, false, false, false, false, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_squad-validation-953", "mrqa_triviaqa-validation-3445", "mrqa_triviaqa-validation-5911", "mrqa_triviaqa-validation-5474", "mrqa_triviaqa-validation-665", "mrqa_triviaqa-validation-7460", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-876", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1647", "mrqa_triviaqa-validation-2340", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3690", "mrqa_triviaqa-validation-6407", "mrqa_triviaqa-validation-6211", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-277", "mrqa_triviaqa-validation-3696", "mrqa_triviaqa-validation-4500", "mrqa_triviaqa-validation-5115", "mrqa_triviaqa-validation-1536", "mrqa_triviaqa-validation-4402", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-3208", "mrqa_triviaqa-validation-7109", "mrqa_naturalquestions-validation-8239", "mrqa_hotpotqa-validation-3529", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-731", "mrqa_searchqa-validation-10306", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-10797", "mrqa_hotpotqa-validation-2366"], "SR": 0.46875, "CSR": 0.5530427631578947, "EFR": 1.0, "Overall": 0.705686677631579}, {"timecode": 38, "before_eval_results": {"predictions": ["rocketry and manned spaceflight", "Jewish", "Peter Yarrow", "Washington", "Zack Snyder", "Mondays", "Cosmopolitan", "Anna Clyne", "Meghan Markle", "terrorist activity", "Commissioner", "August", "Burnley", "Teen Titan Go!", "Evey's mother in the Wachowskis", "Love and Theft", "1978", "SKUM", "Edmonton, Alberta", "Seattle", "The School Boys", "Orchard Central", "Kennedy Center", "commanders of the Great Army", "Environmental Protection Agency", "Humberside", "Diamond Rio", "The Tempest", "Northampton, England", "Mike Greenwell", "2017", "SAS", "polka", "Brian Patrick Friel", "1860", "2004", "Ghanaian national team", "Coronation Street", "Volksb\u00fchne Berlin", "Cold Spring Historic District", "Arctic fox", "Sophie Monk", "The Primettes", "Sunset Publishing Corporation", "Melbourne Storm", "twenty-eighth", "9 November 1967", "Retina", "technical director", "Cincinnati", "Captain B.J. Hunnicutt", "19 June 2018", "naos", "31 - member", "kendo", "Jeffrey Archer", "germany", "little blue booties.", "Majid Movahedi,", "the leader of a drug cartel that set off two grenades during a public celebration in September,", "February 2", "Ninja", "witch", "Mozart"], "metric_results": {"EM": 0.5625, "QA-F1": 0.7151709401709401}, "metric_results_detailed": {"EM": [false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, true, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, true, false, true], "QA-F1": [0.6153846153846153, 0.0, 0.4, 0.5, 1.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.22222222222222224, 0.6666666666666666, 1.0, 0.3333333333333333, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-3812", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-1492", "mrqa_hotpotqa-validation-668", "mrqa_hotpotqa-validation-3421", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-4294", "mrqa_hotpotqa-validation-1298", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-2646", "mrqa_hotpotqa-validation-1088", "mrqa_hotpotqa-validation-5677", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-2868", "mrqa_hotpotqa-validation-3356", "mrqa_hotpotqa-validation-2627", "mrqa_hotpotqa-validation-1300", "mrqa_hotpotqa-validation-2157", "mrqa_hotpotqa-validation-1264", "mrqa_naturalquestions-validation-1533", "mrqa_triviaqa-validation-5545", "mrqa_newsqa-validation-420", "mrqa_newsqa-validation-1646", "mrqa_newsqa-validation-2476", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-3163"], "SR": 0.5625, "CSR": 0.5532852564102564, "EFR": 1.0, "Overall": 0.7057351762820513}, {"timecode": 39, "before_eval_results": {"predictions": ["funding education, sanitation, and traffic control", "Aly Raisman", "2 March 1989", "Seoul, South Korea", "English", "famous Albert Bridge, London", "William Powell \"Bill\"Leary", "Distinguished Service Cross", "Revolver", "Sam Raimi", "\"Martian Manhunter\"", "Eden Valley Railway", "Wolfgang Amadeus Mozart", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Mercer University", "Dame Eileen June Atkins", "June 1800", "\"Little Dixie\"", "1979", "1905", "crafting and voting on legislation, helping to create a state budget, and legislative oversight over state agencies.", "Loughborough University", "Los Angeles", "2.1 million members", "Granada", "Al Horford", "Ade Edmondson", "Mauritian", "Anthony Stephen Burke", "Lorman, Mississippi", "Wilderness Road", "Alfred Edward Housman", "The Killer", "London", "nearly 8 km", "25 October 1921", "\"War & Peace\"", "2016 U.S. Senate election", "Tayeb Salih", "Mickey Gilley's Club", "twin sister of Luke Skywalker", "Eddie Albert", "Akosua Busia", "Gian Carlo Menotti", "Indian origin", "George Draper Dayton House", "Archbishop of Canterbury", "sub-Saharan Africa", "City of Newcastle", "\"cock of the game\"", "Aaliyah Dana Haughton", "Hedwig", "A vanishing point", "16,801", "Lady Gaga", "April", "Georgetown", "The Palm Jumeirah", "consumer confidence", "Japanese Foreign Ministry spokesman Hidenobu Sobashima", "Scarlett Johansson", "Fried Green Tomatoes", "Bon Jovi", "Marcie Blane"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6581439393939394}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, false, false, false, false, false, false, false, true, false, true, true, false, true, true, false, false, false, false, false, false, true, false, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.45454545454545453, 1.0, 1.0, 0.8, 0.0, 0.3333333333333333, 0.0, 0.0, 0.4, 0.6666666666666666, 1.0, 0.4, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666665, 0.0, 0.4, 0.0, 0.4, 0.8, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_squad-validation-7246", "mrqa_hotpotqa-validation-1989", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-4577", "mrqa_hotpotqa-validation-1285", "mrqa_hotpotqa-validation-1610", "mrqa_hotpotqa-validation-2905", "mrqa_hotpotqa-validation-735", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-4351", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2160", "mrqa_hotpotqa-validation-5644", "mrqa_hotpotqa-validation-2127", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-729", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-5790", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-4770", "mrqa_naturalquestions-validation-3427", "mrqa_triviaqa-validation-1074", "mrqa_searchqa-validation-2773"], "SR": 0.515625, "CSR": 0.55234375, "EFR": 1.0, "Overall": 0.705546875}, {"timecode": 40, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1004", "mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1112", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1217", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1325", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1415", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1611", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-182", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-1942", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2076", "mrqa_hotpotqa-validation-21", "mrqa_hotpotqa-validation-217", "mrqa_hotpotqa-validation-222", "mrqa_hotpotqa-validation-2269", "mrqa_hotpotqa-validation-2331", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2477", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2693", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2726", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-3059", "mrqa_hotpotqa-validation-3063", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-3188", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3625", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3737", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4050", "mrqa_hotpotqa-validation-4075", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4648", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5508", "mrqa_hotpotqa-validation-5538", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5681", "mrqa_hotpotqa-validation-569", "mrqa_hotpotqa-validation-5705", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-651", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_hotpotqa-validation-987", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10495", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3412", "mrqa_naturalquestions-validation-3427", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3828", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-4998", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5444", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5767", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6452", "mrqa_naturalquestions-validation-6453", "mrqa_naturalquestions-validation-646", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6843", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7253", "mrqa_naturalquestions-validation-7438", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8251", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8386", "mrqa_naturalquestions-validation-8434", "mrqa_naturalquestions-validation-8607", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9712", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1012", "mrqa_newsqa-validation-1093", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1170", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1281", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1577", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-1971", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2390", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2790", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2879", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3500", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-3772", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-635", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-10259", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-1084", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12714", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-13996", "mrqa_searchqa-validation-14252", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14433", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14791", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16012", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2197", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3163", "mrqa_searchqa-validation-4484", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-5071", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-733", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-789", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-7982", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8602", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9480", "mrqa_squad-validation-10044", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-10326", "mrqa_squad-validation-10425", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1231", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1472", "mrqa_squad-validation-1608", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2006", "mrqa_squad-validation-2036", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-2755", "mrqa_squad-validation-2819", "mrqa_squad-validation-297", "mrqa_squad-validation-3001", "mrqa_squad-validation-3262", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3812", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4078", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-4543", "mrqa_squad-validation-4611", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5079", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5355", "mrqa_squad-validation-5563", "mrqa_squad-validation-5597", "mrqa_squad-validation-5616", "mrqa_squad-validation-5881", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6223", "mrqa_squad-validation-6251", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6696", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7552", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-7952", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8199", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8428", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9768", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1702", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1875", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-244", "mrqa_triviaqa-validation-2573", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3551", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3911", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4496", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4753", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5271", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6517", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6607", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6886", "mrqa_triviaqa-validation-6963", "mrqa_triviaqa-validation-7007", "mrqa_triviaqa-validation-7109", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7167", "mrqa_triviaqa-validation-7223", "mrqa_triviaqa-validation-7296", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.849609375, "KG": 0.50078125, "before_eval_results": {"predictions": ["gilt bronze", "\"The oceans are kind of the last frontier for use and development,\"", "President Obama's", "they are \"still trying to absorb the impact of this week's stunning events,\"", "Pakistan's North West Frontier Province", "$250,000", "\"very diverse\"", "the actor who created one of British television's most surreal thrillers", "Shark River Park", "helicopters and unmanned aerial vehicles", "Mark Sanford", "between 1917 and 1924", "Frank Ricci", "Janet Napolitano", "Masoud Shafiee", "\"utterly baseless\"", "Jacob", "Police", "Microsoft", "lousiana", "prostate cancer", "Eintracht Frankfurt", "Tsvangirai", "the FBI", "GoldenEye", "The Georgia Aquarium", "a dog", "Turkish President Abdullah Gul", "Dame Elizabeth", "Ralph Cifaretto", "not", "may", "17", "a rally", "an empty water bottle", "Val d'Isere, France", "a head injury", "News of the World tabloid", "Swat Valley", "200", "Manny Pacquiao", "1964", "Disney", "Allred", "environmental", "80", "United Nations World Food Program", "Angelo Nieve", "BC Place Stadium", "Gary Player", "200", "Adam", "Erika Mitchell Leonard", "December 1, 1969", "V\u00e1clav Havel", "right", "may", "London", "703", "20 October 1980", "glaciers", "Daley", "Cerberus", "1967"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5369946842373313}, "metric_results_detailed": {"EM": [true, false, true, false, false, true, false, false, false, false, false, true, true, true, false, true, true, true, false, true, true, false, true, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.18181818181818182, 0.5, 0.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.05714285714285715, 1.0, 0.5, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4170", "mrqa_newsqa-validation-1397", "mrqa_newsqa-validation-1598", "mrqa_newsqa-validation-1460", "mrqa_newsqa-validation-2059", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-1443", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-3618", "mrqa_newsqa-validation-4036", "mrqa_newsqa-validation-1507", "mrqa_newsqa-validation-2552", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1672", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-4059", "mrqa_newsqa-validation-1731", "mrqa_newsqa-validation-1282", "mrqa_newsqa-validation-1613", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-4105", "mrqa_newsqa-validation-385", "mrqa_newsqa-validation-6", "mrqa_newsqa-validation-2073", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1159", "mrqa_naturalquestions-validation-10208", "mrqa_naturalquestions-validation-2717", "mrqa_triviaqa-validation-1017", "mrqa_triviaqa-validation-2427", "mrqa_searchqa-validation-2894"], "SR": 0.46875, "CSR": 0.5503048780487805, "EFR": 0.9705882352941176, "Overall": 0.7215223726685795}, {"timecode": 41, "before_eval_results": {"predictions": ["Business Connect", "\"explosion of violence.\"", "40-year-old", "\"It all started when the military arrested one man, and then an hour later he emerged from building barely able to walk from the beating,\"", "\"We tortured (Mohammed al+) Qahtani,\"", "six alleged victims,", "Kevin Kuranyi", "Kenyan and Somali governments", "Aung San Suu Kyi", "legitimacy of that race.", "\"gotten the balance right\" on Myanmar,", "tribute to inspiring people in his new book.\"", "Mashhad", "Islamabad", "the western United States.", "pesos", "former U.S. soldier Steven Green", "millions of Americans", "1000 square meters", "sailors, kite surfers and wind surfers", "they'd get to bring a new puppy with them to the White House in January.", "27-year-old", "Friday,", "Los Ticos", "Seasons of My Heart", "helping on the sandbags lines", "$17,000", "opium", "for not doing more since taking office.\"", "10 years", "$8.8 million", "Noida, located in the outskirts of the capital New Delhi.", "Lisa Brown", "two and a half hours.", "off the front pages for the first time in days.", "Zimbabwean President Robert Mugabe", "31 meters (102 feet)", "London's Heathrow airport", "104 feet", "Transport Workers Union leaders", "state senators", "84-year-old", "to Nieb\u00fcll", "the Southern Baptist Convention,", "Sen. Barack Obama", "a mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Anil Kapoor", "think that someday, they'll try to take over your brain.", "Marie-Therese Walter.", "A mother whose daughter and granddaughter attend Oprah Winfrey's school in South Africa", "The eye of Hurricane Gustav", "Felix Baumgartner ( German : ( \u02c8fe\u02d0l\u026aks \u02c8ba\u028a\u032fm\u02cc\u0261a\u0250\u032ftn\u0250 )", "July 2012", "1273.6 cm", "Ovid's Metamorphoses", "Dutch", "kievan Rus", "Montreal", "1993", "Robert L. Stone", "Nike", "Barbara Bush", "Qwerty", "corpulent"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5927951388888888}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, true, false, true, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, true, true, true, false, true, true, true, true, false, true, false, false, false, true, false, false, true, false, true, true, true, true, false, true, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.1111111111111111, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.75, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.8, 0.0, 0.6666666666666666, 0.5333333333333333, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.16666666666666669, 1.0, 0.13333333333333333, 0.6666666666666666, 0.5714285714285715, 1.0, 0.1111111111111111, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3818", "mrqa_newsqa-validation-2627", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-3485", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1446", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-341", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1427", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-2690", "mrqa_newsqa-validation-3854", "mrqa_newsqa-validation-587", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-1483", "mrqa_newsqa-validation-2634", "mrqa_newsqa-validation-3430", "mrqa_newsqa-validation-2370", "mrqa_newsqa-validation-3800", "mrqa_newsqa-validation-2301", "mrqa_naturalquestions-validation-448", "mrqa_naturalquestions-validation-7458", "mrqa_triviaqa-validation-471", "mrqa_triviaqa-validation-2480", "mrqa_searchqa-validation-7340"], "SR": 0.484375, "CSR": 0.5487351190476191, "EFR": 1.0, "Overall": 0.7270907738095238}, {"timecode": 42, "before_eval_results": {"predictions": ["Wittenberg", "to implement the Prohibition Amendment by defining the process and procedures for banning alcoholic beverages, as well as their production and distribution.", "Bligh", "Parkinson's", "crime", "Tallinn", "Moscow", "germany", "Portugal", "first among equals.", "Friedrich Nietzsche", "the moon", "Moldova", "Zak Starkey", "Craggy Island", "Suez", "otters", "shagbark", "Port Talbot", "Rapa Nui", "The West Wing", "Charlie Cairoli", "Salvador Allende", "Mike Tyson", "Edward Elgar", "conductor", "Boyle\u2019s law", "small crystal ball or shewstone", "fur hat", "Tony Blair", "Adolf Hitler", "Jamaica", "June Brae", "heart", "1066", "crimean", "Jesse James", "Purple Heart Medal", "crime", "Jessica Simpson", "crime", "the South Saskatchewan River", "Robert Devereux", "NASCAR", "Canada", "Delaware", "Nic\u00e9phore Ni\u00e9pce", "Argentina", "Kwame Nkrumah", "The Color Purple", "terrorism", "lithium", "possible combinations of two goods ( such as butter and guns ) that can be produced with constant technology and resources per unit of time", "American singer and songwriter Mariah Carey", "lifetime", "unidentified flying objects", "the Chicago Bears", "grizzly bear", "Turkey", "Pew Research Center", "crime", "Ferrari", "Smoky Mountains National Park", "Bobby Beathard, Robert Brazile, Brian Dawkins, Jerry Kramer, Ray Lewis, Randy Moss, Terrell Owens, and Brian Urlacher"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6268960754158123}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, true, true, true, true, true, true, true, true, false, true, false, true, true, true, true, true, true, true, false, false, false, false, true, false, true, false, false, true, false, true, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, true, false, false], "QA-F1": [0.5, 0.09523809523809523, 0.6666666666666666, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5714285714285715, 0.6666666666666666, 1.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6060606060606061, 0.5, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 0.21052631578947367]}}, "before_error_ids": ["mrqa_squad-validation-2165", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-4034", "mrqa_triviaqa-validation-501", "mrqa_triviaqa-validation-1374", "mrqa_triviaqa-validation-5936", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-4363", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-5069", "mrqa_triviaqa-validation-4378", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-2390", "mrqa_triviaqa-validation-896", "mrqa_triviaqa-validation-5805", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-2307", "mrqa_naturalquestions-validation-2893", "mrqa_naturalquestions-validation-5734", "mrqa_hotpotqa-validation-177", "mrqa_hotpotqa-validation-1607", "mrqa_hotpotqa-validation-797", "mrqa_searchqa-validation-15581", "mrqa_searchqa-validation-9911", "mrqa_naturalquestions-validation-4915"], "SR": 0.53125, "CSR": 0.548328488372093, "EFR": 1.0, "Overall": 0.7270094476744186}, {"timecode": 43, "before_eval_results": {"predictions": ["John M. Grunsfeld", "dancing with the Stars", "psychotropic drugs", "opium", "A growing percentage of the Somali population has become dependent on humanitarian aid.", "10 below", "Democrat", "test scores and graduation rates", "16", "iReporter Rany Freeman", "forgery and flying without a valid license,", "President Bush", "15-year-old's", "seven", "upper respiratory infection", "543", "Kevin Kuranyi", "Amy Bishop Anderson", "Susan Atkins", "Ameneh Bahrami", "Virgin America", "$1,500", "The Al Nisr Al Saudi", "\"We Found Love\"", "his parents", "Ralph Lauren", "iWozniak", "hopes the journalists and the flight crew", "North Korea", "pine beetles", "Old Trafford", "Lillo Brancato Jr.", "Arabic, French and English", "Britain.", "Arsene Wenger", "The FBI's Baltimore field office", "Michael Jackson", "all day starting at 10 a.m.,", "her mom,", "South African police", "a Korean-American missionary", "Palestinian Islamic Army,", "william Obama eventually changes his stance.", "was killed", "cast doubt on Woodward's assertion Tuesday in a conversation with \"American Morning\" host John Roberts.", "consumer confidence", "Phil Spector", "the District of Columbia National Guard", "Australia and New Zealand", "Steven Gerrard", "saw an unprecedented wave of buying amid the elections.", "groin vault", "Hermann Ebbinghaus", "The statesmen", "a dog", "myxoma virus", "Wisconsin", "Battle of Britain and the Battle of Malta", "Viacom Media Networks", "five", "carbon", "'Star-Spangled Banner", "Grace Kelly", "100"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6650015782828282}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, false, false, true, false, true, true, true, false, true, true, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, false, true, false, false, true, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true, false, false, false, true, true, true, true, true, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.45454545454545453, 1.0, 1.0, 0.888888888888889, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-540", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-1994", "mrqa_newsqa-validation-1350", "mrqa_newsqa-validation-4127", "mrqa_newsqa-validation-2288", "mrqa_newsqa-validation-1647", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2837", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-319", "mrqa_newsqa-validation-740", "mrqa_newsqa-validation-3235", "mrqa_newsqa-validation-256", "mrqa_newsqa-validation-509", "mrqa_newsqa-validation-1289", "mrqa_newsqa-validation-2065", "mrqa_naturalquestions-validation-9576", "mrqa_naturalquestions-validation-9516", "mrqa_triviaqa-validation-1401", "mrqa_triviaqa-validation-2853", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-7435"], "SR": 0.609375, "CSR": 0.5497159090909092, "EFR": 0.96, "Overall": 0.7192869318181818}, {"timecode": 44, "before_eval_results": {"predictions": ["the Supreme Court of the United Kingdom", "Seal", "nasal cavity", "undertaker", "63 to 144 inches", "the Wye", "Zorro", "HMS Thetis", "the Last Post", "Karachi", "BMW", "eagle", "Morgan Spurlock", "Tito Jackson", "helps managers understand employees' needs in order to further employees' motivation.", "Prague", "Yellowstone", "Watford", "Nevada", "muezzin", "snake", "Rihanna", "Tintin", "Alexandrina", "22", "Hector BERLIOZ", "Azerbaijan", "Ireland", "Ash", "Madness", "Dalton", "Australia", "Phil Woolas", "bats", "United States", "Penelope Keith", "Alexei Kosygin", "John Galsworthy", "Vinegar Joe", "James Van Allen", "pangram", "the Tyrrhenian Sea", "Steel Beads", "Nicaragua", "Jules Verne", "Hyde Park Corner", "Lancashire", "Edouard Manet", "Burger King", "of Thebes", "Hyundai", "Narin Niruttinanon", "the President", "David Ben - Gurion", "Chow Tai Fook Enterprises", "an organ", "Point of Entry", "1973's \" Raw Power.\"", "a violent government crackdown seeped out.\"", "anyone.", "Hungarian", "China", "China", "Isabella (Belle) Baumfree"], "metric_results": {"EM": 0.640625, "QA-F1": 0.6677083333333333}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, true, false, false, true, true, false, false, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, false, false, false, false, true, true, true, true, true, false, false, false, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.16666666666666669, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-7523", "mrqa_triviaqa-validation-206", "mrqa_triviaqa-validation-6500", "mrqa_triviaqa-validation-6166", "mrqa_triviaqa-validation-2536", "mrqa_triviaqa-validation-5745", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-7055", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-117", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3144", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-1981", "mrqa_triviaqa-validation-5883", "mrqa_triviaqa-validation-4657", "mrqa_naturalquestions-validation-2208", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-3391", "mrqa_searchqa-validation-15957"], "SR": 0.640625, "CSR": 0.5517361111111111, "EFR": 1.0, "Overall": 0.7276909722222221}, {"timecode": 45, "before_eval_results": {"predictions": ["trespassing at a nuclear-missile installation", "vAN WIJK", "2", "skull", "new york", "David Johnston", "Washington", "Nuuk", "Manila", "pool", "China", "Graham Henry", "wool", "South Pacific", "king henry vtoroy", "beans", "Leeds", "wood", "Elizabeth II", "a dog", "llamas", "London Underground Piccadilly Line", "Oklahoma", "barbara david delle bande Nere", "Nepal", "scurvy", "cutters", "Indonesia", "purple coneflower", "d\u00fcsseldorf", "keane", "gauteng", "Harvard", "Pakistan", "Uranus", "barbara boy", "barbara kart", "my favorite martian", "niki lauda", "petronas Towers", "The Daily Mirror", "Eric Morley", "radio waves", "yorkshire", "Manchester City", "football", "Reform Club", "William Shakespeare", "barber", "Trimdon, County Durham", "Tasmania", "approximately 26,000 years", "US - grown fruit", "Orange Juice", "Fifteenth", "Plato", "Richard Feynman", "Florida", "Martin Aloysius Culhane", "400 years", "freelance", "mouth", "barbara dold", "arrested, arraigned and jailed,"], "metric_results": {"EM": 0.453125, "QA-F1": 0.48725490196078436}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, false, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.11764705882352941]}}, "before_error_ids": ["mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-3916", "mrqa_triviaqa-validation-6714", "mrqa_triviaqa-validation-7230", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-7641", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-514", "mrqa_triviaqa-validation-4958", "mrqa_triviaqa-validation-7559", "mrqa_triviaqa-validation-2995", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-79", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-4729", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-4038", "mrqa_triviaqa-validation-1623", "mrqa_triviaqa-validation-1823", "mrqa_triviaqa-validation-7057", "mrqa_triviaqa-validation-2926", "mrqa_triviaqa-validation-6494", "mrqa_triviaqa-validation-5775", "mrqa_triviaqa-validation-5221", "mrqa_naturalquestions-validation-10402", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-5176", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-4100", "mrqa_searchqa-validation-2154", "mrqa_searchqa-validation-11241", "mrqa_newsqa-validation-3806"], "SR": 0.453125, "CSR": 0.5495923913043479, "EFR": 1.0, "Overall": 0.7272622282608696}, {"timecode": 46, "before_eval_results": {"predictions": ["at Konwiktorska Street,", "mashed potato", "Lalo Schifrin", "16 November 2001", "Don McMillan", "7 correct numbers", "Billy Hill", "constantine constantine", "halogenated paraffin hydrocarbons", "the body - centered cubic ( BCC ) lattice", "May 2002", "2010", "virtual reality simulator", "beneath the liver", "between 1881 and 1885", "pre -Columbian times", "early 2014", "Most days are sunny throughout the year", "by chlorine and bromine from manmade organohalogens", "Ego", "1977", "ingredients", "homicidal thoughts of a troubled youth", "The Hunger Games : Mockingjay -- Part 1 ( 2014 )", "constantine terms", "October 22, 2017, and concluded on April 15, 2018, consisting of 16 episodes", "Amitabh Bachchan", "1998", "nine", "Jeff Bezos", "adrenal medulla", "a vertebrate's immune system", "redox", "82.30'E longitude, in Mirzapur, Uttar Pradesh", "produced with constant technology and resources per unit of time", "Lewis Carroll", "January 2004", "Samuel Chase", "Cetshwayo", "1964", "Asuka", "Erica Rivera", "New York", "November 1999", "elocution", "Chris Rea", "mashed potato", "cases that have not been considered by a lower court may be heard by the Supreme Court in the first instance under what is termed original jurisdiction", "three part", "Louisa Johnson", "Mongol - led Yuan dynasty", "candy bars", "1948", "Ruth Rendell", "Hidden America with Jonah Ray", "the United Kingdom", "people working in film and the performing arts", "the crowds", "between 1917 and 1924 when he was in his late 30s and early 40s.", "his lawyer offered testimony Tuesday from a psychiatrist who testified that a mental health review \"did not meet acceptable mental health standards.\"", "my name is Earl", "the Virgin Spring", "cookies", "uncle Juan Nepomuceno Guerra"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6349465870295874}, "metric_results_detailed": {"EM": [true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, false, false, false, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, true, false, true, true, false, false, true, false, false, true, true, true, true, true, false, false, false, true, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.35294117647058826, 1.0, 1.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.7692307692307692, 1.0, 0.08695652173913042, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.18181818181818182, 1.0, 1.0, 0.14285714285714288, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.0, 1.0, 1.0, 0.0, 0.4]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6638", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-6151", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-8227", "mrqa_naturalquestions-validation-4960", "mrqa_naturalquestions-validation-654", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-5452", "mrqa_naturalquestions-validation-10147", "mrqa_naturalquestions-validation-7591", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-6333", "mrqa_naturalquestions-validation-4891", "mrqa_naturalquestions-validation-4007", "mrqa_naturalquestions-validation-7624", "mrqa_naturalquestions-validation-844", "mrqa_naturalquestions-validation-7880", "mrqa_naturalquestions-validation-3605", "mrqa_naturalquestions-validation-8092", "mrqa_naturalquestions-validation-8372", "mrqa_naturalquestions-validation-6321", "mrqa_triviaqa-validation-7778", "mrqa_newsqa-validation-3899", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-2156", "mrqa_searchqa-validation-7061", "mrqa_hotpotqa-validation-4241"], "SR": 0.546875, "CSR": 0.5495345744680851, "EFR": 0.9655172413793104, "Overall": 0.7203541131694792}, {"timecode": 47, "before_eval_results": {"predictions": ["Acadia National Park", "Earl Long", "Luxembourg", "paiyar", "Space Shuttle Challenger", "brian jones", "a son", "lapis lazuli", "The Pentagon", "a valley fold", "snails", "bamboo", "the Vietnam War", "Port Royal", "Gerard Mercator", "peter Venkman", "p.T. Barnum & Bailey", "Dizzy Gillespie", "my nose", "Ernie Els", "Macedonia", "gestation", "boldsky", "Sylvester Stallone", "Herb Alpert", "John Adams", "the Rolling Stones", "Field Marshal Bernard Montgomery", "Zeus", "peter Sinclair's", "Thylacinus cynocephalus", "hatta al-nasr", "terrarium", "cyclotron", "prithee", "prostitutes", "Barcelona", "Yellow Ribbon", "the Indy 500", "\"The Hills\"", "alto", "porter", "Rhode Island", "a sea arm of the North Atlantic Ocean", "Baton Rouge", "Kamehameha I", "bok", "Alan Alda", "heat", "long Metropolitan Park", "sirloin", "New England", "Walter Pauk", "left atrium of the heart", "the American Civil War", "a journal", "the United States", "Rousillon Rupes", "Obafemi Martins", "\"Twice in a Lifetime\".", "two-state solution", "Republican", "a communications breakdown at a Federal Aviation Administration facility,", "The Tempest"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5659722222222222}, "metric_results_detailed": {"EM": [true, true, true, false, false, false, false, true, true, false, true, true, true, false, false, false, false, false, false, true, true, true, false, false, true, true, true, false, true, false, false, false, true, true, false, false, true, false, true, false, true, false, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.2222222222222222, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-847", "mrqa_searchqa-validation-8508", "mrqa_searchqa-validation-11698", "mrqa_searchqa-validation-9392", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-7898", "mrqa_searchqa-validation-13199", "mrqa_searchqa-validation-4024", "mrqa_searchqa-validation-7423", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-7490", "mrqa_searchqa-validation-624", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-4731", "mrqa_searchqa-validation-751", "mrqa_searchqa-validation-16787", "mrqa_searchqa-validation-6431", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-4236", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-3996", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-1195", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-3693", "mrqa_naturalquestions-validation-3893", "mrqa_naturalquestions-validation-3174", "mrqa_triviaqa-validation-7062", "mrqa_hotpotqa-validation-2486", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-904"], "SR": 0.484375, "CSR": 0.5481770833333333, "EFR": 1.0, "Overall": 0.7269791666666666}, {"timecode": 48, "before_eval_results": {"predictions": ["The photoelectric ( optical ) smoke detector", "31", "Mama Said", "The Satavahanas", "a specific individual to operate one or more types of motorized vehicles, such as a motorcycle, car, truck, or bus on a public road", "the alpha efferent neurons", "bohrium", "Wembley Stadium", "Chernobyl Nuclear Power Plant", "Dalveer Bhandari", "Bush", "the center of the Northern Hemisphere", "the Noahic Covenant", "David Tennant", "the Sons of Liberty", "the original timeline is eventually restored", "on Chesapeake Bay", "A footling breech", "October 27, 1964", "1926", "Jonathon Dutton", "1990", "a sport utility vehicles", "Bob Dylan", "qualitative data, quantitative data", "Johannes Gutenberg", "to collect menstrual flow", "a Navy's commissioned ships", "William the Conqueror", "Sir Ernest Rutherford", "Nicole Gale Anderson", "William Chatterton Dix", "April 12, 2017", "The Gupta Empire", "the earlier national arms", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "Gustav Bauer", "a Y chromosome", "Phillip Schofield and Christine Bleakley", "after the title page, copyright notices, and often includes second - level or section titles ( A-heads ) within the chapters as well, and occasionally even third - level titles", "Antarctica", "Henry Haller", "revenge and karma", "the efferent nerves that directly innervate muscles", "1986", "1546", "a crime", "1942", "Prem Lata Agarwal", "John Adams", "early 2014", "The Royal Ballet", "Jessica Smith", "J. M. W. Turner", "\"Big Mamie\"", "six", "\"First Family of Competitive Eating\"", "$2 billion", "The forward's lawyer has appealed against the punishment for the player who had previously admitted in interviews that he had struggled to adapt to the different culture and religious life in Sudan.", "on an above-ground track in the District of Columbia near Takoma Park, Maryland.", "St. Valentine's Day Massacre", "Liza Minelli", "Gabriel", "opposition parties"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5758943829925292}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, true, true, true, true, false, true, true, false, false, false, true, false, false, false, true, true, true, false, true, true, false, true, true, true, true, true, true, false, true, true, false, false, false, false, false, true, false, true, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, true, false, true, true], "QA-F1": [1.0, 0.0, 0.0, 0.0, 0.9500000000000001, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.3404255319148936, 0.06451612903225806, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0689655172413793, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3332", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-6234", "mrqa_naturalquestions-validation-158", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2721", "mrqa_naturalquestions-validation-642", "mrqa_naturalquestions-validation-9741", "mrqa_naturalquestions-validation-6851", "mrqa_naturalquestions-validation-2582", "mrqa_naturalquestions-validation-9144", "mrqa_naturalquestions-validation-220", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-7473", "mrqa_naturalquestions-validation-9150", "mrqa_naturalquestions-validation-9005", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-9237", "mrqa_naturalquestions-validation-3784", "mrqa_naturalquestions-validation-4586", "mrqa_naturalquestions-validation-2571", "mrqa_naturalquestions-validation-8037", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-6786", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-6410", "mrqa_hotpotqa-validation-1210", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1290", "mrqa_searchqa-validation-16493"], "SR": 0.515625, "CSR": 0.5475127551020409, "EFR": 0.967741935483871, "Overall": 0.7203946881171823}, {"timecode": 49, "before_eval_results": {"predictions": ["1912", "Standard Oil", "(Kabuki)", "These Boots Are Made for Walkin", "cretaceous Dinosaurs", "greece", "Nancy Lopez", "ozone", "Who's the Boss?", "Donnie Wahlberg", "Tasmania", "Oriole Park", "Abu Dhabi", "cunard", "Zionism", "Prague", "dressage", "(Isaac) Newton", "Toby Keith", "the accordion", "a swan", "Edith Piaf", "the Stratosphere", "parkinsonism", "(Arturo) Toscanini", "(William) Rehnquist", "Guinevere", "Department of Energy", "tangerine", "the Rhineland", "Dead Ringers", "Johann Strauss II", "Solidarity", "elbert", "(Michael) Cera", "Charles Lindbergh", "nymphaea caerulea", "Disneyland Park", "a journal", "Jack Nicklaus", "civil war", "a vacuum", "Teen Titans Go", "(David) Cowan", "(Ibrahim) Pap Woods", "Mary Poppins", "St. Louis", "Amish", "the Department of Defense", "(Levi) Strauss", "Badminton", "Canada", "the results show moved to Sunday evenings, although it was filmed on Saturday and then broadcast `` as live '' on the Sunday", "autopistas", "Phar Lap", "a dove", "Lewis Carroll", "1983", "Excalibur Hotel and Casino", "the Czech Kingdom", "Arsenal", "Kurdistan Workers' Party,", "humans", "Action Comics"], "metric_results": {"EM": 0.59375, "QA-F1": 0.659375}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, true, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, false, true, false, false, true, false, false, false, true, true, true, false, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1023", "mrqa_searchqa-validation-3755", "mrqa_searchqa-validation-5228", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-11120", "mrqa_searchqa-validation-12322", "mrqa_searchqa-validation-14374", "mrqa_searchqa-validation-12385", "mrqa_searchqa-validation-3069", "mrqa_searchqa-validation-8462", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-1337", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-2122", "mrqa_searchqa-validation-6600", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-6473", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-2104", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-4978", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-15313", "mrqa_naturalquestions-validation-8350", "mrqa_newsqa-validation-1501"], "SR": 0.59375, "CSR": 0.5484375, "EFR": 1.0, "Overall": 0.7270312499999999}, {"timecode": 50, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1283", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1338", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-154", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2420", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2992", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3714", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3876", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4269", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4430", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4581", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5018", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5266", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5350", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5482", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-985", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-1111", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1604", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3745", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4741", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5087", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-5328", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-5620", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6334", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-7716", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7919", "mrqa_naturalquestions-validation-8189", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8350", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9230", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9715", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-9953", "mrqa_newsqa-validation-1034", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-112", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1796", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2102", "mrqa_newsqa-validation-2132", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2435", "mrqa_newsqa-validation-2444", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2920", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3113", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-3132", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4071", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-444", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-537", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-608", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10619", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-11231", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11579", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12265", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12759", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13706", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14179", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-1562", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16869", "mrqa_searchqa-validation-1757", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-2475", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-3025", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-3764", "mrqa_searchqa-validation-4597", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4678", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5710", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6608", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8267", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9430", "mrqa_searchqa-validation-9480", "mrqa_searchqa-validation-9911", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1330", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1830", "mrqa_squad-validation-1876", "mrqa_squad-validation-2048", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-2726", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4065", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-4313", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5296", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-669", "mrqa_squad-validation-6848", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7551", "mrqa_squad-validation-7593", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-818", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1025", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-123", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1678", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2196", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3169", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3197", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3635", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-3934", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-4896", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-590", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6590", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937", "mrqa_triviaqa-validation-955"], "OKR": 0.89453125, "KG": 0.5234375, "before_eval_results": {"predictions": ["Aerosmith", "Willa Cather", "Senate", "The Who", "a science fiction novel", "Bismarck", "analog", "a travel partner", "Luisa Tetrazzini", "Renoir", "the polio vaccine", "Peter Behn", "Chief Justice of the United States", "the bar", "Fyodor Dostoevsky", "Smucker", "Chile", "Russia", "grease", "Hollandaise", "Esau", "Dry ice", "Martin Luther King III", "a type of \"combustion\"", "a catalyst", "Kansas City", "a sergeant in the Palm Beach County sheriff`s Organized Crime Bureau", "Uganda", "senators", "Sappho", "Thermopylae", "the Maccabean", "John Paul Jones", "Hamlet", "a child", "the river Ganga", "New Brunswick", "Copacabana", "Manilow", "We Own the Night", "Russian Empire", "Mr. & Mrs. Smith", "Triceratops", "a pie", "Memphis", "Thomas Mann", "Krackel", "a dog eat dog world", "Dmitri Mendeleev", "Azkaban", "tea leaves", "thia Weil", "`` beloved ''", "the heads of federal executive departments who form the Cabinet of the United States", "Microsoft", "General John J. Pershing", "Ross Kemp", "Taylor Swift", "September 23, 1935", "An invoice, bill or tab", "Martin \"Al\" Culhane,", "will be able to gamble in a casino, buy a drink in a pub or see the horror film \"hostel: Part II,\"", "Hyundai Steel", "2015"], "metric_results": {"EM": 0.53125, "QA-F1": 0.598139880952381}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, true, false, true, false, false, false, true, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, false, true], "QA-F1": [1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.0, 1.0, 0.25000000000000006, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 0.4, 1.0, 0.08333333333333333, 0.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15858", "mrqa_searchqa-validation-3219", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-130", "mrqa_searchqa-validation-16767", "mrqa_searchqa-validation-803", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-4039", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-11713", "mrqa_searchqa-validation-12656", "mrqa_searchqa-validation-14038", "mrqa_searchqa-validation-13163", "mrqa_searchqa-validation-5965", "mrqa_searchqa-validation-9056", "mrqa_searchqa-validation-729", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-16283", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-2780", "mrqa_naturalquestions-validation-8421", "mrqa_naturalquestions-validation-998", "mrqa_triviaqa-validation-6411", "mrqa_hotpotqa-validation-5801", "mrqa_newsqa-validation-762", "mrqa_newsqa-validation-1048"], "SR": 0.53125, "CSR": 0.5481004901960784, "EFR": 1.0, "Overall": 0.7432138480392156}, {"timecode": 51, "before_eval_results": {"predictions": ["Cupcake", "Wilkie Collins", "Chief of Staff", "the Bible", "Helen of Troy", "the Civil War", "the Nobel Prize", "Roussimoff", "Dracula", "Technetium", "Earth", "867-5309", "Miss Havisham", "Thailand", "Whitney Houston", "opal", "Taft", "dense", "air pressure", "echidna", "alkaline", "porcelain", "Synchronicity", "Taft", "bees", "dark energy", "Reptiles", "Taft", "Uranus", "Taecilius Atticus", "Barbara Walters", "Jubal Early", "Perimeter", "pumice", "watermelon", "Cole Porter", "a sockspuppet", "Taft", "drapery", "Cosmopolitan", "Madagascar", "Carl Jung", "Carnarvon", "Ontario", "Olympia", "Copernicus", "Lily Allen", "Candlestick Park", "the Black Death", "Google", "Defense", "because they believed that it violated their rights as Englishmen to `` No taxation without representation '', that is, to be taxed only by their own elected representatives and not by a British parliament in which they were not represented", "1898", "Brevet Colonel Robert E. Lee", "Clough", "Gilda", "the Buddha", "Lancia-Abarth", "1963", "Brittany Snow", "Christopher Savoie", "authorizing killings and kidnappings by paramilitary death squads.", "\"Watchmen\"", "Cork"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6458333333333333}, "metric_results_detailed": {"EM": [false, false, true, false, true, true, false, false, false, false, false, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, false, false, true, false, true, false, false, false, true, true, false, false, false, true, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, false, true, true, true, true, true, true], "QA-F1": [0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11520", "mrqa_searchqa-validation-9236", "mrqa_searchqa-validation-13480", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-3888", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-1771", "mrqa_searchqa-validation-1496", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-5387", "mrqa_searchqa-validation-6272", "mrqa_searchqa-validation-440", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-10284", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-9206", "mrqa_searchqa-validation-3884", "mrqa_searchqa-validation-15299", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-4585", "mrqa_searchqa-validation-4971", "mrqa_triviaqa-validation-7153", "mrqa_hotpotqa-validation-3399"], "SR": 0.59375, "CSR": 0.5489783653846154, "EFR": 0.9615384615384616, "Overall": 0.7356971153846155}, {"timecode": 52, "before_eval_results": {"predictions": ["Deere", "(Ella) VICTORIA", "electron", "the Missouri River", "brandy", "George Babbitt", "GIGO", "(Gioachino) Rossini", "(Hamlet)", "Rome", "the Isle of Wight", "Colorado Springs", "hay", "Possession", "(Etonian) Scott", "(Ben) Franklin", "meters", "a surface-to-air missile", "Vibe", "Pulp Fiction", "yelping", "Frederick Forsyth", "August 15, 1947", "Princess Leia", "Vietnam", "Vince Lombardi", "the global village", "Dubliners", "Sudan", "Kwanzaa", "Warren Buffett", "Charlie\\'s Angels", "(George) Washington", "imagist", "whimper", "obsoleteness", "( Dorothy) Parker", "the Republic of China", "Mickey Spillane", "the Buzz Lightyear", "Jack Bauer", "Bingo", "kidney", "Necessity", "diamonds", "President Eisenhower", "Atlanta", "Texas", "fat", "holidays", "a gem", "prejudice in favour of or against one thing, person, or group compared with another, usually in a way considered to be unfair", "Australia's Sir Donald Bradman", "The Caucasus Mountains", "Rambo", "\"Old Darby\"", "the antelope", "Jung Yun-ho", "The Mauser C96", "a united Ireland", "raping and murdering a woman in Missouri.\"", "troy Livesay", "for a series of wildfires from late summer through autumn in Bastrop County.", "Estadio Victoria"], "metric_results": {"EM": 0.515625, "QA-F1": 0.5946428571428571}, "metric_results_detailed": {"EM": [false, false, true, true, false, true, true, false, false, true, true, true, true, true, false, false, false, false, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, false, true, false, true, false, true, true, false, false, false, true, true, false, true, true, false, false, false, true, false, true, false, false, true, true, false, true, false, false, false, true], "QA-F1": [0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8571428571428571, 1.0, 0.0, 0.4, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.4, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15960", "mrqa_searchqa-validation-15202", "mrqa_searchqa-validation-4179", "mrqa_searchqa-validation-3292", "mrqa_searchqa-validation-7642", "mrqa_searchqa-validation-7782", "mrqa_searchqa-validation-14452", "mrqa_searchqa-validation-10292", "mrqa_searchqa-validation-133", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-14015", "mrqa_searchqa-validation-9935", "mrqa_searchqa-validation-11091", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11759", "mrqa_searchqa-validation-12531", "mrqa_searchqa-validation-12496", "mrqa_searchqa-validation-16518", "mrqa_searchqa-validation-5962", "mrqa_searchqa-validation-6821", "mrqa_searchqa-validation-9744", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-8817", "mrqa_searchqa-validation-14998", "mrqa_naturalquestions-validation-9816", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6562", "mrqa_hotpotqa-validation-3025", "mrqa_newsqa-validation-1271", "mrqa_newsqa-validation-705", "mrqa_newsqa-validation-3364"], "SR": 0.515625, "CSR": 0.5483490566037736, "EFR": 1.0, "Overall": 0.7432635613207548}, {"timecode": 53, "before_eval_results": {"predictions": ["to encourage rebellion against the British authorities", "Debbie Gibson", "three", "February 29", "Ireland", "December 2, 1942", "unknown origin", "heart", "March 26, 1973", "Ancylostoma duodenale", "June 11, 2002", "St Pancras International", "games where the player played, in whole or in part", "\u2212 89.2 \u00b0 C ( \u2212 128.6 \u00b0 F )", "Frank Langella", "Granada Studios in Manchester", "Human fertilization", "Aslan", "16 seasons", "Bill Russell", "the Washington Redskins", "Donald Trump", "vascular cambium", "defined populations", "1895", "`` 50 / 50 '', in which the computer eliminates two of the incorrect answers", "between the Mediterranean Sea to the north and the Red Sea to to the south", "a Border Collie", "the Washington metropolitan area", "Julie Adams", "Gatiman", "John Young", "Kevin Spacey", "novella", "uterine tubes", "Gene MacLellan", "American", "2010", "on location", "Frankie Muniz", "before the first letter of an interrogative sentence or clause to indicate that a question follows", "(Prince) Albert", "Jenna Boyd", "`` never had any meaning other than the obvious one '' and is about the `` loss of innocence in children '', and dismissed the suggestion of association with drugs", "2017", "1978", "a loanword of the Visigothic word guma `` man ''", "air moisture", "birch", "John Brown", "brothers Henry, Jojo and Ringo Garza", "Ken Barlow", "air", "Roman Empire", "a vegetarian dish", "Matt Kemp", "pro-Confederate partisan rangers", "they", "30,000", "\"handful\" of domestic disturbance calls to police since 2000 involving the Damas couple,", "(P Pablo) Picasso", "(Scott) Peterson", "(Prince) Albert", "near the side of your thumb"], "metric_results": {"EM": 0.515625, "QA-F1": 0.609812805372016}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, true, true, true, true, false, true, false, true, true, false, true, false, true, true, false, false, true, false, true, false, false, true, false, true, true, true, true, true, false, true, false, true, true, false, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, true, false, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.3636363636363636, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5714285714285715, 1.0, 0.15, 0.9523809523809523, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.0, 1.0, 0.07407407407407408, 1.0, 1.0, 0.2857142857142857, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.4, 0.5, 0.0, 1.0, 0.08333333333333333, 0.5, 1.0, 0.0, 0.5714285714285715]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5363", "mrqa_naturalquestions-validation-716", "mrqa_naturalquestions-validation-5460", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-2837", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-4036", "mrqa_naturalquestions-validation-5636", "mrqa_naturalquestions-validation-6319", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-56", "mrqa_naturalquestions-validation-10249", "mrqa_naturalquestions-validation-9752", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-1135", "mrqa_naturalquestions-validation-778", "mrqa_naturalquestions-validation-3019", "mrqa_naturalquestions-validation-663", "mrqa_naturalquestions-validation-2418", "mrqa_naturalquestions-validation-4240", "mrqa_triviaqa-validation-6845", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-5886", "mrqa_hotpotqa-validation-951", "mrqa_hotpotqa-validation-3154", "mrqa_hotpotqa-validation-458", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3873", "mrqa_searchqa-validation-15480", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-5691"], "SR": 0.515625, "CSR": 0.5477430555555556, "EFR": 0.9354838709677419, "Overall": 0.7302391353046594}, {"timecode": 54, "before_eval_results": {"predictions": ["Norway", "Ecuador", "Home Improvement", "iron", "annie walker", "Berlin", "Othello", "Fidel Castro", "Pat Garrett", "Montana", "Annenberg Hall at Harvard University", "General Custer", "an arboretum", "Marie Curie", "Abnormal Psychology", "love", "Vice President of the United States", "the Italian flag", "Samuel Butler", "Kitty Kelley", "Abraham Lincoln", "t Teddy bear", "Crouching Tiger, Hidden Dragon", "Baseball Hall of Fame", "upsilon", "the Edo Period", "arizonensis", "Jupiter", "conformation dog shows", "Ziegfeld", "onerebelheart", "Volcanoes", "the Louvre", "Greek", "annie walker", "Phosphorus", "prayer", "the Wessex", "snowmobiling", "Aaron Copland", "blue", "voltage", "Act I of The Royal Ballet", "Tesla", "Lil Jon", "the infield", "a plum", "Lizzie Borden", "Hockey", "Pop-Tarts", "bovine spongiform encephalopathy", "Henry Purcell", "Mankombu Sambasivan Swaminathan", "eight", "Ethiopia", "desert", "Argentina", "Real Madrid and the Spain national team", "1983", "Richard Allen Street", "Brazil forward Ronaldinho", "mpire of the Sun", "whether he should be charged with a crime,", "four"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6463541666666666}, "metric_results_detailed": {"EM": [true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, false, true, true, true, false, true, false, true, false, false, true, false, false, false, false, true, true, false, true, false, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.8, 1.0, 0.33333333333333337, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-10123", "mrqa_searchqa-validation-9879", "mrqa_searchqa-validation-12533", "mrqa_searchqa-validation-8125", "mrqa_searchqa-validation-2256", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-4141", "mrqa_searchqa-validation-9820", "mrqa_searchqa-validation-15248", "mrqa_searchqa-validation-16878", "mrqa_searchqa-validation-10404", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-3773", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13543", "mrqa_searchqa-validation-11324", "mrqa_searchqa-validation-6474", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9692", "mrqa_searchqa-validation-7328", "mrqa_searchqa-validation-5952", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-13757", "mrqa_triviaqa-validation-1529", "mrqa_newsqa-validation-168", "mrqa_newsqa-validation-3273", "mrqa_triviaqa-validation-261"], "SR": 0.5625, "CSR": 0.5480113636363637, "EFR": 1.0, "Overall": 0.7431960227272727}, {"timecode": 55, "before_eval_results": {"predictions": ["Santa Fe", "a Beanie Baby", "kick drum", "chess", "cola", "Berlin", "Comedy Central", "Michael Phelps", "( Modest) Mussorgsky", "Romeo and Juliet", "Cerberus", "lice and roaches", "the Nile", "silver", "Plutarch", "figure skater", "the submarine", "St. Augustine", "Trinity", "the Marshall Islands", "the burnoose", "Tesla", "the Mekong", "the 36th", "Valentina Tereshkova", "Canada", "a crossword", "Missouri", "ribonucleic acid", "Rubeus Hagrid", "Manitoba", "Death of a Salesman", "Chocolate", "inshallah", "Saudi Arabia", "Pamela Anderson", "Lose 30 Pounds", "Idaho", "coppertone", "Edward VI", "the Empire State Building", "laugh", "Tennessee", "the Constitution", "Toronto", "University of Exeter", "(Prince) Ford", "Lawrence of Arabia", "Andy Warhol", "creams", "Tara Reid", "Pac - 12 Conference Champions Stanford Cardinal", "moist temperate climates", "B.F. Skinner", "\"Nokia tune\"", "General Paulus", "Cyprus", "2010", "Tufts University", "The Crips", "Ricardo Urbina", "fears a desperate country with a potential power vacuum that could lash out.", "Monday.", "Cpl. Cesar Laurean's backyard."], "metric_results": {"EM": 0.5625, "QA-F1": 0.623735119047619}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, false, true, false, false, false, true, false, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, true, false, false, false, true, true, false, true, false, false, false, true, true, true, true, true, true, false, false, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.8, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 0.5, 0.5, 1.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-1278", "mrqa_searchqa-validation-12548", "mrqa_searchqa-validation-8030", "mrqa_searchqa-validation-13435", "mrqa_searchqa-validation-4211", "mrqa_searchqa-validation-15699", "mrqa_searchqa-validation-12502", "mrqa_searchqa-validation-15463", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-3517", "mrqa_searchqa-validation-5094", "mrqa_searchqa-validation-9038", "mrqa_searchqa-validation-9777", "mrqa_searchqa-validation-9266", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-13316", "mrqa_searchqa-validation-2463", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-365", "mrqa_searchqa-validation-10897", "mrqa_searchqa-validation-6437", "mrqa_searchqa-validation-5672", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-438", "mrqa_newsqa-validation-1563", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2517"], "SR": 0.5625, "CSR": 0.5482700892857143, "EFR": 1.0, "Overall": 0.7432477678571429}, {"timecode": 56, "before_eval_results": {"predictions": ["former English county of Humberside", "the Federal Bureau of Prisons", "\"Dumb and Dumber\"", "the first trans-Pacific flight from the United States to Australia", "John Hunt", "Walt Disney Productions", "Reinhard Heydrich", "British", "\"Sheen Michaels Entertainment\"", "her sixth studio album", "1770", "Indianola", "2005", "A Bug's Life", "U.S.", "a few", "the Qin dynasty", "Kentucky River", "fourth", "\"The Bob Edwards Show\" on Sirius XM Radio and \"Bob Edwards Weekend\" distributed by Public Radio International to more than 150 public radio stations", "The S7 series", "White Knights of the Ku Klux Klan", "Havana, Cuba", "Charlie Puth", "Fort Albany", "three Golden Globe Awards", "Soviet Union", "the National Society of Daughters of the American Revolution (NSDAR)", "Martin Scorsese", "General Sir John Monash", "Protestant", "the Kentucky RiverBats", "Firestorm", "Agra", "close to 50 million", "Henry II", "Scotty Grainger", "the student to gain physical awareness and experience of music through training that takes place through all of the senses, particularly kinesthetic.", "An agricultural cooperative", "Kairi", "Texas", "the Democratic Unionist Party (DUP)", "five", "Candice Susan Swanepoel", "John C. Bogle", "Chris \"Izzy\" Cole", "multiple awards", "McComb, Mississippi", "\"King of Cool\"", "1995", "\"Losing My Religion\"", "The Royalettes", "three", "April 17, 1982", "Jaguar", "'Q'", "Aintree", "\"Britain's Got Talent\"", "Turkey", "Seoul", "the Charleston", "emerald", "Don Juan", "Pandora"], "metric_results": {"EM": 0.515625, "QA-F1": 0.59765625}, "metric_results_detailed": {"EM": [true, true, true, false, true, true, true, true, true, false, true, false, false, true, true, false, true, true, false, false, true, false, false, false, false, false, true, false, true, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, false, true, false, true, true, false, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5714285714285715, 1.0, 0.9333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 0.09523809523809523, 1.0, 0.25, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2943", "mrqa_hotpotqa-validation-3391", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-2369", "mrqa_hotpotqa-validation-4947", "mrqa_hotpotqa-validation-3183", "mrqa_hotpotqa-validation-2522", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-674", "mrqa_hotpotqa-validation-431", "mrqa_hotpotqa-validation-2559", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-557", "mrqa_hotpotqa-validation-2577", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-934", "mrqa_hotpotqa-validation-3745", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-5850", "mrqa_hotpotqa-validation-983", "mrqa_hotpotqa-validation-1708", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-162", "mrqa_hotpotqa-validation-741", "mrqa_hotpotqa-validation-5716", "mrqa_hotpotqa-validation-314", "mrqa_naturalquestions-validation-7906", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-1229", "mrqa_newsqa-validation-154", "mrqa_searchqa-validation-8091"], "SR": 0.515625, "CSR": 0.5476973684210527, "EFR": 1.0, "Overall": 0.7431332236842105}, {"timecode": 57, "before_eval_results": {"predictions": ["Italian architect and art theorist Leon Battista Alberti", "in a counter clockwise direction", "The episode `` One Son '', Jeffrey finds out that his father, the Smoking Man, forced his mother Cassandra to undergo medical treatments that led to several nervous breakdowns during his childhood years", "December 2, 1942", "Ray Charles", "mid November", "when they qualify as a medical practitioner following graduation with a Bachelor of Medicine, Bachelor of surgery degree", "Daniel Suarez", "its population", "the Central and South regions, and by people of Mexican ancestry living in other places, especially the United States", "Jordan Olssen", "The Death of Archie", "Dan Stevens", "9.7 m", "Guwahati", "Brobee", "based on sovereign states", "efferent nerves", "William Wyler", "Dragon Ball GT", "American country music group The Nitty Gritty Dirt Band", "Donald Fauntleroy Duck", "2013", "Friedman Billings Ramsey", "2018", "skeletal muscle", "Joe Spano", "Spanish moss", "Georges Auguste Escoffier", "Nodar Kumaritashvili", "October 29, 2015", "Madeline Reeves", "New England Patriots", "king of kalinga", "Great Britain", "a charbagh", "an unknown recipient", "Andy Warhol", "Elected Emperor of the Romans", "Dalveer Bhandari", "Middle Eastern alchemy", "the division of Italy into independent states", "Nalini Negi", "The Rashidun Caliphs", "Lituya Bay in Alaska", "The Demon Barber of Fleet Street", "17 -- 15", "George Strait", "the two - dimensional perspective projections ( or drawings ) of mutually parallel lines in three - dimensional space appear to converge", "The United States is a federal republic in which the president, Congress, and federal courts share powers reserved to the national government according to its Constitution", "adversely affect one group of people of a protected characteristic more than another", "Emma Chambers", "Hans Lippershey", "Thermopylae", "August 6, 1845", "an album", "Spain", "Dr. Jennifer Arnold and husband Bill Klein,", "A family friend of a U.S. soldier captured by the Taliban", "60 euros", "Blue", "lump", "a knish", "CBS"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6625534532468246}, "metric_results_detailed": {"EM": [true, false, false, true, true, false, false, false, false, false, false, false, true, true, true, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, true, true, true, true, false, false, true, false, true, true], "QA-F1": [1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.28571428571428575, 0.10526315789473685, 0.0, 0.0, 0.4, 0.0, 0.125, 1.0, 1.0, 1.0, 1.0, 0.07692307692307693, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.16666666666666669, 0.5581395348837209, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-4647", "mrqa_naturalquestions-validation-6028", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-9235", "mrqa_naturalquestions-validation-9129", "mrqa_naturalquestions-validation-3848", "mrqa_naturalquestions-validation-7962", "mrqa_naturalquestions-validation-8889", "mrqa_naturalquestions-validation-6787", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-34", "mrqa_naturalquestions-validation-1155", "mrqa_naturalquestions-validation-8934", "mrqa_naturalquestions-validation-10683", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-4367", "mrqa_naturalquestions-validation-9773", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-31", "mrqa_newsqa-validation-3189", "mrqa_newsqa-validation-419", "mrqa_searchqa-validation-416"], "SR": 0.59375, "CSR": 0.5484913793103448, "EFR": 0.9615384615384616, "Overall": 0.7355997181697613}, {"timecode": 58, "before_eval_results": {"predictions": ["in the five - year time jump for her brother's wedding to Serena van der Woodsen", "111", "Woodrow Wilson", "Uralic", "22", "1954", "Thomas Jefferson", "2018", "Jesus Christ", "14 November 2001", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "24", "the Coriolis force", "Hugh S. Johnson", "Paul Lynde", "Erica Rivera", "Malina Weissman", "the Qi", "Bo\u00f6tes / bo\u028a\u02c8o\u028ati\u02d0z", "1970", "DeWayne Warren", "the nucleus", "1996", "German", "statistical", "Tom Brady", "in pilgrimages to Jerusalem", "1996", "Coconut Cove, where his classmate Dana Matherson starts bullying him", "Curtis Armstrong", "Hollywood, Los Angeles, California", "Category 4", "Rust", "Karen Gillan", "$19.8 trillion", "1,228 km / h ( 763 mph )", "Tommy Shaw", "warplanes", "Nigel Lythgoe, Mia Michaels, and Adam Shankman", "the federal states of Saxony, Thuringia and Saxony - Anhalt", "Atlanta", "Ricky Nelson", "James Chadwick", "Welch, West Virginia", "Tristan Rogers", "15 February 1998", "the Houston Astros", "Americans who served in the armed forces and as civilians during World War II", "it is bounded in the west by the east coast of Queensland, thereby including the Great Barrier Reef", "the middle of the 15th century", "Gladys Knight & the Pips", "Lago de Nicaragua", "priests or the priesthood", "rue", "Delphi Lawrence", "#5", "Edward James Olmos", "In fashionable neighborhoods of Tokyo customers are lining up for vitamin injections", "Scotland", "U.S. troops", "a heart", "James Stewart", "Frank Sinatra", "Salisbury (now Harare)"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7242267271127565}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, false, true, false, true, false, false, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.46153846153846156, 1.0, 0.23529411764705882, 1.0, 0.7272727272727273, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.07999999999999999, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.23529411764705882, 0.6666666666666666, 1.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 0.5]}}, "before_error_ids": ["mrqa_naturalquestions-validation-132", "mrqa_naturalquestions-validation-2399", "mrqa_naturalquestions-validation-3285", "mrqa_naturalquestions-validation-7242", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-7826", "mrqa_naturalquestions-validation-1584", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-6730", "mrqa_naturalquestions-validation-1310", "mrqa_naturalquestions-validation-7744", "mrqa_naturalquestions-validation-6577", "mrqa_naturalquestions-validation-10410", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8625", "mrqa_naturalquestions-validation-9591", "mrqa_naturalquestions-validation-4719", "mrqa_naturalquestions-validation-4134", "mrqa_triviaqa-validation-1517", "mrqa_hotpotqa-validation-2678", "mrqa_newsqa-validation-3325", "mrqa_newsqa-validation-2503", "mrqa_searchqa-validation-8636", "mrqa_searchqa-validation-9866", "mrqa_hotpotqa-validation-3324"], "SR": 0.609375, "CSR": 0.5495233050847458, "EFR": 0.92, "Overall": 0.7274984110169491}, {"timecode": 59, "before_eval_results": {"predictions": ["three French journalists,", "Six", "\"including taking any and all appropriate personnel actions including termination, discipline and referral of any wrongdoing for criminal prosecution.\"", "New Haven, Connecticut, firefighter Frank Ricci,", "anti-doping", "Wednesday.", "Linda Hogan", "not remove for 24 hours.\"", "Crandon, Wisconsin,", "Turkey", "John Demjanjuk", "Somalia's piracy problem was fueled by environmental and political events", "eight", "U.S.", "Missouri", "\"perezagruzka,\"", "Haiti,", "9", "many as 250,000", "Maj. Nidal Malik Hasan,", "Claud Neilson", "to stop the Afghan opium trade", "Nick Adenhart", "order", "his father", "\"disagreements\" with the Port Authority of New York and New Jersey,", "Operation Pipeline Express", "died in the Holmby Hills, California, mansion he rented.", "Susan Boyle", "10-person", "last April,", "the three masked men who stole four Impressionist paintings worth about $163 million (180 million Swiss francs)", "promotes fuel economy and safety while boosts the economy.", "gasoline", "to do jobs that Arizonans wouldn't do.", "a \"prostitute\"", "digging", "Tottenham", "U.S.", "Barack Obama", "3-2", "\"The deceased appeared to have been there for some time.\"", "that is resonating with those tuning into programming aimed at and featuring the plus-sized.", "56,", "\"And then I joined and then they got this record deal and look what happened.\"", "15-month", "intravenously in operating rooms", "summer", "give detainees greater latitude in selecting legal representation", "heavy turbulence", "Zac Efron", "drivers who were Daytona Pole Award winners, former Daytona 500 pole winners who competed full - time in 2017, and drivers who qualified for the 2017 Playoffs", "the spectroscopic notation for the associated atomic orbitals", "reflects the idea that it should be possible to deliver the summary in the time span of an elevator ride, or approximately thirty seconds to two minutes", "sound and light", "2", "John Buchan", "NCAA Division I Football Bowl Subdivision", "Kristoffer Kristofferson", "Ben Savage", "The Lion, the Witch and the Wardrobe", "Bering Sea", "Eugene", "Ayahuasca"], "metric_results": {"EM": 0.4375, "QA-F1": 0.5178829544638368}, "metric_results_detailed": {"EM": [false, true, false, false, false, true, false, false, false, true, false, true, false, false, false, false, true, true, true, false, false, false, true, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, false, false, false, false, false, true, false, true, false, true, true, true, true, false, true, false, true, false, true, false, false, true, true, true, false, false], "QA-F1": [0.0, 1.0, 0.06666666666666667, 0.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.6666666666666666, 0.11764705882352941, 0.875, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.09523809523809525, 1.0, 0.1, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6111111111111112, 1.0, 0.8205128205128205, 1.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-920", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-1648", "mrqa_newsqa-validation-2391", "mrqa_newsqa-validation-3079", "mrqa_newsqa-validation-2324", "mrqa_newsqa-validation-2118", "mrqa_newsqa-validation-1742", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-1308", "mrqa_newsqa-validation-3593", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-239", "mrqa_newsqa-validation-3614", "mrqa_newsqa-validation-4130", "mrqa_newsqa-validation-3832", "mrqa_newsqa-validation-4030", "mrqa_newsqa-validation-4039", "mrqa_newsqa-validation-318", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-182", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3627", "mrqa_newsqa-validation-3556", "mrqa_newsqa-validation-2122", "mrqa_newsqa-validation-3615", "mrqa_naturalquestions-validation-9149", "mrqa_naturalquestions-validation-4387", "mrqa_triviaqa-validation-7393", "mrqa_hotpotqa-validation-1546", "mrqa_hotpotqa-validation-3871", "mrqa_searchqa-validation-9222", "mrqa_searchqa-validation-13957"], "SR": 0.4375, "CSR": 0.54765625, "EFR": 0.9444444444444444, "Overall": 0.7320138888888889}, {"timecode": 60, "UKR": 0.72265625, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-101", "mrqa_hotpotqa-validation-1066", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-116", "mrqa_hotpotqa-validation-1215", "mrqa_hotpotqa-validation-1223", "mrqa_hotpotqa-validation-1261", "mrqa_hotpotqa-validation-1314", "mrqa_hotpotqa-validation-1398", "mrqa_hotpotqa-validation-1413", "mrqa_hotpotqa-validation-1468", "mrqa_hotpotqa-validation-1475", "mrqa_hotpotqa-validation-1529", "mrqa_hotpotqa-validation-1586", "mrqa_hotpotqa-validation-1672", "mrqa_hotpotqa-validation-1770", "mrqa_hotpotqa-validation-1816", "mrqa_hotpotqa-validation-186", "mrqa_hotpotqa-validation-1865", "mrqa_hotpotqa-validation-2038", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2150", "mrqa_hotpotqa-validation-2187", "mrqa_hotpotqa-validation-2193", "mrqa_hotpotqa-validation-223", "mrqa_hotpotqa-validation-2326", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2615", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2798", "mrqa_hotpotqa-validation-2892", "mrqa_hotpotqa-validation-2983", "mrqa_hotpotqa-validation-2997", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3025", "mrqa_hotpotqa-validation-31", "mrqa_hotpotqa-validation-3123", "mrqa_hotpotqa-validation-3131", "mrqa_hotpotqa-validation-3164", "mrqa_hotpotqa-validation-3165", "mrqa_hotpotqa-validation-3172", "mrqa_hotpotqa-validation-319", "mrqa_hotpotqa-validation-3205", "mrqa_hotpotqa-validation-3238", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3390", "mrqa_hotpotqa-validation-3445", "mrqa_hotpotqa-validation-3454", "mrqa_hotpotqa-validation-3489", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3559", "mrqa_hotpotqa-validation-36", "mrqa_hotpotqa-validation-3642", "mrqa_hotpotqa-validation-3788", "mrqa_hotpotqa-validation-3790", "mrqa_hotpotqa-validation-380", "mrqa_hotpotqa-validation-3963", "mrqa_hotpotqa-validation-3984", "mrqa_hotpotqa-validation-4047", "mrqa_hotpotqa-validation-4101", "mrqa_hotpotqa-validation-4206", "mrqa_hotpotqa-validation-4210", "mrqa_hotpotqa-validation-4418", "mrqa_hotpotqa-validation-4533", "mrqa_hotpotqa-validation-4567", "mrqa_hotpotqa-validation-4726", "mrqa_hotpotqa-validation-4760", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-5111", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5176", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5274", "mrqa_hotpotqa-validation-5365", "mrqa_hotpotqa-validation-5516", "mrqa_hotpotqa-validation-5553", "mrqa_hotpotqa-validation-5599", "mrqa_hotpotqa-validation-5601", "mrqa_hotpotqa-validation-5602", "mrqa_hotpotqa-validation-563", "mrqa_hotpotqa-validation-5669", "mrqa_hotpotqa-validation-5801", "mrqa_hotpotqa-validation-5804", "mrqa_hotpotqa-validation-5840", "mrqa_hotpotqa-validation-5890", "mrqa_hotpotqa-validation-5893", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-813", "mrqa_hotpotqa-validation-831", "mrqa_hotpotqa-validation-834", "mrqa_hotpotqa-validation-887", "mrqa_hotpotqa-validation-961", "mrqa_hotpotqa-validation-994", "mrqa_naturalquestions-validation-10013", "mrqa_naturalquestions-validation-10268", "mrqa_naturalquestions-validation-10364", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10412", "mrqa_naturalquestions-validation-10455", "mrqa_naturalquestions-validation-10525", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1163", "mrqa_naturalquestions-validation-1203", "mrqa_naturalquestions-validation-1344", "mrqa_naturalquestions-validation-1423", "mrqa_naturalquestions-validation-1430", "mrqa_naturalquestions-validation-1479", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1786", "mrqa_naturalquestions-validation-1834", "mrqa_naturalquestions-validation-1955", "mrqa_naturalquestions-validation-2119", "mrqa_naturalquestions-validation-2269", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2351", "mrqa_naturalquestions-validation-2400", "mrqa_naturalquestions-validation-2565", "mrqa_naturalquestions-validation-2665", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2894", "mrqa_naturalquestions-validation-3016", "mrqa_naturalquestions-validation-3182", "mrqa_naturalquestions-validation-3296", "mrqa_naturalquestions-validation-3613", "mrqa_naturalquestions-validation-3841", "mrqa_naturalquestions-validation-3891", "mrqa_naturalquestions-validation-3953", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4002", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4408", "mrqa_naturalquestions-validation-4619", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4917", "mrqa_naturalquestions-validation-5051", "mrqa_naturalquestions-validation-5304", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-5555", "mrqa_naturalquestions-validation-5564", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-6254", "mrqa_naturalquestions-validation-6289", "mrqa_naturalquestions-validation-6349", "mrqa_naturalquestions-validation-6496", "mrqa_naturalquestions-validation-6572", "mrqa_naturalquestions-validation-6660", "mrqa_naturalquestions-validation-7050", "mrqa_naturalquestions-validation-7110", "mrqa_naturalquestions-validation-7115", "mrqa_naturalquestions-validation-7350", "mrqa_naturalquestions-validation-7367", "mrqa_naturalquestions-validation-7627", "mrqa_naturalquestions-validation-777", "mrqa_naturalquestions-validation-7857", "mrqa_naturalquestions-validation-7910", "mrqa_naturalquestions-validation-8201", "mrqa_naturalquestions-validation-8220", "mrqa_naturalquestions-validation-8239", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8444", "mrqa_naturalquestions-validation-8619", "mrqa_naturalquestions-validation-8688", "mrqa_naturalquestions-validation-8695", "mrqa_naturalquestions-validation-8847", "mrqa_naturalquestions-validation-8884", "mrqa_naturalquestions-validation-8947", "mrqa_naturalquestions-validation-8958", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-919", "mrqa_naturalquestions-validation-922", "mrqa_naturalquestions-validation-9275", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-941", "mrqa_naturalquestions-validation-9419", "mrqa_naturalquestions-validation-9457", "mrqa_naturalquestions-validation-946", "mrqa_naturalquestions-validation-9620", "mrqa_naturalquestions-validation-9652", "mrqa_naturalquestions-validation-9773", "mrqa_newsqa-validation-1076", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1177", "mrqa_newsqa-validation-1248", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1444", "mrqa_newsqa-validation-1473", "mrqa_newsqa-validation-1524", "mrqa_newsqa-validation-1560", "mrqa_newsqa-validation-1637", "mrqa_newsqa-validation-1675", "mrqa_newsqa-validation-1705", "mrqa_newsqa-validation-1770", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1804", "mrqa_newsqa-validation-1860", "mrqa_newsqa-validation-2009", "mrqa_newsqa-validation-2032", "mrqa_newsqa-validation-2065", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2161", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2225", "mrqa_newsqa-validation-2230", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2243", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-2301", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2335", "mrqa_newsqa-validation-2412", "mrqa_newsqa-validation-2431", "mrqa_newsqa-validation-2455", "mrqa_newsqa-validation-2456", "mrqa_newsqa-validation-2606", "mrqa_newsqa-validation-2607", "mrqa_newsqa-validation-2702", "mrqa_newsqa-validation-2730", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-2840", "mrqa_newsqa-validation-2908", "mrqa_newsqa-validation-2967", "mrqa_newsqa-validation-2979", "mrqa_newsqa-validation-3069", "mrqa_newsqa-validation-3273", "mrqa_newsqa-validation-3300", "mrqa_newsqa-validation-3354", "mrqa_newsqa-validation-3392", "mrqa_newsqa-validation-3491", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3507", "mrqa_newsqa-validation-3519", "mrqa_newsqa-validation-3581", "mrqa_newsqa-validation-3659", "mrqa_newsqa-validation-3720", "mrqa_newsqa-validation-38", "mrqa_newsqa-validation-3862", "mrqa_newsqa-validation-3873", "mrqa_newsqa-validation-3917", "mrqa_newsqa-validation-3941", "mrqa_newsqa-validation-3947", "mrqa_newsqa-validation-3976", "mrqa_newsqa-validation-4050", "mrqa_newsqa-validation-4058", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4100", "mrqa_newsqa-validation-4159", "mrqa_newsqa-validation-445", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-475", "mrqa_newsqa-validation-540", "mrqa_newsqa-validation-574", "mrqa_newsqa-validation-583", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-649", "mrqa_newsqa-validation-659", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-783", "mrqa_newsqa-validation-909", "mrqa_newsqa-validation-92", "mrqa_newsqa-validation-93", "mrqa_newsqa-validation-987", "mrqa_searchqa-validation-10088", "mrqa_searchqa-validation-1017", "mrqa_searchqa-validation-10461", "mrqa_searchqa-validation-10585", "mrqa_searchqa-validation-10620", "mrqa_searchqa-validation-10667", "mrqa_searchqa-validation-10689", "mrqa_searchqa-validation-10881", "mrqa_searchqa-validation-11019", "mrqa_searchqa-validation-11030", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11302", "mrqa_searchqa-validation-11423", "mrqa_searchqa-validation-11491", "mrqa_searchqa-validation-11693", "mrqa_searchqa-validation-11749", "mrqa_searchqa-validation-1178", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11890", "mrqa_searchqa-validation-119", "mrqa_searchqa-validation-11925", "mrqa_searchqa-validation-12105", "mrqa_searchqa-validation-12155", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12416", "mrqa_searchqa-validation-12441", "mrqa_searchqa-validation-13042", "mrqa_searchqa-validation-13244", "mrqa_searchqa-validation-13367", "mrqa_searchqa-validation-13691", "mrqa_searchqa-validation-13915", "mrqa_searchqa-validation-14097", "mrqa_searchqa-validation-14323", "mrqa_searchqa-validation-1442", "mrqa_searchqa-validation-14450", "mrqa_searchqa-validation-14481", "mrqa_searchqa-validation-15228", "mrqa_searchqa-validation-15948", "mrqa_searchqa-validation-16035", "mrqa_searchqa-validation-1607", "mrqa_searchqa-validation-16155", "mrqa_searchqa-validation-16205", "mrqa_searchqa-validation-16382", "mrqa_searchqa-validation-16388", "mrqa_searchqa-validation-16571", "mrqa_searchqa-validation-16963", "mrqa_searchqa-validation-1817", "mrqa_searchqa-validation-2294", "mrqa_searchqa-validation-246", "mrqa_searchqa-validation-2634", "mrqa_searchqa-validation-2888", "mrqa_searchqa-validation-2890", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3038", "mrqa_searchqa-validation-3299", "mrqa_searchqa-validation-3497", "mrqa_searchqa-validation-3693", "mrqa_searchqa-validation-4656", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4861", "mrqa_searchqa-validation-5048", "mrqa_searchqa-validation-544", "mrqa_searchqa-validation-5543", "mrqa_searchqa-validation-5612", "mrqa_searchqa-validation-5975", "mrqa_searchqa-validation-6091", "mrqa_searchqa-validation-6188", "mrqa_searchqa-validation-6223", "mrqa_searchqa-validation-697", "mrqa_searchqa-validation-7021", "mrqa_searchqa-validation-706", "mrqa_searchqa-validation-748", "mrqa_searchqa-validation-7603", "mrqa_searchqa-validation-7707", "mrqa_searchqa-validation-7764", "mrqa_searchqa-validation-7773", "mrqa_searchqa-validation-793", "mrqa_searchqa-validation-8014", "mrqa_searchqa-validation-8198", "mrqa_searchqa-validation-8328", "mrqa_searchqa-validation-8480", "mrqa_searchqa-validation-8488", "mrqa_searchqa-validation-8501", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8850", "mrqa_searchqa-validation-892", "mrqa_searchqa-validation-8942", "mrqa_searchqa-validation-9852", "mrqa_searchqa-validation-9911", "mrqa_searchqa-validation-9935", "mrqa_squad-validation-10121", "mrqa_squad-validation-10128", "mrqa_squad-validation-10158", "mrqa_squad-validation-10167", "mrqa_squad-validation-1020", "mrqa_squad-validation-10232", "mrqa_squad-validation-10300", "mrqa_squad-validation-1088", "mrqa_squad-validation-1131", "mrqa_squad-validation-1160", "mrqa_squad-validation-1264", "mrqa_squad-validation-1368", "mrqa_squad-validation-1419", "mrqa_squad-validation-1876", "mrqa_squad-validation-2106", "mrqa_squad-validation-2165", "mrqa_squad-validation-2293", "mrqa_squad-validation-239", "mrqa_squad-validation-2395", "mrqa_squad-validation-2480", "mrqa_squad-validation-2491", "mrqa_squad-validation-2652", "mrqa_squad-validation-2668", "mrqa_squad-validation-3001", "mrqa_squad-validation-3390", "mrqa_squad-validation-3597", "mrqa_squad-validation-3626", "mrqa_squad-validation-3680", "mrqa_squad-validation-3932", "mrqa_squad-validation-4080", "mrqa_squad-validation-4176", "mrqa_squad-validation-4264", "mrqa_squad-validation-435", "mrqa_squad-validation-4418", "mrqa_squad-validation-4472", "mrqa_squad-validation-48", "mrqa_squad-validation-4815", "mrqa_squad-validation-4857", "mrqa_squad-validation-4947", "mrqa_squad-validation-4956", "mrqa_squad-validation-5173", "mrqa_squad-validation-5236", "mrqa_squad-validation-5991", "mrqa_squad-validation-6079", "mrqa_squad-validation-6182", "mrqa_squad-validation-6284", "mrqa_squad-validation-6342", "mrqa_squad-validation-6435", "mrqa_squad-validation-6509", "mrqa_squad-validation-7028", "mrqa_squad-validation-7244", "mrqa_squad-validation-7306", "mrqa_squad-validation-7395", "mrqa_squad-validation-7474", "mrqa_squad-validation-7704", "mrqa_squad-validation-7827", "mrqa_squad-validation-7869", "mrqa_squad-validation-8174", "mrqa_squad-validation-8230", "mrqa_squad-validation-8278", "mrqa_squad-validation-8383", "mrqa_squad-validation-8433", "mrqa_squad-validation-8475", "mrqa_squad-validation-8636", "mrqa_squad-validation-8705", "mrqa_squad-validation-8815", "mrqa_squad-validation-882", "mrqa_squad-validation-902", "mrqa_squad-validation-9047", "mrqa_squad-validation-9142", "mrqa_squad-validation-9209", "mrqa_squad-validation-9315", "mrqa_squad-validation-9499", "mrqa_squad-validation-9570", "mrqa_squad-validation-9661", "mrqa_squad-validation-9711", "mrqa_squad-validation-9912", "mrqa_squad-validation-9918", "mrqa_squad-validation-9935", "mrqa_squad-validation-9986", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1145", "mrqa_triviaqa-validation-1251", "mrqa_triviaqa-validation-1298", "mrqa_triviaqa-validation-1470", "mrqa_triviaqa-validation-1498", "mrqa_triviaqa-validation-1516", "mrqa_triviaqa-validation-1575", "mrqa_triviaqa-validation-1710", "mrqa_triviaqa-validation-1841", "mrqa_triviaqa-validation-2019", "mrqa_triviaqa-validation-203", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2350", "mrqa_triviaqa-validation-241", "mrqa_triviaqa-validation-288", "mrqa_triviaqa-validation-3145", "mrqa_triviaqa-validation-319", "mrqa_triviaqa-validation-3354", "mrqa_triviaqa-validation-3371", "mrqa_triviaqa-validation-3376", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3579", "mrqa_triviaqa-validation-3581", "mrqa_triviaqa-validation-3722", "mrqa_triviaqa-validation-3845", "mrqa_triviaqa-validation-3896", "mrqa_triviaqa-validation-4001", "mrqa_triviaqa-validation-4093", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4151", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4212", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-4481", "mrqa_triviaqa-validation-4573", "mrqa_triviaqa-validation-4595", "mrqa_triviaqa-validation-4630", "mrqa_triviaqa-validation-4649", "mrqa_triviaqa-validation-4675", "mrqa_triviaqa-validation-5006", "mrqa_triviaqa-validation-5089", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5358", "mrqa_triviaqa-validation-5509", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5726", "mrqa_triviaqa-validation-5781", "mrqa_triviaqa-validation-5821", "mrqa_triviaqa-validation-5886", "mrqa_triviaqa-validation-5973", "mrqa_triviaqa-validation-5994", "mrqa_triviaqa-validation-6024", "mrqa_triviaqa-validation-6127", "mrqa_triviaqa-validation-6186", "mrqa_triviaqa-validation-6206", "mrqa_triviaqa-validation-6238", "mrqa_triviaqa-validation-626", "mrqa_triviaqa-validation-6315", "mrqa_triviaqa-validation-6508", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-670", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6981", "mrqa_triviaqa-validation-7128", "mrqa_triviaqa-validation-7145", "mrqa_triviaqa-validation-7393", "mrqa_triviaqa-validation-7457", "mrqa_triviaqa-validation-7477", "mrqa_triviaqa-validation-7488", "mrqa_triviaqa-validation-7602", "mrqa_triviaqa-validation-7710", "mrqa_triviaqa-validation-937"], "OKR": 0.86328125, "KG": 0.4765625, "before_eval_results": {"predictions": ["$7.8 million", "prostate cancer,", "Donald Duck", "Whitney Houston", "newly discovered DNA evidence", "South Africa", "consumer confidence", "Saturn", "Prague", "35,000.", "Osama", "The EU naval force", "Kerstin Fritzl,", "threatening messages", "in the west African nation", "misdemeanor", "New Haven, Connecticut,", "$273 million", "the mastermind behind the September 11, 2001, terrorist attacks on the United States.", "Haiti", "air support.", "20", "$250,000 for Rivers' charity: God's Love We Deliver.", "February 5,", "two", "Blacks and Hispanics", "Australian officials", "Alberto Espinoza Barron,", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "Rev. Alberto Cutie", "meter reader who led authorities last week to remains believed to be those of Caylee Anthony called police four months ago,", "the man facing up, with his arms out to the side.", "Garth Brooks", "Monday's", "Friday,", "10,000", "Ryan Adams.", "$1.5 million.", "three out of four", "Six alleged victims, who are relatives of the five suspects,", "a dad.", "581 points", "his health and about a comeback.", "up", "\"utterly baseless.\"", "Mexicans who are unemployed or underemployed", "Caylee,", "1-0", "her boyfriend,", "Lonnie", "the first", "either by being sprinkled over their heads or, in English - speaking countries", "Tokyo for the 2020 Summer Olympics", "extends the benefits of the US privacy Act to Europeans and gives them access to US courts", "Doncaster Rovers", "Usain Bolt", "1973", "2,099", "PPG Paints Arena", "2017", "lactic acid", "The Greatest Show on Earth", "a prophet", "Germany"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7411409469154034}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, true, true, false, true, false, true, false, true, false, true, false, true, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, false, true, true, true, false, false, false, false, true, true, false, false, false, true, true, true, false, false, false, true, true, true, true, true, true, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 0.19999999999999998, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.18181818181818182, 1.0, 1.0, 1.0, 1.0, 1.0, 0.17391304347826084, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.6153846153846153, 0.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 0.33333333333333337, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-110", "mrqa_newsqa-validation-56", "mrqa_newsqa-validation-2823", "mrqa_newsqa-validation-500", "mrqa_newsqa-validation-2906", "mrqa_newsqa-validation-3425", "mrqa_newsqa-validation-2710", "mrqa_newsqa-validation-2998", "mrqa_newsqa-validation-378", "mrqa_newsqa-validation-2817", "mrqa_newsqa-validation-467", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-4028", "mrqa_newsqa-validation-1729", "mrqa_newsqa-validation-451", "mrqa_newsqa-validation-3446", "mrqa_newsqa-validation-999", "mrqa_newsqa-validation-3229", "mrqa_naturalquestions-validation-251", "mrqa_naturalquestions-validation-7635", "mrqa_naturalquestions-validation-7987", "mrqa_searchqa-validation-12647"], "SR": 0.640625, "CSR": 0.5491803278688525, "EFR": 1.0, "Overall": 0.7223360655737705}, {"timecode": 61, "before_eval_results": {"predictions": ["Windows Easy Transfer", "John Cooper Clarke", "Charlotte of Mecklenburg - Strelitz", "O'Meara", "Judi Dench", "the efficient and effective management of money ( funds ) in such a manner as to accomplish the objectives of the organization", "Omar Khayyam", "P.V. Sindhu", "1665 to 1666", "Saturday", "1982", "Siddharth Arora / Vibhav Roy", "1949", "mitosis", "Butter Island off North Haven, Maine in the Penobscot Bay", "if the player busts, the player loses, regardless of whether the dealer subsequently busts", "Pat McCormick", "10,605", "U.S. Electoral College", "American country music artists Reba McEntire and Linda Davis", "August Darnell", "provinces along the Yangtze River and in provinces in the south", "New York City", "July 21, 1861", "Nashville, Tennessee", "Paspahegh Indians", "cella", "about 375 miles ( 600 km ) south of Newfoundland", "April 12, 2017", "October 2012", "Dawn French, Timomatic and Geri Halliwell", "John Joseph Patrick Ryan", "49 cents", "counter clockwise", "Kit Harington", "Anatomy", "above the light source and under the sample in an upright microscope", "divergent tectonic", "Organisms in the domains of Archaea and Bacteria", "Pure water is neutral, at pH 7 ( 25 \u00b0 C ), being neither an acid nor a base", "Speaker of the House of Representatives", "1877", "18", "SUR FaceA of ROOTS", "norm that sovereigns had no internal equals within a defined territory and no external superiors as the ultimate authority within the territory's sovereign borders", "winter festivals", "a recognized group of people who jointly oversee the activities of an organization, which can be either a for - profit business, nonprofit organization, or a government agency", "the 1820s", "The Royalettes", "Katherine Kiernan Maria", "Fred E. Ahlert", "John Terry", "\"Land of the Rising Sun\".", "James Hogg", "Lawrence", "\"Realty Bites\"", "national aviation branch", "Thessaloniki and Athens,", "prostate cancer,", "a stagecoach", "chicken Little", "Saturn", "Russia", "Isolde"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6613611043752758}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, true, true, false, true, true, true, true, false, true, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3636363636363636, 0.0, 0.6153846153846153, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.6428571428571429, 0.6666666666666666, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.9302325581395349, 0.5, 0.625, 1.0, 1.0, 0.7499999999999999, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-3302", "mrqa_naturalquestions-validation-7862", "mrqa_naturalquestions-validation-10603", "mrqa_naturalquestions-validation-3672", "mrqa_naturalquestions-validation-8845", "mrqa_naturalquestions-validation-9107", "mrqa_naturalquestions-validation-3962", "mrqa_naturalquestions-validation-7714", "mrqa_naturalquestions-validation-9078", "mrqa_naturalquestions-validation-4132", "mrqa_naturalquestions-validation-2951", "mrqa_naturalquestions-validation-10357", "mrqa_naturalquestions-validation-8652", "mrqa_naturalquestions-validation-6337", "mrqa_naturalquestions-validation-1704", "mrqa_naturalquestions-validation-3559", "mrqa_naturalquestions-validation-7266", "mrqa_naturalquestions-validation-2426", "mrqa_naturalquestions-validation-3163", "mrqa_naturalquestions-validation-9559", "mrqa_triviaqa-validation-406", "mrqa_triviaqa-validation-1975", "mrqa_hotpotqa-validation-2181", "mrqa_hotpotqa-validation-2374", "mrqa_hotpotqa-validation-3093", "mrqa_newsqa-validation-117", "mrqa_newsqa-validation-3398", "mrqa_newsqa-validation-3859", "mrqa_triviaqa-validation-6243"], "SR": 0.546875, "CSR": 0.5491431451612903, "EFR": 0.9310344827586207, "Overall": 0.7085355255839823}, {"timecode": 62, "before_eval_results": {"predictions": ["1976", "Bacon", "from 1922 to 1991", "73", "Gibraltar", "1 January 1904", "Thebes", "Brooke Wexler", "October 2", "in the 1980s", "Alabama", "Evermoist", "in the mid - to late 1920s", "differential erosion", "Kanawha River", "Graham McTavish", "Thomas Alva Edison", "since been adopted by five other countries", "the rise of literacy", "Richard Masur", "Frankie Valli", "one", "JackScanlon", "Saturday", "Gametes", "sometime in 2018", "2015", "Sarah Josepha Hale", "Billy Gibbons", "Ledger", "known locally as the International Border ( IB )", "in a 1945 NCAA game between Columbia and Fordham", "2017", "permanently absorbed the superhuman powers and the psyche of Carol Danvers", "in a brownstone in Brooklyn Heights, New York", "Clare Torry", "6 March 1983", "during Christmas season in the late 1970s", "1986", "1939", "Himadri Station", "to YouTube", "birch", "February 2017 in Japan and in March 2018 in North America and Europe", "FaZe Rug", "at the fictional elite conservative Vermont boarding school Welton Academy", "1973", "9 February 2018", "The long - hair gene is recessive", "94 by 50 feet", "Tom\u00e1s de Torquemada", "1981", "Kent", "Spanish", "Dusty Dvoracek", "South America", "Los Angeles", "has an inspiration: U.S. President Barack Obama.", "the foyer of the BBC building in Glasgow, Scotland", "to disrupt the inauguration,", "Frederic Chopin", "spring", "pesos", "The Rev. Alberto Cutie"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6740688131313131}, "metric_results_detailed": {"EM": [false, true, false, false, true, false, false, true, false, true, false, true, true, false, true, true, true, false, true, true, true, false, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, false, true, false, true, true, true, true, true, false, true, true, true, true, true, false, false, false, true, true, true, true], "QA-F1": [0.0, 1.0, 0.0, 0.0, 1.0, 0.35294117647058826, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.5, 0.8181818181818181, 0.7058823529411764, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2666666666666667, 1.0, 0.9411764705882353, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.22222222222222224, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-10034", "mrqa_naturalquestions-validation-7080", "mrqa_naturalquestions-validation-2124", "mrqa_naturalquestions-validation-2168", "mrqa_naturalquestions-validation-1284", "mrqa_naturalquestions-validation-2095", "mrqa_naturalquestions-validation-4139", "mrqa_naturalquestions-validation-1969", "mrqa_naturalquestions-validation-1433", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-4995", "mrqa_naturalquestions-validation-6931", "mrqa_naturalquestions-validation-9384", "mrqa_naturalquestions-validation-7095", "mrqa_naturalquestions-validation-1018", "mrqa_naturalquestions-validation-9246", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-7214", "mrqa_naturalquestions-validation-4433", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-7084", "mrqa_naturalquestions-validation-3329", "mrqa_naturalquestions-validation-5791", "mrqa_triviaqa-validation-4726", "mrqa_newsqa-validation-3076", "mrqa_newsqa-validation-219", "mrqa_newsqa-validation-1330"], "SR": 0.578125, "CSR": 0.5496031746031746, "EFR": 1.0, "Overall": 0.722420634920635}, {"timecode": 63, "before_eval_results": {"predictions": ["a Malay Peninsula", "Reggie Jackson", "Nova Scotia", "a chainmaille", "osca", "a port", "Morocco", "\"The Wings of the Dove\"", "WolfeScience.com", "osca", "Pop art", "embalming", "Port of Portland", "Rihanna", "Dionysus", "a coral reef fish", "symbiosis", "space shuttle", "a American politician and businessman who was the 46th Vice President", "When You Look Me In The Eyes", "The Lost World", "Prince Edward Island", "New York Presbyterian Hospital", "the Bosporus", "Red Heat", "Atlas Mountains", "kafkaesque", "Heather Mills McCartney", "snow", "Paris", "Mont Blanc On", "Rene Lacoste", "preemption", "the Nobel Prize", "osca", "osca", "Jawaharlal Nehru", "around 5:00 p.m., the end", "The Agony and the Ecstasy", "a cat", "congruent", "Spain", "toad", "San Francisco", "A Brief History of Time", "a crossword puzzle", "Macy's", "a wife of Bath's Tale", "a psychic midget", "\" Hillary's America\"", "Benazir Bhutto", "CBS", "Gibraltar", "Orographic lift", "Virginia Plain", "strawberry", "Venice", "Central African Republic", "Arnold M\u00e6rsk Mc- Kinney M\u00f8ller Foundation", "Stephen King", "Pat Quinn", "murder in the beating death of a company boss who fired them.", "\"Walk -- Don't Run\"", "the Government of South Australia"], "metric_results": {"EM": 0.484375, "QA-F1": 0.548251488095238}, "metric_results_detailed": {"EM": [false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, false, true, false, false, false, true, true, false, true, true, true, true, false, false, true, false, true, true, true, false, false, true, false, false, false, true, true, false, true, true, true, true, false, false, false, true, true, true, true, true, true, true, false, false, true, true, false, false, false], "QA-F1": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5714285714285715, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0]}}, "before_error_ids": ["mrqa_searchqa-validation-8210", "mrqa_searchqa-validation-994", "mrqa_searchqa-validation-7517", "mrqa_searchqa-validation-7176", "mrqa_searchqa-validation-1923", "mrqa_searchqa-validation-11561", "mrqa_searchqa-validation-4487", "mrqa_searchqa-validation-11842", "mrqa_searchqa-validation-661", "mrqa_searchqa-validation-8555", "mrqa_searchqa-validation-11205", "mrqa_searchqa-validation-6076", "mrqa_searchqa-validation-3664", "mrqa_searchqa-validation-1684", "mrqa_searchqa-validation-3641", "mrqa_searchqa-validation-1986", "mrqa_searchqa-validation-1712", "mrqa_searchqa-validation-3059", "mrqa_searchqa-validation-14989", "mrqa_searchqa-validation-7824", "mrqa_searchqa-validation-16099", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-777", "mrqa_searchqa-validation-16392", "mrqa_searchqa-validation-9589", "mrqa_searchqa-validation-15031", "mrqa_searchqa-validation-4348", "mrqa_hotpotqa-validation-3558", "mrqa_hotpotqa-validation-5688", "mrqa_newsqa-validation-3564", "mrqa_newsqa-validation-2308", "mrqa_hotpotqa-validation-883"], "SR": 0.484375, "CSR": 0.548583984375, "EFR": 1.0, "Overall": 0.722216796875}, {"timecode": 64, "before_eval_results": {"predictions": ["a person of Latin American", "Bonnie and Clyde", "Forrest Gump", "a relationship with a man who proves to", "The Gunman", "Thomas Beekman", "I Have No Mouth", "Friday Night Lights", "contractions", "skull and crossbones", "India", "Florida State", "Ukraine", "Boston", "Tibetan", "The Godfather", "a bolt", "Australia", "Napalm", "Roald Dahl", "Mount Kenya", "John Lennon", "the Stamp Act", "Princeton University", "CO2", "The Battle of Thermopylae", "Buenos Aires", "Mulberry Street", "Romeo & Juliet", "Prescott", "Helen Hayes", "Wesley Clark", "cobalt", "Sing Sing", "salmon", "the falling star", "Herman Melville", "Abercrombie & Fitch", "Beatrix Potter", "the Romaunt", "a cassowary", "the Gadsden Purchase", "the umbilical cord", "trees", "Sweden", "the House of Lords", "the Red Cross", "terrorists", "The Sunshine Band", "the Somme", "Graceland", "David Tennant", "Chesapeake Bay", "pulmonary heart disease", "1", "squash", "Facebook", "Gospel Starlighter", "500-room", "Wal-Mart Canada Corp.", "Tuesday,", "Diversity,", "were directly involved in an Internet broadband deal with a Chinese firm.", "gang rape"], "metric_results": {"EM": 0.546875, "QA-F1": 0.636156798245614}, "metric_results_detailed": {"EM": [false, false, true, false, false, false, false, false, true, false, true, true, false, false, false, false, true, true, true, true, true, true, true, false, false, false, false, true, true, false, false, true, true, true, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true], "QA-F1": [0.0, 0.4, 1.0, 0.0, 0.0, 0.0, 0.0, 0.8, 1.0, 0.8, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.9473684210526316, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-11200", "mrqa_searchqa-validation-10759", "mrqa_searchqa-validation-15728", "mrqa_searchqa-validation-6468", "mrqa_searchqa-validation-2776", "mrqa_searchqa-validation-3221", "mrqa_searchqa-validation-16325", "mrqa_searchqa-validation-691", "mrqa_searchqa-validation-16100", "mrqa_searchqa-validation-8741", "mrqa_searchqa-validation-5378", "mrqa_searchqa-validation-9441", "mrqa_searchqa-validation-4727", "mrqa_searchqa-validation-11743", "mrqa_searchqa-validation-2681", "mrqa_searchqa-validation-6512", "mrqa_searchqa-validation-13467", "mrqa_searchqa-validation-12394", "mrqa_searchqa-validation-6678", "mrqa_searchqa-validation-7699", "mrqa_searchqa-validation-15724", "mrqa_searchqa-validation-15707", "mrqa_searchqa-validation-14787", "mrqa_searchqa-validation-3727", "mrqa_naturalquestions-validation-5912", "mrqa_triviaqa-validation-724", "mrqa_triviaqa-validation-1283", "mrqa_hotpotqa-validation-2565", "mrqa_newsqa-validation-3111"], "SR": 0.546875, "CSR": 0.5485576923076922, "EFR": 1.0, "Overall": 0.7222115384615384}, {"timecode": 65, "before_eval_results": {"predictions": ["the Blue Ridge Parkway", "3,384,569", "Vishal Bhardwaj", "around 169 CE", "Ed O'Neill", "Milwaukee Bucks", "138,535 people", "Dennis Hull, as well as painter Manley MacDonald.", "Max Martin, Savan Kotecha and Ilya Salmanzadeh", "a jersey", "Love Letter", "Brazil", "March 14, 1928", "September 6, 1961", "Stacey Kent", "Shenandoah National Park", "Regional League North", "Campeonato Brasileiro S\u00e9rie A", "Samantha Spiro", "William Shakespeare", "1.6 million", "the Yule goat", "West Africa", "West Tambaram", "Portal A Interactive", "Graduados", "Sada Carolyn Thompson", "World Health Organization", "Chow Tai Fook Enterprises", "Michelle Anne Sinclair", "2011", "2012", "Honolulu", "Lalit", "Kal Ho Naa Ho", "in Kolkata", "Hollywood actor", "musicology", "left", "1835", "1926 Paris during the period of the Lost Generation", "Erreway", "Forbes", "January 28, 2016", "69.7 million litres", "500-room", "Ukraine", "2027 Fairmount Avenue", "southern portion of Carroll County", "Black Panther Party", "globetrotters", "in the United Kingdom", "into the intermembrane space", "Butter Island off North Haven, Maine in the Penobscot Bay", "eye", "Leo Tolstoy", "gizzard", "the trip had caused fury among some in the military who saw it as a waste of time and money at a time when British forces are thinly-stretched, fighting in Iraq and Afghanistan.", "2,000", "Some have complained that his wins are too routine, and purists grouse that he does not poses the quality of \"hinkaku,\"", "baron", "Shirley Jackson", "Sue Miller", "Dan Parris,"], "metric_results": {"EM": 0.515625, "QA-F1": 0.63383722899729}, "metric_results_detailed": {"EM": [false, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, false, true, true, false, true, true, false, true, true, false, true, true, false, true, true, false, false, true, true, false, false, true, false, true, true, true, false, true, false, false, false, false, false, true, true, true, false, false, true, false, false, true, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 1.0, 0.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.4, 0.4444444444444445, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.4, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4878048780487806, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4122", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1161", "mrqa_hotpotqa-validation-4852", "mrqa_hotpotqa-validation-1363", "mrqa_hotpotqa-validation-2341", "mrqa_hotpotqa-validation-2328", "mrqa_hotpotqa-validation-5154", "mrqa_hotpotqa-validation-2360", "mrqa_hotpotqa-validation-2970", "mrqa_hotpotqa-validation-1526", "mrqa_hotpotqa-validation-5108", "mrqa_hotpotqa-validation-4634", "mrqa_hotpotqa-validation-963", "mrqa_hotpotqa-validation-4553", "mrqa_hotpotqa-validation-2134", "mrqa_hotpotqa-validation-5573", "mrqa_hotpotqa-validation-3587", "mrqa_hotpotqa-validation-530", "mrqa_hotpotqa-validation-3343", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-1978", "mrqa_hotpotqa-validation-5265", "mrqa_hotpotqa-validation-4162", "mrqa_naturalquestions-validation-6912", "mrqa_naturalquestions-validation-180", "mrqa_triviaqa-validation-3498", "mrqa_newsqa-validation-1287", "mrqa_newsqa-validation-1122", "mrqa_searchqa-validation-11103", "mrqa_newsqa-validation-2296"], "SR": 0.515625, "CSR": 0.5480587121212122, "EFR": 0.967741935483871, "Overall": 0.7156601295210165}, {"timecode": 66, "before_eval_results": {"predictions": ["musician", "Captain Hans Geering", "the 50JJB Sports Fitness Clubs and the attached retail stores", "October 2015", "the Jacksonville Aviation Authority", "(Han) Sung-soo", "Bhushan Patel", "the widow of veteran film director Yash Chopra", "Mark O'Connor", "Kinnairdy Castle", "South African", "Agent 99", "The 2008\u201309 UEFA Champions League", "National Hockey League", "his advocacy", "Parlophone Records", "a Soldier in Truck", "eight", "Cuban", "arts manager", "\"The Royal Family\".", "girls aged 11 to 18 in Boston, Lincolnshire, England", "Jackie Harris", "water", "the National Basketball Development League", "Operation Overlord", "invoice", "Sir Christopher Wren", "1851", "the first month of World War I", "12-year", "World War II", "Graham Payn", "Martin Truex Jr.", "twice", "Malayalam cinema", "47,818", "Every Rose Has Its Thorn", "13", "The chain is now part of the Ahold Delhaize group based in the Netherlands.", "1953", "German", "Sc scapegoat Mountain", "the recording debut of future AC/DC founders Angus Young and Malcolm Young", "Boston Celtics", "1912", "Dutch", "Bill Curry", "the youngest publicly documented people to be identified as transgender, and for being the youngest person to become a national transgender figure.", "1968", "311", "Jason Lee", "Hans Zimmer", "secession", "the Florida Current", "The History Boys", "a Siamese", "Mexico", "3,500", "'overcharged.'\"", "Dame Melba", "Gary", "Henry Hudson", "the optic disc"], "metric_results": {"EM": 0.484375, "QA-F1": 0.610639880952381}, "metric_results_detailed": {"EM": [false, true, false, false, false, false, false, false, true, true, false, false, false, false, false, true, false, false, true, false, true, false, false, true, true, false, true, false, true, false, false, true, true, true, true, false, true, true, false, false, true, true, false, false, true, true, true, false, false, true, true, true, true, false, false, true, true, true, false, true, false, true, true, false], "QA-F1": [0.6666666666666666, 1.0, 0.4615384615384615, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0, 1.0, 0.4, 0.5, 1.0, 0.0, 1.0, 0.7142857142857143, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5384615384615384, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_hotpotqa-validation-3870", "mrqa_hotpotqa-validation-5603", "mrqa_hotpotqa-validation-1698", "mrqa_hotpotqa-validation-3796", "mrqa_hotpotqa-validation-4167", "mrqa_hotpotqa-validation-164", "mrqa_hotpotqa-validation-4087", "mrqa_hotpotqa-validation-1034", "mrqa_hotpotqa-validation-1991", "mrqa_hotpotqa-validation-2080", "mrqa_hotpotqa-validation-5587", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-4661", "mrqa_hotpotqa-validation-4517", "mrqa_hotpotqa-validation-1187", "mrqa_hotpotqa-validation-5830", "mrqa_hotpotqa-validation-4061", "mrqa_hotpotqa-validation-3917", "mrqa_hotpotqa-validation-656", "mrqa_hotpotqa-validation-2842", "mrqa_hotpotqa-validation-1807", "mrqa_hotpotqa-validation-5841", "mrqa_hotpotqa-validation-5724", "mrqa_hotpotqa-validation-5294", "mrqa_hotpotqa-validation-4906", "mrqa_hotpotqa-validation-3127", "mrqa_hotpotqa-validation-5251", "mrqa_naturalquestions-validation-685", "mrqa_triviaqa-validation-2823", "mrqa_newsqa-validation-2935", "mrqa_searchqa-validation-7226", "mrqa_naturalquestions-validation-3368"], "SR": 0.484375, "CSR": 0.5471082089552239, "EFR": 1.0, "Overall": 0.7219216417910448}, {"timecode": 67, "before_eval_results": {"predictions": ["classical", "biochemist and academic Dr. Alberto Taquini", "democracy and personal freedom", "Rudolf Kehrer", "2015", "Austria Wien", "926 East McLemore Avenue", "Edward Albert Heimberger", "Squam Lake", "lambics", "Croatian", "the Harpe brothers", "Stargate", "Marvel's Agent Carter", "Everton", "shorthand writing", "twelfth", "coal mining town", "31 July 1975", "Dark Heresy", "Theodore Robert Bundy", "1943", "university", "Humvee", "Malta", "East Knoyle", "Philadelphia", "Maria Brink", "Jyothika Sadanah", "\"Sippin' on Some Sizzurp", "24 January 76 \u2013 10 July 138", "Leonard Cohen", "General Theological Seminary", "BraveStarr", "25 million records", "Paul Avery", "Sunflower County", "848 km", "Ellesmere Port", "Homer Hickam, Jr.", "South America", "Toronto", "Eugene", "Chief of the Operations Staff of the Armed Forces High Command", "CBS News", "Philadelphia, Pennsylvania", "Parlophone", "June", "the best known globetrotters", "Henry Lau", "John Schlesinger", "Pasek & Paul", "Diary of a Wimpy Kid", "Ed Sheeran", "Sarah Palin", "Ub Iwerks", "The Royal", "onto the college campus", "14", "one American diplomat to a \"prostitute\"", "touchpad", "Beaker", "Damascus", "fire at her apartment near Fort Bragg in North Carolina."], "metric_results": {"EM": 0.546875, "QA-F1": 0.6678639069264068}, "metric_results_detailed": {"EM": [false, false, true, false, false, true, false, true, true, true, true, true, true, false, false, false, false, false, true, true, false, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, true, false, true, true, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, false, true, false, false], "QA-F1": [0.18181818181818182, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.4, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 1.0, 0.8, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.9090909090909091, 0.0, 1.0, 0.0, 0.28571428571428564]}}, "before_error_ids": ["mrqa_hotpotqa-validation-1530", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-705", "mrqa_hotpotqa-validation-259", "mrqa_hotpotqa-validation-4805", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1571", "mrqa_hotpotqa-validation-5456", "mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-4350", "mrqa_hotpotqa-validation-5695", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2747", "mrqa_hotpotqa-validation-3638", "mrqa_hotpotqa-validation-4046", "mrqa_hotpotqa-validation-5445", "mrqa_hotpotqa-validation-1448", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-1127", "mrqa_hotpotqa-validation-5014", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-5385", "mrqa_naturalquestions-validation-714", "mrqa_triviaqa-validation-6828", "mrqa_newsqa-validation-3940", "mrqa_searchqa-validation-12226", "mrqa_searchqa-validation-5330", "mrqa_newsqa-validation-1398"], "SR": 0.546875, "CSR": 0.5471047794117647, "EFR": 1.0, "Overall": 0.7219209558823529}, {"timecode": 68, "before_eval_results": {"predictions": ["Stephen T. Kay", "Trey Parker and Matt Stone", "Jay Gruden", "Wayman Tisdale", "New Boston Air Force Station", "the Corps of Discovery", "sarod", "Fleetwood Mac", "County Louth", "Chelmsford", "2009", "Comedy Central", "five", "Lazio region", "Mick Jackson", "The Livingston family", "U.S. saloon-keeper", "Dutch House of Orange-Nassau", "\"Kitty Hawk\"", "the Qin dynasty", "best known as the star of the self-produced sitcoms \"I Love Lucy\",", "France", "a Francophone and French poet", "Matthew Perry", "Cricket fighting", "Jaguar Land Rover Limited", "new buildings, structures, projects, or even designs that are deemed to be comparable to the seven Wonders of the World", "Noel Gallagher", "1966", "Vogue", "a band director", "Ford Field in Detroit, Michigan", "draft", "Mary O'Connell", "Bolton, England", "February 9, 1994", "Las Vegas", "Afro-American", "Missouri River", "World War II", "Ector County", "Norse", "Mercedes-Benz Superdome in New Orleans, Louisiana", "1979", "He now works as a classroom specialist for the Peace Corps of America", "December 12, 1967", "rural", "Lombardy region", "August 14, 1848", "Punjabi/Pashtun", "41st President of the United States", "the ruling city of the Northern Kingdom of Israel, Samaria", "He chose to charter a plane to reach their next venue in Moorhead, Minnesota", "1948", "The A4 class", "Sherlock Holmes", "Tokyo", "It will join Facebook and Google, which both have their headquarters in the Irish capital.", "near Warsaw, Kentucky,", "FARC", "(W. Somerset) Maugham", "a tree species related to pine", "not a fast typist", "fotografa de Monticello"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5927714646464646}, "metric_results_detailed": {"EM": [true, true, false, true, false, true, false, true, true, true, true, false, true, true, true, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, false, false, false, false, true, true, false, true, true, true, true, false, true, false, true, true, false, false, true, false, true, false, true, false, true, true, false, false, false, false, false, false, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 1.0, 0.0, 0.5, 1.0, 1.0, 0.0, 0.0, 0.888888888888889, 0.0, 0.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.33333333333333337, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.18181818181818182, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-4450", "mrqa_hotpotqa-validation-4724", "mrqa_hotpotqa-validation-203", "mrqa_hotpotqa-validation-5114", "mrqa_hotpotqa-validation-616", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-2284", "mrqa_hotpotqa-validation-3904", "mrqa_hotpotqa-validation-2526", "mrqa_hotpotqa-validation-1592", "mrqa_hotpotqa-validation-1111", "mrqa_hotpotqa-validation-309", "mrqa_hotpotqa-validation-5779", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-5352", "mrqa_hotpotqa-validation-78", "mrqa_hotpotqa-validation-1626", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-5173", "mrqa_hotpotqa-validation-107", "mrqa_hotpotqa-validation-693", "mrqa_hotpotqa-validation-717", "mrqa_hotpotqa-validation-4720", "mrqa_hotpotqa-validation-1316", "mrqa_naturalquestions-validation-7939", "mrqa_triviaqa-validation-1118", "mrqa_newsqa-validation-192", "mrqa_newsqa-validation-2573", "mrqa_newsqa-validation-1037", "mrqa_searchqa-validation-3993", "mrqa_searchqa-validation-16102", "mrqa_searchqa-validation-9564", "mrqa_searchqa-validation-1104"], "SR": 0.484375, "CSR": 0.5461956521739131, "EFR": 1.0, "Overall": 0.7217391304347827}, {"timecode": 69, "before_eval_results": {"predictions": ["November 1999", "the senior-most judge of the supreme court", "the Norman given name Robert", "March 16, 2018", "Irsay", "Britain", "relieves the driving motor from the load of holding the elevator cab", "federal", "Matt Monro", "December 2, 2013", "before the beginning of the seventh century", "the original title of the novelization of the 1977 film Star Wars", "2018", "Walter Pauk", "Walter Egan", "1973", "maquila", "1959", "Schwarzenegger", "Salman Khan", "2016", "2003", "Isekai wa Sum\u0101tofon", "Representatives and Delegates", "a recognized group of people who jointly oversee the activities of an organization", "New Zealand", "a large roasted turkey", "currently a free agent", "S\u00e9rgio Mendes", "Times Square in New York City west to Lincoln Park in San Francisco", "1 BC", "Viceroyalty of New Spain", "the mid-1970s", "Florida and into the town of Coconut Cove", "the economy", "tropical desert climate", "1988", "in the 1970s", "A blighted ovum or anembryonic gestation", "February 27, 2015", "John Hancock", "the angel Balthazar changes history in the sixth season episode `` My Heart Will Go On '' so that the Titanic never sank", "lakes or reservoirs at high altitudes", "March 5, 2014", "FIGG Bridge Engineers", "Carol Ann Susi", "in the absence of a catalyst", "a crown cutting of the fruit", "Chris Rea", "2017", "16 August 1975", "a pistil", "Goliath", "Frankenstein", "Dutch", "Winecoff", "India Today", "Jet Republic,", "as many as 50,000", "Kabul", "Prison Break", "the Lone Ranger", "Ethiopia", "President Obama and Britain's Prince Charles"], "metric_results": {"EM": 0.546875, "QA-F1": 0.6259398496240602}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, false, true, false, false, false, true, true, true, false, false, true, false, false, true, true, false, false, true, true, true, true, false, false, false, false, true, true, false, true, true, false, false, true, true, true, false, false, true, true, false, false, true, true, true, true, false, false, true, false, true, true, false, true, true, true, false, false], "QA-F1": [1.0, 1.0, 0.3333333333333333, 1.0, 1.0, 1.0, 0.16666666666666666, 0.0, 1.0, 0.0, 0.4000000000000001, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.631578947368421, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.8333333333333333, 1.0, 1.0, 1.0, 0.42857142857142855, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 1.0, 0.0, 0.26666666666666666]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6514", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-7785", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-7715", "mrqa_naturalquestions-validation-6806", "mrqa_naturalquestions-validation-3386", "mrqa_naturalquestions-validation-2106", "mrqa_naturalquestions-validation-712", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-6352", "mrqa_naturalquestions-validation-6019", "mrqa_naturalquestions-validation-683", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-8534", "mrqa_naturalquestions-validation-9895", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-7398", "mrqa_triviaqa-validation-1511", "mrqa_triviaqa-validation-2261", "mrqa_hotpotqa-validation-886", "mrqa_newsqa-validation-2671", "mrqa_searchqa-validation-12778", "mrqa_newsqa-validation-2497"], "SR": 0.546875, "CSR": 0.5462053571428571, "EFR": 0.896551724137931, "Overall": 0.7010514162561576}, {"timecode": 70, "UKR": 0.73046875, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1169", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1451", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2262", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2301", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2393", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2476", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2640", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4068", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4286", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-465", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4680", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4863", "mrqa_hotpotqa-validation-4883", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4904", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5043", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-5306", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-5843", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-728", "mrqa_hotpotqa-validation-757", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10213", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10285", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10574", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-2284", "mrqa_naturalquestions-validation-2309", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2569", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-371", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3902", "mrqa_naturalquestions-validation-3926", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4419", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4871", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5538", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-863", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-8993", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9563", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2580", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2668", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3072", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3815", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4192", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-646", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-724", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-975", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10731", "mrqa_searchqa-validation-10872", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11809", "mrqa_searchqa-validation-11846", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14111", "mrqa_searchqa-validation-14440", "mrqa_searchqa-validation-14501", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16049", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-226", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7278", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-7679", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9461", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1330", "mrqa_squad-validation-1472", "mrqa_squad-validation-1506", "mrqa_squad-validation-1949", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-2791", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3262", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-3507", "mrqa_squad-validation-3626", "mrqa_squad-validation-363", "mrqa_squad-validation-3647", "mrqa_squad-validation-3687", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4301", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-562", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-6362", "mrqa_squad-validation-66", "mrqa_squad-validation-6962", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7693", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-855", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_squad-validation-9923", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2510", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-4134", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4657", "mrqa_triviaqa-validation-4659", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4764", "mrqa_triviaqa-validation-4941", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-7726"], "OKR": 0.802734375, "KG": 0.49375, "before_eval_results": {"predictions": ["jujitsu", "Peter Stuyvesant", "Cornell", "cactus", "NASCAR", "Vivaldi", "seven", "Grace Slick", "London", "Sweden", "Phil Lynott", "purple", "Zachary Taylor", "Kempton Park", "Piero da Vinci", "(Sillais) Millais", "Belfast", "coconut shy", "Fulham", "Kent", "bryophyta", "(Somerset) Hookham", "King Solomon", "fondue", "glockenspiel", "(Samuela)Arbelaez", "William Shakespeare", "Mackinac Bridge", "a Poseidon", "sunshine", "tea", "Joan Crawford", "red", "Alexandria", "1969", "the queen", "boxing", "(Lord Beaconsfield)", "dukedom", "Babylon", "Nottingham", "George III", "25", "(S. Jimmy) Beck", "Antoine Lavoisier", "Australia", "Phoenician king of Tyre", "X-Men Origins: Wolverine", "Jimmy Carter", "David Mitchell", "King William IV", "December 15, 2017", "Laura Bertram", "Fox Ranch in Malibu Creek State Park, northwest of Los Angeles", "Loretta Lynn", "National Association for the Advancement of Colored People", "gGmbH", "Fernando Gonzalez", "Alaska or Hawaii.", "\"falling space debris,\"", "jury dutyserve with pride", "ova", "Brooke Shields", "Nepal"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6711956521739131}, "metric_results_detailed": {"EM": [false, true, true, true, true, true, true, true, false, true, false, false, true, true, false, false, false, false, false, true, true, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, false, false, true, true, true, true, false, true, false, false, false, true, true, false, true, false, false, false, false, false, true, false, true, false, false, true, true], "QA-F1": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 0.8, 0.6666666666666666, 0.0, 0.0, 0.5, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.9565217391304348, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.3333333333333333, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2231", "mrqa_triviaqa-validation-7597", "mrqa_triviaqa-validation-1914", "mrqa_triviaqa-validation-7642", "mrqa_triviaqa-validation-3041", "mrqa_triviaqa-validation-4081", "mrqa_triviaqa-validation-2099", "mrqa_triviaqa-validation-2349", "mrqa_triviaqa-validation-968", "mrqa_triviaqa-validation-4317", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-254", "mrqa_triviaqa-validation-4458", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6959", "mrqa_triviaqa-validation-3280", "mrqa_triviaqa-validation-2955", "mrqa_triviaqa-validation-6054", "mrqa_triviaqa-validation-6308", "mrqa_naturalquestions-validation-10107", "mrqa_naturalquestions-validation-468", "mrqa_hotpotqa-validation-4252", "mrqa_hotpotqa-validation-1720", "mrqa_hotpotqa-validation-3467", "mrqa_newsqa-validation-3347", "mrqa_searchqa-validation-9986", "mrqa_searchqa-validation-14926"], "SR": 0.578125, "CSR": 0.5466549295774648, "EFR": 0.9259259259259259, "Overall": 0.6999067961006781}, {"timecode": 71, "before_eval_results": {"predictions": ["four", "Wyoming", "tobacco", "Oprah Winfrey", "Phil Spector", "Margaret Beckett", "Robin Hood and Monty Python's A Holy Grail", "Rapa Nui", "Fringillidae", "Greyfriars", "eddie and director Michael Curtiz", "fusilli", "ritchie", "eddie", "eddie perpetuus", "Kelly Gang", "Yorkshire", "cuiheng, Xiangshan (later Zhongshan ) county, Guangzhou prefecture", "dame Catherine Cookson", "London", "a trumpet", "$10", "king of phrygia", "sheep", "the French Open", "cactus", "a child", "peaches", "greece", "bone", "a bruise", "barber", "terra di siena", "british republic", "states and federal district are in central North America between Canada and Mexico, with the state of Alaska in the northwestern part of North America", "daedalus", "Tommy Roe", "cork", "barleycorn", "Kopassus", "Uranus", "cressida", "main characters'exploits on the Island", "behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing", "pascal", "brain", "ash", "1985", "sisyphus", "william c. Taylor", "Honolulu", "usually in May", "the notion that an English parson may'have his nose up in the air ', upturned like the chicken's rear end", "parthenogenesis", "Derry City F.C.", "URO VAMTAC", "Christian", "Stop the War Coalition", "CNN's Michael Ware", "has used Twitter to share personal information.\"", "lexicographer", "a great blue heron", "will review the promise made by Lew Wallace to Billy the Kid,\"", "As mayor of Seoul from 2002 to 2004,"], "metric_results": {"EM": 0.421875, "QA-F1": 0.5141955266955267}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, false, true, false, true, false, true, false, false, false, false, false, false, false, false, true, false, false, true, true, true, true, false, true, false, true, true, false, false, false, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, false, true, false, true, false, true, true, false, false, false, true, false, false, false], "QA-F1": [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.8, 0.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.7878787878787877, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 0.33333333333333337, 0.14285714285714288]}}, "before_error_ids": ["mrqa_triviaqa-validation-3510", "mrqa_triviaqa-validation-3823", "mrqa_triviaqa-validation-4377", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-1155", "mrqa_triviaqa-validation-5806", "mrqa_triviaqa-validation-7364", "mrqa_triviaqa-validation-6772", "mrqa_triviaqa-validation-1755", "mrqa_triviaqa-validation-207", "mrqa_triviaqa-validation-7241", "mrqa_triviaqa-validation-5220", "mrqa_triviaqa-validation-6642", "mrqa_triviaqa-validation-7199", "mrqa_triviaqa-validation-3356", "mrqa_triviaqa-validation-4924", "mrqa_triviaqa-validation-3439", "mrqa_triviaqa-validation-5401", "mrqa_triviaqa-validation-7272", "mrqa_triviaqa-validation-5578", "mrqa_triviaqa-validation-6550", "mrqa_triviaqa-validation-6112", "mrqa_triviaqa-validation-2578", "mrqa_triviaqa-validation-7704", "mrqa_triviaqa-validation-5425", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-4699", "mrqa_triviaqa-validation-55", "mrqa_triviaqa-validation-6028", "mrqa_naturalquestions-validation-5831", "mrqa_hotpotqa-validation-1630", "mrqa_newsqa-validation-218", "mrqa_newsqa-validation-510", "mrqa_newsqa-validation-2547", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-13940", "mrqa_newsqa-validation-3686"], "SR": 0.421875, "CSR": 0.544921875, "EFR": 0.972972972972973, "Overall": 0.7089695945945945}, {"timecode": 72, "before_eval_results": {"predictions": ["a curmudgeonly senior citizen, Carl, tries to cope with the enthusiasm of Russell, a young boy.", "Colombia", "Afghan homes and compounds,", "refused to refer the case of Mohammed al-Qahtani to prosecutors because of that assessment,", "they'd get to bring a new puppy with them to the White House in January.", "three", "Kurdish militant group in Turkey", "closing these racial gaps.", "Barbara Streisand's", "U.S. President-elect Barack Obama", "can vote online, via phone calls or by text messaging,", "travels four hours to reach a government-run health facility that provides her with free drug treatment.", "Friday night", "off the coast", "Pixar's", "Friday", "state senators who will decide whether to remove him from office", "\"The Rosie Show,\"", "Louela Binlac", "South Africa", "Hungary", "The official said deciding the duties of the new prime minister has been a sticking point in the negotiations.", "with rising disposable income and an increasing interest in leisure pursuits, a growing number of courses, more television coverage and availability of EU funds,", "Clifford Harris,", "Jeffrey Jamaleldine", "to sniff out cell phones.", "21 percent", "five", "Karen Floyd", "around 8 p.m. local time Thursday", "15-year-old", "150", "not achieve the level of fame as Tiger Woods, but we can surely learn more from his fall from grace than Tiger's.", "Ennis, County Clare", "haitians", "start a dialogue of peace based on the conversations she had with Americans along the way.\"", "Daniel Radcliffe", "1 million", "55-year-old", "Southeast,", "Venus Williams", "Russian bombers", "was planning to conduct attacks in Karachi, according to Karachi Police Chief Waseem Ahmad.", "is \"still in denial\" about his conduct.", "small child", "March 22,", "ordered the eventual closure of Guant Bay prison and CIA \"black site\" prisons, and placed interrogation in all American facilities by all U.S. personnel under the guidelines of the Army Field Manual.", "Bill Stanton", "prisoners at the South Dakota State Penitentiary", "Woosuk Ken Choi,", "Apple employees", "In December 1971", "Norman Greenbaum", "17 December 1968", "good Morning Frog", "north yorkshire", "karmen", "Matthew Ward Winer", "on the property of the University of Hawai\u02bbi at M\u0101noa in Honolulu CDP", "Sun Woong", "darts", "Joe Louis", "16th", "civil wars and political conflict"], "metric_results": {"EM": 0.53125, "QA-F1": 0.5983597961844189}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, false, true, false, false, false, false, true, true, false, true, false, true, false, false, false, true, true, true, false, true, true, true, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, true, false, true, true, false, false, false, true, false, true, false, true, false, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0689655172413793, 0.6666666666666666, 0.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.0, 1.0, 0.0, 0.923076923076923, 0.18749999999999997, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.1, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6363636363636364, 0.0, 0.0, 1.0, 0.28571428571428575, 1.0, 0.2857142857142857, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.16666666666666669, 1.0, 0.0, 1.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1509", "mrqa_newsqa-validation-786", "mrqa_newsqa-validation-2701", "mrqa_newsqa-validation-4068", "mrqa_newsqa-validation-3219", "mrqa_newsqa-validation-973", "mrqa_newsqa-validation-560", "mrqa_newsqa-validation-1433", "mrqa_newsqa-validation-342", "mrqa_newsqa-validation-3389", "mrqa_newsqa-validation-491", "mrqa_newsqa-validation-1305", "mrqa_newsqa-validation-2984", "mrqa_newsqa-validation-1740", "mrqa_newsqa-validation-403", "mrqa_newsqa-validation-601", "mrqa_newsqa-validation-2389", "mrqa_newsqa-validation-1095", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-463", "mrqa_newsqa-validation-1114", "mrqa_newsqa-validation-1148", "mrqa_naturalquestions-validation-1000", "mrqa_triviaqa-validation-5043", "mrqa_triviaqa-validation-4232", "mrqa_triviaqa-validation-3654", "mrqa_hotpotqa-validation-3757", "mrqa_searchqa-validation-2301", "mrqa_searchqa-validation-10852", "mrqa_hotpotqa-validation-1958"], "SR": 0.53125, "CSR": 0.544734589041096, "EFR": 1.0, "Overall": 0.7143375428082192}, {"timecode": 73, "before_eval_results": {"predictions": ["12", "Sodra nongovernmental organization,", "the United States", "not feel Misty Cummings has told them everything she knows.", "Mogadishu", "\"horrible crime that is designed to sabotage reconciliatory efforts by the Iraqi people, who, I am confident, will continue on the road of dialogue.\"", "more than 100", "The three gunshot wound, Van Hollen said, struck Peterson in the left bicep", "Scarlett Keeling", "took on water", "two", "Omar", "Dr. Cade", "Christian farmer", "social media", "growing crowded, and governments are increasingly trying to plan their use.", "relatives of the five suspects,", "165-room", "U.S. Holocaust Memorial Museum,", "the simple puzzle video game,", "curfew in Jaipur", "40-year-old", "stand down.", "Kingman Regional Medical Center", "different women coping with breast cancer in five vignettes.", "two years", "269,000", "Harare", "the UK", "\"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "intelligence were gathering information about the rebels to give to the Colombian military.", "Hundreds of militants, believed to be foreign fighters, launched attacks on various military check posts in Pakistan's border with Afghanistan", "they'd get to bring a new puppy with them to the White House in January.", "Thessaloniki", "in Austin, Texas,", "forgery and flying without a valid license,", "Kurt Cobain", "ALS6,", "he was one of 10 gunmen who attacked several targets in Mumbai", "a rabbit hole,", "software magnate Larry Ellison,", "anesthetic", "10", "space for aspiring entrepreneurs to brainstorm with like-minded people.", "39,", "supplies power to almost 9 million Americans, \"has not fully implemented appropriate security practices to protect the control systems used to operate its critical infrastructures,\"", "Old Trafford", "12.3 million", "Stratfor", "Seasons of My Heart", "will be at the front of the line, self-righteously driving under the speed limit on his or her way to save the world.\"", "Justice A.K Mathur", "somatic cell nuclear transfer ( SCNT )", "1956", "fox", "Argentina", "Nicolas cage", "Donald Richard \"Don\" DeLillo", "10 Years", "\"Queen City\"", "Carl Sagan", "hawaii", "Copacabana", "seven"], "metric_results": {"EM": 0.609375, "QA-F1": 0.7121141403196392}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, true, false, true, true, true, false, true, false, false, false, true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, false, true, false, false, false, true, true, true, true, false, true, true, true, true, false, false, true, true, true, false, true, false, true, false, false, true, true, true, true, true, false, false, true], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.2105263157894737, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 0.12500000000000003, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.8750000000000001, 1.0, 1.0, 0.0, 1.0, 0.9743589743589743, 0.13333333333333333, 0.09523809523809523, 1.0, 0.0, 0.8, 0.2857142857142857, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.45161290322580644, 0.0, 1.0, 1.0, 1.0, 0.06896551724137931, 1.0, 0.888888888888889, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-534", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2320", "mrqa_newsqa-validation-504", "mrqa_newsqa-validation-3095", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-1616", "mrqa_newsqa-validation-443", "mrqa_newsqa-validation-1137", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-1035", "mrqa_newsqa-validation-2883", "mrqa_newsqa-validation-113", "mrqa_newsqa-validation-4000", "mrqa_newsqa-validation-2100", "mrqa_newsqa-validation-2464", "mrqa_newsqa-validation-3607", "mrqa_newsqa-validation-1265", "mrqa_newsqa-validation-2397", "mrqa_naturalquestions-validation-5109", "mrqa_triviaqa-validation-4654", "mrqa_triviaqa-validation-345", "mrqa_searchqa-validation-4997", "mrqa_searchqa-validation-8780"], "SR": 0.609375, "CSR": 0.5456081081081081, "EFR": 0.96, "Overall": 0.7065122466216216}, {"timecode": 74, "before_eval_results": {"predictions": ["brazil", "north yorkshire", "Lou Gehrig", "Goat Island", "Loretta Lynn", "a bat", "electronic junk mail or junk newsgroup posting", "Andrew Lloyd Webber", "east of Eden", "Sir Henry Neville", "Mark Hamill", "Aslan", "rugby", "Act I", "kvetch", "Coupe Van der Straeten Ponthoz", "Harold Wilson", "Bleak House", "Handley Page", "Aunty entity", "tetrodotoxin", "Sheffield United", "Capricorn", "Jack Kennedy", "the bluebird", "Toy Story", "Tom Waits", "black", "Kiel Canal", "colombia", "Avro Lancaster", "Sarah Vaughan", "Abu Dhabi", "33 miles", "Emily Davison", "Marc Brunel", "Aberystwyth", "oasis", "Peter Sellers", "the Indus Valley", "the Ghent-Terneuzen Canal", "an even break", "David Bowie", "Lorne Greene", "1709", "colombia", "Thai", "Viola", "\u00e1stron", "Ramadan", "sewing machines", "Frederick County", "11 January 1923", "Byzantine Greek culture", "1994", "Two Pi\u00f1a Coladas", "Tamara Ecclestone Rutland", "Kaka,", "comments he made after his new boss, President Scott, defeated Woods at the Bridgestone Invitational in Ohio in August.", "16,", "noncommissioned", "William Henry Harrison", "the shaft", "1982"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6678819444444445}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, true, true, false, true, true, false, false, true, false, true, true, true, false, false, true, true, false, true, true, false, false, true, false, false, true, true, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, false, true, false, true, true, false, true, true, false, true, false, false, false, true, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0, 1.0, 0.8, 1.0, 0.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-90", "mrqa_triviaqa-validation-1135", "mrqa_triviaqa-validation-1615", "mrqa_triviaqa-validation-6505", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-476", "mrqa_triviaqa-validation-3668", "mrqa_triviaqa-validation-821", "mrqa_triviaqa-validation-1459", "mrqa_triviaqa-validation-2599", "mrqa_triviaqa-validation-1642", "mrqa_triviaqa-validation-1808", "mrqa_triviaqa-validation-5567", "mrqa_triviaqa-validation-6458", "mrqa_triviaqa-validation-2905", "mrqa_triviaqa-validation-5335", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-1167", "mrqa_triviaqa-validation-3882", "mrqa_naturalquestions-validation-4416", "mrqa_hotpotqa-validation-4672", "mrqa_newsqa-validation-2813", "mrqa_newsqa-validation-2719", "mrqa_searchqa-validation-9552", "mrqa_searchqa-validation-15435"], "SR": 0.609375, "CSR": 0.5464583333333333, "EFR": 1.0, "Overall": 0.7146822916666666}, {"timecode": 75, "before_eval_results": {"predictions": ["African National Congress Deputy President Kgalema Motlanthe,", "President Jose Manuel Zelaya could return to power within days,", "Dead Weather's \"Horehound\"", "two", "Oprah Winfrey's school", "twice.", "Misty Cummings,", "the eradication of the Zetas cartel", "Hamas,", "a bank", "Rima Fakih", "against using injectable vitamin supplements because the quantities are not regulated.", "copenhagen", "Arthur E. Morgan III,", "job training", "her landlord defaulted on the mortgage and the house fell into foreclosure.", "\"Taxman,\" \"While My Guitar Gently Weeps,\" \"Something\" and \"Here Comes the Sun.\"", "22", "1969", "diabetes and hypertension,", "Tillakaratne Dilshan scored his sixth Test century of a remarkable year to give Sri Lanka a fine start to the third match of their series against India in Mumbai on Wednesday.", "Tuesday,", "100,000", "Rescue workers have pulled a body from underneath the rubble of a collapsed apartment building in Cologne, Germany,", "17,000", "President Obama", "Caylee Anthony", "the foyer of the BBC building in Glasgow, Scotland", "hopes the journalists and the flight crew will be freed,", "AMD, a competitor, launched this in Europe (and in Japan and South Korea)", "returning combat veterans", "September 21.", "by Thursday.", "246", "40", "stole", "maximum allowed", "elephant sanctuary", "15", "\"Dr. No\"", "Arabic, French and English", "$40", "many as 250,000", "state senators", "Iran", "Mohammed Mohsen Zayed,", "the 1950s,", "Orbiting Carbon Observatory,", "2.5 million", "5,600", "in Yemen,", "Dallas Cowboys and the Minnesota Vikings", "heroin", "contemporary Earth", "b\u00e9la daniels", "refrigerator", "blackcurrant", "Premier League club Everton", "the Magic Band", "Fat Man", "messenger", "the beaver", "Jan Hus", "R2-D2"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6463105060144534}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false, true, true, true, true, false, true, true, false, false, true, true, true, false, false, false, false, true, false, true, true, true, false, true, true, true, true, false, false, false, false, false, false, false, true, true, false, true, true, true, true], "QA-F1": [0.4444444444444445, 0.10000000000000002, 0.0, 1.0, 0.28571428571428575, 1.0, 0.5, 0.4, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.4, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.846153846153846, 1.0, 1.0, 1.0, 1.0, 0.9333333333333333, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 0.5454545454545454, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.17543859649122806, 0.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1382", "mrqa_newsqa-validation-3880", "mrqa_newsqa-validation-2049", "mrqa_newsqa-validation-3799", "mrqa_newsqa-validation-3770", "mrqa_newsqa-validation-2796", "mrqa_newsqa-validation-1420", "mrqa_newsqa-validation-3322", "mrqa_newsqa-validation-1917", "mrqa_newsqa-validation-869", "mrqa_newsqa-validation-2692", "mrqa_newsqa-validation-2850", "mrqa_newsqa-validation-2898", "mrqa_newsqa-validation-3246", "mrqa_newsqa-validation-919", "mrqa_newsqa-validation-3004", "mrqa_newsqa-validation-3454", "mrqa_newsqa-validation-10", "mrqa_newsqa-validation-1085", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3499", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-817", "mrqa_newsqa-validation-1493", "mrqa_naturalquestions-validation-5819", "mrqa_naturalquestions-validation-4422", "mrqa_naturalquestions-validation-2207", "mrqa_triviaqa-validation-1848", "mrqa_triviaqa-validation-7344", "mrqa_triviaqa-validation-5192", "mrqa_hotpotqa-validation-5388"], "SR": 0.515625, "CSR": 0.5460526315789473, "EFR": 0.9354838709677419, "Overall": 0.7016979255093378}, {"timecode": 76, "before_eval_results": {"predictions": ["Hawaii", "\"The Real Housewives of Atlanta\"", "commission, led by former U.S. Attorney Patrick Collins,", "outbreak", "was released Friday and taken to the Australian embassy in Bangkok, where he stayed until leaving for Australia at about midnight.", "Sheikh Sharif Sheikh Ahmed", "two", "golf", "ended his playing career at his original club of Argentinos Juniors in 2007 and has been coaching at Independiente.", "Nigeria", "Ameneh Bahrami", "the chief executive officer,", "Daytime Emmy Lifetime Achievement Award.", "launch", "Zubaydah had been waterboarded for \"about 30 seconds, 35 seconds\" and agreed to cooperate with interrogators the following day.", "35,000.", "Roger Federer", "March 22,", "Venezuela", "Hutu militias and members of the general population sought out Tutsis and moderate Hutus -- and went on a 100-day killing massacre.", "power-sharing talks", "\"Operation Crank Call,\"", "body of the aircraft", "Rima Fakih", "more than 100", "Obama and McCain camps", "an acid attack by a spurned suitor.", "about 5:20 p.m. at Terminal C when a man walked through an exit on the public side to the secure \"sterile\" side for passengers who had cleared screening,", "President Bill Clinton", "Haleigh Cummings,", "Transportation Security Administration", "pulling on the top-knot of an opponent,", "response to the HIV/AIDS fight has been widely praised and adopted as a model around the world.", "a one-of-a-kind navy dress with red lining by the American-born Lintner,", "was killed", "Kindle Fire", "Islamabad", "three", "Nigeria,", "Jose Whitehouse,", "fill a million sandbags and place 700,000 around our city,\"", "workers of the dependable Camry", "15-year-old's", "as a precaution.", "David Bowie,", "More than 15,000", "looked depressed", "Franklin", "said such joint exercises between nations are not unusual.", "Authorities in Fayetteville, North Carolina,", "Saturday.", "Lake Powell", "Lew Brown", "Rockwell", "her 3rd cousin George of Hanover", "Neighbours", "Pesach", "Tamworth", "Dunlop India Ltd.", "Hern\u00e1n Crespo", "Iceland", "( Giuseppe) Verdi", "Serengeti National Park", "Switzerland"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7226937736026324}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, true, false, true, true, false, true, false, false, true, true, true, true, false, false, true, true, true, true, true, false, false, true, true, true, false, true, false, true, false, true, true, true, false, true, false, true, false, true, true, true, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, false, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.6206896551724138, 1.0, 1.0, 1.0, 0.08333333333333333, 1.0, 1.0, 0.0, 1.0, 0.0, 0.9473684210526316, 1.0, 1.0, 1.0, 1.0, 0.0, 0.3636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.18181818181818185, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-703", "mrqa_newsqa-validation-3047", "mrqa_newsqa-validation-249", "mrqa_newsqa-validation-2664", "mrqa_newsqa-validation-820", "mrqa_newsqa-validation-3662", "mrqa_newsqa-validation-655", "mrqa_newsqa-validation-1644", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-1121", "mrqa_newsqa-validation-3781", "mrqa_newsqa-validation-3314", "mrqa_newsqa-validation-3713", "mrqa_newsqa-validation-2396", "mrqa_newsqa-validation-1673", "mrqa_newsqa-validation-2808", "mrqa_newsqa-validation-3488", "mrqa_newsqa-validation-2519", "mrqa_triviaqa-validation-781", "mrqa_triviaqa-validation-648", "mrqa_hotpotqa-validation-5708", "mrqa_searchqa-validation-1821", "mrqa_searchqa-validation-11541"], "SR": 0.640625, "CSR": 0.5472808441558441, "EFR": 0.9130434782608695, "Overall": 0.6974554894833427}, {"timecode": 77, "before_eval_results": {"predictions": ["Thursday,", "Charlotte Gainsbourg and Willem Dafoe", "Patrick McGoohan,", "flooding and debris", "Vicente Dale Coutinho, commander of Brazil's 4th Army, reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Woosuk Ken Choi,", "a head injury.", "1994,", "Mawise Gumba", "his bodyguard-turned-informant delivered three machine guns and two silencers to the hip-hop star,", "at least 25 dead", "At least 15", "at least nine", "shows the world that you love the environment and hate using fuel,\"", "\"falling space debris,\"", "Gloria Allred,", "$10 billion", "\"The Orchid Thief\"", "102", "his business dealings for possible securities violations requested the temporary restraining order in Hamilton County Superior Court,", "Andrade", "A receptionist with a gunshot wound in her stomach played dead under her desk and called 911 on Friday after a shooting massacre in a Binghamton, New York, immigration center.", "new generation of innovative, exciting skyscrapers set to appear all over the world over the next 10 years.", "the United States", "financial gain,", "Arnold Drummond", "trading goods and services without exchanging money", "Laura Ling and Euna Lee,", "Another high tide -- expected to reach about 4 meters (13 feet) high,", "July", "16", "January 3.", "100 to 150", "reached an agreement late Thursday to form a government of national reconciliation.", "Jose Manuel Zelaya could return to power within days,", "privileged ethnicity,", "At the end of a biology department faculty meeting at the University of Alabama in Huntsville,", "prison inmates.", "Cannes,", "Mark Obama Ndesandjo", "Ronald Reagan UCLA Medical Center,", "one of five", "Afghan", "in a muddy barley field owned by farmer Alan Graham outside Bangor,", "executive director of the Americas Division of Human Rights Watch,", "\"Empire of the Sun,\"", "Basel", "\"@\"", "Jeffrey Jamaleldine", "Heshmatollah Attarzadeh", "At least 14", "Daryl Sabara", "July 2014", "Gibraltar", "Barry White", "John Adams", "Anita Brookner", "9", "a Canadian comedian, and the father of British comedian", "consulting", "flamboyant", "Orson Welles", "barbed wire", "loyalty"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5872814046333783}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, true, false, false, true, false, true, true, false, true, false, false, false, false, false, false, true, true, false, true, true, false, true, true, false, false, true, false, false, false, false, false, false, false, false, false, false, true, true, true, true, false, true, false, true, false, true, true, false, true, false, false, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.7894736842105263, 1.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.3157894736842105, 0.6666666666666666, 0.0, 0.3157894736842105, 1.0, 1.0, 0.0, 1.0, 1.0, 0.4, 1.0, 1.0, 0.0, 0.0, 1.0, 0.15384615384615383, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.33333333333333337, 0.5, 0.5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.6666666666666666, 0.4444444444444445, 0.0, 1.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2217", "mrqa_newsqa-validation-3865", "mrqa_newsqa-validation-1246", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-1295", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-1566", "mrqa_newsqa-validation-2803", "mrqa_newsqa-validation-2682", "mrqa_newsqa-validation-2023", "mrqa_newsqa-validation-2000", "mrqa_newsqa-validation-745", "mrqa_newsqa-validation-1828", "mrqa_newsqa-validation-1210", "mrqa_newsqa-validation-4016", "mrqa_newsqa-validation-1255", "mrqa_newsqa-validation-3882", "mrqa_newsqa-validation-3661", "mrqa_newsqa-validation-2287", "mrqa_newsqa-validation-2259", "mrqa_newsqa-validation-626", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-1497", "mrqa_newsqa-validation-1405", "mrqa_newsqa-validation-2177", "mrqa_newsqa-validation-3097", "mrqa_newsqa-validation-148", "mrqa_newsqa-validation-795", "mrqa_naturalquestions-validation-9330", "mrqa_triviaqa-validation-635", "mrqa_hotpotqa-validation-893", "mrqa_hotpotqa-validation-814", "mrqa_hotpotqa-validation-632", "mrqa_searchqa-validation-12008"], "SR": 0.46875, "CSR": 0.5462740384615384, "EFR": 0.9705882352941176, "Overall": 0.7087630797511312}, {"timecode": 78, "before_eval_results": {"predictions": ["dress shop", "callable bonds", "British Columbia, Canada", "45 %", "September 14, 2008", "to prevent further offense by convincing the offender that their conduct was wrong", "Mike Alstott", "the closing of the atrioventricular valves and semilunar valves", "Ethiopia ( Abyssinia ), the Dervish state ( a portion of present - day Somalia ) and Liberia still being independent", "a compiler", "Dalveer Bhandari", "bone marrow", "to collect menstrual flow", "the Kansas City Chiefs", "the episode `` Killer Within ''", "Filipino", "the 15th century", "Wakanda", "the Roman Empire", "Joel", "Maganlal Daiya", "Gertrude Niesen", "Australia", "1983", "Dr. Addison Montgomery", "March 12, 2013", "photoelectric", "cockerel", "jazz", "sedimentary", "$2 million", "Michael Buffer", "Eric Clapton", "115", "c. 3000 BC", "Andrew Garfield", "each team has either selected a player or traded its draft position", "in Collier's Weekly magazine", "a router", "the rez", "England", "Glenn Close", "Kenneth Cook", "Tim Russert", "spinal nerve segments above the point of entry", "a large, high - performance luxury coupe", "positive, zero, or negative scalar quantity", "Second Continental Congress", "the winter solstice", "a mixture of phencyclidine and cocaine", "an integral membrane protein that builds up a proton gradient across a biological membrane", "Lewis Carroll", "lithium", "Apprentice", "A123 Systems, LLC", "41st President of the United States", "Andrew James West", "34", "Caylee Anthony", "Michael Brewers,", "a drum or bugle", "Tennessee Williams", "Hawthorne", "Friday,"], "metric_results": {"EM": 0.484375, "QA-F1": 0.6545371295371296}, "metric_results_detailed": {"EM": [true, true, true, true, true, false, false, false, false, false, true, false, true, true, false, false, true, false, true, true, false, false, true, true, false, true, false, false, false, false, false, true, true, false, false, true, false, false, false, false, true, true, true, true, false, false, false, true, false, false, true, true, false, true, true, false, false, true, true, false, false, true, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.9333333333333333, 0.8, 0.0, 1.0, 0.8, 1.0, 1.0, 0.8, 0.6666666666666666, 1.0, 0.2857142857142857, 1.0, 1.0, 0.3636363636363636, 0.0, 1.0, 1.0, 0.0, 1.0, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 1.0, 1.0, 0.0, 0.8, 1.0, 0.3076923076923077, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.7777777777777778, 0.33333333333333337, 0.0, 1.0, 0.4, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-7935", "mrqa_naturalquestions-validation-2813", "mrqa_naturalquestions-validation-2555", "mrqa_naturalquestions-validation-5928", "mrqa_naturalquestions-validation-9885", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-3162", "mrqa_naturalquestions-validation-6015", "mrqa_naturalquestions-validation-1805", "mrqa_naturalquestions-validation-2183", "mrqa_naturalquestions-validation-1053", "mrqa_naturalquestions-validation-3498", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-8560", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-10537", "mrqa_naturalquestions-validation-8203", "mrqa_naturalquestions-validation-902", "mrqa_naturalquestions-validation-8596", "mrqa_naturalquestions-validation-1277", "mrqa_naturalquestions-validation-9597", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-7511", "mrqa_naturalquestions-validation-2164", "mrqa_naturalquestions-validation-5368", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-5437", "mrqa_triviaqa-validation-5536", "mrqa_hotpotqa-validation-350", "mrqa_hotpotqa-validation-1433", "mrqa_newsqa-validation-3438", "mrqa_searchqa-validation-82"], "SR": 0.484375, "CSR": 0.545490506329114, "EFR": 1.0, "Overall": 0.7144887262658227}, {"timecode": 79, "before_eval_results": {"predictions": ["Bury, Greater Manchester, England", "the Battle of the Rosebud", "Rabat", "Potomac River", "Hermione Baddeley", "Harmony Korine", "December 1993", "New Jersey", "rock and roll", "Red and Assiniboine Rivers", "King George IV and the Duke of Wellington", "June 24, 1935", "The Washington Post", "odd-eyed cat", "2001", "Fresh 92.7 (Fresh FM (Australia)", "1999", "tempo", "Presbyterian Church (USA)", "2002", "English", "Southaven", "Anheuser-Busch InBev", "Kansas City, Missouri", "Francis the Talking Mule", "Leslie James \"Les\" Clark", "Francesco Maria Piave", "County Louth", "Gal Gadot", "Kurt Vonnegut Jr.", "top division", "film", "in Laurel, Mississippi", "Vincent Anthony Guaraldi", "2007", "Grave Digger", "Mulberry", "Isabella (Belle) Baumfree", "Jay Park", "The final of 2011 AFC Asian Cup", "Mel Blanc", "Centers for Medicare & Medicaid Services", "Pakistan", "\"Godspell\"", "Steven Selling", "Scunthorpe", "Australian", "Argentinian", "Jack St. Clair Kilby", "throughout the 1970s and 1980s", "\"Personal History\",", "Timothy B. Schmit", "Hakeem Olajuwon of Nigeria", "the direction from which the wind is blowing", "France", "Shropshire", "s. molloyi", "Alfredo Astiz,", "current and historic conflict zones, including Iraq, Rwanda and most recently the Gaza Strip,", "Kim Clijsters", "Catherine of Aragon", "Little Boy Blue.", "Cheyenne", "March 27, 2017"], "metric_results": {"EM": 0.59375, "QA-F1": 0.7588474025974026}, "metric_results_detailed": {"EM": [true, true, false, true, true, true, true, true, true, true, false, true, false, false, true, false, true, true, false, false, true, false, false, false, false, false, false, true, true, false, false, true, false, true, false, true, true, true, true, true, true, false, true, true, true, true, true, true, false, false, true, true, false, true, true, true, false, true, false, false, false, true, true, false], "QA-F1": [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.8, 0.5, 0.0, 0.0, 1.0, 1.0, 0.8, 0.0, 1.0, 0.8, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.8571428571428571, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-70", "mrqa_hotpotqa-validation-334", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-3176", "mrqa_hotpotqa-validation-598", "mrqa_hotpotqa-validation-1925", "mrqa_hotpotqa-validation-426", "mrqa_hotpotqa-validation-4248", "mrqa_hotpotqa-validation-2769", "mrqa_hotpotqa-validation-4273", "mrqa_hotpotqa-validation-1084", "mrqa_hotpotqa-validation-718", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-5596", "mrqa_hotpotqa-validation-351", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-1803", "mrqa_hotpotqa-validation-5515", "mrqa_hotpotqa-validation-2518", "mrqa_naturalquestions-validation-601", "mrqa_triviaqa-validation-4205", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-809", "mrqa_searchqa-validation-5939", "mrqa_naturalquestions-validation-5649"], "SR": 0.59375, "CSR": 0.54609375, "EFR": 1.0, "Overall": 0.714609375}, {"timecode": 80, "UKR": 0.75, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1264", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1308", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-193", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2055", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2935", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-313", "mrqa_hotpotqa-validation-314", "mrqa_hotpotqa-validation-3140", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3196", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3273", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3972", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4076", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5685", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-792", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-10719", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-165", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1735", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2821", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5420", "mrqa_naturalquestions-validation-5437", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-567", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6442", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-685", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-7852", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-832", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8585", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2386", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2586", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-2628", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-2772", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3503", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3771", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-482", "mrqa_newsqa-validation-604", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10094", "mrqa_searchqa-validation-10999", "mrqa_searchqa-validation-11035", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11416", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-11939", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-13558", "mrqa_searchqa-validation-13583", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14668", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16383", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4735", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5585", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-82", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8749", "mrqa_searchqa-validation-8869", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10260", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1222", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4313", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-453", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4878", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7521", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8507", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9772", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1288", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2737", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2868", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3882", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4462", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4690", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5654", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6223", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6251", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6932", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7618", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.814453125, "KG": 0.51328125, "before_eval_results": {"predictions": ["Russell Crowe", "Two Greedy Italians,", "Russ Conway", "element", "Mel Brooks", "Agent 007", "Edward Woodward", "Scotland", "Fiat", "three", "Bob Anderson", "Andre Agassi", "katherine Bridges", "India", "Giovanni Bizzelli", "Mark Darcy", "California Chrome", "Reggie", "Milan", "Tony Meo", "Bash Street", "Frankie Howerd", "Robin Hood", "Me and My Girl", "Yeshua", "Augustus Caesar", "Richard Frost", "Titanic", "Peter Falk", "tax collector", "Robert Maxwell", "Mikhail Gorbachev", "Pocahontas", "Noah Beery, Jr.", "Argentina", "peripheral nerves", "myxomatosis", "an avocado", "World War I", "Captain America", "HARIBO", "rag\u00f9", "New Zealand", "Eva Braun", "Sindh", "Devon Loch", "special administrative zones", "Bruce Willis", "Kwame Nkrumah", "Fifth", "cording", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "pneumonoultramicroscopicsilicovolcanoconiosis", "Hercules", "Philip Livingston", "2010", "\"The Process\"", "Ameneh Bahrami", "Nearly eight in 10", "a Mumbai slum", "camels", "Shrek", "Seoul", "Asia"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7018229166666667}, "metric_results_detailed": {"EM": [false, true, true, false, true, false, true, true, true, true, true, true, false, false, false, true, false, false, true, false, true, false, true, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, true, false, true, true, false, true, false, true, true, true, false, true, true, true, true, true, true, true, true, false, true, false, true, false], "QA-F1": [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.6666666666666666, 1.0, 0.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-2463", "mrqa_triviaqa-validation-3270", "mrqa_triviaqa-validation-6950", "mrqa_triviaqa-validation-5483", "mrqa_triviaqa-validation-5919", "mrqa_triviaqa-validation-5713", "mrqa_triviaqa-validation-3186", "mrqa_triviaqa-validation-7276", "mrqa_triviaqa-validation-7244", "mrqa_triviaqa-validation-5365", "mrqa_triviaqa-validation-4836", "mrqa_triviaqa-validation-6255", "mrqa_triviaqa-validation-6748", "mrqa_triviaqa-validation-6057", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-4922", "mrqa_triviaqa-validation-1693", "mrqa_newsqa-validation-3518", "mrqa_searchqa-validation-11588", "mrqa_searchqa-validation-457"], "SR": 0.6875, "CSR": 0.5478395061728395, "EFR": 1.0, "Overall": 0.7251147762345679}, {"timecode": 81, "before_eval_results": {"predictions": ["London", "his writings about the outdoors, especially mountain-climbing", "50th anniversary of the founding of the National Basketball Association", "Roger Thomas Staubach", "World Health Organization", "Grammy", "Argentine", "Pittsburgh Steelers", "Kim Jong-hyun", "Las Vegas", "Scandinavian design", "romantic comedy", "Anthony Davis", "Tsung-Dao Lee", "December 19, 1967", "South African-born", "Betty Lynn", "1822", "1926 Paris", "Bulgarian", "Quahog, Rhode Island", "Operation Neptune", "This road is best known for the Abbey Road Studios and the 1969 album, \"Abbey Road\", by The Beatles.", "Roslyn Castle", "the Battle of Dresden", "2015", "Violet", "Free Range Films", "Mondays", "Chrysler K platform", "Edinburgh", "Laurel, Mississippi", "Matt Flynn", "Camber Sands", "Jaguar Land Rover Limited", "base of support", "Adelaide", "Kathleen O'Brien", "\"The Fault in Our Stars\"", "sixteen", "Crips", "Deftones", "Doctor of Philosophy", "Donald Richard \"Don\" DeLillo", "Hong Kong Mak\u00e9l\u00e9l\u00e9", "#364", "the Magic Band", "the Salzburg Festival", "German princely Battenberg", "military leader", "Nassau Herald", "on the Fox Ranch in Malibu Creek State Park, northwest of Los Angeles", "Sufi verse", "pineapple", "asia", "Peter Principle", "Poland", "Steven Green", "Steve Williams", "Umar Farouk AbdulMutallab", "Dr. No", "the spine", "Big Brown", "ties to paramilitary groups,"], "metric_results": {"EM": 0.6875, "QA-F1": 0.7700706845238094}, "metric_results_detailed": {"EM": [true, false, false, false, true, false, true, true, false, true, true, true, true, true, false, true, false, true, true, true, false, true, true, true, true, false, true, true, true, true, true, true, true, true, false, false, true, true, true, true, true, true, false, true, false, true, true, true, false, false, true, true, false, true, false, true, true, true, true, false, true, false, true, false], "QA-F1": [1.0, 0.2857142857142857, 0.375, 0.6, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.3333333333333333, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.5, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_hotpotqa-validation-5862", "mrqa_hotpotqa-validation-5237", "mrqa_hotpotqa-validation-4795", "mrqa_hotpotqa-validation-4630", "mrqa_hotpotqa-validation-4316", "mrqa_hotpotqa-validation-4384", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3055", "mrqa_hotpotqa-validation-4259", "mrqa_hotpotqa-validation-3348", "mrqa_hotpotqa-validation-840", "mrqa_hotpotqa-validation-5297", "mrqa_hotpotqa-validation-4833", "mrqa_hotpotqa-validation-209", "mrqa_hotpotqa-validation-2925", "mrqa_naturalquestions-validation-3587", "mrqa_triviaqa-validation-3641", "mrqa_newsqa-validation-1205", "mrqa_searchqa-validation-11199", "mrqa_newsqa-validation-877"], "SR": 0.6875, "CSR": 0.5495426829268293, "EFR": 0.95, "Overall": 0.7154554115853659}, {"timecode": 82, "before_eval_results": {"predictions": ["Kenny Young", "40 million", "\"American Idol\"", "brother-in-law", "Terence Winter", "Jeff Tremaine, Shanna Zablow, Dimitry Elyashkevich, Lance Bangs, Nick Weidenfeld and Keith Crofford", "Daimler-Benz", "Edward Moore", "private", "Lee Seok-hoon", "Two Pi\u00f1a Coladas", "early traditions", "Kazakh presidential election", "Eucritta melanolimnetes", "Umberto II", "1866", "1860", "Attorney General and as Lord Chancellor of England", "Sexred", "British", "Westfield Tea Tree Plaza", "924", "1951", "Darci Kistler", "1966", "Potomac River", "Europe", "Kingkiller Chronicle", "Gateways", "England", "Black Panthers", "Extended play", "\"Sausage Party\"", "May 5 to July 8, 2014", "The Supremes", "The Chiltern Shakespeare Company", "Wolf Creek", "Sky News", "High Court of Admiralty", "Chelsea Lately", "bobsledder", "Double Agent", "Oregon Ducks", "Aksel Sandemose", "Theme Park World", "the Earth", "Eric Whitacre", "Sullivan University College of Pharmacy", "24 January 76 \u2013 10 July 138", "Marvel Comics", "Shire River", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "1961 during the Cold War", "September 14, 2008", "Harry Truman", "The Kentucky Derby", "Gianni Versace", "London", "his former Boca Juniors teammate and national coach Diego Maradona,", "his club", "pink", "Beaker", "Claddagh", "Emperor Concerto"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7482142857142857}, "metric_results_detailed": {"EM": [true, true, true, true, false, true, true, false, true, true, true, false, false, true, true, true, true, true, false, false, false, true, true, false, true, true, true, false, true, false, true, true, false, false, false, false, true, true, false, false, true, true, true, true, false, true, true, true, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, false], "QA-F1": [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.2, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.0, 0.6666666666666666, 0.0, 0.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.4, 1.0, 0.8, 1.0, 1.0, 1.0, 0.9523809523809523, 0.4, 1.0, 1.0, 1.0, 0.6666666666666666]}}, "before_error_ids": ["mrqa_hotpotqa-validation-2753", "mrqa_hotpotqa-validation-5066", "mrqa_hotpotqa-validation-516", "mrqa_hotpotqa-validation-3084", "mrqa_hotpotqa-validation-4293", "mrqa_hotpotqa-validation-3530", "mrqa_hotpotqa-validation-5864", "mrqa_hotpotqa-validation-3175", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-900", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-3956", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-3975", "mrqa_hotpotqa-validation-547", "mrqa_hotpotqa-validation-2533", "mrqa_hotpotqa-validation-4974", "mrqa_naturalquestions-validation-3898", "mrqa_triviaqa-validation-1828", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-1461", "mrqa_searchqa-validation-10992"], "SR": 0.640625, "CSR": 0.5506400602409638, "EFR": 1.0, "Overall": 0.7256748870481927}, {"timecode": 83, "before_eval_results": {"predictions": ["Thunder Road", "the former Kingdom of Strathclyde who spoke Cumbric, a close relative of the Welsh language, or possibly an incomer from Wales, or the Welsh Marches", "between the Eastern Ghats and the Bay of Bengal", "Times Square in New York City west to Lincoln Park in San Francisco", "New York University", "the oral mucosa ( a mucous membrane ) lining the mouth and also on the tongue and palates and mouth floor", "Eydie Gorm\u00e9", "Werner Ruchti", "Stephen Lang", "used their knowledge of Native American languages as a basis to transmit coded messages", "the English", "Himadri Station", "the government abolished jury trials soon after in most cases", "the ancient Chinese military strategist Sun Tzu ( `` Master Sun '', also spelled Sunzi )", "their son Jack ( short for Jack - o - Lantern )", "multiple alternative realities rather than a novel", "Pope Gregory I the Great", "1966", "1955", "1970", "Warren Hastings", "The mixing of sea water and fresh water", "water ice", "June 1992", "John Vincent Calipari", "1975", "Charles Darwin and Alfred Russel Wallace", "Left Behind", "1936", "Judith Cynthia Aline Keppel ( born 18 August 1942 ) was the first one - million - pound winner on the television game show Who Wants to Be a Millionaire? in the United Kingdom", "the 1820s", "May 18, 2018", "Russia", "the five states which the UN Charter of 1945 grants a permanent seat on the UN Security Council ( UNSC )", "a qui tam provision that allows people who are not affiliated with the government, called `` relators '' under the law, to file actions on behalf of the government", "Fusajiro Yamauchi", "Bart Cummings", "along the Californian coast at The Inn at Newport Ranch, a resort and cattle ranch to the north of San Francisco", "pre-Christian festivals that were celebrated around the winter solstice", "Speaker of the House of Representatives", "the NFL", "the Italian Campaign", "St. Louis Blues", "practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral", "Thomas Weston", "Blue with a harp of gold", "Florida and into the town of Coconut Cove", "S - shaped basin", "September 9, 2010", "the shooter must be at least 18 or 21 years old ( or have a legal guardian present )", "Lori Rom", "Jennifer Eccles", "Sinclair Lewis", "Akon", "Lily Hampton", "Hawaii Five-0", "Gregg Harper", "23-year-old", "200.", "Transportation Security Administration", "the Vaio Z Canvas 2-in-1", "Newman", "LaGrone", "East Knoyle"], "metric_results": {"EM": 0.578125, "QA-F1": 0.6450840994714351}, "metric_results_detailed": {"EM": [true, false, true, false, true, false, true, true, true, false, true, true, false, false, false, false, true, true, false, false, true, false, true, true, true, true, false, true, false, false, true, true, true, false, false, true, true, true, true, true, true, false, false, true, false, false, true, false, false, false, true, true, true, true, true, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 1.0, 0.631578947368421, 1.0, 0.125, 1.0, 1.0, 1.0, 0.06666666666666667, 1.0, 1.0, 0.0, 0.3076923076923077, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.4444444444444445, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.2666666666666667, 1.0, 1.0, 1.0, 0.13333333333333333, 0.16, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.4, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-1426", "mrqa_naturalquestions-validation-707", "mrqa_naturalquestions-validation-2684", "mrqa_naturalquestions-validation-5352", "mrqa_naturalquestions-validation-1404", "mrqa_naturalquestions-validation-6308", "mrqa_naturalquestions-validation-4751", "mrqa_naturalquestions-validation-2729", "mrqa_naturalquestions-validation-7362", "mrqa_naturalquestions-validation-3602", "mrqa_naturalquestions-validation-1814", "mrqa_naturalquestions-validation-550", "mrqa_naturalquestions-validation-10656", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-3442", "mrqa_naturalquestions-validation-993", "mrqa_naturalquestions-validation-734", "mrqa_naturalquestions-validation-5719", "mrqa_naturalquestions-validation-2812", "mrqa_naturalquestions-validation-3804", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-6378", "mrqa_naturalquestions-validation-8617", "mrqa_hotpotqa-validation-1447", "mrqa_newsqa-validation-1613", "mrqa_searchqa-validation-8173", "mrqa_searchqa-validation-6235"], "SR": 0.578125, "CSR": 0.5509672619047619, "EFR": 1.0, "Overall": 0.7257403273809524}, {"timecode": 84, "before_eval_results": {"predictions": ["to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body", "A pulmonary artery", "Grand Inquisition", "Theodore Roosevelt", "the United States, its NATO allies and others", "Anna Faris", "the s - block", "late - night", "Emma Watson", "April 10, 2018", "the New York Yankees", "Pope Gregory I the Great", "on the table", "Ireland", "Saphira", "Pre-evaluation, strategic planning, operative planning, implementation", "Saint Peter", "active absorption", "1983", "restricted naturalization to `` free white persons '' of `` good moral character ''", "Tachycardia, also called tachyarrhythmia", "Ptolemy", "1986", "the Battle of Antietam", "Evan Spiliotopoulos", "The Wizard of Oz", "if the car is slowed initially by manual use of the automatic gear box", "Thomas Mundy Peterson", "Filipino Americans", "Glynis Johns", "The Continental Congress", "eusebeia", "Maximilien Robespierre", "Lagaan", "A blighted ovum", "September 19 - 22, 2017", "the New Testament", "Kirsten Simone Vangsness", "asphyxia", "in 1902", "from Camp Green Lake, Theodore `` Armpit '' Johnson", "A patent", "1999", "10,605", "the Southeastern United States", "the Reverse - Flash", "March 1930", "John Donne", "Category 4", "season four", "During the reign of King Beorhtric of Wessex", "the Lawrence Sheriff Grammar School", "gold", "Doctor Dolittle", "Premier League", "Christian Kern", "50 best cities to live in", "the Interior Department's inspector general,", "Miguel Cotto", "Average scraped together his last salary, some money he made from trading sugar bought at a discount from the supermarket where he worked, and funds borrowed from friends to secure a visitor's visa and bus ticket to Johannesburg.", "Rocky", "the Cumberland Gap", "salinity", "George Fox"], "metric_results": {"EM": 0.484375, "QA-F1": 0.5685267857142857}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, true, false, true, false, true, false, false, false, true, true, true, false, false, false, false, false, true, false, true, false, false, false, false, true, true, false, false, true, true, true, true, false, false, true, false, false, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.0, 0.5, 0.8, 0.4444444444444445, 0.0, 0.4444444444444445, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 0.28571428571428575, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.16666666666666669, 0.4444444444444445, 1.0, 0.8, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-5552", "mrqa_naturalquestions-validation-3157", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-5180", "mrqa_naturalquestions-validation-1910", "mrqa_naturalquestions-validation-1688", "mrqa_naturalquestions-validation-10161", "mrqa_naturalquestions-validation-4556", "mrqa_naturalquestions-validation-2024", "mrqa_naturalquestions-validation-8374", "mrqa_naturalquestions-validation-1682", "mrqa_naturalquestions-validation-10131", "mrqa_naturalquestions-validation-5966", "mrqa_naturalquestions-validation-3429", "mrqa_naturalquestions-validation-10284", "mrqa_naturalquestions-validation-3022", "mrqa_naturalquestions-validation-7024", "mrqa_naturalquestions-validation-4881", "mrqa_naturalquestions-validation-3288", "mrqa_naturalquestions-validation-2967", "mrqa_naturalquestions-validation-8117", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-3351", "mrqa_naturalquestions-validation-4990", "mrqa_naturalquestions-validation-3604", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-6670", "mrqa_naturalquestions-validation-4919", "mrqa_naturalquestions-validation-4863", "mrqa_triviaqa-validation-6894", "mrqa_triviaqa-validation-3826", "mrqa_hotpotqa-validation-3900", "mrqa_newsqa-validation-2653"], "SR": 0.484375, "CSR": 0.5501838235294118, "EFR": 0.9393939393939394, "Overall": 0.7134624275846703}, {"timecode": 85, "before_eval_results": {"predictions": ["John Marshall", "Pirates of the Caribbean: At World's End", "Samuel de Champlain", "Louis XIV", "the Nine Day Queen", "the Barbary Coast", "Iceland", "Excalibur", "Richard Cory", "the Volkswagen Passat", "baldness", "Athens", "rum", "tea rose", "Aida", "give love a bad name", "sindh", "Madame Dficit", "rotunda", "the magnolia", "haryana", "bicentennial", "the Gallic War", "auction", "the peace sign", "Michael Dell", "Pizza Hut", "Lusitania", "1972", "a hurricane", "Amish", "the Rocky Mountains", "carbon", "Materials", "Boston", "Wu-Tang Clan", "Tudor", "Jose de San", "a whale", "Salt Lake City", "Luxembourg", "Texas", "drag", "the Knight of Ni", "a chrie", "Las Vegas", "Laura", "The New Yorker", "the Sarajevo Haggadah", "a spoiled brat", "Tufts", "The results of the Avery -- MacLeod -- McCarty experiment", "the cast", "the buttock", "Ming-Na", "magnesium", "The Truman Show", "Robert Redford", "Great Lakes and Midwestern", "three people", "five", "said. \"It appears that there was a struggle between the victim and the suspect in the threshold of the hotel room immediately prior to the shooting,\"", "March 3, 2008,", "Elizabeth Birnbaum"], "metric_results": {"EM": 0.609375, "QA-F1": 0.6899553571428572}, "metric_results_detailed": {"EM": [true, false, false, true, false, false, true, false, true, false, true, true, true, true, true, true, false, false, true, true, false, true, false, false, false, true, false, true, true, true, true, false, true, true, true, true, true, false, true, true, true, true, false, false, false, true, true, true, false, false, true, true, true, false, false, false, true, true, true, false, true, true, false, true], "QA-F1": [1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.8, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-12392", "mrqa_searchqa-validation-3768", "mrqa_searchqa-validation-9029", "mrqa_searchqa-validation-8046", "mrqa_searchqa-validation-7327", "mrqa_searchqa-validation-6995", "mrqa_searchqa-validation-7272", "mrqa_searchqa-validation-1981", "mrqa_searchqa-validation-1979", "mrqa_searchqa-validation-186", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-15481", "mrqa_searchqa-validation-4152", "mrqa_searchqa-validation-3485", "mrqa_searchqa-validation-16693", "mrqa_searchqa-validation-10398", "mrqa_searchqa-validation-10968", "mrqa_searchqa-validation-4535", "mrqa_searchqa-validation-11933", "mrqa_naturalquestions-validation-10378", "mrqa_triviaqa-validation-5727", "mrqa_triviaqa-validation-2130", "mrqa_hotpotqa-validation-3415", "mrqa_newsqa-validation-4210"], "SR": 0.609375, "CSR": 0.5508720930232558, "EFR": 1.0, "Overall": 0.7257212936046511}, {"timecode": 86, "before_eval_results": {"predictions": ["the Granite", "the Bull", "Horse Feathers", "Bleak House", "Chaillot", "Do the Right Thing", "coloring", "Asteroids", "a bad peace", "Yves Saint Laurent", "Iceland", "England", "Lend-Lease Act", "Spanglish", "Monica Lewinsky", "Friday night", "Google", "the Medusa", "the vest", "Prince Rogers Nelson", "a gull", "Hammurabi", "Nixon", "precipitation", "(Prince) Kesselring", "sleep disorder", "Ned", "the 747", "Terry Bradshaw", "Chris Evert", "Azerbaijan", "Mamma Mia!", "Fallingwater", "Alanis Morissette", "commas", "a barrel", "Etna", "a law clerk", "the town-house", "Louisiana", "George Orwell", "Nepal", "Toro", "Stalin", "Metallica", "change horses", "Get Smart", "Lafayette", "Nick Carraway", "Captain Kangaroo", "Kosher", "1996", "XIX", "Gabrielle - Suzanne Barbot de Villeneuve", "a dragon", "carbonate", "South Africa", "Cartoon Network Too", "Via Port Rotterdam", "Bruce Grobbelaar", "Tehran, Iran", "The Palm Jumeirah", "35,000.", "Mashhad"], "metric_results": {"EM": 0.625, "QA-F1": 0.7192708333333333}, "metric_results_detailed": {"EM": [false, true, true, false, true, true, false, false, true, true, false, true, true, true, true, false, true, true, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, false, false, true, true, false, true, true, true, true, true, false, false, true, false, true, false, true, false, false, true, true, false, true, false, true, true, true], "QA-F1": [0.6666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-4406", "mrqa_searchqa-validation-10869", "mrqa_searchqa-validation-4847", "mrqa_searchqa-validation-12019", "mrqa_searchqa-validation-285", "mrqa_searchqa-validation-15017", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-7312", "mrqa_searchqa-validation-5255", "mrqa_searchqa-validation-2504", "mrqa_searchqa-validation-3", "mrqa_searchqa-validation-517", "mrqa_searchqa-validation-16876", "mrqa_searchqa-validation-14140", "mrqa_searchqa-validation-4740", "mrqa_searchqa-validation-13220", "mrqa_searchqa-validation-10150", "mrqa_searchqa-validation-4307", "mrqa_naturalquestions-validation-288", "mrqa_triviaqa-validation-3809", "mrqa_triviaqa-validation-1471", "mrqa_hotpotqa-validation-337", "mrqa_newsqa-validation-3141"], "SR": 0.625, "CSR": 0.5517241379310345, "EFR": 1.0, "Overall": 0.7258917025862069}, {"timecode": 87, "before_eval_results": {"predictions": ["Oblivion", "a chiffon", "Corpus Christi", "Grover Cleveland", "the eye", "the Federalist Papers", "Martin Luther King", "transitive", "California", "the Central Pacific", "ACTIVE", "Tom Cruise", "Sicilian pizza", "a panda", "Risk", "brown rice", "Kansas State", "scrabble", "1945", "Kentucky Bourbon", "a stork", "the Lord of the Rings", "a rat", "anime", "Daisy Miller", "Icelandic", "Cary", "Mercury and Venus", "the Skoda Kodiaq", "the Stars and Stripes Forever", "One Hundred Years of Solitude", "lethal", "Henry Cavendish", "vanilla", "terminal", "Italy", "Night of the Iguana", "Rhode Island", "Baseball", "Anne Rice", "the root", "Judges", "1066", "Darby Wren", "a Bull", "the hip", "a hearse", "City Slickers", "Ned Kelly", "without worries", "a doge", "can negatively affect a person's personal, work, or school life, as well as sleeping, eating habits, and general health", "the southeastern United States", "5.0 - litre, naturally aspirated V8 - engine with electronic fuel injection", "Italy", "Pakistan International Airlines", "sculpture", "1998", "\"Peshwa\" (Prime Minister)", "Sleeping Beauty", "the Beatles", "ceo Herbert Hainer", "Mugabe and Tsvangirai", "Consumer Reports"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7833829365079366}, "metric_results_detailed": {"EM": [true, true, true, true, true, true, false, false, true, true, true, true, false, false, true, false, true, false, true, false, true, false, true, false, true, true, false, false, false, false, false, true, true, true, true, true, true, true, false, true, true, true, true, false, true, false, true, true, true, false, true, false, true, true, false, false, true, true, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8571428571428571, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.5, 1.0, 1.0, 0.0, 0.8, 0.0, 0.8571428571428571, 0.888888888888889, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.4, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_searchqa-validation-15068", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-13890", "mrqa_searchqa-validation-4561", "mrqa_searchqa-validation-2042", "mrqa_searchqa-validation-5164", "mrqa_searchqa-validation-15006", "mrqa_searchqa-validation-11025", "mrqa_searchqa-validation-13444", "mrqa_searchqa-validation-11517", "mrqa_searchqa-validation-5301", "mrqa_searchqa-validation-2206", "mrqa_searchqa-validation-6173", "mrqa_searchqa-validation-8657", "mrqa_searchqa-validation-727", "mrqa_searchqa-validation-12728", "mrqa_searchqa-validation-12454", "mrqa_searchqa-validation-16917", "mrqa_naturalquestions-validation-9444", "mrqa_triviaqa-validation-6679", "mrqa_triviaqa-validation-3663", "mrqa_newsqa-validation-3054"], "SR": 0.65625, "CSR": 0.5529119318181819, "EFR": 1.0, "Overall": 0.7261292613636363}, {"timecode": 88, "before_eval_results": {"predictions": ["Sputnik", "\"Let It Snow! Let It Snow!\"", "Kinks", "Gorbachev", "Jerez de la Frontera", "Buncefield Depot", "vatican city", "cable", "Westminster Abbey", "Cast", "a king of hearts", "Hawaii", "World War II", "aromatherapy", "s Sierra One from Sierra Oscar", "Downton Abbey", "Bobby Darin", "France", "Montmorency", "Kent", "Cliff Thorburn", "chamonix", "three", "cymbals", "violin", "Ireland", "Venus", "beetles", "wrinkles", "eight", "Japanese silvergrass", "Swindon Town", "A&M", "Happy Birthday to You", "Everton", "a sash", "marc", "Staraya Russa", "Makepeace Thackeray", "Mud", "Dumbo", "Jimmy Knapp", "Pelham 1-2-3 (most don't even recall the made-for-TV version filmed in Toronto - with good reason)", "4", "7,926 miles", "John Galliano", "Mangog", "Richard Seddon", "Chiricahua", "Albert Reynolds", "Aug. 24, 1572", "in the very late 1980s", "Taylor Michel Momsen", "Malayalam", "2008", "Canada", "Dutch", "the peace with Israel", "Jiverly Wong,", "28", "Catherine Zeta-Jones", "Hannibal", "jam or jelly", "Michael Edward \" Mike\" Mills"], "metric_results": {"EM": 0.625, "QA-F1": 0.6867187499999999}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, false, true, true, true, false, true, true, true, false, true, true, true, true, true, true, false, false, true, true, true, true, true, false, false, false, true, false, true, true, false, true, false, false, true, true, true, false, false, false, true, false, true, false, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false], "QA-F1": [1.0, 0.6666666666666666, 1.0, 1.0, 0.4, 0.8, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0, 1.0, 1.0, 0.25, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.6666666666666666]}}, "before_error_ids": ["mrqa_triviaqa-validation-2755", "mrqa_triviaqa-validation-2186", "mrqa_triviaqa-validation-5664", "mrqa_triviaqa-validation-3033", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-2694", "mrqa_triviaqa-validation-2162", "mrqa_triviaqa-validation-4314", "mrqa_triviaqa-validation-7590", "mrqa_triviaqa-validation-58", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3502", "mrqa_triviaqa-validation-2027", "mrqa_triviaqa-validation-5008", "mrqa_triviaqa-validation-5603", "mrqa_triviaqa-validation-3035", "mrqa_triviaqa-validation-1026", "mrqa_triviaqa-validation-2131", "mrqa_triviaqa-validation-3944", "mrqa_triviaqa-validation-7086", "mrqa_triviaqa-validation-4019", "mrqa_searchqa-validation-1265", "mrqa_searchqa-validation-3464", "mrqa_hotpotqa-validation-4878"], "SR": 0.625, "CSR": 0.5537219101123596, "EFR": 0.9583333333333334, "Overall": 0.7179579236891386}, {"timecode": 89, "before_eval_results": {"predictions": ["18", "A third beluga whale belonging to the world's largest aquarium has died", "Los Ticos", "reacted to this by saying that the United States obviously wanted Brazil \"to do the dirty work,\"", "Alicia Keys", "around Haiti,", "Rwanda", "dance", "new materials -- including ultra-high-strength steel and boron", "Azzam the American,", "10", "fortune", "between 5 and 10 knots an hour.", "helping to plan the September 11, 2001, terror attacks,", "to provide security as needed.\"", "Los Ticos", "some dental work done,", "workers walked off the job January 28 to protest the hiring of hundreds of foreign workers for a construction project at the Lindsey oil refinery in eastern England.", "fill a million sandbags", "helping on the sandbag lines", "Immigration Minister Eric Besson", "since 1983", "28", "promotes fuel economy and safety while boosting the economy.", "growing crowded,", "Jewish", "12.3 million", "The Ski Train", "two years", "Robert Park", "Piedad Cordoba, right,", "eight", "Itawamba County School District", "Frank Ricci,", "Bobby Darin,", "$199", "from the capital, Dhaka, to their homes in Bhola for the Muslim festival of Eid al-Adha.", "1983", "13", "The Valley Swim Club", "\"still trying to absorb the impact of this week's stunning events.\"", "International Polo Club Palm Beach in Wellington, Florida,", "relatives of the five suspects,", "Sharon Bialek", "not", "John Dillinger,", "five", "New Year's Day", "Iggy Pop", "two", "gayslord Opryland", "Garfield Sobers", "R.E.M.", "the Cow Palace", "John Donne", "127 Hours", "Kiel Canal", "\"Slaughterhouse-Five\"", "1885", "Germanic", "hearsay", "buffoon", "17th century", "the Book of Esther"], "metric_results": {"EM": 0.515625, "QA-F1": 0.6507695123503947}, "metric_results_detailed": {"EM": [true, false, true, true, false, false, true, false, false, false, true, false, false, false, false, false, true, false, false, true, true, false, false, true, true, true, true, true, true, true, false, true, false, true, false, false, false, true, false, true, false, false, true, true, true, true, false, false, false, false, false, true, true, false, true, true, true, true, true, false, true, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.7272727272727273, 0.0, 1.0, 0.0, 0.11764705882352942, 0.8571428571428571, 0.0, 0.0, 1.0, 0.4, 0.5, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.5, 1.0, 0.9090909090909091, 0.9333333333333333, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5]}}, "before_error_ids": ["mrqa_newsqa-validation-3619", "mrqa_newsqa-validation-439", "mrqa_newsqa-validation-1674", "mrqa_newsqa-validation-3558", "mrqa_newsqa-validation-455", "mrqa_newsqa-validation-289", "mrqa_newsqa-validation-767", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3820", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-336", "mrqa_newsqa-validation-2802", "mrqa_newsqa-validation-3842", "mrqa_newsqa-validation-377", "mrqa_newsqa-validation-3115", "mrqa_newsqa-validation-513", "mrqa_newsqa-validation-383", "mrqa_newsqa-validation-1331", "mrqa_newsqa-validation-3315", "mrqa_newsqa-validation-320", "mrqa_newsqa-validation-1097", "mrqa_newsqa-validation-1396", "mrqa_newsqa-validation-4124", "mrqa_newsqa-validation-2749", "mrqa_newsqa-validation-2780", "mrqa_newsqa-validation-1031", "mrqa_newsqa-validation-490", "mrqa_newsqa-validation-268", "mrqa_naturalquestions-validation-878", "mrqa_hotpotqa-validation-4986", "mrqa_triviaqa-validation-6731"], "SR": 0.515625, "CSR": 0.5532986111111111, "EFR": 1.0, "Overall": 0.7262065972222221}, {"timecode": 90, "UKR": 0.736328125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1047", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1158", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1278", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1358", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2867", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3469", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3728", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4152", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-439", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-469", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5474", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10009", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-10670", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2548", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2686", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3004", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4872", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-581", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5958", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-767", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8792", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-135", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-1702", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2002", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2165", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-229", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2327", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-245", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-753", "mrqa_newsqa-validation-772", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-957", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-987", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11541", "mrqa_searchqa-validation-11764", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12146", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-1307", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15698", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-161", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-16756", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2436", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4509", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6286", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8580", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4158", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4562", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-620", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-2083", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3870", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-5180", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6741", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726"], "OKR": 0.82421875, "KG": 0.47890625, "before_eval_results": {"predictions": ["15,000", "on your social networking sites", "Mark Sanford", "Russian air force,", "Chevron", "one", "serving its fast burgers and fries in the Carrousel du Louvre,", "the Kooyong Classic in Melbourne.", "peanuts, nuts, shellfish, peanuts, tree nuts, wheat and soy.", "opium", "Tuesday.", "acute stress disorder in Iraq", "the fact that the teens were charged as adults.", "Wednesday,", "fastest circumnavigation of the globe in a powerboat", "order", "General Motors", "London, Ontario,", "Da Vinci Code", "Nearly eight in 10", "66th annual Golden Globe Awards", "no motive has been determined for the killing,", "Opry Mills,", "striker", "Jan Brewer.", "the oceans,", "Columbia", "advertising copy that ranges from the provocative and the political to the lighthearted and cutesy.", "weren't taking it well.", "Leo Frank,", "Ralph Lauren,", "the American Civil Liberties Union", "Islamabad", "\"To My Mother\"", "June 2004", "of Olympia", "\"Percy Jackson & The Olympians,\"", "Frank Ricci,", "U.S. Navy", "Russian air force,", "co-chair of the Genocide Prevention Task Force.", "she wonders if part of the appeal of plus-sized", "Turkey can play an important role in Afghanistan as a reliable NATO ally.", "Jet Republic,", "Jaipur", "Venezuela", "likening one American diplomat to a \"prostitute\"", "meeting with the president to discuss her son.", "Toffelmakaren.", "she also believed police were trying to cover up the truth behind her daughter's murder,", "buckling under pressure from the ruling party.", "16 seasons", "12.9 - kilometre ( 8 mi )", "1939", "Afghanistan", "Alessandro Allori", "100-point", "Nicolas Winding Refn", "Father Dougal McGuire", "Parliamentarians (\"Roundheads\") and Royalists (\"Cavaliers\")", "alfalfa", "Thomas Jefferson", "William Friedkin", "Adolphe Adam"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6793803505658611}, "metric_results_detailed": {"EM": [false, false, false, true, false, true, false, false, false, true, true, false, true, false, false, true, false, false, true, true, false, false, true, true, false, true, true, false, false, true, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, true, true, false, false, true], "QA-F1": [0.5, 0.0, 0.8, 1.0, 0.0, 1.0, 0.888888888888889, 0.0, 0.5714285714285714, 1.0, 1.0, 0.7499999999999999, 1.0, 0.4, 0.2857142857142857, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.26086956521739124, 1.0, 1.0, 0.8, 1.0, 1.0, 0.1951219512195122, 0.0, 1.0, 1.0, 0.0, 1.0, 0.2222222222222222, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.17391304347826086, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9655172413793104, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-4145", "mrqa_newsqa-validation-1182", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-2950", "mrqa_newsqa-validation-1368", "mrqa_newsqa-validation-3733", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-1962", "mrqa_newsqa-validation-2541", "mrqa_newsqa-validation-2937", "mrqa_newsqa-validation-353", "mrqa_newsqa-validation-105", "mrqa_newsqa-validation-2727", "mrqa_newsqa-validation-3580", "mrqa_newsqa-validation-1117", "mrqa_newsqa-validation-1893", "mrqa_newsqa-validation-131", "mrqa_newsqa-validation-3086", "mrqa_newsqa-validation-4189", "mrqa_newsqa-validation-3649", "mrqa_newsqa-validation-2723", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-2504", "mrqa_newsqa-validation-2983", "mrqa_triviaqa-validation-3262", "mrqa_searchqa-validation-1104", "mrqa_searchqa-validation-15070"], "SR": 0.5625, "CSR": 0.5533997252747253, "EFR": 0.9642857142857143, "Overall": 0.711427712912088}, {"timecode": 91, "before_eval_results": {"predictions": ["British Airways", "Linus van Pelt", "chestnut", "almond", "Mark Twain", "Oslo", "Humphrey Bogart", "Hawaii", "glockenspiel", "George Orwell", "Goldtrail", "The Archers", "MythBusters Kari Byron, Grant Imahara and Tory Belleci", "Jack Nicholson", "photography", "ginger ale", "Taiwan", "Willem de Zwijger (William the Silent)", "Oliver Stone", "President Nixon", "Oregon", "your Excellency", "Nikola Tesla", "De Quincey", "Susie Dent", "Pancho Villa", "The Crusades", "Ivan Owen", "1919", "copper", "Pickwick", "Bluebell Girls", "Columbus", "austria", "Ann Darrow", "blue and White", "the Flying Pickets", "St Moritz", "marham", "Vietnam", "1985", "Bogota", "the Pentagon", "James Murdoch", "Crystal Palace", "Belfast", "And you can tell everbody that this is your song", "Thermopylae", "Elton John", "Seattle", "Marshalsea", "verification code ( CVC )", "Frenchmen : the engineer \u00c9mile Gagnan and Naval Lieutenant ( `` lieutenant de vaisseau '' ) Jacques Cousteau", "Tandi, in Lahaul", "94", "August 6, 1845 - October 6, 1931", "Ted", "24", "Tuesday", "heroin labs in neighboring countries and along trafficking routes.", "Ham", "resuscitation", "Hudson", "Subway"], "metric_results": {"EM": 0.65625, "QA-F1": 0.7276589912280702}, "metric_results_detailed": {"EM": [true, false, false, true, true, true, true, true, true, true, true, true, false, true, true, false, true, false, true, false, true, true, true, true, true, true, true, true, false, true, true, true, true, false, false, false, true, false, false, true, true, false, false, true, true, true, false, true, true, true, true, false, false, true, true, false, true, false, true, false, true, false, false, true], "QA-F1": [1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 0.0, 0.7368421052631579, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.6666666666666666, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-3978", "mrqa_triviaqa-validation-4026", "mrqa_triviaqa-validation-835", "mrqa_triviaqa-validation-6962", "mrqa_triviaqa-validation-3273", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-6451", "mrqa_triviaqa-validation-5295", "mrqa_triviaqa-validation-1539", "mrqa_triviaqa-validation-7218", "mrqa_triviaqa-validation-1028", "mrqa_triviaqa-validation-7060", "mrqa_triviaqa-validation-6739", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-7195", "mrqa_naturalquestions-validation-6916", "mrqa_naturalquestions-validation-5143", "mrqa_hotpotqa-validation-5628", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-2175", "mrqa_searchqa-validation-13767", "mrqa_searchqa-validation-8329"], "SR": 0.65625, "CSR": 0.5545176630434783, "EFR": 0.9090909090909091, "Overall": 0.7006123394268775}, {"timecode": 92, "before_eval_results": {"predictions": ["orangutans", "lowestoft", "jeremy", "new zealand", "net worth", "netherlands", "king charles II", "the \u201cGodfather of Italian cooking\u201d", "NASA\u2019s Hubble Space Telescope", "France", "george r Reeves", "a window", "dennis truman", "a coffee house", "the little dog laughed", "baseball cards", "bansk\u00e1 \u0160tiavnica", "brazil Teena", "Neighbours", "kursk", "Jessica Simpson", "3-4-5-6", "blind beggar", "Mark Darcy", "one Thousand and One", "Homo floresiensis", "benjamin baird", "Andropov", "unitedia", "Theo Walcott", "anabaptists", "netherlands", "santa santa", "surrey", "petula Clark", "Dr Tamseel", "dice", "Saturn", "Sinclair Lewis", "the river Fleet", "surseen", "james chadwick", "netherlands", "1879", "a weasel", "table tennis", "a bison", "alberich", "geomorphology", "mars", "tina turner", "com TLD", "abdicated", "an edited version of a film ( or television episode, music video, commercial, or video game ) that is supposed to represent the director's own approved edit", "Guangzhou", "29, 1985", "Jane Mayer", "the storm,", "a member of the band for more than 40 years", "American", "avanti", "Coors Field", "the Chrysler Building", "the USS \"Enterprise\""], "metric_results": {"EM": 0.484375, "QA-F1": 0.529234871031746}, "metric_results_detailed": {"EM": [true, true, false, true, false, false, true, false, false, true, false, true, false, true, false, true, false, false, false, true, true, false, true, true, true, false, false, false, false, true, false, false, false, false, true, false, true, true, true, false, false, true, false, true, false, true, false, true, false, true, true, true, false, false, false, false, true, true, false, true, false, true, true, true], "QA-F1": [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.8571428571428571, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.125, 0.22222222222222218, 0.6666666666666666, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_triviaqa-validation-4320", "mrqa_triviaqa-validation-4018", "mrqa_triviaqa-validation-7714", "mrqa_triviaqa-validation-5157", "mrqa_triviaqa-validation-1504", "mrqa_triviaqa-validation-1795", "mrqa_triviaqa-validation-654", "mrqa_triviaqa-validation-3094", "mrqa_triviaqa-validation-6855", "mrqa_triviaqa-validation-6329", "mrqa_triviaqa-validation-904", "mrqa_triviaqa-validation-4877", "mrqa_triviaqa-validation-4682", "mrqa_triviaqa-validation-3562", "mrqa_triviaqa-validation-5742", "mrqa_triviaqa-validation-4668", "mrqa_triviaqa-validation-3954", "mrqa_triviaqa-validation-7321", "mrqa_triviaqa-validation-4082", "mrqa_triviaqa-validation-5263", "mrqa_triviaqa-validation-6995", "mrqa_triviaqa-validation-1588", "mrqa_triviaqa-validation-6048", "mrqa_triviaqa-validation-2010", "mrqa_triviaqa-validation-3380", "mrqa_triviaqa-validation-2988", "mrqa_triviaqa-validation-7470", "mrqa_naturalquestions-validation-10721", "mrqa_naturalquestions-validation-3342", "mrqa_hotpotqa-validation-4032", "mrqa_hotpotqa-validation-2717", "mrqa_newsqa-validation-3990", "mrqa_searchqa-validation-6690"], "SR": 0.484375, "CSR": 0.553763440860215, "EFR": 0.9393939393939394, "Overall": 0.7065221010508309}, {"timecode": 93, "before_eval_results": {"predictions": ["Robert Barnett,", "a public housing project,", "a hospital in Amstetten,", "Saudi Arabia", "Zimbabwean", "a one-shot victory in the Bob Hope Classic", "on supporting full marriage equality,\"", "he had been in a canoeing accident with some friends,\"", "Her husband and attorney, James Whitehouse,", "80,", "she is God-sent,\"", "Iraqi", "Passers-by", "subscribers to a daily publication which is the primary service of Stratfor,\"", "Oprah Winfrey.", "Former Mobile County Circuit Judge Herman Thomas", "the kind of bipartisan rhetoric Obama has espoused on the campaign trail.", "heavy flannel or wool", "Diego Milito", "Kit of Elsinore (Rodergatan 2)", "snowstorm", "ownership of the Falklands.", "war funding", "three", "finance", "Indonesian", "The Charlie Daniels Band,", "three out of four", "trading goods and services without exchanging money.", "gasoline", "cambodia", "South Africa", "Harrison Ford", "Jacob,", "Cash for Clunkers", "100 meter", "Shenzhen in southern China.", "fluoroquinolone drugs,", "Chinese and international laws", "the player", "\"Great Charter\" in Latin.", "27,", "five", "Secretary of State", "The ACLU", "it was unjustifiable", "his father", "\"The Sopranos,\"", "finance", "$60 billion", "managing his time.", "bachata music", "U.S. state of Georgia", "currently a free agent", "john s Sullivan", "bill bryson", "argument form", "1961", "Ariel Ram\u00edrez", "\"Boston Herald\" Rumor Clinic", "the Yangtze River", "oija", "the Andes Mountains", "autumnal equinox"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6750210274327122}, "metric_results_detailed": {"EM": [true, false, false, false, false, false, true, false, false, true, false, true, true, false, false, true, true, true, false, false, true, false, false, false, false, true, true, true, true, true, false, true, true, true, true, true, true, false, true, true, false, true, true, true, true, true, false, true, true, true, false, false, false, true, false, true, false, true, true, false, false, false, false, false], "QA-F1": [1.0, 0.8571428571428571, 0.5, 0.0, 0.0, 0.8, 1.0, 0.17391304347826086, 0.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.7499999999999999, 1.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.08695652173913043, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.6666666666666666, 0.4, 1.0, 0.8, 1.0, 0.0, 1.0, 1.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2572", "mrqa_newsqa-validation-2903", "mrqa_newsqa-validation-3233", "mrqa_newsqa-validation-664", "mrqa_newsqa-validation-2858", "mrqa_newsqa-validation-2684", "mrqa_newsqa-validation-3716", "mrqa_newsqa-validation-3802", "mrqa_newsqa-validation-3022", "mrqa_newsqa-validation-4003", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-1920", "mrqa_newsqa-validation-3887", "mrqa_newsqa-validation-162", "mrqa_newsqa-validation-3587", "mrqa_newsqa-validation-360", "mrqa_newsqa-validation-308", "mrqa_newsqa-validation-1807", "mrqa_newsqa-validation-2602", "mrqa_newsqa-validation-501", "mrqa_newsqa-validation-4073", "mrqa_naturalquestions-validation-4925", "mrqa_naturalquestions-validation-3344", "mrqa_triviaqa-validation-4841", "mrqa_triviaqa-validation-3004", "mrqa_hotpotqa-validation-1657", "mrqa_searchqa-validation-13597", "mrqa_searchqa-validation-13769", "mrqa_searchqa-validation-6906", "mrqa_triviaqa-validation-2535"], "SR": 0.53125, "CSR": 0.5535239361702128, "EFR": 1.0, "Overall": 0.7185954122340426}, {"timecode": 94, "before_eval_results": {"predictions": ["Larry King", "Immigration Minister Eric Besson", "$500,000", "The BBC is refusing to broadcast a plea from leading British charities for aid to Gaza,", "bankruptcy", "At least 38", "hooked up with Mildred, a younger woman of about 80, in March.", "Eleven", "41,", "McDonald's", "an organization strong enough to haul supporters out of their homes on a frigid January night to debate, harangue and cajole their neighbors into backing him.", "10.1,\"", "Democrats and Republicans", "Zac Efron", "the two were embedded with the rebels while working on a story about the region.", "Sylt", "\"Piers Morgan Tonight\"", "Congress", "Cash for Clunkers", "11th year in a row.", "three thousand", "\"face of the peace initiative has been attacked.\"", "after Wood went missing off Catalina Island, near the California coast,", "grossed $55.7 million during its first frame,", "Tulsa, Oklahoma.", "almost 100", "Department of Homeland Security Secretary Janet Napolitano", "56,", "There's no chance", "Barack Obama,", "gas emissions,", "in power since the country's independence from Britain in 1980,", "30,000", "31 meters (102 feet) long and 15 meters (49 feet) wide,", "24.1 million,", "of the Movement for Democratic Change,", "cartel from the state of Veracruz, Mexico,", "Orbiting Carbon Observatory,", "the IAAF", "Pakistan", "Kim", "the explosion of a train seconds after it leaves the Liverpool Street Station heading for Aldgate East.", "Daryeel Bulasho Guud", "onto the college campus.", "a body", "in the Gaslight Theater.", "Caylee Anthony", "in the neighboring country of Djibouti,", "Harry Potter in \"Harry Potter and the Order of the Phoenix\"", "school,", "Bush-era Justice Department", "to collect menstrual flow", "they also reduce trade and adversely affect consumers in general ( by raising the cost of imported goods ), and harm the producers and workers in export sectors, both in the country implementing protectionist policies, and in the countries protected against", "Massachusetts", "Ukraine", "Muhammad Ali", "Ub Iwerks", "Croatan, Nantahala, and Nebo", "4,613", "English rock band", "raytheon", "the Lion King", "Xurbia Endless", "Johnny Got His Gun"], "metric_results": {"EM": 0.59375, "QA-F1": 0.6739020867423584}, "metric_results_detailed": {"EM": [true, true, false, false, false, false, false, true, true, true, false, false, false, true, false, true, false, true, true, true, true, true, false, true, true, true, false, true, false, false, false, false, true, true, true, false, true, true, true, true, true, true, true, true, true, false, true, false, false, true, false, true, false, false, true, true, true, false, true, false, true, true, false, true], "QA-F1": [1.0, 1.0, 0.0, 0.35294117647058826, 0.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0.11320754716981132, 0.0, 0.0, 1.0, 0.09090909090909091, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.4444444444444445, 1.0, 0.8, 0.0, 0.0, 0.19999999999999998, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.33333333333333337, 0.0, 1.0, 0.0, 1.0, 0.24489795918367346, 0.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.5, 1.0, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-221", "mrqa_newsqa-validation-1947", "mrqa_newsqa-validation-2767", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-2227", "mrqa_newsqa-validation-2123", "mrqa_newsqa-validation-3151", "mrqa_newsqa-validation-2047", "mrqa_newsqa-validation-2582", "mrqa_newsqa-validation-3966", "mrqa_newsqa-validation-4154", "mrqa_newsqa-validation-2213", "mrqa_newsqa-validation-2228", "mrqa_newsqa-validation-3973", "mrqa_newsqa-validation-2652", "mrqa_newsqa-validation-661", "mrqa_newsqa-validation-269", "mrqa_newsqa-validation-2341", "mrqa_newsqa-validation-759", "mrqa_newsqa-validation-821", "mrqa_naturalquestions-validation-86", "mrqa_naturalquestions-validation-644", "mrqa_hotpotqa-validation-3339", "mrqa_hotpotqa-validation-3036", "mrqa_searchqa-validation-9921"], "SR": 0.59375, "CSR": 0.5539473684210526, "EFR": 1.0, "Overall": 0.7186800986842106}, {"timecode": 95, "before_eval_results": {"predictions": ["Ford", "two amino acids joined by a single peptide bond or one amino acid with two peptide bonds", "near the Afghan - Pakistan border, from central Afghanistan to northern Pakistan", "around 3,000 - 5,000 program - erase cycles, but some flash drives have single - level cell ( SLC ) based memory that is good for around 100,000 writes", "a legal case in certain legal systems written by one or more judges expressing disagreement with the majority opinion of the court which gives rise to its judgment", "a two - axled vehicle with a drivetrain capable of providing torque to all its wheels simultaneously", "October 6, 2017", "autopistas", "Bemis Heights", "Fusajiro Yamauchi", "1854", "Tim McGraw and Kenny Chesney", "Qutab - ud - din Aibak", "the Devastator", "Abraham Gottlob Werner", "lacteal", "\" Fix You ''", "the early 20th century", "Britain", "the House of Representatives", "architecture", "the fictional town of West Egg on prosperous Long Island in the summer of 1922", "the arms of Ireland", "a pop ballad", "2010", "12 - 10", "Annette Strean", "Charles Path\u00e9", "eukaryotic", "During World War II", "Bactrian", "2003", "in 1651", "2015", "Rodney Crowell", "Spanish moss", "Guant\u00e1namo or GTMO ( / \u02c8\u0261\u026atmo\u028a / )", "to remove an electron from a solid to a point in the vacuum immediately outside the solid surface", "961", "The Lightning Thief", "milling", "The Cornett family", "Allison Janney", "2011", "Patrick Swayze", "McFerrin, Robin Williams, and Bill Irwin", "Ann Gillespie", "Lula", "2017", "March 15, 1945", "Adwaita", "Oliver Goldsmith", "Brussels", "indus Valley", "Seventeen", "Cartoon Network", "What You Will", "Kurt Cobain", "Madonna's", "Brian Smith.", "Shakespeare", "Bob Hope", "three", "$40 and a loaf of bread."], "metric_results": {"EM": 0.578125, "QA-F1": 0.6777646032427822}, "metric_results_detailed": {"EM": [true, false, true, true, false, true, true, true, false, true, false, false, true, true, false, true, true, false, false, false, true, false, true, true, false, false, true, true, false, true, false, true, false, true, true, true, false, false, true, true, true, true, true, false, true, false, true, false, true, false, false, true, true, false, true, true, true, false, false, true, false, true, false, true], "QA-F1": [1.0, 0.2222222222222222, 1.0, 1.0, 0.2758620689655173, 1.0, 1.0, 1.0, 0.3076923076923077, 1.0, 0.6666666666666666, 0.5714285714285715, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.8181818181818181, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.25, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 0.0, 0.8125000000000001, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.5, 1.0, 0.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-6481", "mrqa_naturalquestions-validation-186", "mrqa_naturalquestions-validation-6461", "mrqa_naturalquestions-validation-346", "mrqa_naturalquestions-validation-1748", "mrqa_naturalquestions-validation-291", "mrqa_naturalquestions-validation-9723", "mrqa_naturalquestions-validation-3214", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-2299", "mrqa_naturalquestions-validation-5449", "mrqa_naturalquestions-validation-53", "mrqa_naturalquestions-validation-10355", "mrqa_naturalquestions-validation-8046", "mrqa_naturalquestions-validation-9024", "mrqa_naturalquestions-validation-5036", "mrqa_naturalquestions-validation-3184", "mrqa_naturalquestions-validation-8964", "mrqa_naturalquestions-validation-3325", "mrqa_naturalquestions-validation-430", "mrqa_naturalquestions-validation-6266", "mrqa_naturalquestions-validation-3309", "mrqa_triviaqa-validation-3684", "mrqa_newsqa-validation-1963", "mrqa_newsqa-validation-613", "mrqa_searchqa-validation-533", "mrqa_searchqa-validation-7262"], "SR": 0.578125, "CSR": 0.55419921875, "EFR": 0.9629629629629629, "Overall": 0.7113230613425926}, {"timecode": 96, "before_eval_results": {"predictions": ["Swamp Soccer", "stepped into the museum with a rifle and began firing.", "Another high tide", "April 22,", "\"Hawaii Five-O\"", "a number of calls,", "30", "U.S. senators", "The son of Gabon's former president", "chairman of the House Budget Committee,", "could be secretly working on a nuclear weapon", "Abu Sayyaf,", "Islamabad", "travel,", "Harkat-ul-Jihad al-Islami (HuJi)", "always hot and humid and it rains almost every day of the year.", "a hospital", "in Fayetteville, North Carolina,", "\"We're not going to forget you in Washington, D.C.\"", "Ewan McGregor", "September,", "\"It doesn't appear as though there was an opportunity for intervention,\"", "genocide, crimes against humanity, and war crimes.", "any person who has been abused by any priest of the Diocese of Cloyne during my time as bishop or at any time,\"", "Barney Stinson,", "19-year-old", "Dr. Death in Germany", "collaborating with the Colombian government,", "the U.S. Holocaust Memorial Museum,", "sportswear,", "peppermint-oil", "outside the municipal building of Abu Ghraib in western Baghdad", "Unseeded Frenchwoman Aravane Rezai", "researchers", "five", "30-minute", "Grease", "in good spirits, especially comforted to be receiving care from talented doctors in a world-class hospital named in honor of her late husband,\"", "gunned down four Lakewood, Washington, police officers Sunday.", "the home,", "that students often know ahead of time when and where violence will flare up on campus.", "between 1917 and 1924", "some work rule issues.", "Kerstin Fritzl,", "the children of street cleaners and firefighters.", "for his efforts to help male veterans struggling with homelessness and addiction.", "clean up Washington State's decommissioned Hanford nuclear site,", "over 1,000 pounds", "3,000 kilometers (1,900 miles)", "Two UH-60 Blackhawk helicopters", "Marc by Marc Jacobs", "Lorazepam", "the Coppolas and, technically, the Farrow / Previn / Allens", "To capitalize on her publicity", "Church", "The Men Behaving Badly", "bunch grasses", "\u00c6thelwald Moll", "The Bears", "Nikolai Trubetzkoy", "8", "Charles Dana Gibson", "the Constitution", "an eccentric bouncing character called Zebedee,"], "metric_results": {"EM": 0.46875, "QA-F1": 0.5337769435425685}, "metric_results_detailed": {"EM": [true, false, true, true, true, false, false, true, false, false, true, false, true, false, false, false, true, true, false, false, true, false, false, false, true, true, false, false, true, true, false, false, false, false, false, true, true, false, true, false, true, true, false, true, false, false, true, true, true, true, false, false, false, false, true, false, false, true, true, true, false, true, true, false], "QA-F1": [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.5, 0.0, 0.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.125, 1.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 1.0, 1.0, 0.5454545454545455, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 0.0, 0.2857142857142857, 0.22222222222222224, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0]}}, "before_error_ids": ["mrqa_newsqa-validation-2940", "mrqa_newsqa-validation-3970", "mrqa_newsqa-validation-4200", "mrqa_newsqa-validation-3923", "mrqa_newsqa-validation-2657", "mrqa_newsqa-validation-3409", "mrqa_newsqa-validation-2943", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-2781", "mrqa_newsqa-validation-1439", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-983", "mrqa_newsqa-validation-407", "mrqa_newsqa-validation-1206", "mrqa_newsqa-validation-2019", "mrqa_newsqa-validation-1043", "mrqa_newsqa-validation-98", "mrqa_newsqa-validation-2485", "mrqa_newsqa-validation-3285", "mrqa_newsqa-validation-2372", "mrqa_newsqa-validation-1454", "mrqa_newsqa-validation-1495", "mrqa_newsqa-validation-3343", "mrqa_newsqa-validation-1894", "mrqa_newsqa-validation-3690", "mrqa_newsqa-validation-3789", "mrqa_newsqa-validation-1918", "mrqa_naturalquestions-validation-4463", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-2130", "mrqa_triviaqa-validation-205", "mrqa_triviaqa-validation-3543", "mrqa_searchqa-validation-114", "mrqa_triviaqa-validation-7222"], "SR": 0.46875, "CSR": 0.5533182989690721, "EFR": 1.0, "Overall": 0.7185542847938144}, {"timecode": 97, "before_eval_results": {"predictions": ["Michael Schumacher", "Spaniard Carlos Moya", "Six", "girls", "David Bowie,", "1957,", "behind the counter.", "\"We want to reset our relationship and so we will do it together.'\"", "heavy brush,", "1959.", "Phoenix, Arizona,", "\"project work\"", "Buddhism", "40", "we are not confident to what degree our sincerity", "one of Africa's most stable nations.", "Molotov cocktails, rocks and glass.", "Alfredo Astiz,", "over a budgetary dispute,", "the northeastern Iranian city of Mashhad", "wars in Iraq and Afghanistan", "nearly $106.5 million", "Obama should have met with the Dalai Lama.", "Matthew Fisher,", "autonomy.", "the head", "government efforts at control and censorship remain rife across the Middle East and North Africa,", "Guinea, Myanmar, Sudan and Venezuela.", "Brazil,", "Uzbekistan.", "45 minutes,", "calling on NATO to do more to stop the Afghan opium trade", "took on water", "ash and rubble in place of their homes.", "July", "Visitors aren't allowed", "an auxiliary lock", "Diego Milito's", "music video", "Robert Mugabe", "alleged gang rape", "\"wacko.\"", "threatening messages", "Mexico", "more than 1.2 million people.", "Sunday.", "tennis", "Osama bin Laden's sons", "Chesley \"Sully\" Sullenberger", "Gary Player,", "Karen Floyd", "Asuka", "Authority ( derived from the Latin word auctoritas )", "Prince William, Duke of Cambridge", "Astor family", "Gryffindor", "Rowan Atkinson", "841", "Adelaide", "gender queer", "Windsor Castle", "Tad Hamilton", "Israel", "McFerrin"], "metric_results": {"EM": 0.5625, "QA-F1": 0.6940789194465664}, "metric_results_detailed": {"EM": [true, false, true, false, true, true, false, false, false, true, false, false, true, false, false, true, true, true, false, false, true, false, false, false, true, true, false, false, false, true, false, false, true, false, true, true, true, true, true, true, false, true, true, true, false, false, true, false, true, true, true, true, false, false, false, true, true, true, true, false, true, true, true, true], "QA-F1": [1.0, 0.8, 1.0, 0.33333333333333337, 1.0, 1.0, 0.8, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.6666666666666666, 0.3636363636363636, 1.0, 1.0, 1.0, 0.0, 0.888888888888889, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.35294117647058826, 0.0, 0.0, 1.0, 0.5714285714285715, 0.2727272727272727, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.28571428571428575, 1.0, 1.0, 1.0, 1.0, 0.2857142857142857, 0.4444444444444445, 0.6666666666666666, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1362", "mrqa_newsqa-validation-1371", "mrqa_newsqa-validation-2276", "mrqa_newsqa-validation-2348", "mrqa_newsqa-validation-3625", "mrqa_newsqa-validation-2024", "mrqa_newsqa-validation-4064", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-246", "mrqa_newsqa-validation-2214", "mrqa_newsqa-validation-1775", "mrqa_newsqa-validation-900", "mrqa_newsqa-validation-3486", "mrqa_newsqa-validation-2154", "mrqa_newsqa-validation-4079", "mrqa_newsqa-validation-2491", "mrqa_newsqa-validation-4069", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-2183", "mrqa_newsqa-validation-1435", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-1274", "mrqa_newsqa-validation-648", "mrqa_naturalquestions-validation-1255", "mrqa_naturalquestions-validation-2337", "mrqa_triviaqa-validation-5311", "mrqa_hotpotqa-validation-2217"], "SR": 0.5625, "CSR": 0.5534119897959184, "EFR": 1.0, "Overall": 0.7185730229591838}, {"timecode": 98, "before_eval_results": {"predictions": ["Cambodian officials", "monarchy", "\"pleased\"", "Umar Farouk AbdulMutallab", "partially submerged in a stream in Shark River Park in Monmouth County", "consumer confidence", "anesthetic", "Madonna", "Egyptian striker", "Iran's parliament speaker", "18,", "as soon as 2050,", "Australian", "Janet Napolitano", "that 75 percent of utilities had taken steps to mitigate the Aurora vulnerability,", "break up ice jams.", "likely to top $60 million by the time the Presidents Day holiday weekend is over.", "unable to pass significant", "Islamabad", "the 3rd District of Utah.", "75.", "Nineteen", "\"If Russian long-range bombers should need to land in Venezuela, we would not object to that either.", "nearly $2 billion", "near Garacad, Somalia,", "Al-Shabaab,", "February 12", "Kenneth Cole", "Clifford Harris,", "heavy turbulence", "killing of a 15-year-old boy", "sniff out cell phones.", "California, Texas and Florida,", "the Bush family political dynasty, the British royal family, Frank Sinatra, Elizabeth Taylor, Jacqueline Kennedy Onassis and Nancy Reagan.", "Brewer", "\"executioners\"", "suicides", "Jaime Andrade", "Aung San Suu Kyi", "$81,8709", "Patrick McGoohan,", "\"GoldenEye\"", "J.Crew,", "Irish capital.", "1995", "Chester Arthur Stiles", "journalists and the flight crew will be freed,", "a skilled hacker", "$7.8 million", "AMD,", "\"Argentina has always claimed sovereignty over the islands and invaded them in 1982, prompting a war in which more than 600 Argentinean and 255 British military personnel died.", "last starring role was as Boston police detective Barry Frost on the TNT police drama series Rizzoli & Isles", "September 2017", "Norman occupational surname ( meaning tailor ) in France", "Monopoly", "cogito ergo sum", "hydrocephalus", "India Today", "Paul LePage", "7 January 1936", "Bran Mak Morn", "Istanbul", "Washington Redskins", "Mrs. Miniver"], "metric_results": {"EM": 0.53125, "QA-F1": 0.6146304419552102}, "metric_results_detailed": {"EM": [true, false, false, false, false, true, false, true, false, true, true, true, false, false, false, false, false, false, true, false, true, true, false, true, false, true, true, true, true, true, true, false, false, true, false, false, true, true, true, false, false, true, true, false, true, true, true, true, true, false, false, false, true, false, true, false, true, true, false, true, false, true, false, true], "QA-F1": [1.0, 0.0, 0.0, 0.5, 0.9, 1.0, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4444444444444445, 0.9565217391304348, 0.0, 0.375, 0.0, 1.0, 0.0, 1.0, 1.0, 0.11764705882352941, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.888888888888889, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15384615384615385, 0.1, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.5, 1.0]}}, "before_error_ids": ["mrqa_newsqa-validation-1578", "mrqa_newsqa-validation-1806", "mrqa_newsqa-validation-1204", "mrqa_newsqa-validation-868", "mrqa_newsqa-validation-3613", "mrqa_newsqa-validation-8", "mrqa_newsqa-validation-641", "mrqa_newsqa-validation-4153", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3460", "mrqa_newsqa-validation-3645", "mrqa_newsqa-validation-157", "mrqa_newsqa-validation-3175", "mrqa_newsqa-validation-3489", "mrqa_newsqa-validation-2559", "mrqa_newsqa-validation-690", "mrqa_newsqa-validation-2338", "mrqa_newsqa-validation-3439", "mrqa_newsqa-validation-1038", "mrqa_newsqa-validation-4198", "mrqa_newsqa-validation-2061", "mrqa_newsqa-validation-187", "mrqa_newsqa-validation-3918", "mrqa_newsqa-validation-3888", "mrqa_naturalquestions-validation-6853", "mrqa_naturalquestions-validation-8858", "mrqa_triviaqa-validation-3864", "mrqa_hotpotqa-validation-978", "mrqa_searchqa-validation-8905", "mrqa_searchqa-validation-6956"], "SR": 0.53125, "CSR": 0.5531881313131313, "EFR": 0.9666666666666667, "Overall": 0.7118615845959596}, {"timecode": 99, "UKR": 0.6953125, "OKR_sampled_ids": ["mrqa_hotpotqa-validation-1011", "mrqa_hotpotqa-validation-1053", "mrqa_hotpotqa-validation-1065", "mrqa_hotpotqa-validation-1100", "mrqa_hotpotqa-validation-1102", "mrqa_hotpotqa-validation-1116", "mrqa_hotpotqa-validation-1142", "mrqa_hotpotqa-validation-1143", "mrqa_hotpotqa-validation-1166", "mrqa_hotpotqa-validation-1224", "mrqa_hotpotqa-validation-125", "mrqa_hotpotqa-validation-1272", "mrqa_hotpotqa-validation-1274", "mrqa_hotpotqa-validation-1329", "mrqa_hotpotqa-validation-1887", "mrqa_hotpotqa-validation-195", "mrqa_hotpotqa-validation-2024", "mrqa_hotpotqa-validation-2042", "mrqa_hotpotqa-validation-2146", "mrqa_hotpotqa-validation-2167", "mrqa_hotpotqa-validation-2169", "mrqa_hotpotqa-validation-2170", "mrqa_hotpotqa-validation-2236", "mrqa_hotpotqa-validation-2245", "mrqa_hotpotqa-validation-2251", "mrqa_hotpotqa-validation-2252", "mrqa_hotpotqa-validation-2273", "mrqa_hotpotqa-validation-2289", "mrqa_hotpotqa-validation-2337", "mrqa_hotpotqa-validation-2415", "mrqa_hotpotqa-validation-2447", "mrqa_hotpotqa-validation-2459", "mrqa_hotpotqa-validation-2513", "mrqa_hotpotqa-validation-2517", "mrqa_hotpotqa-validation-2518", "mrqa_hotpotqa-validation-2570", "mrqa_hotpotqa-validation-2622", "mrqa_hotpotqa-validation-2712", "mrqa_hotpotqa-validation-2760", "mrqa_hotpotqa-validation-2871", "mrqa_hotpotqa-validation-2913", "mrqa_hotpotqa-validation-2915", "mrqa_hotpotqa-validation-2978", "mrqa_hotpotqa-validation-30", "mrqa_hotpotqa-validation-3080", "mrqa_hotpotqa-validation-3182", "mrqa_hotpotqa-validation-3207", "mrqa_hotpotqa-validation-3277", "mrqa_hotpotqa-validation-3324", "mrqa_hotpotqa-validation-3441", "mrqa_hotpotqa-validation-3443", "mrqa_hotpotqa-validation-3459", "mrqa_hotpotqa-validation-3507", "mrqa_hotpotqa-validation-3541", "mrqa_hotpotqa-validation-3657", "mrqa_hotpotqa-validation-3685", "mrqa_hotpotqa-validation-3741", "mrqa_hotpotqa-validation-3775", "mrqa_hotpotqa-validation-3869", "mrqa_hotpotqa-validation-3890", "mrqa_hotpotqa-validation-3976", "mrqa_hotpotqa-validation-3993", "mrqa_hotpotqa-validation-4049", "mrqa_hotpotqa-validation-4062", "mrqa_hotpotqa-validation-4185", "mrqa_hotpotqa-validation-4330", "mrqa_hotpotqa-validation-4399", "mrqa_hotpotqa-validation-45", "mrqa_hotpotqa-validation-4503", "mrqa_hotpotqa-validation-4523", "mrqa_hotpotqa-validation-4596", "mrqa_hotpotqa-validation-461", "mrqa_hotpotqa-validation-4632", "mrqa_hotpotqa-validation-4667", "mrqa_hotpotqa-validation-4672", "mrqa_hotpotqa-validation-4679", "mrqa_hotpotqa-validation-47", "mrqa_hotpotqa-validation-4700", "mrqa_hotpotqa-validation-4711", "mrqa_hotpotqa-validation-4713", "mrqa_hotpotqa-validation-4770", "mrqa_hotpotqa-validation-4835", "mrqa_hotpotqa-validation-4864", "mrqa_hotpotqa-validation-4961", "mrqa_hotpotqa-validation-5113", "mrqa_hotpotqa-validation-5123", "mrqa_hotpotqa-validation-5187", "mrqa_hotpotqa-validation-540", "mrqa_hotpotqa-validation-5565", "mrqa_hotpotqa-validation-562", "mrqa_hotpotqa-validation-5628", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5773", "mrqa_hotpotqa-validation-5778", "mrqa_hotpotqa-validation-582", "mrqa_hotpotqa-validation-67", "mrqa_hotpotqa-validation-708", "mrqa_hotpotqa-validation-990", "mrqa_naturalquestions-validation-10039", "mrqa_naturalquestions-validation-10277", "mrqa_naturalquestions-validation-10303", "mrqa_naturalquestions-validation-10377", "mrqa_naturalquestions-validation-10470", "mrqa_naturalquestions-validation-10565", "mrqa_naturalquestions-validation-1182", "mrqa_naturalquestions-validation-1538", "mrqa_naturalquestions-validation-1569", "mrqa_naturalquestions-validation-1603", "mrqa_naturalquestions-validation-1679", "mrqa_naturalquestions-validation-1801", "mrqa_naturalquestions-validation-1904", "mrqa_naturalquestions-validation-1961", "mrqa_naturalquestions-validation-2137", "mrqa_naturalquestions-validation-2170", "mrqa_naturalquestions-validation-222", "mrqa_naturalquestions-validation-2256", "mrqa_naturalquestions-validation-232", "mrqa_naturalquestions-validation-2499", "mrqa_naturalquestions-validation-2506", "mrqa_naturalquestions-validation-2670", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-2730", "mrqa_naturalquestions-validation-2769", "mrqa_naturalquestions-validation-2830", "mrqa_naturalquestions-validation-2851", "mrqa_naturalquestions-validation-288", "mrqa_naturalquestions-validation-2976", "mrqa_naturalquestions-validation-3143", "mrqa_naturalquestions-validation-3189", "mrqa_naturalquestions-validation-324", "mrqa_naturalquestions-validation-3261", "mrqa_naturalquestions-validation-3344", "mrqa_naturalquestions-validation-337", "mrqa_naturalquestions-validation-3484", "mrqa_naturalquestions-validation-3485", "mrqa_naturalquestions-validation-361", "mrqa_naturalquestions-validation-3698", "mrqa_naturalquestions-validation-3782", "mrqa_naturalquestions-validation-3860", "mrqa_naturalquestions-validation-3964", "mrqa_naturalquestions-validation-3989", "mrqa_naturalquestions-validation-3993", "mrqa_naturalquestions-validation-4077", "mrqa_naturalquestions-validation-4099", "mrqa_naturalquestions-validation-4114", "mrqa_naturalquestions-validation-4214", "mrqa_naturalquestions-validation-4239", "mrqa_naturalquestions-validation-4286", "mrqa_naturalquestions-validation-4558", "mrqa_naturalquestions-validation-4701", "mrqa_naturalquestions-validation-4728", "mrqa_naturalquestions-validation-4729", "mrqa_naturalquestions-validation-4792", "mrqa_naturalquestions-validation-4823", "mrqa_naturalquestions-validation-4930", "mrqa_naturalquestions-validation-5004", "mrqa_naturalquestions-validation-5040", "mrqa_naturalquestions-validation-5135", "mrqa_naturalquestions-validation-5143", "mrqa_naturalquestions-validation-5477", "mrqa_naturalquestions-validation-5512", "mrqa_naturalquestions-validation-5649", "mrqa_naturalquestions-validation-5700", "mrqa_naturalquestions-validation-587", "mrqa_naturalquestions-validation-5915", "mrqa_naturalquestions-validation-5980", "mrqa_naturalquestions-validation-6084", "mrqa_naturalquestions-validation-6106", "mrqa_naturalquestions-validation-6391", "mrqa_naturalquestions-validation-64", "mrqa_naturalquestions-validation-6483", "mrqa_naturalquestions-validation-6708", "mrqa_naturalquestions-validation-6782", "mrqa_naturalquestions-validation-688", "mrqa_naturalquestions-validation-720", "mrqa_naturalquestions-validation-7223", "mrqa_naturalquestions-validation-7227", "mrqa_naturalquestions-validation-7298", "mrqa_naturalquestions-validation-7535", "mrqa_naturalquestions-validation-7628", "mrqa_naturalquestions-validation-8253", "mrqa_naturalquestions-validation-833", "mrqa_naturalquestions-validation-8526", "mrqa_naturalquestions-validation-857", "mrqa_naturalquestions-validation-8851", "mrqa_naturalquestions-validation-9342", "mrqa_naturalquestions-validation-9383", "mrqa_naturalquestions-validation-9400", "mrqa_naturalquestions-validation-948", "mrqa_naturalquestions-validation-9516", "mrqa_naturalquestions-validation-9688", "mrqa_naturalquestions-validation-976", "mrqa_naturalquestions-validation-9802", "mrqa_naturalquestions-validation-9821", "mrqa_naturalquestions-validation-998", "mrqa_newsqa-validation-1004", "mrqa_newsqa-validation-1037", "mrqa_newsqa-validation-1048", "mrqa_newsqa-validation-1056", "mrqa_newsqa-validation-1102", "mrqa_newsqa-validation-1118", "mrqa_newsqa-validation-1138", "mrqa_newsqa-validation-1172", "mrqa_newsqa-validation-13", "mrqa_newsqa-validation-1306", "mrqa_newsqa-validation-133", "mrqa_newsqa-validation-1330", "mrqa_newsqa-validation-1383", "mrqa_newsqa-validation-1422", "mrqa_newsqa-validation-1484", "mrqa_newsqa-validation-1535", "mrqa_newsqa-validation-1596", "mrqa_newsqa-validation-1611", "mrqa_newsqa-validation-1671", "mrqa_newsqa-validation-174", "mrqa_newsqa-validation-1757", "mrqa_newsqa-validation-1779", "mrqa_newsqa-validation-1781", "mrqa_newsqa-validation-1784", "mrqa_newsqa-validation-1823", "mrqa_newsqa-validation-1835", "mrqa_newsqa-validation-185", "mrqa_newsqa-validation-1856", "mrqa_newsqa-validation-1926", "mrqa_newsqa-validation-1978", "mrqa_newsqa-validation-1983", "mrqa_newsqa-validation-2012", "mrqa_newsqa-validation-2082", "mrqa_newsqa-validation-214", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2142", "mrqa_newsqa-validation-2147", "mrqa_newsqa-validation-2156", "mrqa_newsqa-validation-2263", "mrqa_newsqa-validation-231", "mrqa_newsqa-validation-2352", "mrqa_newsqa-validation-2365", "mrqa_newsqa-validation-2458", "mrqa_newsqa-validation-2486", "mrqa_newsqa-validation-2489", "mrqa_newsqa-validation-2506", "mrqa_newsqa-validation-2517", "mrqa_newsqa-validation-2519", "mrqa_newsqa-validation-2577", "mrqa_newsqa-validation-2616", "mrqa_newsqa-validation-268", "mrqa_newsqa-validation-2705", "mrqa_newsqa-validation-2755", "mrqa_newsqa-validation-2756", "mrqa_newsqa-validation-2762", "mrqa_newsqa-validation-279", "mrqa_newsqa-validation-2791", "mrqa_newsqa-validation-3012", "mrqa_newsqa-validation-3046", "mrqa_newsqa-validation-3093", "mrqa_newsqa-validation-3100", "mrqa_newsqa-validation-3119", "mrqa_newsqa-validation-3141", "mrqa_newsqa-validation-3144", "mrqa_newsqa-validation-3155", "mrqa_newsqa-validation-3165", "mrqa_newsqa-validation-3177", "mrqa_newsqa-validation-3261", "mrqa_newsqa-validation-3262", "mrqa_newsqa-validation-3283", "mrqa_newsqa-validation-3394", "mrqa_newsqa-validation-340", "mrqa_newsqa-validation-3423", "mrqa_newsqa-validation-3457", "mrqa_newsqa-validation-3480", "mrqa_newsqa-validation-3548", "mrqa_newsqa-validation-3563", "mrqa_newsqa-validation-3571", "mrqa_newsqa-validation-3605", "mrqa_newsqa-validation-3745", "mrqa_newsqa-validation-3850", "mrqa_newsqa-validation-3888", "mrqa_newsqa-validation-3891", "mrqa_newsqa-validation-391", "mrqa_newsqa-validation-3940", "mrqa_newsqa-validation-3977", "mrqa_newsqa-validation-3983", "mrqa_newsqa-validation-4", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4010", "mrqa_newsqa-validation-4026", "mrqa_newsqa-validation-4044", "mrqa_newsqa-validation-4054", "mrqa_newsqa-validation-4078", "mrqa_newsqa-validation-4094", "mrqa_newsqa-validation-4111", "mrqa_newsqa-validation-4166", "mrqa_newsqa-validation-4173", "mrqa_newsqa-validation-433", "mrqa_newsqa-validation-446", "mrqa_newsqa-validation-568", "mrqa_newsqa-validation-648", "mrqa_newsqa-validation-715", "mrqa_newsqa-validation-736", "mrqa_newsqa-validation-739", "mrqa_newsqa-validation-805", "mrqa_newsqa-validation-839", "mrqa_newsqa-validation-84", "mrqa_newsqa-validation-880", "mrqa_newsqa-validation-920", "mrqa_newsqa-validation-942", "mrqa_newsqa-validation-951", "mrqa_newsqa-validation-977", "mrqa_newsqa-validation-994", "mrqa_searchqa-validation-10696", "mrqa_searchqa-validation-10796", "mrqa_searchqa-validation-11045", "mrqa_searchqa-validation-11048", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11103", "mrqa_searchqa-validation-11349", "mrqa_searchqa-validation-11774", "mrqa_searchqa-validation-11936", "mrqa_searchqa-validation-12013", "mrqa_searchqa-validation-12043", "mrqa_searchqa-validation-12203", "mrqa_searchqa-validation-12220", "mrqa_searchqa-validation-12661", "mrqa_searchqa-validation-12778", "mrqa_searchqa-validation-12921", "mrqa_searchqa-validation-13103", "mrqa_searchqa-validation-13218", "mrqa_searchqa-validation-13355", "mrqa_searchqa-validation-13454", "mrqa_searchqa-validation-1368", "mrqa_searchqa-validation-13963", "mrqa_searchqa-validation-14087", "mrqa_searchqa-validation-1409", "mrqa_searchqa-validation-14789", "mrqa_searchqa-validation-14839", "mrqa_searchqa-validation-14991", "mrqa_searchqa-validation-15097", "mrqa_searchqa-validation-15246", "mrqa_searchqa-validation-15435", "mrqa_searchqa-validation-15519", "mrqa_searchqa-validation-15526", "mrqa_searchqa-validation-15716", "mrqa_searchqa-validation-16275", "mrqa_searchqa-validation-16417", "mrqa_searchqa-validation-16458", "mrqa_searchqa-validation-16665", "mrqa_searchqa-validation-2036", "mrqa_searchqa-validation-2435", "mrqa_searchqa-validation-2538", "mrqa_searchqa-validation-2732", "mrqa_searchqa-validation-2940", "mrqa_searchqa-validation-3330", "mrqa_searchqa-validation-36", "mrqa_searchqa-validation-3698", "mrqa_searchqa-validation-387", "mrqa_searchqa-validation-4249", "mrqa_searchqa-validation-4496", "mrqa_searchqa-validation-4594", "mrqa_searchqa-validation-4635", "mrqa_searchqa-validation-4792", "mrqa_searchqa-validation-4927", "mrqa_searchqa-validation-4995", "mrqa_searchqa-validation-5158", "mrqa_searchqa-validation-5300", "mrqa_searchqa-validation-5393", "mrqa_searchqa-validation-5745", "mrqa_searchqa-validation-5752", "mrqa_searchqa-validation-5944", "mrqa_searchqa-validation-6558", "mrqa_searchqa-validation-6940", "mrqa_searchqa-validation-7226", "mrqa_searchqa-validation-7535", "mrqa_searchqa-validation-8208", "mrqa_searchqa-validation-8226", "mrqa_searchqa-validation-8277", "mrqa_searchqa-validation-8471", "mrqa_searchqa-validation-8671", "mrqa_searchqa-validation-8907", "mrqa_searchqa-validation-8915", "mrqa_searchqa-validation-9260", "mrqa_searchqa-validation-9607", "mrqa_searchqa-validation-9613", "mrqa_searchqa-validation-9630", "mrqa_searchqa-validation-9730", "mrqa_searchqa-validation-9964", "mrqa_squad-validation-10310", "mrqa_squad-validation-1036", "mrqa_squad-validation-1088", "mrqa_squad-validation-1472", "mrqa_squad-validation-2015", "mrqa_squad-validation-2036", "mrqa_squad-validation-2468", "mrqa_squad-validation-2568", "mrqa_squad-validation-278", "mrqa_squad-validation-283", "mrqa_squad-validation-3001", "mrqa_squad-validation-3193", "mrqa_squad-validation-3304", "mrqa_squad-validation-3420", "mrqa_squad-validation-363", "mrqa_squad-validation-4078", "mrqa_squad-validation-4326", "mrqa_squad-validation-435", "mrqa_squad-validation-4518", "mrqa_squad-validation-4718", "mrqa_squad-validation-4960", "mrqa_squad-validation-506", "mrqa_squad-validation-5236", "mrqa_squad-validation-547", "mrqa_squad-validation-5490", "mrqa_squad-validation-5544", "mrqa_squad-validation-5801", "mrqa_squad-validation-5908", "mrqa_squad-validation-6079", "mrqa_squad-validation-6091", "mrqa_squad-validation-6124", "mrqa_squad-validation-6223", "mrqa_squad-validation-6342", "mrqa_squad-validation-66", "mrqa_squad-validation-7039", "mrqa_squad-validation-7725", "mrqa_squad-validation-7725", "mrqa_squad-validation-7751", "mrqa_squad-validation-8199", "mrqa_squad-validation-8278", "mrqa_squad-validation-8428", "mrqa_squad-validation-8964", "mrqa_squad-validation-9567", "mrqa_squad-validation-9847", "mrqa_triviaqa-validation-1065", "mrqa_triviaqa-validation-109", "mrqa_triviaqa-validation-1138", "mrqa_triviaqa-validation-1372", "mrqa_triviaqa-validation-1376", "mrqa_triviaqa-validation-157", "mrqa_triviaqa-validation-1619", "mrqa_triviaqa-validation-1699", "mrqa_triviaqa-validation-1731", "mrqa_triviaqa-validation-1740", "mrqa_triviaqa-validation-1794", "mrqa_triviaqa-validation-1867", "mrqa_triviaqa-validation-2079", "mrqa_triviaqa-validation-2125", "mrqa_triviaqa-validation-217", "mrqa_triviaqa-validation-2183", "mrqa_triviaqa-validation-2252", "mrqa_triviaqa-validation-2337", "mrqa_triviaqa-validation-2542", "mrqa_triviaqa-validation-2585", "mrqa_triviaqa-validation-261", "mrqa_triviaqa-validation-2823", "mrqa_triviaqa-validation-2862", "mrqa_triviaqa-validation-2871", "mrqa_triviaqa-validation-3058", "mrqa_triviaqa-validation-3114", "mrqa_triviaqa-validation-3121", "mrqa_triviaqa-validation-3161", "mrqa_triviaqa-validation-3287", "mrqa_triviaqa-validation-3324", "mrqa_triviaqa-validation-3461", "mrqa_triviaqa-validation-3531", "mrqa_triviaqa-validation-3641", "mrqa_triviaqa-validation-3705", "mrqa_triviaqa-validation-3758", "mrqa_triviaqa-validation-3893", "mrqa_triviaqa-validation-3918", "mrqa_triviaqa-validation-3961", "mrqa_triviaqa-validation-4030", "mrqa_triviaqa-validation-4174", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4244", "mrqa_triviaqa-validation-4292", "mrqa_triviaqa-validation-4460", "mrqa_triviaqa-validation-4555", "mrqa_triviaqa-validation-4651", "mrqa_triviaqa-validation-4698", "mrqa_triviaqa-validation-4736", "mrqa_triviaqa-validation-4840", "mrqa_triviaqa-validation-5049", "mrqa_triviaqa-validation-521", "mrqa_triviaqa-validation-5253", "mrqa_triviaqa-validation-5339", "mrqa_triviaqa-validation-5371", "mrqa_triviaqa-validation-560", "mrqa_triviaqa-validation-5613", "mrqa_triviaqa-validation-562", "mrqa_triviaqa-validation-5649", "mrqa_triviaqa-validation-5746", "mrqa_triviaqa-validation-5748", "mrqa_triviaqa-validation-5804", "mrqa_triviaqa-validation-5809", "mrqa_triviaqa-validation-5819", "mrqa_triviaqa-validation-5935", "mrqa_triviaqa-validation-6036", "mrqa_triviaqa-validation-6070", "mrqa_triviaqa-validation-6089", "mrqa_triviaqa-validation-6169", "mrqa_triviaqa-validation-6243", "mrqa_triviaqa-validation-6523", "mrqa_triviaqa-validation-6717", "mrqa_triviaqa-validation-6802", "mrqa_triviaqa-validation-6828", "mrqa_triviaqa-validation-6913", "mrqa_triviaqa-validation-6998", "mrqa_triviaqa-validation-7009", "mrqa_triviaqa-validation-7225", "mrqa_triviaqa-validation-7325", "mrqa_triviaqa-validation-7336", "mrqa_triviaqa-validation-7715", "mrqa_triviaqa-validation-7726", "mrqa_triviaqa-validation-859", "mrqa_triviaqa-validation-904"], "OKR": 0.8203125, "KG": 0.49375, "before_eval_results": {"predictions": ["detritus", "1648 - 51", "In 2011, the IRB approved the use of the Millennium Stadium, despite being outside of the host country", "Scott Schwartz", "a certified question or proposition of law from one of the United States Courts of Appeals", "Lana Del Rey", "pour point", "Madison", "May 29, 2018", "Dan Stevens", "in Ephesus in AD 95", "Bulgaria", "lead", "11 February 2012", "2018", "The symbol consists of three dots placed in an upright triangle and is read therefore", "2010", "The 14th game of this series", "Clarence Anglin", "Ariana Clarice Richards", "James Intveld", "Darlene Cates", "one", "mashed potato", "2018", "in the dress shop", "George Harrison", "the Seton Hall Pirates men's basketball team", "July 2014", "Universal Pictures and Focus Features", "Cyanea capillata", "enabled business applications to be developed with Flash", "prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned", "the county seat and commercial center of Lee County, Florida", "the digestive systems of many organisms", "Theodosius I", "Christopher Allen Lloyd", "South Africa", "Mark Jackson", "Fall 1998", "TLC", "Inequality of opportunity", "In 1929", "Abid Ali Neemuchwala", "Bhupendranath Dutt", "May 19, 2017", "the people of France", "an armature of piped masonry often carved in decorative patterns", "from its headwaters in the Brazilian state of Mato Grosso to its confluence with the Paran\u00e1 River north of Corrientes and Resistencia", "Gustav Bauer", "either in front or on top of the brainstem", "leicestershire", "The Lady of Shalott", "15", "Mike Pence", "Soviet Union", "Heinkel Flugzeugwerke", "Her returned to Pakistan in October after President Pervez Musharraf signed an amnesty lifting corruption charges.", "Zac Efron", "Aryan Airlines Flight 1625", "Ocean's Twelve", "Trajan's Column", "Anne Rice", "Wilkie Collins"], "metric_results": {"EM": 0.640625, "QA-F1": 0.7421328569306511}, "metric_results_detailed": {"EM": [true, true, false, false, false, true, true, true, true, true, false, true, false, false, true, false, true, false, true, true, true, true, false, true, false, false, true, false, false, true, false, false, false, false, false, false, true, true, true, true, true, true, false, true, true, true, false, true, true, true, true, true, true, true, true, true, true, false, true, true, true, false, true, true], "QA-F1": [1.0, 1.0, 0.0, 0.0, 0.11764705882352941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.0, 0.0, 1.0, 0.22222222222222224, 1.0, 0.16666666666666666, 1.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.0, 0.8, 1.0, 0.5882352941176471, 0.0, 1.0, 0.6666666666666666, 0.761904761904762, 0.5, 0.3636363636363636, 0.5714285714285715, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.5714285714285715, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]}}, "before_error_ids": ["mrqa_naturalquestions-validation-8610", "mrqa_naturalquestions-validation-8260", "mrqa_naturalquestions-validation-7995", "mrqa_naturalquestions-validation-9428", "mrqa_naturalquestions-validation-2239", "mrqa_naturalquestions-validation-1038", "mrqa_naturalquestions-validation-6556", "mrqa_naturalquestions-validation-8483", "mrqa_naturalquestions-validation-8383", "mrqa_naturalquestions-validation-1784", "mrqa_naturalquestions-validation-10102", "mrqa_naturalquestions-validation-805", "mrqa_naturalquestions-validation-9346", "mrqa_naturalquestions-validation-2794", "mrqa_naturalquestions-validation-2245", "mrqa_naturalquestions-validation-5934", "mrqa_naturalquestions-validation-8409", "mrqa_naturalquestions-validation-7226", "mrqa_naturalquestions-validation-1147", "mrqa_naturalquestions-validation-2713", "mrqa_naturalquestions-validation-594", "mrqa_newsqa-validation-846", "mrqa_searchqa-validation-15139"], "SR": 0.640625, "CSR": 0.5540625, "EFR": 0.9565217391304348, "Overall": 0.7039918478260869}]}